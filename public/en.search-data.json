{"/utho-docs/docs/add-txt-record-in-plesk/":{"data":{"":"","conclusion#conclusion":"Hopefully, now you have learned how to add TXT record in Plesk.\nAlso read: How to add MX record in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to add TXT record in Plesk.\nA TXT record, which is an abbreviation for ‚Äútext record,‚Äù is a specific kind of resource record that is used by the Domain Name System (DNS) to provide the ability to associate random text with a host or other name. This text can include information that is readable by humans about a server, network, data centre, or other accounting information. TXT records are modified as ‚ÄúTXT records.‚Äù\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to¬†Hosting and DNS under the menu of websites and domains, then click on DNS settings.\nStep 3. Click on ‚ÄúAdd Record.‚Äù\nStep 4. In the record type drop-down menu, select TXT record.\nIn the domain field here, we are not using any subdomains, so for the main domain, which is micro.com, we are keeping this field empty.\nIn the TTL field, you can assign any value in seconds; we have entered 3600. in the TXT record field, entered the TXT record. then select OK.¬†NOTE: Use your main domain instead of micro.com."},"title":"How to add TXT record in Plesk"},"/utho-docs/docs/analysis-of-postgresql-and-mysql-a-comparative-study/":{"data":{"":"","few-reasons-why-more-and-more-businesses-are-choosing-postgresql-as-their-go-to-database#\u003cstrong\u003eFew reasons why more and more businesses are choosing PostgreSQL as their go-to database\u003c/strong\u003e.":"","how-can-utho-support-your-postgresql-and-mysql-requirements#\u003cstrong\u003eHow can Utho support your PostgreSQL and MySQL requirements?\u003c/strong\u003e":"PostgreSQL and MySQL are among the most highly regarded open-source relational databases in the modern software landscape. PostgreSQL is highly advantageous for enterprise-level applications that involve frequent write operations and intricate queries. Its robust features make it a top choice for handling large and complex databases. On the other hand, MySQL is more suitable for beginners due to its easier learning curve. It allows for quicker development of new database projects from scratch.¬†Few reasons why more and more businesses are choosing PostgreSQL as their go-to database. PostgreSQL simplifies the process of setting up and utilizing databases, whether on-premises or in the cloud. In environments with a significant number of databases, both private and public, automating the creation of PostgreSQL instances can significantly save time.\nPostgreSQL is reliable: PostgreSQL implements a Write-Ahead Log mechanism to safeguard against system crashes. This ensures that any committed transactions, that have not yet been written to the database, can be recovered by replaying the Write-Ahead Log. As a result, these transactions are successfully committed without loss of data.\nPostgreSQL is extensible : PostgreSQL is like a customizable toolbox. Its extensibility means you can easily add new tools or features to tailor it to your specific needs. You can create custom data types, operators, and functions, and even plug in entire extensions, making it flexible and adaptable for different uses.\nPostgreSQL is fast: PostgreSQL is a performance-enhancing feature that includes enhanced partition handling, increased efficiency in parallel processing, faster indexing speeds, and the elimination of concurrency barriers.¬†PostgreSQL is secure: PostgreSQL is widely recognized for its strong focus on data integrity and robust security features, including row-level security and advanced authentication mechanisms. This reputation positions it as a highly secure database system.\nWhat is MySQL and what is it used for? MySQL is a highly efficient, dependable, and easily expandable open-source relational database system. It is specially developed to manage demanding, critical production applications with heavy data loads. As it is managed by a robust RDMS, MySQL is a widely used and user-friendly database that requires minimal resources in terms of memory, disk space, and CPU usage.\nMySQL is widely compatible: Despite its frequent association with internet applications and web services, MySQL was purposefully designed to have broad compatibility with a range of technologies and architectures. It is supported on all major computing platforms, including Unix-based systems like various Linux distributions and Mac OS, as well as Windows.\nMySQL databases are relational: Databases such as MySQL store data in various tables that are highly structured and separated, rather than relying on a single comprehensive repository or collections of unstructured documents. This design enables RDBMSs to efficiently optimize processes such as data retrieval, information updates, and more advanced actions such as aggregations.\nMySQL is open-source: These options offer organizations a higher degree of flexibility when considering utilizing MySQL. The open-source nature of the releases, which are publicly and community-driven, enhances MySQL‚Äôs documentation and online support culture, while also guaranteeing that any new or ongoing developments remain in line with current user demands.\nWhich is better MySQL or PostgreSQL?\nAlthough MySQL and PostgreSQL possess several similarities, the variances between them are substantial and may cause confusion for both novice and expert database managers. It is evident that MySQL has effectively enhanced itself to remain pertinent, while PostgreSQL stands out for its lack of licensing requirements and additional features such as table inheritance, rules systems, custom data types, and database events. As a result, PostgreSQL takes the lead over MySQL in this regard.\nHow can Utho support your PostgreSQL and MySQL requirements? ","utho-provides-a-range-of-services-to-fulfill-your-mysql-and-postgresql-needs#Utho provides a range of services to fulfill your MySQL and PostgreSQL needs.":"Utho Relational Database Service is a comprehensive suite of managed services designed to streamline the process of setting up, managing, and expanding relational databases in the cloud. Through our flagship offering, Utho RDS for MySQL, users can quickly deploy scalable MySQL servers with minimal effort and cost-effective options for resizing hardware capacity.\nSimilarly, Utho Relational Database Service for PostgreSQL makes it easy to set up, operate, and scale PostgreSQL deployments in the cloud. Furthermore, it efficiently handles intricate and labor-intensive administrative responsibilities, including upgrading PostgreSQL software, managing storage, and performing backups to ensure disaster recovery.\nGet started with MySQL and PostgreSQL on Utho by creating a free account today.","what-is-mysql-and-what-is-it-used-for#\u003cstrong\u003eWhat is MySQL and what is it used for?\u003c/strong\u003e":""},"title":"Analysis of PostgreSQL and MySQL: A Comparative Study"},"/utho-docs/docs/apache-virtual-hosts-setup-on-centos-7/":{"data":{"":"","create-new-virtual-host-files#Create New Virtual Host Files":"This is the virtual host file to specify how the Apache web server is to answer the various domain requests and how our separate websites are configured.\nIn order for us to start by setting the directory in which our virtual hosts are saved, and the directory in which Apache is told that the virtual host is ready to serve visitors. All of our virtual host files are maintained by the sites-available directory, while the sites-enabled directory has symbolic links to virtual hosts that we want to publish. By typing, we can create both directories:\n[root@Microhost]# mkdir /etc/httpd/sites-available [root@Microhost]# mkdir /etc/httpd/sites-enabled [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]The Debian contributor introduced this directory layout but we include it here to make it even more flexible with our virtual hosts management (as this is easier to allow and uncheck virtual hosts temporarily)..[/ht_message]\nIn sites-enabled directory, we should tell Apache to search for virtual hosts. In order to achieve this, we will edit the main Apache configuration file and add an optional line for further configuration of files. We can edit the file by following command.\n[root@Microhost]# /etc/httpd/conf/httpd.conf Add the following line at the end of the file content.\nIncludeOptional sites-enabled/*.conf save the file and exit from the text editor.","create-the-first-virtual-host-file#Create the First Virtual Host File":"Create a configuration file for domain1.com at the below location.\n[root@Microhost]# vi /etc/httpd/sites-available/domain1.com.conf The configuration file should look like :\n\u003cVirtualHost *:80\u003e ServerName domain1.com ServerAlias www.domain1.com DocumentRoot /var/www/domain1.com/public_html ErrorLog /var/www/domain1.com/error.log CustomLog /var/www/domain1.com/requests.log combined \u003c/VirtualHost\u003e save the file and exit from the text editor.\nNow do the same process for another domain.\n[root@Microhost]# vi /etc/httpd/sites-available/domain2.com.conf The configuration file should look like\n\u003cVirtualHost *:80\u003e ServerName domain2.com ServerAlias www.domain2.com DocumentRoot /var/www/domain2.com/public_html ErrorLog /var/www/domain2.com/error.log CustomLog /var/www/domain2.com/requests.log combined \u003c/VirtualHost\u003e save the file and exit from the text editor.","creation-of-test-pages-for-each-domains#Creation of test pages for each domains":"Let us now create some content to serve, as we now have a directory structure in place.\nOur pages are very simple because this is only for demonstration and testing. For each site that identifies the given domain, we will only make an index.php page.\nStart with domain1.com. Let‚Äôs start. In our editor, we can open an index.php file by typing:\n[root@Microhost]# vi /var/www/domain1.com/public_html/index.php copy the below content and paste it into the file.\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003ch1\u003eMy first PHP page\u003c/h1\u003e \u003c?php echo \"Hello World!\"; ?\u003e \u003c/body\u003e \u003c/html\u003e Save the file using :wq and exit from the text editor.\nWe can perform the above steps for domain2 also, only we have to create a file on domain2.com location as given below:\n[root@Microhost]# vi /var/www/domain2.com/public_html/index.php copy the below content and paste it into the file.\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003ch1\u003eMy Second PHP page\u003c/h1\u003e \u003c?php echo \"Hello World!\"; ?\u003e \u003c/body\u003e \u003c/html\u003e Save the file and exit from the text editor.","creation-of-virtual-host-directory#Creation of virtual host directory":"Our document root is set to individual directories in the /var/www directory and is the top level directory that Apache looks to find the content to serve. For each of the virtual host we plan to make, we will create a directory here.\nWe will create a public html directory that contains our current files in each of these directories.\nThese directories can be made by the mkdir command (using the -p flag to create a folder with a nested folder inside it):\n[root@Microhost]# mkdir -p /var/www/domain1.com/public_html [root@Microhost]# mkdir -p /var/www/domain2.com/public_html We should also change some of our permissions to ensure that the general web directory and all the files and folders inside are allowed access to pages so that they can be served correctly:\n[root@Microhost]# chmod -R 755 /var/www You should now have the permissions on your web server to serve content and the content of the corresponding folders should be created by your user.","enable-the-new-virtual-host-files#Enable the New Virtual Host Files":"We must now enable our virtual host files so that Apache knows how to serve visitors. In this way, in the sites-enabled directory, we can create a symbolic link for each virtual host:\n[root@Microhost]# ln -s /etc/httpd/sites-available/domain1.com.conf /etc/httpd/sites-enabled/domain1.com.conf [root@Microhost]# ln -s /etc/httpd/sites-available/domain2.com.conf /etc/httpd/sites-enabled/domain2.com.conf Now restart the apache services.\n[root@Microhost]# systemctl restart httpd Thank You :)","installation-of-apache#Installation of Apache":" [root@Microhost]# yum -y install httpd Now the Apache server has been installed. We will enable the apache service so that it will automatically up in boot time.\n[root@Microhost]# systemctl enable httpd.service Now we will start the service of apache server by following command:\n[root@Microhost]# systemctl start httpd.service [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]The setup example in this guide makes one virtual host for domain1.com and another for domain2.com . These are mentioned in the entire guide, but you should replace your own domains or values as you follow. .[/ht_message]","introduction#Introduction":"The web server of Apache is the most popular way to deliver web content. It serves over half of all active websites in the Internet and is extremely powerful and flexible.\nApache divides its features and components into separate units, which can be independently customized and set up. A virtual Host is called the basic unit describing a single site or domain. Virtual hosts allow one server, through a matching system, to host several domains or interfaces. This is important for those who want to host more than one VPS site.\nEach configured domain will direct the visitor to a certain directory that contains information on this website without indicating that the same server is also responsible for other websites. This scheme can be expanded without software limits provided the traffic of all the sites is handled by your server.","prerequisites#Prerequisites":" Log in to the server with the ssh. The account should have the root privilege. Update the server packages using yum update -y "},"title":"Apache Virtual Hosts setup on CentOS 7"},"/utho-docs/docs/bash-vs-cmd-decoding-the-battle-of-command-line-titans/":{"data":{"":"","how-does-bash-function#\u003cstrong\u003eHow does Bash function?\u003c/strong\u003e":"","what-are-the-advantages-of-using-bash#**What are the advantages of using Bash?":"","what-are-the-advantages-of-utilizing-a-command-prompt#\u003cstrong\u003eWhat are the advantages of utilizing a Command Prompt?\u003c/strong\u003e":"Bash and CMD are important tools in the world of computing. Bash, found in Unix-like systems, helps users efficiently navigate and control their systems using text-based commands. CMD, associated with Windows, offers a similar approach, providing a toolkit for executing commands. Both are crucial for system management, used by administrators, developers, and enthusiasts. Join us as we explore the unique features of Bash and CMD in this brief overview and discover which one is the better command-line interface.\nWhat is Bash? Bash serves as a UNIX shell and a command-line interpreter, simultaneously playing the roles of both. Recognized as a commonly utilized programming language, Bash supports a range of functions, variables, loops, and conditional statements, resembling features found in several other programming languages. Users can leverage Bash to interpret commands and execute multiple actions.\nHow does Bash function? From a technical standpoint, Bash serves as a command interpreter, processing and executing basic system commands like ls or mkdir. This interaction is the primary way of working with Bash. Additionally, there‚Äôs a second method involving batch files, containing Bash code. Mastering Bash scripting, which involves writing and executing batch files, provides a significant advantage, allowing automation of tasks and the creation of complex system commands.\n**What are the features of Bash? **\n**Here are fundamental concepts in Bash that every user should acquaint themselves with:\nCommands:** A command serves as an instruction directing the shell‚Äôs actions, and it can range from simple to complex, entered into the terminal through typing.\nArguments: Arguments consist of supplementary information provided to a command to alter its behavior, encompassing options, filenames, or other types of data.\nVariables: Variables serve as storage for data utilized by the shell or scripts, capable of being assigned values and employed within commands or scripts.\nFunctions: Functions are employed to group commands together, enabling the execution of specific tasks. They can be invoked either from the command line or within a Bash script.\nRedirection: Redirection is the process of directing a command‚Äôs output to a file or another command. This functionality enables users to save the output to a file or utilize it as input for another command in the command prompt.\nWildcards: Wildcards serve the purpose of matching patterns in filenames or other data, allowing the selection of multiple files or the execution of operations on groups of files.\n**What are the advantages of using Bash? **\nThe introduction of windows and menus was a significant advancement in computer software development, so why revert to using CLIs like Bash? CLI usage persists due to several distinct advantages over GUIs. Let‚Äôs delve into some of these advantages.\nEnhance your operating system access efficiency: Individuals opt for Bash when they seek to manage their computer or OS without navigating through GUI menus, options, and windows. Additionally, using Bash instead of a GUI is more resource-efficient, as it eliminates the need for the computer to allocate resources to render graphical output. This makes Bash an appealing choice when running multiple programs, a virtual machine, or working with limited computing resources.\n**\nInput and output with text files:** Bash simplifies the creation and editing of text files, including CSVs. Given that text files are among the most prevalent means of storing and processing data, Bash proves to be excellent for tasks such as organizing and refining data,¬†sorting and filtering data, scrubbing and refreshing data.\nAutomate with ease: Bash facilitates the automation of tasks on your computer, particularly beneficial when your job entails repetitive functions.\nWhat are the primary use cases of Bash? Key Applications of Bash:\nScripting: Bash scripting empowers users to create scripts, sequences of commands, enabling the automation of repetitive tasks, system administration, and the development of intricate workflows.\n**\nFile and Directory Management:** Bash simplifies file and directory operations, encompassing tasks such as creating, deleting, copying, moving, and renaming files and directories.\nRemote Server Management: Bash is commonly employed to establish secure connections to remote servers through SSH (Secure Shell) and execute operations on distant systems.\nSoftware Development: Bash scripts find application in software development workflows, handling tasks such as build automation, deployment, and testing.\nWhat are the primary use cases of CMD? System Information: CMD provides commands like systeminfo to retrieve detailed information about the system, including hardware and software configurations.\nNetwork Troubleshooting: Commands like ipconfig, ping, and tracert help diagnose and troubleshoot network-related issues.\nTask Management: CMD provides commands like tasklist and taskkill to view and manage running processes and applications.\nRemote Access: CMD supports remote access and management of other systems using commands like psexec and ssh.\nWhat is CMD (Command Prompt)? CMD (Command Prompt) serves as a command-line interpreter on Windows operating systems, offering a text-based interface for executing diverse system and application commands, as well as facilitating scripting and automation tasks. It is commonly known as the ‚ÄúWindows command prompt‚Äù or simply the ‚Äúcommand prompt.‚Äù\nWhat is the functioning mechanism of Command Prompt? The command-line interface (CLI) accepts text commands entered through a keyboard. Although CLIs may have varying syntaxes, they generally carry out similar operations. Upon command execution, the computer interprets and performs the specified actions, while the CLI offers user feedback, including error messages or output from the executed commands.\nWhat are the advantages of utilizing a Command Prompt? Using a command-line interface (CLI) offers numerous advantages, with the most notable being:\nSpeed: The CLI allows for swift execution of commands, enabling the combination of multiple commands into a single line of text for program execution. This efficiency surpasses the navigation through menus in a GUI.\nResources: The CLI demands fewer computing resources for executing commands compared to a graphical interface.\nRepetitive Tasks: The CLI proves effective in automating repetitive tasks, allowing the creation of batch files to automate tasks at any specified time.\nPower-user: A CLI is well-suited for power users as it grants access to commands not available in a GUI. For instance, certain system-protected tasks cannot be accessed through a GUI.","what-are-the-features-of-bash#**What are the features of Bash?":"","what-are-the-primary-use-cases-of-bash#\u003cstrong\u003eWhat are the primary use cases of Bash?\u003c/strong\u003e":"","what-are-the-primary-use-cases-of-cmd#\u003cstrong\u003eWhat are the primary use cases of CMD?\u003c/strong\u003e":"","what-is-bash#\u003cstrong\u003eWhat is Bash?\u003c/strong\u003e":"","what-is-cmd-command-prompt#\u003cstrong\u003eWhat is CMD (Command Prompt)?\u003c/strong\u003e":"","what-is-the-functioning-mechanism-of-command-prompt#\u003cstrong\u003eWhat is the functioning mechanism of Command Prompt?\u003c/strong\u003e":""},"title":"Bash vs. CMD: Decoding the Battle of Command Line Titans"},"/utho-docs/docs/basics-information-of-apache-configuration/":{"data":{"":"\nThe Apache HTTP web server is the de facto standard for general HTTP services in many respects. Thanks to its large number of modules, it offers versatile support for proxy servers, URL rewriting, and granular access control. In addition, web developers also prefer Apache to support server-side scripting using CGI, FastCGI, and embedded interpreters. Such features make it easier to execute complex code quickly and efficiently. Although there are many popular alternatives to Apache, also within the bounds of open source, the scope of Apache use is remarkable.\nApache does not without any expense the exceptional degree of flexibility; this takes on the form of a configuration system that is often confusing and sometimes complicated. We have therefore developed this document and a number of other guides to tackle this issue and explore some more advanced and optional Apache HTTP Sever features.\nIf you would like to have a working web server and install Apache for the first time, we suggest using the corresponding ‚ÄúApache development guide‚Äù for your Linux distribution. Try trying a suitable LAMP guide for your delivery, if you need a more full LAMP stack. This guide assumes that you have an up to date Linux setup, installed Apache successfully, and logged into a root access shell session.","apache-basics#Apache Basics":"The default Apache settings vary widely between different Linux distributions. All distributions from Debian, Ubuntu, and Gentoo refer to Apache as ‚ÄúApache2‚Äù and bring configuration files into the directory /etc/apache2/. Certain distributions such as Fedora, CentOS and Arch Linux refer to Apache as ‚Äúhttpd,‚Äù and store /etc/httpd/ configuration files. While we allow you to learn about the default configuration of your Apache server, most configurational choices do not differ between the operating systems. The main obstacles to Apache‚Äôs setup are to grasp the regular configurations of the implementations and their variations from the upstream Apache.\nYou may use the ‚Äúinit‚Äù scripts to manage basic Apache features to safely and quickly start, stop or restart the server. The init script can also reload the configuration and check the server status. Edit the correct command to access these functions:\n/etc/init.d/apache2 start /etc/init.d/apache2 stop /etc/init.d/apache2 restart /etc/init.d/apache2 reload /etc/init.d/apache2 status If you use a distribution referred to as httpd by Apache, the commands are as follows:\n/etc/init.d/httpd start /etc/init.d/httpd stop /etc/init.d/httpd restart /etc/init.d/httpd reload /etc/init.d/httpd status The path to the script may be /etc/rc.d/init.d/¬†for some versions instead of /etc/init.d/.\nIn the init script with the command mod_disk_cache¬†on Debian-based distributions contain the functions to monitor the htcache functionality:\n/etc/init.d/apache2 start-htcacheclean /etc/init.d/apache2 stop-htcacheclean Further functions from the command line interface are also supported. You can use the following command in Debian and Ubuntu systems to test the syntax of your Apache configuration files without restarting the server and trying it:\napache2ctl -t In CentOS and Fedora systems, use the following form:\nhttpd -t Furthermore, the apache2ctl -S¬†or httpd -S¬†Commands provide an update on virtual machines that currently run, which include the port the host listens on, the virtual host‚Äôs name (i.e. the domain) and site configuration information including file names and line numbers.\nThe ‚Äúmaster‚Äù file for Apache is usually located in the httpd.conf. This file is in the apache2.conf¬†file in Debian distributions, with a user-specific configuration in the httpd.conf¬†file, including the master file, the master file contains some other files. To receive a list of the following items, submit an order according to your distribution:\ngrep \"Include\" /etc/apache2/apache2.conf grep \"Include\" /etc/apache2/httpd.conf grep \"Include\" /etc/httpd/httpd.conf Note that the order of these files can affect the web server behavior. When later options contradict options set in previous files, then earlier options are overridden. Knowing the current default configuration can be a valuable learning experience.","configuration-file-organization#Configuration File Organization":"One of the most common use cases for the Apache web server is to use its ‚Äúvirtual hosting‚Äù capabilities, which allow a single instance of Apache to serve numerous websites and subdomains. Because most websites don‚Äôt tend to use a significant amount of system resources, virtual hosting is often a great way to fully utilize a web server. As a result of this capability, configuration files for virtual hosts can be complex and difficult to organize. There are two major approaches to the solution for this problem.","create-a-single-virtual-hosts-file#Create a Single Virtual Hosts file":"For each virtual host, the Debian method of maintaining a single configuration file can be useful in managing many noninterlinked sites or for the editing of various distinct and unprivileged users. However, there are circumstances in which too many virtual host files may cause confusion and increase the maintenance burden by setting the same configuration group for a collection of hosts.\nFor these instances, the easiest way to keep Apache installed would be to provide a single file. This is the chosen host organization in some distributions, such as CentOS, Fedora, and Arch Linux. When checking Apache‚Äôs default distribution configuration, there‚Äôs normally a conf.d/directory where you can store user-created configuration. When you want to merge multiple virtual host configuration files into one file, create a vhosts.conf¬†in the conf.d/¬†folder and add the entire configuration options in that file. Theconf.d/folder is located within the /etc/ directory of Apache, either: /etc/apache2/conf.d/or, where your distribution is concerned, /etc/httpd/conf.d/¬†.\nBoth settings are essentially similar, but can be implemented conveniently in different scenarios. The structure of the configuration file you choose depends on the specifications of your particular application.","symbolic-links-and-the-debian-way#Symbolic Links and the Debian Way":"To order to enhance usability, the Debian project enables users to effectively prevent changing the web server‚Äôs ‚Äúfoundation configuration.‚Äù As a result, Debian and Ubuntu use symbolic links to allow and disable various configuration options for administrators without fully erasing configuration options.\nMake sure that your apache.conf or httpd.conf file has the correct line if you are using an operating system other than Debian or choose to use the sites-enabled organization to customize your settings.\nInclude /etc/httpd/sites-enabled/ Include /etc/apache2/sites-enabled/ You need to use a command like: mkdir -p /etc/httpd/sites-enabled/. if you haven‚Äôt created this directory yet. Now the setup options for any files that you store in those directories will be included on your Apache server. Please use the following command to create a connection to this directory:\nln -s /etc/httpd/vhosts/abc.com /etc/httpd/sites-enabled/abc.com The syntax for creating symbolic links is ln -s¬†followed by the original or ‚Äúgoal‚Äù file and the path to the link to be connected. If you skip the final word, ln¬†will build a connection in the current directory with the same name as the target file. The original file will not be affected if you delete a connection. Apache can follow multiple layers of symbolic connections, but this can be confusing for itself.\nOne advantage of this is that a virtual host can keep its configuration files close to the other associated host files. All your virtual host-related services are mostly located in a /srv/www/abc.com/ directory. Under the DocumentRoot,¬†directory, logs directory and application support directories, public_html/,logs/and application/ are stored. This organization helps you to find it convent to store your configuration file with your virtual host in /srv/www/abc.com/. This makes it easy to back up and transfer a virtual host, since all files are in one directory. The symbolic connection can be generated as following if the virtual host set up file is located at /srv/www/abc.com/apache.conf you might create the symbolic link as follows:\nln -s /srv/www/abc.com/apache.conf /etc/apache2/sites-available/abc.com You can use the a2ensite¬†and a2dissite¬†software to handle virtual host files if you are using a Debian distribution. You can also use/etc/apache2/sites-enabled/abc.com to manually connect your configuration files. When you do not use a Debian distribution, the symbolic relation may look the same, but file names and locations can change somewhat:\nln -s /srv/www/abc.com/apache.conf /etc/httpd/conf.d/abc.conf "},"title":"Basics Information of Apache Configuration"},"/utho-docs/docs/blog/5-best-practices-for-configuring-and-managing-a-load-balancer/":{"data":{"":"","1-monitor-your-servers-closely#1. Monitor Your Servers Closely":"","2-know-your-traffic-patterns#2. Know Your Traffic Patterns":"","3-use-autoscaling-when-possible#3. Use Autoscaling When Possible":"","4-utilize-automated-monitoring-tools#4. Utilize Automated Monitoring Tools":"","5-keep-backup-systems-in-place#5. Keep Backup Systems In Place":"A load balancer is an essential tool in any business‚Äôs IT infrastructure. It ensures that traffic is distributed evenly across servers, helping to prevent performance bottlenecks that can lead to outages. As such, it‚Äôs important to configure and manage your load balancer correctly. Here are five tips for doing just that.¬†Must Read : 6 Benefits of Deploying a Load Balancer on your server.\n1. Monitor Your Servers Closely To ensure peak performance from your load balancer, you need to monitor the servers it‚Äôs connected to. This means monitoring all of the server‚Äôs resources (CPU usage, memory utilization, etc.) on a regular basis so that you can quickly identify any potential issues and address them before they cause major problems.¬†2. Know Your Traffic Patterns Load balancers are most effective when they‚Äôre configured according to specific traffic patterns. So, take some time to study the traffic coming into your website or application and adjust your configuration accordingly. Doing so will allow you to optimize your setup for maximum efficiency and minimize potential outages due to unexpected spikes in traffic or other irregularities.¬†3. Use Autoscaling When Possible Autoscaling is a great way to ensure that your load balancer always has enough capacity to handle incoming traffic without bogging down the system or causing outages due to overloading resources. Not only does it help save on costs by allowing you scale up or down as needed, but it also makes sure that users always have access to the services they need when they need them most.¬†4. Utilize Automated Monitoring Tools Automated monitoring tools can be used in conjunction with your load balancer configuration in order to detect any issues before they become serious problems and make sure everything is running smoothly at all times. The more data you collect from these tools, the better informed decisions you‚Äôll be able make when it comes time for maintenance or upgrades down the line.¬†5. Keep Backup Systems In Place Nothing lasts forever, including your load balancer configuration and hardware setup ‚Äì which is why having backup systems in place is so important! This could mean anything from having multiple failover systems ready in case of an emergency or simply keeping redundant copies of all configurations and settings so that you can quickly restore service should something go wrong with the primary setup.¬†A load balancer can be a powerful tool for managing traffic on your website or application. By following these best practices, you can ensure that your load balancer is properly configured and able to handle the traffic demands of your users. If you do not have a load balancer in place, we recommend considering one as part of your infrastructure."},"title":"5 Best practices for configuring and managing a Load Balancer"},"/utho-docs/docs/blog/5-most-effective-ways-to-avoid-cloud-bill-shocks/":{"data":{"":"","1-establish-a-budget#¬†\u003cstrong\u003e1. Establish a budget:\u003c/strong\u003e":"","2-monitor-usage#\u003cstrong\u003e2. Monitor usage:\u003c/strong\u003e":"","3-use-reserved-instances#\u003cstrong\u003e3. Use reserved instances:\u003c/strong\u003e":"","4-optimize-resources#\u003cstrong\u003e4 .Optimize resources:\u003c/strong\u003e":"","5-search-for-an-alternative#\u003cstrong\u003e5 Search for an alternative:\u003c/strong\u003e¬†":"Cloud bill shock is a common phenomenon among IT managers, small and medium-sized businesses. A cloud bill shock can arise when an unexpected, large bill arrives from your cloud service provider. It can be an unpleasant surprise that erodes your budget and disrupts operations. But you don‚Äôt have to let these shocks derail your business. By following these Five simple strategies, you can avoid the majority of cloud bill shocks and optimize costs in the long run.¬†Most Effective Ways to Avoid Cloud Bill Shocks.\n1. Establish a budget: The first step in avoiding cloud bill shocks is to establish a budget for your cloud services expenditures. Make sure that this budget is realistic, based on past usage and anticipated future needs. This will give you a clear understanding of what you‚Äôre willing to spend and help you plan accordingly throughout the year.¬†2. Monitor usage: Monitoring your usage of cloud services is key to avoiding costly surprises down the line. Investing in tools or services that provide real-time insights into resource utilization can help ensure that you never exceed your allocated budget‚Äîand save you money in the long run.¬†3. Use reserved instances: Reserved instances are pre-purchased contracts that guarantee capacity availability at fixed prices over a specified period of time‚Äîoften one or three years‚Äîfrom the original purchase date. By taking advantage of such offerings, you can lower costs significantly by locking in discounted rates on commonly used services and resources over extended periods of time.¬†4 .Optimize resources: Optimizing resources wisely can make a big difference when it comes to managing costs associated with cloud services utilization over time as well as minimizing unexpected bills from month to month..¬†For example, optimizing storage space by archiving data no longer being used but still needed for reference or compliance purposes could help reduce bills significantly over time . Additionally, shutting down idle virtual machines during low usage times could also greatly reduce costs long term .¬†5 Search for an alternative:¬†Switching cloud providers can be a daunting task for many business owners, but it can save them from the pain of high bill shocks. When selecting a new cloud provider, one of the most important criteria should be that your provider offers a predictable billing model. With such a model, you will have full control and visibility over your costs, allowing you to create budget plans and cost forecasts more accurately. Additionally, with predictable billing you will know exactly what to expect each month in terms of pricing and will be able to plan accordingly without being subject to sudden changes or unexpected surprises and Microhost is the right cloud provider for this.\nWith careful planning and effective management, it‚Äôs possible to avoid most instances of cloud bill shock."},"title":"5 Most Effective Ways to Avoid Cloud Bill Shocks."},"/utho-docs/docs/blog/5-proven-strategies-for-disaster-recovery-and-business-continuity-in-the-cloud/":{"data":{"":"","1backup-and-recovery#\u003cstrong\u003e1.Backup and Recovery\u003c/strong\u003e":"","2-replication#\u003cstrong\u003e2. Replication\u003c/strong\u003e":"","3multi-cloud#\u003cstrong\u003e3.Multi-Cloud\u003c/strong\u003e":"","4high-availability#\u003cstrong\u003e4.High Availability\u003c/strong\u003e":"","5-disaster-recovery-as-a-service-draas#\u003cstrong\u003e5. Disaster Recovery as a Service (DRaaS)\u003c/strong\u003e":"Cloud disaster recovery is more than just backing up your data to a remote server. It requires a holistic approach that encompasses people, processes, and technology. Several key elements can make or break your recovery efforts, from risk assessment to testing and automation. To help you get it right, we‚Äôve compiled a list of 5 proven strategies for disaster recovery and business continuity in the cloud that you can start implementing today.¬†1.Backup and Recovery The first strategy for disaster recovery and business continuity in the cloud is to implement a regular backup and recovery process for critical data and applications. This involves creating copies of critical data and applications and storing them in a secure cloud environment.\nBy doing this, in an outage, businesses can quickly and easily restore their data and applications from the cloud, minimizing downtime and ensuring business continuity. It is important to test the restoration process regularly to ensure that the data and applications can be recovered quickly and accurately.\nThe cloud provides several advantages for backup and recovery, such as easy scalability, cost-effectiveness, and the ability to store data in different geographic locations for redundancy. This strategy can help businesses to mitigate the risk of data loss and downtime, protecting their reputation and minimizing the impact on customers and partners.\n2. Replication This means creating a copy of critical data and applications in a different location from the primary system. In the cloud, you can replicate data and applications across different regions or availability zones within the same cloud service provider or multiple providers. This ensures that your data and applications remain accessible during an outage in the primary system.\nTo keep the replicated data and applications up to date, cloud-based replication solutions use technologies such as asynchronous data replication and real-time synchronization. As a result, if an outage occurs, you can failover to the replicated data and applications quickly and easily, minimizing the impact on your business and customers.\nImplementing a cloud-based replication solution helps businesses achieve a high level of resilience and disaster recovery capability while minimizing the need for complex and costly backup and restore processes.\n3.Multi-Cloud This means using multiple cloud service providers to ensure redundancy and disaster recovery across different regions and availability zones to minimize the impact of an outage. When relying on a single cloud service provider, businesses risk outages due to natural disasters, system failures, or cyber-attacks that may occur within the provider‚Äôs infrastructure. However, businesses can mitigate this risk by using multiple cloud service providers and ensuring that their data and applications remain available and accessible even in an outage in one provider‚Äôs infrastructure.\nA multi-cloud strategy also enables businesses to take advantage of different cloud providers‚Äô strengths, such as geographical reach, pricing, and service offerings. It also avoids vendor lock-in, allowing businesses to switch providers and avoid disruptions.\nTo implement a multi-cloud approach, businesses must carefully evaluate the costs and complexities of managing multiple cloud service providers. They must also ensure that their applications are compatible with multiple cloud platforms and have the necessary redundancy and failover mechanisms.\nBusinesses can use a multi-cloud approach to ensure a high level of resilience and disaster recovery capability while minimizing the risk of downtime and data loss during an outage.\n4.High Availability Deploy highly available architectures, such as auto-scaling and load-balancing, to ensure that applications remain available and responsive during an outage.\nAuto-scaling and load-balancing allow applications to adjust dynamically to changes in demand, ensuring that resources are allocated efficiently and that the application remains available and responsive to users. Auto-scaling automatically adds or removes compute resources based on workload demand, while load-balancing distributes traffic across multiple servers to prevent any single server from becoming overloaded.\nIn disaster recovery and business continuity, these techniques can be used to ensure that critical applications are highly available and can handle increased traffic or demand during an outage. For example, suppose an application server fails. Auto-scaling can quickly spin up additional servers to take over the workload, while load-balancing ensures that traffic is routed to the available servers.\nTo implement highly available architectures in the cloud, businesses must design their applications with resilience, including redundancy, failover mechanisms, and fault-tolerant design. They must also monitor their applications to continue identifying and mitigating potential issues before they lead to downtime.\n5. Disaster Recovery as a Service (DRaaS) DRaaS is a cloud-based service that provides businesses with a complete disaster recovery solution. This solution includes backup, replication, and failover, without the need for businesses to invest in their infrastructure.\nBy replicating critical data and applications to a secondary site or cloud environment, DRaaS ensures that systems can quickly fail in an outage or disaster. DRaaS providers often offer a range of service levels, from basic backup and recovery to comprehensive disaster recovery solutions with near-zero recovery time (RTOs) and recovery point objectives (RPOs).\nOne of the key benefits of DRaaS is that it reduces the need for businesses to invest in their disaster recovery infrastructure, which can be costly and complex to manage. DRaaS providers can also help businesses develop and test their disaster recovery plans, ensuring they are fully prepared for a potential disaster.\nTo implement DRaaS, businesses must carefully evaluate their disaster recovery requirements, including their RTOs and RPOs, and choose a provider that meets their specific needs. They must also ensure that their data and applications are compatible with the DRaaS provider‚Äôs environment and have a plan for testing and maintaining their disaster recovery solution.\nUsing DRaaS, businesses can ensure a high level of resilience and disaster recovery capability without the need for significant capital investment and complex infrastructure management.\nBy following these strategies, businesses can significantly reduce the risk of data loss and downtime in an outage, ensuring business continuity and minimizing the impact on customers, employees, and partners."},"title":"5 Proven Strategies for Disaster Recovery and Business Continuity in the Cloud"},"/utho-docs/docs/blog/6-benefits-of-deploying-a-load-balancer-on-your-server/":{"data":{"":"","1-improved-scalability#\u003cstrong\u003e1) Improved scalability:\u003c/strong\u003e":"","2-reduced-costs#\u003cstrong\u003e2) Reduced costs:\u003c/strong\u003e":"","3-increased-security#\u003cstrong\u003e3) Increased security:\u003c/strong\u003e":"","4-improved-performance#\u003cstrong\u003e4) Improved performance:\u003c/strong\u003e":"Here we will discuss the benefits of deploying a load balancer on your server and what is a load balancer. A load balancer distributes network or application traffic across a cluster of servers. A load balancer sits between client devices and backend servers, receiving and then routing requests to the appropriate server.\nThe main benefit of using a load balancer is improved availability and performance. By balancing traffic across multiple servers, a load balancer helps ensure that no single server is overwhelmed by requests and that all servers are available to handle client requests.\nBenefits of using a Load Balancer\nOther Benefits of using a load balancer include the following: 1) Improved scalability: By distributing traffic across multiple servers, a load balancer makes it possible to scale up your infrastructure to accommodate increased traffic without overburdening any single server.\n2) Reduced costs: You can reduce your overall hosting costs by using multiple lower-powered servers instead of a single, more powerful server.\n3) Increased security: By sitting between client devices and backend servers, a load balancer can act as a ‚Äúgateway‚Äù through which all traffic must pass. This makes it possible to implement security measures such as firewalls and intrusion detection/prevention systems (IDS/IPS) that can help protect your backend servers from attacks.\n4) Improved performance: By balancing traffic across multiple servers, a load balancer helps to ensure that no single server is overwhelmed by requests. This can lead to faster response times for clients and reduced latency.","5-better-utilization-of-resources#5) Better utilization of resources:":"By using multiple lower-powered servers instead of a single, more powerful server, you can better utilize each server‚Äôs processing power and memory. This can lead to improved performance for your applications.","6-flexibility#6) Flexibility:":"Load balancers can be used with on-premises infrastructure or in the cloud, allowing you to scale up or down as needed without making significant changes to your infrastructure.¬†There are many Benefits of Deploying a Load Balancer that can improve the availability, performance, scalability, security, and utilization of your applications and infrastructure. Microhost‚Äôs load balancers are easy to deploy, just like our cloud servers, and are ready to use the moment they are deployed. If you don‚Äôt already have one, consider deploying one today.","other-benefits-of-using-a-load-balancer-include-the-following#Other Benefits of using a load balancer include the following:":""},"title":"6 Benefits of Deploying a Load Balancer on your server."},"/utho-docs/docs/blog/6-cloud-computing-myths-busted/":{"data":{"":"","1-myth-the-cloud-is-expensive#\u003cstrong\u003e1. Myth: The cloud is expensive.\u003c/strong\u003e":"","2-myth-the-cloud-is-unreliable#\u003cstrong\u003e2. Myth: The cloud is unreliable.\u003c/strong\u003e":"","3-myth-the-cloud-is-insecure#\u003cstrong\u003e3. Myth: The cloud is insecure.\u003c/strong\u003e¬†":"","4-myth-the-cloud-is-complicated#\u003cstrong\u003e4. Myth: The cloud is complicated.\u003c/strong\u003e¬†":"","5-myth-the-cloud-is-only-for-big-businesses#\u003cstrong\u003e5. Myth: The cloud is only for big businesses.\u003c/strong\u003e¬†":"","6-myth-the-cloud-is-only-for-storage#\u003cstrong\u003e6. Myth: The cloud is only for storage.\u003c/strong\u003e¬†":"There‚Äôs a lot of misinformation out there about cloud computing. We‚Äôre here to set the record straight and bust some of the most common cloud computing myths!\nWhat is cloud computing?¬†Before busting the myths, let‚Äôs understand what cloud computing is! Cloud computing delivers servers, storage, databases, networking, software, analytics, and intelligence services over the Internet (‚Äúthe cloud‚Äù) to offer faster innovation, flexible resources, and economies of scale.¬†With cloud computing, businesses can avoid the upfront investment in hardware and software infrastructure and instead pay for only the resources they use on a pay-as-you-go basis. This flexibility enables organizations to respond quickly to changing business needs while controlling IT costs. In addition, cloud services can be scaled up or down as needed without extensive planning or reengineering.¬†The myths of cloud computing. most common cloud computing myths\n1. Myth: The cloud is expensive. Busted! Moving to the cloud can save your business money in the long run. With on-premise infrastructure, you must pay for power, cooling, and physical space to house your servers. With the cloud, you only pay for the resources you use. Plus, you don‚Äôt have to worry about upkeep and maintenance costs.\n2. Myth: The cloud is unreliable. Busted! Cloud providers invest heavily in ensuring that their data centers are reliable and always available. Clouds are often more reliable than on-premise infrastructure because they have built-in redundancy mechanisms. For example, if one data center goes down, your cloud provider can quickly spin up another to take its place.\n3. Myth: The cloud is insecure.¬†Busted! Cloud providers go to great lengths to secure their data centers and keep customer data safe. They have teams of security experts who constantly monitor for threats and implement new security measures as needed. Plus, the data stored in the cloud is typically encrypted, making it even more difficult for hackers to access.\n4. Myth: The cloud is complicated.¬†Busted! Cloud providers offer a variety of tools and resources to help customers get started with the cloud and make the most of its features. Plus, many providers offer 24/7 support if you encounter any technical issues.¬†5. Myth: The cloud is only for big businesses.¬†Busted! While enterprises are certainly embracing the cloud, it‚Äôs also being used by small businesses and individuals who need a simple, cost-effective way to store and share data online.¬†6. Myth: The cloud is only for storage.¬†Busted! While storage is one of the most popular uses for the cloud, it‚Äôs far from the only use case. Many businesses rely on the cloud for computing resources, application hosting, data analysis, etc.¬†Conclusion:¬†Moving to the cloud can be beneficial for your business in terms of costs, reliability, performance, scalability, and security. Don‚Äôt let myths about the cloud stop you from exploring what it can do for your organization. Microhost provides top-notch cloud computing services. The best part is that you can try it out for seven days without risk.","the-myths-of-cloud-computing#\u003cstrong\u003eThe myths of cloud computing.\u003c/strong\u003e":"","what-is-cloud-computing#\u003cstrong\u003eWhat is cloud computing?\u003c/strong\u003e¬†":""},"title":"6 Cloud Computing Myths, Busted!"},"/utho-docs/docs/blog/7-reasons-why-cloud-infrastructure-is-important-for-startups/":{"data":{"":"","1-scalability#\u003cstrong\u003e1. Scalability\u003c/strong\u003e":"","2-flexibility#\u003cstrong\u003e2. Flexibility\u003c/strong\u003e":"","3-reliability#\u003cstrong\u003e3. Reliability\u003c/strong\u003e":"","4-cloud-infrastructure-is-cost-effective#\u003cstrong\u003e4. Cloud infrastructure\u003c/strong\u003e \u003cstrong\u003eis Cost-effective\u003c/strong\u003e":"","5-security#\u003cstrong\u003e5. Security\u003c/strong\u003e":"","6-compliance#\u003cstrong\u003e6. Compliance\u003c/strong\u003e":"","7-support#\u003cstrong\u003e7. Support\u003c/strong\u003e":"Cloud Infrastructure is Important for Startups, and many factors contribute to a startup‚Äôs success; one of the most important is having a strong infrastructure. Have you ever wondered why some startups succeed while others fail?\nThat‚Äôs where cloud infrastructure comes in. Cloud infrastructure can provide startups with the scalability, flexibility, and reliability they need to grow and thrive. Here are seven reasons why cloud infrastructure is so important for startups:\nAdvantages of cloud infrastructure for startups\n1. Scalability One of the biggest challenges for startups is predicting future growth. Will your user base double in the next six months? What about next year? Trying to forecast that kind of growth can be difficult, and if you underestimate it, you could end up with an infrastructure that can‚Äôt handle the demand.\nWith cloud infrastructure, you only pay for the resources you use, so it‚Äôs easy to scale up or down as your needs change. That gives you the flexibility to respond quickly to changes in user demand, without having to over-provision your infrastructure and waste money on unused resources.\n2. Flexibility Another challenge for startups is the need to be agile and respond quickly to changes in the market. With a traditional infrastructure, it can take weeks or even months to provision new resources or make changes to your existing setup. That‚Äôs not ideal when you need to move fast to stay ahead of the competition.\nCloud infrastructure provides the flexibility you need to make changes quickly and easily. If you need to add new servers or storage, you can do it in minutes instead of weeks. And if you need to reduce your capacity, you can do that just as easily. That means you can respond quickly to changes in the market and keep your startup agile.\n3. Reliability Startups need to be able to rely on their infrastructure to keep their business running smoothly. Downtime can cost you money, so it‚Äôs important to have an infrastructure that is reliable and always available.\nWith cloud infrastructure, you can take advantage of the same high-availability features that are used by some of the largest companies in the world. That means your startup can have the same level of reliability and availability, without having to invest in expensive hardware and software.\n4. Cloud infrastructure is Cost-effective A traditional infrastructure can be costly to set up and maintain. Startups often don‚Äôt have the capital to invest in their own data center, so they have to lease space from a third-party provider. That can be expensive, and it can limit the amount of control you have over your infrastructure.\nWith cloud infrastructure, you only pay for the resources you use, so it‚Äôs more cost-effective than a traditional infrastructure. And because you don‚Äôt have to invest in your own data center, you can use that money to invest in other areas of your business.\n5. Security Startups need to be able to protect their data and applications from cyberattacks. With a traditional infrastructure, you have to manage your own security, which can be a challenge if you don‚Äôt have the resources or expertise.\nWith cloud infrastructure, you can take advantage of the security features that are provided by the provider. That means you don‚Äôt have to worry about managing your own security, and you can focus on other aspects of your business.\n6. Compliance Startups need to be able to comply with industry regulations. With a traditional infrastructure, you have to manage your own compliance, which can be a challenge if you‚Äôre not familiar with the regulations.\nWith cloud infrastructure, you can take advantage of the compliance features that are provided by the provider. That means you don‚Äôt have to worry about managing your own compliance, and you can focus on other aspects of your business.\n7. Support When you‚Äôre running a startup, you need to be able to get help when you need it. With a traditional infrastructure, you have to manage your own support, which can be a challenge if you‚Äôre not familiar with the technology.\nWith cloud infrastructure, you can take advantage of the support that is provided by the provider. That means you don‚Äôt have to worry about managing your own support, and you can focus on other aspects of your business.\nMicrohost is a cloud infrastructure provider that offers all of these features to startups. We make it easy for startups to get started with cloud infrastructure, and we offer the tools and resources they need to be successful. Contact us to learn more about how we can help your startup."},"title":"7 Reasons Why Cloud Infrastructure is Important for Startups"},"/utho-docs/docs/blog/advantages-and-challenges-of-developing-cloud-native-applications/":{"data":{"":"","advantages-of-cloud-native-applications#\u003cstrong\u003eAdvantages of Cloud-Native Applications\u003c/strong\u003e":"","best-practices-for-developing-cloud-native-applications#\u003cstrong\u003eBest Practices for Developing Cloud-Native Applications\u003c/strong\u003e":"","challenges-of-cloud-native-applications#\u003cstrong\u003eChallenges of Cloud-Native Applications\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction In recent years, there has been a growing trend in software development towards cloud-native applications. These applications are designed specifically to run in the cloud and take full advantage of its benefits, including scalability, reliability, and cost-efficiency. However, developing cloud-native applications also presents its own set of challenges that must be overcome. In this article, we will explore the advantages and challenges of developing cloud-native applications and provide best practices for doing so.\nAdvantages of Cloud-Native Applications Scalability One of the most significant advantages of cloud-native applications is their ability to scale easily and efficiently. Cloud providers offer services such as auto-scaling and load balancing that allow applications to automatically adjust their resources based on demand. This means that cloud-native applications can handle sudden spikes in traffic without any downtime or performance issues.\nReliability Cloud providers also offer high levels of reliability, with service level agreements (SLAs) that guarantee a certain level of uptime. Cloud-native applications can take advantage of this reliability by using services such as redundant data storage and automatic failover. This means that applications can continue to operate even if there is a failure in one part of the system.\nCost-Efficiency Another advantage of cloud-native applications is their cost efficiency. By using cloud services, developers can avoid the upfront costs of purchasing and maintaining their own hardware. Additionally, cloud providers offer pay-as-you-go pricing models, which means that developers only pay for the resources they actually use. This can result in significant cost savings, especially for smaller businesses or startups.\nChallenges of Cloud-Native Applications Complexity Developing cloud-native applications can be more complex than traditional applications. Cloud-native applications often use microservices architecture, which involves breaking an application into smaller, more manageable components. This can increase the complexity of the application as a whole, as each component must be designed and developed independently.\nSecurity Cloud-native applications can also present security challenges. With multiple components running in different environments, it can be more difficult to ensure that all components are secure. Additionally, cloud providers offer their own security measures, but it is still the responsibility of the developer to ensure that their application is secure.\nVendor Lock-In Finally, developing cloud-native applications can result in vendor lock-in. This occurs when a developer uses a specific cloud provider‚Äôs services to develop their application and becomes reliant on those services. If the developer wants to switch to a different provider, they may face challenges in porting their application over.\nBest Practices for Developing Cloud-Native Applications Use a Containerization Platform Using a containerization platform such as Docker or Kubernetes can help to address the complexity of developing cloud-native applications. Containers provide a lightweight and portable way to package and deploy individual components of an application.\nImplement Security Best Practices Developers should implement security best practices to ensure that their application is secure. This includes using encryption for data in transit and at rest, enforcing\naccess controls, and regularly testing for vulnerabilities.\nUse Open-Source Technologies Using open-source technologies can help to avoid vendor lock-in and provide more flexibility in developing cloud-native applications. Additionally, open-source technologies often have a large community of developers contributing to them, which can result in faster innovation and bug fixes.\nConclusion Developing cloud-native applications can provide many benefits, including scalability, reliability, and cost-efficiency. However, it also presents its own set of challenges, including complexity, security, and vendor lock-in. By following best practices such as using a containerization platform, implementing security best practices, and using open-source technologies, developers can overcome these challenges and take full advantage of the benefits of cloud-native applications.\nAlso Read: Best Practices for Managing and Securing Edge Computing Devices","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":""},"title":"Advantages and Challenges of Developing Cloud-Native Applications"},"/utho-docs/docs/blog/advantages-and-challenges-of-implementing-a-hybrid-cloud-solution/":{"data":{"":"","advantages-of-implementing-a-hybrid-cloud-solution#\u003cstrong\u003eAdvantages of Implementing a Hybrid Cloud Solution\u003c/strong\u003e":"","challenges-of-implementing-a-hybrid-cloud-solution#\u003cstrong\u003eChallenges of Implementing a Hybrid Cloud Solution\u003c/strong\u003e":"\nWith businesses moving to the cloud, it‚Äôs no surprise that hybrid cloud solutions have gained popularity in recent years. The hybrid cloud offers a combination of public and private clouds that provide flexibility and control over data, applications, and infrastructure. However, like any technology solution, hybrid cloud implementation comes with its own set of advantages and challenges.\nAdvantages of Implementing a Hybrid Cloud Solution 1. Scalability and Flexibility One of the primary advantages of a hybrid cloud solution is its scalability and flexibility. It allows businesses to scale resources up or down as per their needs, whether it‚Äôs adding or removing resources in a public or private cloud environment. With the ability to balance workloads across different environments, businesses can ensure optimal performance and cost-efficiency.\n2. Cost-Effective Implementing a hybrid cloud solution can be cost-effective for businesses as it allows them to use public cloud services for non-critical workloads and private cloud services for mission-critical workloads. By leveraging the public cloud, businesses can avoid the high costs associated with building and maintaining their own infrastructure. At the same time, private cloud services provide greater security and control over sensitive data and applications.\n3. Increased Security The hybrid cloud solution offers increased security as businesses can keep sensitive data and applications on their private cloud, which is less accessible to external threats. At the same time, the public cloud can be used for less sensitive data and applications, where security concerns are relatively lower.\nChallenges of Implementing a Hybrid Cloud Solution 1. Complexity Implementing a hybrid cloud solution can be complex as it involves managing resources across different environments. Businesses need to ensure that their hybrid cloud environment is properly configured, and applications are designed to run across different clouds seamlessly. This requires skilled professionals who understand the intricacies of the hybrid cloud.\n2. Integration Issues Integrating different cloud environments can be challenging as it requires businesses to ensure compatibility between different technologies, protocols, and standards. This can result in delays and additional costs associated with re-architecting applications to make them work in a hybrid environment.\n3. Data Management Managing data in a hybrid cloud environment can be challenging as businesses need to ensure that data is synchronized across different environments. This requires businesses to implement proper data management policies to ensure that data is consistent and up-to-date across different environments.","conclusion#Conclusion":"In conclusion, implementing a hybrid cloud solution can offer businesses greater flexibility, scalability, cost-effectiveness, and security. However, it also comes with its own set of challenges that businesses need to be aware of. To maximize the benefits of a hybrid cloud, businesses need to have the right resources, skills, and expertise to manage and operate their hybrid cloud environment effectively.\nIf you‚Äôre looking to implement a hybrid cloud solution, MicroHost can help you navigate the complexities of the cloud and ensure that you get the most out of your hybrid cloud environment. Visit our website at https://utho.com/ to learn more about our cloud solutions and how we can help you achieve your business goals.\nRead Also: 7 Reasons Why Cloud Infrastructure is Important for Startups"},"title":"Advantages and Challenges of Implementing a Hybrid Cloud Solution"},"/utho-docs/docs/blog/advantages-and-challenges-of-implementing-edge-computing-in-your-organization/":{"data":{"":"","advantages-of-edge-computing#\u003cstrong\u003eAdvantages of Edge Computing\u003c/strong\u003e":"","challenges-of-edge-computing#\u003cstrong\u003eChallenges of Edge Computing\u003c/strong\u003e":"","choosing-the-right-edge-computing-solution#\u003cstrong\u003eChoosing the Right Edge Computing Solution\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nEdge computing is rapidly gaining momentum as a valuable addition to modern-day computing infrastructures. Technology has come a long way since it was first introduced, with many companies now embracing it for its advantages. However, as with any new technology, there are also challenges associated with implementing edge computing in your organization. In this article, we‚Äôll explore the advantages and challenges of implementing edge computing in your organization.\nWhat is Edge Computing? Edge computing is a distributed computing model that brings data processing and storage closer to the devices where it‚Äôs being generated. In traditional computing models, data is sent to a central location for processing and analysis. With edge computing, the processing and analysis of data happen closer to the source, which can be a device, sensor, or gateway.\nAdvantages of Edge Computing 1. Reduced Latency One of the biggest advantages of edge computing is reduced latency. Since the data processing and analysis happen closer to the source, there‚Äôs less distance for the data to travel, which results in faster response times. This is especially important for applications that require real-time data processing, such as autonomous vehicles, drones, and IoT devices.\n2. Improved Security Edge computing can also improve security by keeping sensitive data closer to the source. Instead of sending data to a central location, where it can be intercepted and compromised, data is processed and analyzed on the device or gateway itself. This can greatly reduce the risk of data breaches and cyber-attacks.\n3. Cost Savings Edge computing can also lead to cost savings by reducing the amount of data that needs to be transmitted over a network. Instead of sending all data to a central location, only relevant data is sent, which can result in lower bandwidth costs and reduced storage requirements.\nChallenges of Edge Computing 1. Complexity One of the main challenges of edge computing is complexity. Since data processing and analysis happen closer to the source, there are often multiple devices, gateways, and systems involved in the process. This can make it difficult to manage and maintain, especially for organizations that don‚Äôt have a lot of experience with edge computing.\n2. Security Concerns While edge computing can improve security, it can also introduce new security concerns. With data being processed and analyzed on multiple devices, gateways, and systems, there are more potential points of vulnerability that need to be secured.\n3. Scalability Scalability is another challenge of edge computing. As more devices and systems are added to the edge computing infrastructure, it can become difficult to scale the system effectively. This can result in performance issues and a decrease in overall efficiency.\nChoosing the Right Edge Computing Solution When it comes to choosing the right edge computing solution for your organization, it‚Äôs important to consider the specific needs of your business. Some factors to consider include the size of your organization, the number of devices you have, and your budget.\nIt‚Äôs also important to choose a solution that is scalable, secure, and easy to manage. At Microhost, we offer a range of edge computing solutions that are designed to meet the needs of businesses of all sizes. Our solutions are secure, scalable, and easy to manage, making it easy for you to implement edge computing in your organization.\nConclusion In conclusion, edge computing has many advantages, including reduced latency, improved security, and cost savings. However, there are also challenges associated with implementing edge computing, including complexity, security concerns, and scalability. By choosing the right edge computing solution for your organization and partnering with an experienced provider like Microhost, you can reap the benefits of edge computing while minimizing the challenges.\nRead Also:\nRead Also: 6 Cloud Computing Myths, Busted!","what-is-edge-computing#\u003cstrong\u003eWhat is Edge Computing?\u003c/strong\u003e":""},"title":"Advantages and challenges of implementing edge computing in your organization"},"/utho-docs/docs/blog/advantages-and-challenges-of-using-ai-and-machine-learning-in-the-cloud/":{"data":{"":"","advantages-of-using-ai-and-ml-in-the-cloud#\u003cstrong\u003eAdvantages of using AI and ML in the cloud\u003c/strong\u003e":"","challenges-of-using-ai-and-ml-in-the-cloud#\u003cstrong\u003eChallenges of using AI and ML in the cloud\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction As the world becomes increasingly data-driven, businesses are turning to artificial intelligence (AI) and machine learning (ML) to gain insights and make more informed decisions. The cloud has become a popular platform for deploying AI and ML applications due to its scalability, flexibility, and cost-effectiveness. In this article, we‚Äôll explore the advantages and challenges of using AI and ML in the cloud.\nAdvantages of using AI and ML in the cloud Scalability One of the primary advantages of using AI and ML in the cloud is scalability. Cloud providers offer the ability to scale up or down based on demand, which is essential for AI and ML applications that require large amounts of processing power. This allows businesses to easily increase or decrease the resources allocated to their AI and ML applications, reducing costs and increasing efficiency.\nFlexibility Another advantage of using AI and ML in the cloud is flexibility. Cloud providers offer a wide range of services and tools for developing, testing, and deploying AI and ML applications. This allows businesses to experiment with different technologies and approaches without making a significant upfront investment.\nCost-effectiveness Using AI and ML in the cloud can also be more cost-effective than deploying on-premises. Cloud providers offer a pay-as-you-go model, allowing businesses to pay only for the resources they use. This eliminates the need for businesses to invest in expensive hardware and software, reducing upfront costs.\nImproved performance Cloud providers also offer access to high-performance computing resources that can significantly improve the performance of AI and ML applications. This includes specialized hardware, such as graphics processing units (GPUs) and tensor processing units (TPUs), which are designed to accelerate AI and ML workloads.\nEasy integration Finally, using AI and ML in the cloud can be easier to integrate with other cloud-based services and applications. This allows businesses to create more comprehensive and powerful solutions that combine AI and ML with other technologies such as analytics and data warehousing.\nChallenges of using AI and ML in the cloud Data security and privacy One of the primary challenges of using AI and ML in the cloud is data security and privacy. Cloud providers are responsible for ensuring the security and privacy of customer data, but businesses must also take steps to protect their data. This includes implementing strong access controls, encryption, and monitoring to detect and respond to potential threats.\nTechnical complexity Another challenge of using AI and ML in the cloud is technical complexity. Developing and deploying AI and ML applications can be complex, requiring specialized knowledge and expertise. This can be a barrier to entry for businesses that lack the necessary skills and resources.\nDependence on the cloud provider Using AI and ML in the cloud also means dependence on the cloud provider. Businesses must rely on the cloud provider to ensure the availability, reliability, and performance of their AI and ML applications. This can be a concern for businesses that require high levels of uptime and reliability.\nLatency and bandwidth limitations Finally, using AI and ML in the cloud can be limited by latency and bandwidth. AI and ML applications require large amounts of data to be transferred between the cloud and the end-user device. This can lead to latency and bandwidth limitations, particularly for applications that require real-time processing.\nConclusion Using AI and ML in the cloud offers numerous advantages, including scalability, flexibility, cost-effectiveness, improved performance, and easy integration. However, it also presents several challenges, including data security and privacy, technical complexity, dependence on the cloud provider, and latency and bandwidth limitations. Businesses must carefully consider these factors when deciding whether to use AI and ML in the cloud.\nAt Microhost, we offer a range of cloud-based solutions and services to help businesses harness the power of AI and machine learning. Our team of experts can help you navigate the challenges and complexities of implementing these technologies in the cloud, and ensure that you are maximizing their potential.\nWhether you are looking to develop custom machine learning models, or simply need help with integrating AI-powered applications into your existing infrastructure, our solutions are tailored to meet your specific needs. With a focus on security, scalability, and performance, we can help you build a robust and future-proof cloud environment that will drive your business forward.\nRead Also: Challenges of Cloud Server Compliance","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":""},"title":"Advantages and Challenges of Using AI and Machine Learning in the Cloud"},"/utho-docs/docs/blog/advantages-and-challenges-of-using-cloud-based-analytics-tools/":{"data":{"":"\nIn today‚Äôs data-driven world, organizations are constantly seeking effective ways to extract valuable insights from their vast amounts of data. Cloud-based analytics tools have emerged as a popular solution, providing a scalable and cost-effective way to analyze data. In this article, we will explore the advantages and challenges of using cloud-based analytics tools.","advantages-of-cloud-based-analytics-tools#Advantages of Cloud-Based Analytics Tools":"1. Scalability and Flexibility One of the significant advantages of cloud-based analytics tools is their scalability and flexibility. These tools can easily handle large volumes of data and accommodate fluctuations in data processing requirements. Cloud platforms offer the ability to scale up or down resources based on demand, ensuring that organizations have the necessary computing power to analyze data effectively.\n2. Cost Efficiency Cloud-based analytics tools offer cost efficiency by eliminating the need for organizations to invest in and maintain their own on-premises infrastructure. With a pay-as-you-go model, organizations only pay for the resources they consume, avoiding upfront infrastructure costs. This allows businesses of all sizes to access sophisticated analytics capabilities without significant capital investments.\n3. Enhanced Performance Cloud platforms provide high-performance computing resources that enable faster data processing and analysis. With distributed computing and parallel processing, organizations can process vast amounts of data efficiently, enabling quicker insights and decision-making. The advanced infrastructure of cloud providers ensures reliable performance and reduces the processing time for complex analytics tasks.\n4. Advanced Analytics Capabilities Cloud-based analytics tools often come equipped with advanced analytics capabilities, such as machine learning, artificial intelligence, and predictive modeling. These tools empower organizations to gain deeper insights from their data, uncover patterns and trends, and make data-driven decisions. The integration of these advanced analytics capabilities within the cloud environment simplifies the implementation and utilization of such technologies.\n5. Data Security and Compliance Cloud providers prioritize data security and have robust security measures in place to protect sensitive data. They employ encryption, access controls, and regular security audits to ensure the confidentiality and integrity of data. Cloud platforms also comply with industry-specific regulations, making it easier for organizations to meet compliance requirements and maintain data privacy.","challenges-of-using-cloud-based-analytics-tools#Challenges of Using Cloud-Based Analytics Tools":"While cloud-based analytics tools offer numerous advantages, organizations should be aware of the following challenges:\n1. Data Integration and Connectivity Integrating data from various sources and ensuring connectivity with cloud-based analytics tools can be complex. Organizations must address data compatibility issues, data quality concerns, and establish robust data integration processes to ensure seamless and accurate data flow.\n2. Bandwidth and Latency Issues Analyzing large volumes of data in the cloud requires substantial bandwidth. In cases where organizations have limited internet connectivity or deal with massive datasets, transferring data to and from the cloud may result in latency issues. Organizations need to consider the network infrastructure and bandwidth requirements for efficient data processing.\n3. Data Governance and Privacy When using cloud-based analytics tools, organizations must carefully manage data governance and privacy. This includes understanding where data is stored, ensuring compliance with data protection regulations, and implementing appropriate access controls and data handling practices.\n4. Vendor Lock-In Migrating data and applications to the cloud involves a certain level of vendor lock-in. Organizations should evaluate vendor lock-in risks and ensure they have an exit strategy in case they decide to switch providers or bring the analytics capabilities in-house.","conclusion#Conclusion":"Cloud-based analytics tools provide significant advantages in terms of scalability, cost efficiency, performance, advanced analytics capabilities, and data security. However, organizations must also address challenges related to data integration, bandwidth, data governance, and vendor lock-in.\nMicrohost offers a comprehensive range of cloud-based analytics tools and services to help organizations unlock the full potential of their data. To learn more about Microhost‚Äôs offerings and how they can support your organization‚Äôs analytics needs, visit their website at utho.com. With Microhost‚Äôs expertise and advanced cloud-based analytics solutions, you can harness the power of data to drive informed decisions and gain a competitive edge in today‚Äôs data-driven business landscape.\nRemember, choosing the right cloud-based analytics tools is crucial for unlocking the true value of your data. Evaluate your organization‚Äôs requirements, consider the advantages and challenges discussed in this article, and partner with a trusted provider like Microhost to make the most out of your analytics journey in the cloud.\nEmbrace the power of cloud-based analytics tools and unlock the insights hidden within your data today!\nAlso Read: Advantages and Challenges of Using Kubernetes and Containers in the Cloud"},"title":"Advantages and Challenges of Using Cloud-Based Analytics Tools"},"/utho-docs/docs/blog/advantages-and-challenges-of-using-kubernetes-and-containers-in-the-cloud/":{"data":{"":"","advantages-of-using-kubernetes-and-containers-in-the-cloud#\u003cstrong\u003eAdvantages of Using Kubernetes and Containers in the Cloud\u003c/strong\u003e":"","challenges-of-using-kubernetes-and-containers-in-the-cloud#\u003cstrong\u003eChallenges of Using Kubernetes and Containers in the Cloud\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction Kubernetes and containers are two buzzwords that have been floating around in the tech industry for a while now. But what exactly are they and how can they benefit your cloud infrastructure? In this article, we will explore the advantages and challenges of using Kubernetes and containers in the cloud.\nWhat are Kubernetes and Containers? Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. Containers, on the other hand, are a lightweight and portable way to package and run applications, along with all of their dependencies.\nAdvantages of Using Kubernetes and Containers in the Cloud Improved Scalability Kubernetes and containers can help improve the scalability of your cloud infrastructure by allowing you to easily deploy and manage multiple instances of your application. This is especially useful during times of high traffic when you need to quickly scale your resources up or down.\nIncreased Portability Containers are designed to be highly portable, meaning they can run on any cloud infrastructure regardless of the underlying hardware or operating system. This allows for greater flexibility and makes it easier to move your applications between different cloud providers or environments.\nImproved Resource Utilization Containers are highly efficient and can be run on a single machine or distributed across multiple machines, which helps to improve resource utilization and reduce costs.\nFaster Deployment Times Kubernetes and containers can help to reduce deployment times by providing a standardized way to package and deploy applications. This means that you can quickly and easily deploy your applications without having to worry about the underlying infrastructure.\nChallenges of Using Kubernetes and Containers in the Cloud Complexity One of the biggest challenges of using Kubernetes and containers is their complexity. Setting up and managing a Kubernetes cluster can be a daunting task, and requires a deep understanding of the underlying infrastructure and networking.\nSecurity Containers are designed to be lightweight and portable, which makes them vulnerable to security threats if not properly secured. It is important to ensure that your containers are properly configured and secured to prevent unauthorized access or data breaches.\nResource Overhead Kubernetes and containers require additional resources to run, such as memory and CPU. This can lead to increased costs if not properly managed.\nConclusion Kubernetes and containers offer a number of advantages for managing your cloud infrastructure, including improved scalability, increased portability, improved resource utilization, and faster deployment times. However, they also come with their own set of challenges, such as complexity, security, and resource overhead. It is important to carefully consider these factors when deciding whether or not to use Kubernetes and containers in your cloud environment.\nIf you are interested in learning more about Kubernetes and containers, or need help managing them in your cloud environment, consider contacting MicroHost. As a leading cloud hosting provider, MicroHost offers a range of cloud hosting solutions to help you manage and scale your applications with ease. Visit our website at https://utho.com/ to learn more.\nRead Also: Best VPS Hosting Providers in 2023","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","what-are-kubernetes-and-containers#\u003cstrong\u003eWhat are Kubernetes and Containers?\u003c/strong\u003e":""},"title":"Advantages and Challenges of Using Kubernetes and Containers in the Cloud"},"/utho-docs/docs/blog/benefits-of-cloud-computing-for-small-businesses/":{"data":{"":"","#":"Cloud computing is becoming an increasingly popular choice for small businesses due to the many benefits it offers, including increased efficiency, cost savings, and greater flexibility. Whether you‚Äôre looking to transition your entire business to the cloud or just want to take advantage of its many benefits, there are a number of things you should consider when making this transition.\nIncrease Efficiency One of the biggest benefits of cloud computing is increased efficiency. Cloud-based systems enable employees to collaborate more easily and work from anywhere, making it easier for them to get their work done on time and meet deadlines. This also means less time wasted on communication issues or ineffective tools that slow down productivity.\nFor instance, one set of cloud-based systems that have made a significant impact are cloud HR tools. To highlight this point, companies like SnackNation have compiled lists of their top 10 cloud HR tools, showcasing how they contribute to increasing overall efficiency, collaboration, and flexibility in the modern workforce environment.\nCost Savings Cost savings is another key benefit of cloud computing for small businesses. Because cloud-based systems require less up-front investment on your part, you can get the same features and functionality as larger organizations without having to pay a premium. Additionally, many cloud service providers offer subscription-based pricing plans that are more cost-effective in the long run compared to traditional software licenses.\nGreater Flexibility Finally, cloud computing also offers greater flexibility for small businesses. With cloud hosting solutions, it‚Äôs easy to scale up or down depending on your needs, meaning you can add or remove resources as needed without having to invest in additional equipment or make expensive upgrades.\nWhether you‚Äôre looking to expand your business or scale back during slower periods, the cloud is a great option that gives you the flexibility to adapt quickly and stay competitive. If you‚Äôre ready to take advantage of the many benefits of cloud computing for your small business, consider partnering with a trusted cloud service provider to help you make the transition. With their expertise and support, you can be confident that your move to the cloud will be a success.\nRead Also: 6 Cloud Computing Myths, Busted!"},"title":"Benefits of Cloud Computing for Small Businesses"},"/utho-docs/docs/blog/benefits-of-using-cloud-servers-compared-to-physical-servers/":{"data":{"":"","here-are-some-of-the-key-benefits-of-using-cloud-servers-#\u003cstrong\u003eHere are some of the key benefits of using cloud servers:-\u003c/strong\u003e":"\nThere are many benefits of using cloud servers compared to physical servers. Cloud servers are more scalable and flexible and provide better performance. They are also more secure and offer better uptime.¬†Here are some of the key benefits of using cloud servers:- Cloud servers offer scalability and flexibility, allowing businesses to easily adjust their storage and computing power as needed.¬†Cloud servers provide cost savings, as there is no need for expensive hardware or maintenance costs.¬†Cloud servers offer improved security measures, with built-in backups and disaster recovery plans.¬†Cloud servers allow for remote access and collaboration, making it easy for teams to work together from anywhere.\nCloud servers have a high uptime, ensuring reliable and consistent performance.\nWith cloud servers, software and system updates are automatic and seamless.¬†Cloud servers offer enhanced accessibility, as information can be accessed from any device with internet connection.¬†Cloud servers provide the ability to test and develop new applications without impacting the current system.\nCloud servers offer improved disaster recovery capabilities, as data can be easily restored in the event of a security breach or natural disaster.¬†Cloud servers allow for better data management and organization.¬†Cloud servers offer enhanced collaboration opportunities with partners and clients.¬†Cloud servers provide improved agility and responsiveness to changing business needs.¬†Cloud servers offer increased cost-effectiveness for businesses, as they only pay for the resources they use.¬†Cloud servers allow businesses to focus on their core competencies, rather than managing IT infrastructure.\nCloud server technology is constantly evolving and improving, offering even more benefits for businesses.\nAs India‚Äôs first cloud platform, Microhost offers all of these benefits and more. With top-notch security measures, 24/7 support, and a user-friendly interface, Microhost is the premier choice for your cloud server needs. Visit our website to learn more about how we can help your business succeed in today‚Äôs digital world.\nAlso Read: 7 Reasons Why Cloud Infrastructure is Important for Startups"},"title":"Benefits of using Cloud Servers compared to Physical Servers"},"/utho-docs/docs/blog/best-cloud-platform-for-your-business/":{"data":{"":"","how-to-choose-the-best-cloud-platform-for-your-business-needs#How to Choose the Best Cloud Platform for Your Business Needs":"As more and more businesses shift to the cloud, choosing the right cloud platform for your business can be a daunting task. There are several cloud platforms to choose from, each with its unique features and capabilities. In this blog post, we‚Äôll explore the factors you need to consider when choosing the best cloud platform for your business needs.\nIdentify Your Business Needs The first step in choosing the right cloud platform for your business is to identify your business needs. What are your business objectives, and what kind of workloads are you looking to run on the cloud? Different cloud platforms offer different services and capabilities, so it‚Äôs essential to know what you need before you start evaluating platforms.\nConsider Scalability and Flexibility Scalability and flexibility are critical factors to consider when choosing a cloud platform. Your business needs may change over time, and you‚Äôll want a platform that can accommodate those changes. Look for a cloud platform that allows you to easily scale up or down, depending on your business needs.\nEvaluate Security and Compliance Security and compliance are crucial considerations when choosing a cloud platform. Your cloud platform should offer robust security features to protect your data and infrastructure. Additionally, if your business operates in a regulated industry, you‚Äôll want to ensure that the platform complies with the necessary regulations.\nEvaluate Costs Cost is another critical factor to consider when choosing a cloud platform. Look for a platform that offers transparent pricing and allows you to pay only for the services you use. Some platforms offer discounts for long-term commitments, so be sure to evaluate the total cost of ownership before making a decision.\nEvaluate Support and Documentation Support and documentation are essential considerations when choosing a cloud provider. Look for a platform that offers comprehensive documentation and support to help you get started quickly and troubleshoot any issues that may arise.\nConsider Vendor Lock-In Vendor lock-in is a risk. You don‚Äôt want to be locked into a platform that doesn‚Äôt meet your business needs or one that is difficult to migrate from. Look for a provider that offers standard APIs and open standards to avoid vendor lock-in.\nConclusion Choosing the best cloud provider for your business needs requires careful consideration of several factors. Identify your business needs, evaluate scalability and flexibility, consider security and compliance, evaluate costs, evaluate support and documentation, and consider vendor lock-in. By following these guidelines, you‚Äôll be able to choose the right cloud provider for your business.\nAt MicroHost, we offer a comprehensive cloud platform that meets all of these requirements. Our platform offers scalable and flexible services with robust security features, transparent pricing, comprehensive documentation and support, and open standards to avoid vendor lock-in. Contact us today to learn more about our cloud services"},"title":"Best Cloud Platform for Your Business"},"/utho-docs/docs/blog/best-practices-for-implementing-serverless-computing-in-your-organization/":{"data":{"":"","development-and-deployment#\u003cstrong\u003eDevelopment and Deployment\u003c/strong\u003e":"","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","operations-and-monitoring#\u003cstrong\u003eOperations and Monitoring\u003c/strong\u003e":"\nIntroduction Serverless computing has become an increasingly popular option for organizations looking to build and run applications more efficiently. With serverless computing, developers can focus on writing code without having to worry about infrastructure management. However, implementing serverless computing in your organization requires careful planning and execution. In this article, we‚Äôll discuss some best practices for implementing serverless computing in your organization.\nPlanning and Architecture 1. Identify the Right Use Cases Serverless computing can be a great fit for certain types of workloads, such as event-driven applications or applications with unpredictable traffic patterns. Before implementing serverless computing, it‚Äôs important to identify the use cases that will benefit most from this approach. Consider factors such as workload characteristics, resource requirements, and application dependencies.\n2. Design for Scalability and Resiliency Serverless computing provides automatic scaling and failover, but it‚Äôs important to design your applications to take advantage of these features. Use a microservices architecture with small, loosely-coupled components that can be scaled independently. Use monitoring and logging to identify and diagnose issues quickly.\n3. Understand Your Limits Serverless computing imposes some limits on resource usage, such as function execution time, memory usage, and network bandwidth. It‚Äôs important to understand these limits and design your applications accordingly. Use optimization techniques such as caching, batching, and asynchronous processing to minimize resource usage.\nDevelopment and Deployment 1. Use Best Practices for Code Quality and Security Serverless computing requires developers to write high-quality, secure code. Follow industry best practices for code quality and security, such as using code reviews, automated testing, and security scanning tools.\n2. Optimize for Performance and Cost Serverless computing charges based on resource usage, so it‚Äôs important to optimize your applications for performance and cost. Use techniques such as caching, lazy loading, and serverless-specific optimizations such as cold start reduction techniques.\n3. Implement CI/CD Pipeline for Serverless Applications Implement a CI/CD pipeline to streamline the development and deployment of serverless applications. Use automation tools to build, test, and deploy your applications in a consistent and repeatable way. This will help you catch issues early and reduce the time it takes to deliver new features.\nOperations and Monitoring 1. Monitor and Optimize Resource Usage Serverless computing provides automatic scaling, but it‚Äôs important to monitor and optimize resource usage to avoid unnecessary costs. Use monitoring tools to track resource usage and identify opportunities for optimization, such as reducing memory usage, optimizing database queries, and reducing function execution time.\n2. Establish Clear Governance and Policies Establish clear governance and policies for serverless computing to ensure compliance with regulatory requirements and best practices. Use tools to enforce policies for resource allocation, security, and access control.\n3. Use Cloud-Native Tools and Services Serverless computing is part of the larger cloud computing ecosystem. Take advantage of cloud-native tools and services such as cloud storage, databases, and analytics to enhance the capabilities of your serverless applications.\n**Conclusion** Implementing serverless computing in your organization can offer significant benefits in terms of cost savings, scalability, and agility. However, it requires careful planning and execution to achieve these benefits. By following these best practices for planning, architecture, development, deployment, operations, and monitoring, you can successfully implement serverless computing in your organization.\nIf you‚Äôre interested in learning more about serverless computing and how it can benefit your organization, visit Microhost‚Äôs website. Their cloud computing experts can help you navigate the serverless landscape and find the right solution for your needs.\nAlso Read: 5 Most Effective Ways to Avoid Cloud Bill Shocks.","planning-and-architecture#\u003cstrong\u003ePlanning and Architecture\u003c/strong\u003e":""},"title":"Best Practices for Implementing Serverless Computing in Your Organization"},"/utho-docs/docs/blog/best-practices-for-managing-and-securing-edge-computing-devices/":{"data":{"":"","about-microhost#\u003cstrong\u003eAbout MicroHost\u003c/strong\u003e":"\nIntroduction In recent years, edge computing has become increasingly popular as organizations seek to bring computing capabilities closer to where data is generated and consumed. Edge computing devices, which can range from simple sensors to advanced servers, have the potential to provide significant benefits in terms of reduced latency, improved performance, and enhanced security. However, managing and securing these devices can be challenging, especially when they are distributed across multiple locations.\nBest Practices for Managing Edge Computing Devices 1. Develop a comprehensive inventory It‚Äôs important to have a comprehensive inventory of all edge computing devices deployed in your organization. This includes not only the physical devices but also their software and firmware versions, network addresses, and other relevant details. This information can be used to identify potential vulnerabilities and ensure that devices are updated and patched in a timely manner.\n2. Establish a standard configuration Having a standard configuration for edge computing devices can simplify management and reduce the risk of misconfiguration. This includes things like standard operating systems, software versions, and security settings. By establishing a standard configuration, you can ensure that all devices are secure and compliant with your organization‚Äôs policies.\n3. Implement remote management tools Remote management tools can help you manage edge computing devices more efficiently, especially when they are distributed across multiple locations. These tools allow you to monitor device status, configure settings, and perform updates and patches remotely, reducing the need for on-site visits.\n4. Monitor device performance Monitoring the performance of edge computing devices can help you identify potential issues before they become serious problems. This includes monitoring CPU and memory usage, network traffic, and other key metrics. By proactively identifying and addressing performance issues, you can ensure that your devices are operating at peak efficiency.\nBest Practices for Securing Edge Computing Devices 1. Use secure boot and firmware validation Secure boot and firmware validation can help ensure that edge computing devices are running only authorized software and firmware. This can prevent attackers from installing malicious code that can compromise the security of your devices and your organization‚Äôs data.\n2. Implement strong access controls Implementing strong access controls is critical for securing edge computing devices. This includes things like requiring strong passwords, limiting access to authorized users, and using multi-factor authentication. By implementing strong access controls, you can reduce the risk of unauthorized access and prevent data breaches.\n3. Encrypt data in transit and at rest Encrypting data in transit and at rest is essential for protecting the confidentiality and integrity of your organization‚Äôs data. This includes using protocols like SSL/TLS to encrypt data in transit and using strong encryption algorithms to encrypt data at rest. By encrypting your data, you can ensure that it remains secure even if it is intercepted by attackers.\n4. Regularly update and patch devices Regularly updating and patching edge computing devices is essential for ensuring that they remain secure. This includes updating both software and firmware, as well as applying security patches as soon as they become available. By keeping your devices up-to-date, you can reduce the risk of known vulnerabilities being exploited by attackers.\nConclusion Managing and securing edge computing devices can be challenging, but by following these best practices, you can ensure that your devices are operating at peak efficiency and are protected from potential security threats. By developing a comprehensive inventory, establishing standard configurations, implementing remote management tools, monitoring device performance, using secure boot and firmware validation, implementing strong access controls, encrypting data, and regularly updating and patching devices, you can ensure that your edge computing devices are secure and compliant with your organization‚Äôs policies.\nAbout MicroHost If you‚Äôre looking for a reliable and secure edge computing solution, MicroHost offers a range of services to meet your needs. Our team of experts can help you design and implement an edge computing strategy that is tailored to your specific requirements. With our state-of-the-art infrastructure and industry-leading security measures, you can rest assured that your edge computing devices are protected against cyber threats and data breaches. Contact us today to learn more about how we can help you manage and secure your edge computing devices.\nAlso Read: Benefits of Cloud Computing for Small Businesses","best-practices-for-managing-edge-computing-devices#\u003cstrong\u003eBest Practices for Managing Edge Computing Devices\u003c/strong\u003e":"","best-practices-for-securing-edge-computing-devices#\u003cstrong\u003eBest Practices for Securing Edge Computing Devices\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":""},"title":"Best Practices for Managing and Securing Edge Computing Devices"},"/utho-docs/docs/blog/best-vps-hosting-providers-in-2023/":{"data":{"":"","best-vps-hosting-providers-in-2023-a-comprehensive-guide#\u003cstrong\u003eBest VPS Hosting Providers in 2023: A Comprehensive Guide\u003c/strong\u003e":"Virtual Private Server (VPS) hosting is a popular choice for businesses and individuals looking for more control and flexibility than shared hosting provides. With VPS hosting, you get your own virtual server with dedicated resources and the ability to install your own software and applications.\nChoosing the right VPS hosting provider can be a daunting task, with many options available. In this comprehensive guide, we‚Äôll take a closer look at the top VPS hosting providers in 2023 and the factors you should consider when making your choice.\nBest VPS Hosting Providers in 2023: A Comprehensive Guide Factors to Consider When Choosing a VPS Hosting Provider Before we dive into the top VPS hosting providers, let‚Äôs look at the factors you should consider when making your choice: Reliability and uptime guarantees: Look for a provider that offers high uptime guarantees, ideally 99.9% or higher. This ensures that your website or application will be available to users at all times.\nPerformance and speed: Choose a provider with fast server hardware and solid-state drives (SSDs) for faster load times and better performance.\nScalability and flexibility: Look for a provider that allows you to easily scale your resources as your needs grow, without having to migrate to a new server.\nCustomer support: Make sure the provider offers reliable customer support, ideally 24/7, to help you quickly resolve any issues.\nPrice and value for money: Compare prices and features to find a provider that offers good value for money.\nTop VPS Hosting Providers in 2023 Based on the above factors, here are the top VPS hosting providers for 2023:\nUtho: Utho is a popular choice for developers and small businesses, offering a user-friendly control panel, SSDs, 24/7 Human Support, hourly billing and flexible pricing plans starting at $36 per month. They also have a robust API and offer one-click app installations for popular software.¬†DigitalOcean: DigitalOcean offering a user-friendly control panel, SSDs, and flexible pricing plans starting at $48 per month. They also have a robust API and offer one-click app installations.\nLinode: Linode offers high-performance VPS hosting with SSDs, 40 Gbps network connectivity, and a user-friendly control panel. They offer flexible pricing plans starting at $40 per month and excellent customer support.\nVultr: Vultr offers fast VPS hosting with SSDs, 100% Intel CPUs, and 16 data center locations worldwide. They also offer hourly billing and flexible pricing plans starting at $48 per month.\nComparison of VPS Hosting Providers Here‚Äôs a side-by-side comparison of the top VPS hosting providers:¬†ProviderConfigurationPriceUthoCloud Server ‚Äì 4 vCPU, 8 GB RAM, 160 GB Disk, 1000 GB Traffic$36/moDigitalOceanBasic Droplet ‚Äì 4 vCPU, 8 GB RAM, 160 GB SSD, 5 TB Traffic$48/moLinodeShared CPU ‚Äì 4 vCPU, 8 GB RAM, 160 GB SSD, 5 TB Traffic$40/moVultrCloud Compute ‚Äì 4 vCPU, 8 GB RAM, 180 GB NVMe SSD, 6 TB Traffic$48/mo Note that pricing plans and features may vary depending on the provider and the specific VPS hosting plan you choose.\nConclusion When choosing a VPS hosting provider, it‚Äôs important to consider factors such as reliability, performance, scalability, customer support, and price.¬†Ultimately, the right provider for you will depend on your specific needs and budget. We encourage you to take advantage of free trials and compare features and pricing to find the best fit for your business or personal use.\nWe hope this comprehensive guide has helped you make an informed decision on the best VPS hosting provider for your needs. Good luck with your hosting journey!\nRead Also: Benefits of Cloud Computing for Small Businesses"},"title":"Best VPS Hosting Providers in 2023"},"/utho-docs/docs/blog/beyond-boundaries-ensuring-safety-with-ipsec-tunnels/":{"data":{"":"","heading#":"","how-does-utho-cloud-facilitate-ipsec-tunnel-implementation-to-enhance-security-for-businesses#\u003cstrong\u003eHow does Utho Cloud facilitate \u003cstrong\u003eIPsec\u003c/strong\u003e tunnel implementation to enhance security for businesses?\u003c/strong\u003e":"In today‚Äôs digital world, businesses and organizations rely more and more on cloud services to store and use their data. But this means they need strong security to keep their information safe from cyber threats. That‚Äôs where IPsec tunnels come in! They‚Äôre like secure paths that let data travel safely between different places, even if they‚Äôre far apart. So, no matter where your data goes, you can trust it‚Äôs staying safe thanks to these IPsec tunnels.\nWhat is IPsec, and how does it play a crucial role in securing cloud server connections? IPsec, or Internet Protocol Security, is a set of protocols used to secure internet communication by authenticating and encrypting each IP packet of a data stream. It plays a crucial role in securing cloud server connections by providing several key features:\nAuthentication: IPsec verifies the identities of communicating parties, ensuring that only authorized users or devices can access the cloud servers.\nEncryption: It encrypts the data packets exchanged between the client and server, protecting sensitive information from unauthorized access or interception.\nIntegrity: IPsec ensures that data remains intact during transmission by detecting and preventing tampering or modification of packets.\nTunneling: It enables the creation of secure tunnels between endpoints, allowing remote users or branch offices to securely connect to cloud servers over the internet.\nBy implementing IPsec, cloud server connections are fortified against various cyber threats such as eavesdropping, data tampering, and unauthorized access, thereby ensuring the confidentiality, integrity, and availability of data in cloud environments.\n**What are the primary benefits of utilizing IPsec tunnels for cloud server connections compared to other security protocols? **\nUtilizing IPsec tunnels for cloud server connections offers several primary benefits compared to other security protocols:\nStrong Encryption: IPsec employs robust encryption algorithms to protect data transmitted over the internet, ensuring that sensitive information remains confidential and secure from potential eavesdropping or interception.\nAuthentication Mechanisms: IPsec provides robust authentication mechanisms, including pre-shared keys, digital certificates, or more advanced methods like IKEv2, to verify the identities of communicating parties, thereby preventing unauthorized access to cloud servers.\nData Integrity: IPsec ensures the integrity of data by detecting and preventing tampering or modification during transmission, guaranteeing that the information received at the cloud server is the same as that sent by the client.\nEnd-to-End Security: IPsec establishes secure tunnels between endpoints, creating a virtual private network (VPN) over the public internet. This ensures end-to-end security for cloud server connections, regardless of the underlying network infrastructure.\nFlexibility and Interoperability: IPsec is a widely adopted industry standard protocol supported by various networking devices and operating systems. This ensures interoperability between different systems and allows for flexible deployment in diverse cloud environments.\nScalability: IPsec tunnels can easily scale to accommodate increasing traffic or expanding cloud infrastructures, making it suitable for small businesses as well as large enterprises with dynamic computing needs.\nOverall, the utilization of IPsec tunnels for cloud server connections offers a comprehensive security solution that combines encryption, authentication, integrity, and scalability, making it an ideal choice for protecting sensitive data and ensuring secure communication in cloud environments.\nWhat challenges and myths exist with Internet Protocol Security tunnels for cloud servers, and how can we overcome them?\nImplementing IPsec tunnels for cloud server connections may pose some common challenges or misconceptions, which can be addressed through careful consideration and proactive measures:\nComplex Configuration: Setting up IPsec tunnels can be complex, especially for users with limited networking knowledge. Solution: Utilize simplified configuration interfaces provided by cloud service providers or employ automated deployment tools to streamline the setup process.\nPerformance Overhead: Encrypting and decrypting data within IPsec tunnels can introduce latency and overhead, impacting network performance. Solution: Optimize IPsec configurations by selecting appropriate encryption algorithms and key exchange methods that balance security with performance. Additionally, leverage hardware acceleration or specialized VPN appliances to offload encryption tasks and improve throughput.\nInteroperability Issues: Compatibility issues may arise when establishing IPsec tunnels between different vendor devices or across heterogeneous cloud environments. Solution: Ensure compatibility and interoperability by selecting IPsec-compliant devices and adhering to standardized configurations. Additionally, leverage industry best practices and conduct thorough testing to validate interoperability before deployment.\nKey Management Complexity: Managing cryptographic keys and certificates for IPsec tunnels can be challenging, leading to security vulnerabilities if not properly handled. Solution: Implement robust key management practices, such as regularly rotating keys, using secure key storage mechanisms, and employing certificate revocation mechanisms to mitigate risks associated with key compromise.\nScalability Constraints: Scaling IPsec tunnels to accommodate growing network traffic or expanding cloud deployments may pose scalability challenges. Solution: Design IPsec architectures with scalability in mind by implementing load balancing, redundant tunnel configurations, and dynamic routing protocols to efficiently manage increasing traffic demands and scale resources as needed.\nBy addressing these common challenges and misconceptions associated with implementing IPsec tunnels for cloud server connections, organizations can enhance security, optimize performance, and ensure seamless connectivity across their cloud environments.\nWhat types of industries commonly utilize IPsec tunnels for securing their network communications? Various industries rely on IPsec tunnels to secure their network communications. Some common examples include:\nHealthcare: Hospitals, clinics, and healthcare organizations use IPsec tunnels to safeguard patient data transmitted between medical devices, electronic health record (EHR) systems, and cloud servers.\nFinance: Banks, financial institutions, and payment processing companies utilize IPsec tunnels to encrypt sensitive financial transactions and protect customer information from unauthorized access.\nGovernment: Government agencies and departments employ IPsec tunnels to secure communications between offices, data centers, and cloud-based systems, ensuring the confidentiality of classified information.\nTechnology: Technology companies, including software development firms and IT service providers, implement IPsec tunnels to secure client data, communications, and access to cloud-based infrastructure.\nManufacturing: Manufacturing companies leverage IPsec tunnels to protect proprietary designs, production data, and supply chain information exchanged between facilities and cloud-based systems.\nEducation: Schools, universities, and educational institutions use IPsec tunnels to secure student records, research data, and administrative communications transmitted over network connections.\nRetail: Retailers and e-commerce businesses utilize IPsec tunnels to secure online transactions, customer data, and inventory management systems hosted on cloud servers.\nOverall, IPsec tunnels are essential for securing network communications across various industries, ensuring the confidentiality, integrity, and availability of sensitive data and resources.\nWhat future trends in IPsec tunnels for cloud servers could boost security and reliability? In the realm of Internet Protocol Security tunnels for cloud server connections, several advancements and trends are expected to further enhance security and reliability:\nIntegration with SD-WAN: There‚Äôs a growing trend towards integrating IPsec tunnels with Software-Defined Wide Area Network (SD-WAN) technology. This integration enables dynamic routing and optimization of traffic between different cloud servers, improving both security and performance.\nZero Trust Network Access (ZTNA): With the increasing adoption of Zero Trust principles, IPsec tunnels are likely to evolve to support ZTNA architectures. This approach focuses on strict identity verification and access controls, ensuring that only authorized users and devices can establish connections to cloud servers via IPsec tunnels.\nEnhanced Encryption Algorithms: As cyber threats continue to evolve, there will be advancements in encryption algorithms used within IPsec tunnels. Expect to see the adoption of stronger encryption standards, such as post-quantum cryptography, to better safeguard data against emerging security risks.\nAutomation and Orchestration: Automation and orchestration tools will play a crucial role in managing and provisioning IPsec tunnels for cloud server connections. This trend will streamline deployment processes, improve scalability, and enhance overall network agility while maintaining security and reliability.\nMulti-Cloud Support: With many organizations adopting multi-cloud strategies, IPsec tunnels will need to support seamless connectivity across different cloud providers. Look for advancements that enable easy configuration and management of IPsec tunnels in heterogeneous cloud environments, ensuring consistent security and reliability regardless of the cloud platform.\nImproved Monitoring and Analytics: Expect advancements in monitoring and analytics capabilities for IPsec tunnels, providing real-time visibility into traffic patterns, performance metrics, and security events. This proactive approach enables quicker detection and response to potential threats or network issues, further enhancing overall security and reliability.\nThe future of IPsec tunnels for cloud server connections will be characterized by integration with SD-WAN, adoption of Zero Trust principles, advancements in encryption, automation, and orchestration, support for multi-cloud environments, and improved monitoring and analytics capabilities, all aimed at enhancing security and reliability in an increasingly dynamic and interconnected digital landscape.\nHow does Utho Cloud facilitate IPsec tunnel implementation to enhance security for businesses? Utho Cloud offers a straightforward process for businesses to implement Internet Protocol Security tunnels, bolstering security. Here‚Äôs how:\nUser-Friendly Interface: Utho Cloud provides a user-friendly interface that guides businesses through the setup of IPsec tunnels. This interface simplifies the configuration process, making it accessible even for users without extensive networking expertise.\nFlexible Deployment Options: Utho Cloud offers flexible deployment options for IPsec tunnels, allowing businesses to establish secure connections between their on-premises infrastructure and Utho Cloud services, such as virtual machines, databases, and storage.\nRobust Encryption Standards: Utho Cloud ensures robust encryption standards for IPsec tunnels, leveraging industry-leading algorithms to encrypt data in transit. This encryption mitigates the risk of unauthorized access and data breaches, safeguarding sensitive business information.\nScalability and Reliability: Utho Cloud‚Äôs infrastructure is designed for scalability and reliability, ensuring that businesses can deploy IPsec tunnels to support growing workloads and maintain consistent connectivity with minimal downtime.\nComprehensive Security Features: In addition to IPsec tunnels, Utho Cloud offers a range of complementary security features, including network security groups, web application firewalls, and identity and access management controls. These features work together to provide comprehensive protection against cybersecurity threats.\nBy leveraging Utho Cloud‚Äôs IPsec tunnel implementation, businesses can enhance their security posture, protect their data, and meet compliance requirements with ease.\nIPsec tunnels are like our trustworthy guardians, always keeping our data safe in today‚Äôs changing digital world. As we keep moving forward and trying new things, we can feel confident knowing that IPsec tunnels will always be there, making sure our data stays secure no matter where it goes.","what-are-the-primary-benefits-of-utilizing-ipsec-tunnels-for-cloud-server-connections-compared-to-other-security-protocols#**What are the primary benefits of utilizing IPsec tunnels for cloud server connections compared to other security protocols?":"","what-future-trends-in-ipsec-tunnels-for-cloud-servers-could-boost-security-and-reliability#\u003cstrong\u003eWhat future trends in IPsec tunnels for cloud servers could boost security and reliability?\u003c/strong\u003e":"","what-is-ipsec-and-how-does-it-play-a-crucial-role-in-securing-cloud-server-connections#\u003cstrong\u003eWhat is IPsec, and how does it play a crucial role in securing cloud server connections?\u003c/strong\u003e":"","what-types-of-industries-commonly-utilize-ipsec-tunnels-for-securing-their-network-communications#\u003cstrong\u003eWhat types of industries commonly utilize IPsec tunnels for securing their network communications?\u003c/strong\u003e":""},"title":"Beyond Boundaries: Ensuring Safety with IPsec Tunnels"},"/utho-docs/docs/blog/can-artificial-intelligence-replace-teachers-the-future-of-education-with-ai/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\n**Introduction** The rapidly changing landscape of education is being shaped by artificial intelligence (AI), a force with the potential to revolutionize traditional teaching methodologies. AI-driven tools and platforms have already demonstrated their ability to automate tasks, tailor learning experiences, and enhance educational outcomes. However, the question that looms is whether AI can ever fully supplant the role of teachers.\nThe Rise of AI in Education The adoption of AI in education is experiencing exponential growth. Valued at $1.2 billion in 2021, the global AI in education market is projected to reach an astonishing $12.8 billion by 2028. The rationale behind this surge is manifold. AI possesses the capability to streamline laborious and repetitive tasks for teachers, such as grading assignments and crafting lesson plans. Moreover, AI‚Äôs capacity to customize learning for individual students based on their distinct needs and preferences further bolsters its appeal.\nThe Benefits of AI in Education An array of potential advantages accompanies the integration of AI into education. Some key benefits include:\n1. Automated Grading: AI can automate the grading of papers, quizzes, and exams. This efficiency translates to more time for teachers to focus on vital tasks, such as delivering personalized instruction.\n2. Instant Feedback: AI provides instantaneous feedback to students on their assignments, fostering more effective learning and pinpointing areas requiring additional attention.\n3. Adaptive Learning: Personalizing learning to suit each student‚Äôs pace and preferences, AI helps facilitate more efficient comprehension and mastery of subjects.\n4. Cost Savings: By automating administrative chores like class scheduling and student registration, AI contributes to reducing the overall cost of education.\n5. Enhanced Accessibility: AI‚Äôs real-time translation capabilities can augment the accessibility of education for students with disabilities, ensuring inclusivity.\nThe Limitations of AI in Education While the benefits are clear, limitations persist within AI-infused education:\n1. Emotional Intelligence Deficit: AI lacks the emotional intelligence inherent to human teachers, impeding its capacity to offer comparable levels of support and motivation.\n2. Bias Concerns: AI can perpetuate biases rooted in its training data, potentially exacerbating inequalities and stereotypes within the educational landscape.\n3. Privacy Considerations: Privacy concerns arise regarding the use of AI to track students‚Äô online activities and gather personal information.\nThe Role of Teachers in the AI Era Despite these limitations, human teachers retain an indispensable role in the AI-driven educational landscape. Their responsibilities encompass:\n1. AI Oversight: Teachers must possess proficiency in utilizing AI effectively and ethically, ensuring responsible use within the classroom.\n2. Human Touch: Human teachers provide irreplaceable emotional support, motivation, and guidance that AI cannot replicate.\n3. Collaboration with AI: A symbiotic relationship between teachers and AI will foster a more personalized and impactful learning environment.\nFinding the Balance: AI and Teachers as Partners The foreseeable future of education is one of synergy between AI and human teachers. While AI can streamline tasks, individualize learning experiences, and elevate educational outcomes, human teachers remain pivotal for emotional connections and effective collaboration with AI.\nConclusion AI‚Äôs transformative potential in education is undeniable, yet its implementation is still in its nascent stages. By striking the right equilibrium between AI and human educators, we can forge a path toward a personalized, efficient, and universally accessible learning journey for all students.\nRead Also: Ethics and Regulation of AI: The Next Frontier for Artificial Intelligence","finding-the-balance-ai-and-teachers-as-partners#\u003cstrong\u003eFinding the Balance: AI and Teachers as Partners\u003c/strong\u003e":"","the-benefits-of-ai-in-education#\u003cstrong\u003eThe Benefits of AI in Education\u003c/strong\u003e":"","the-limitations-of-ai-in-education#\u003cstrong\u003eThe Limitations of AI in Education\u003c/strong\u003e":"","the-rise-of-ai-in-education#\u003cstrong\u003eThe Rise of AI in Education\u003c/strong\u003e":"","the-role-of-teachers-in-the-ai-era#\u003cstrong\u003eThe Role of Teachers in the AI Era\u003c/strong\u003e":""},"title":"Can Artificial Intelligence Replace Teachers? The Future of Education with AI"},"/utho-docs/docs/blog/challenges-of-cloud-server-compliance/":{"data":{"":"","the-challenges-of-cloud-server-compliance-navigating-regulatory-requirements-and-standards#\u003cstrong\u003eThe Challenges of Cloud Server Compliance: Navigating Regulatory Requirements and Standards\u003c/strong\u003e":"\nThe Challenges of Cloud Server Compliance: Navigating Regulatory Requirements and Standards As more businesses move their data to the cloud, compliance with regulatory requirements and standards becomes a significant challenge. Cloud service providers must meet various compliance requirements to ensure the security, privacy, and integrity of the data they store. In this article, we‚Äôll explore the challenges of cloud server compliance and how businesses can navigate them.\nThe Need for Cloud Server Compliance Compliance requirements for cloud servers are essential to protect data privacy and security. Many industries, such as healthcare and finance, have strict regulations that require specific data handling procedures. For example, the Health Insurance Portability and Accountability Act (HIPAA) requires healthcare providers to keep patient data secure and confidential. Failure to comply with these regulations can result in hefty fines and legal penalties.\nCloud Server Compliance Challenges One of the significant challenges of cloud server compliance is navigating the various regulatory requirements and standards. Different countries and industries have different regulations and standards, making it challenging to comply with all of them. Additionally, the regulatory landscape is constantly changing, and businesses must keep up with the latest requirements to avoid compliance breaches.\nAnother challenge is the complexity of compliance requirements. Many compliance regulations are highly technical and require specific configurations and controls. Cloud service providers must implement these controls while ensuring that they don‚Äôt impact performance or usability.\nNavigating Cloud Server Compliance To navigate cloud server compliance, businesses should start by conducting a compliance audit. The audit will identify any compliance gaps and help businesses prioritize their compliance efforts. Businesses should also work with a cloud service provider that has experience in complying with relevant regulations and standards.\nCloud service providers can help businesses implement compliance controls and configurations. They can also provide regular compliance reports to show that they‚Äôre meeting regulatory requirements. Additionally, cloud service providers can help businesses stay up to date with the latest regulatory changes and ensure that their systems remain compliant.\nConclusion Compliance with regulatory requirements and standards is a significant challenge for cloud server providers. However, businesses can navigate these challenges by conducting compliance audits, working with experienced cloud service providers, and staying up to date with the latest regulatory changes. With the right approach, businesses can ensure the security, privacy, and integrity of their data in the cloud.\nMicrohost is a reliable cloud service provider that can help businesses navigate cloud server compliance challenges. With experience in complying with various regulations and standards, Microhost can provide compliant cloud solutions for businesses of all sizes. To learn more about Microhost‚Äôs cloud solutions, visit our website at https://utho.com/."},"title":"Challenges of Cloud Server Compliance"},"/utho-docs/docs/blog/cloud-or-in-house-server-which-is-best-for-your-business/":{"data":{"":"","what-do-in-house-servers-and-cloud-computing-refer-to#\u003cstrong\u003eWhat do In-House Servers and Cloud Computing refer to\u003c/strong\u003e":"","what-is-the-ideal-server-solution-for-your-small-to-medium-sized-business#\u003cstrong\u003eWhat is the ideal server solution for your small to medium-sized business?\u003c/strong\u003e":"\nThe topic of hosting infrastructure holds significant importance in the digital era due to the essential role of data and online services in enterprises.\nBusinesses often face challenges when making the decision between using in-house servers or opting for cloud hosting. They aim to provide flawless client experiences while also ensuring data security and scalability.\nThe traditional approach to hosting has involved companies investing in hardware, software, and dedicated IT teams for their in-house servers. However, a more flexible and scalable solution is now available through cloud hosting. This method delivers computing resources as a service over the internet, eliminating the need for physical infrastructure on-site.\nCloud or In-House Server options have their respective strengths and weaknesses, making the decision a multi-faceted one.\nWhat do In-House Servers and Cloud Computing refer to An in-house server, also known as an on-premise server, is a physical infrastructure owned and operated by a business within its own premises or data center. This dedicated computing system is utilized for the storage, processing, and management of data and applications for internal use. In-house servers are typically equipped with specialized hardware, operating systems, and software applications tailored to meet the specific computing requirements of the organization.\nCloud hosting, on the other hand, is a technology framework that utilizes external servers, typically managed by third-party suppliers, to store, handle, and process data and applications. It operates on a cloud computing infrastructure that employs virtualization strategies to generate numerous virtual servers on shared physical hardware, rendering versatile and expandable resources easily accessible via the internet.\nWhat is the primary distinction between Cloud and Physical Servers? When considering whether to use a cloud server or an in-house physical server model, it is essential to understand the key distinctions between these options. This understanding is crucial as it allows for a thorough evaluation of the value each choice can bring.\nData Recoverability: Cloud providers employ data redundancy measures to safeguard customer data from potential losses caused by hardware malfunctions, natural calamities, or unforeseen circumstances.¬†Flexibility: If your business is anticipating rapid growth or planning to relocate data in the near future, it may be beneficial to consider a cloud-based environment. With all of your important files securely stored in the cloud, they can easily be migrated or transferred as needed.\nScalability: Cloud environments provide a more flexible and scalable option for growth compared to physical servers that rely on routers and switches for customization. With just a few clicks, users can easily add resources in a cloud environment, whereas physical servers require manual hardware modifications when upgrading server capabilities.\nWhat is the ideal server solution for your small to medium-sized business? When considering options for server solutions in your business, it is crucial to select the most suitable option that meets both present and future needs.¬†Weighing between hybrid servers, Cloud or In-House Server requires careful consideration to ensure an optimal fit for your business.\nUtho, India‚Äôs leading public cloud provider, began as a web hosting company in 2010 and launched India‚Äôs first cloud platform in 2018. Through simple, secure, and reliable cloud solutions, businesses can break free of vendor lock-in, slow speeds, high costs, and complexity with Utho. With more than a decade of experience, we prioritize simplicity, security, and customer service.\nSince we understand the pain of businesses and customers who use in-house servers, we have assisted over 3,000+ SMBs in migrating to the cloud.\nMoving your data to the cloud from in-house servers?¬†Get a free consultation with our cloud expert today.\nRead Also: 7 Reasons Why Cloud Infrastructure is Important for Startups","what-is-the-primary-distinction-between-cloud-and-physical-servers#\u003cstrong\u003eWhat is the primary distinction between Cloud and Physical Servers?\u003c/strong\u003e":""},"title":"Cloud or In-House Server: Which is best for your business?"},"/utho-docs/docs/blog/collaborating-devops-and-sre-for-efficient-cloud-migration/":{"data":{"":"","how-does-utho-do-devops-and-sre-internally#\u003cstrong\u003eHow does Utho do DevOps and SRE internally?\u003c/strong\u003e":"\nCloud migration is the procedure of transferring current applications and data from on-premise data centers to cloud infrastructure. Cloud migration has grown as more and more companies realize their digital transformation goals. This method provides a broad range of customized services to meet the unique needs of different applications, such as PaaS, SaaS, and IaaS.\nWhat is the correlation between DevOps and SRE teams Site Reliability Engineering (SRE) collaborates closely with Development and Operations (DevOps) by offering a comprehensive framework of procedures, guidelines, and automated protocols for DevOps teams to efficiently oversee updates and promptly address system complications in complex, geographically dispersed systems.\nDevOps seeks to achieve a delicate equilibrium between speed and stability, while SRE emphasizes reliability. Both approaches place importance on metrics, but DevOps takes into account factors such as deployment frequency, cycle time, and change fail rates. At the same time, SRE focuses on managing service level agreements (SLAs), service level objectives (SLOs), and service level indicators (SLIs) by utilizing error budgets. While there are distinct differences between DevOps and SRE, they should not be viewed as competing solutions. Instead, they can do specific work for the overall success of the company.\nWhat are the benefits of combining SRE and DevOps? DevOps and SRE must work together to accelerate software delivery, break organizational boundaries, and simplify operations. In today‚Äôs ever-evolving landscape, where user demands continue to escalate and competition relentlessly progresses, system crashes are unacceptable. Therefore, in order to maximize productivity and deliver great service, firms need to combine their DevOps and SRE teams.\nWhat are the challenges addressed by DevOps teams and SREs Implementing DevOps comes with its own set of challenges and risks. These include cultural resistance to change, insufficient communication and collaboration between teams, lack of standardization and automation, potential security and compliance risks, and difficulties in effectively measuring success.\nAn SRE (Site Reliability Engineer) plays a crucial role in contributing to a business by utilizing automation to eliminate and alter unnecessary tasks and positions. This helps to minimize overall expenses through resource optimization and enhancing the mean time to repair (MTTR).\nHow does Utho do DevOps and SRE internally? Utho strongly emphasizes automation and leveraging its own services in its approach to DevOps and SRE. Using these methods, they try to achieve optimal reliability and scalability.‚Äã\nUtho maintains a specialized SRE team tasked with guaranteeing the dependability and accessibility of their services. The primary goal of this team is to proactively identify and resolve potential issues while implementing robust protocols and technologies to improve the overall reliability of the service.\nUtho DevOps includes a comprehensive set of tools and methods designed to facilitate the efficient and automated development, deployment, and management of Teams software applications using the Utho cloud platform.‚Äã\nRead Also: Cloud or In-House Server: Which is best for your business?","what-are-the-benefits-of-combining-sre-and-devops#\u003cstrong\u003eWhat are the benefits of combining SRE and DevOps?\u003c/strong\u003e":"","what-are-the-challenges-addressed-by-devops-teams-and-sres#\u003cstrong\u003eWhat are the challenges addressed by DevOps teams and SREs\u003c/strong\u003e":"","what-is-the-correlation-between-devops-and-sre-teams#\u003cstrong\u003eWhat is the correlation between DevOps and SRE teams\u003c/strong\u003e":""},"title":"Collaborating DevOps and SRE for Efficient Cloud Migration"},"/utho-docs/docs/blog/compliance-in-the-cloud-understanding-your-responsibilities/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIn today‚Äôs fast-paced digital world, businesses of all sizes are turning to the cloud to streamline their operations and harness the power of scalability. But with great convenience comes great responsibility, especially when it comes to compliance and safeguarding sensitive data. Understanding your obligations regarding compliance in the cloud is vital to ensure the utmost security and maintain your customers‚Äô trust. In this article, we‚Äôll take a closer look at why compliance matters in the cloud and provide practical insights into fulfilling your compliance responsibilities.\nWhy Compliance in the Cloud is Crucial Cloud computing brings a multitude of benefits, including cost savings, flexibility, and improved accessibility. However, it also introduces unique challenges in terms of data security and compliance. To operate within specific industries or regions, businesses must adhere to regulations like HIPAA, GDPR, or PCI DSS, depending on the nature of their operations. Non-compliance can lead to severe consequences such as hefty fines, reputational damage, and even legal actions.\nUnderstanding Your Compliance Responsibilities To navigate the complex landscape of compliance in the cloud, organizations must be well-informed about their responsibilities. Here are some essential factors to consider:\n1. Stay Up-to-Date with Regulations Start by understanding the regulations that apply to your industry and the type of data you handle. Stay informed about any changes or updates to these regulations to ensure ongoing compliance.\n2. Implement Effective Data Classification and Governance Establish a robust framework for classifying and governing your data. Categorize data based on its sensitivity and define appropriate controls for storage, transmission, and retention.\n3. Choose a Compliant Cloud Provider When selecting a cloud provider, make sure they have the necessary certifications and compliance measures in place. Look for industry-recognized certifications such as ISO 27001 and SOC 2 to ensure the provider‚Äôs commitment to security and data protection.\n4. Embrace Data Encryption Utilize encryption techniques to safeguard your data both in transit and at rest. Encryption adds an extra layer of protection, making it significantly harder for unauthorized individuals to access and decipher sensitive information.\n5. Establish Strong Access Controls and User Management Implement robust access controls to ensure that only authorized personnel can access your cloud resources. Enforce strong passwords, multi-factor authentication, and regularly review user access privileges to minimize the risk of unauthorized access.\n6. Regular Monitoring and Auditing Maintain ongoing monitoring and auditing of your cloud environment to identify any potential vulnerabilities or security incidents. Implement comprehensive logging and auditing mechanisms to track user activities and promptly detect any unauthorized access attempts.\nConclusion Compliance in the cloud is not a choice; it‚Äôs a necessity for safeguarding your business and customer data. By understanding your responsibilities and implementing best practices, you can ensure the highest levels of security, integrity, and confidentiality for your cloud-based operations. Remember to keep yourself informed about relevant regulations, establish data classification and governance protocols, select a reputable cloud provider, employ encryption measures, enforce strict access controls, and maintain regular monitoring and auditing.\nIf you‚Äôre searching for a reliable and compliant cloud provider, look no further than MicroHost. Their commitment to security and comprehensive range of compliance measures make them an excellent choice for your cloud needs. Discover more about their secure cloud solutions by visiting microhost.com.\nRead Also: 6 Benefits of Deploying a Load Balancer on your server.","understanding-your-compliance-responsibilities#\u003cstrong\u003eUnderstanding Your Compliance Responsibilities\u003c/strong\u003e":"","why-compliance-in-the-cloud-is-crucial#\u003cstrong\u003eWhy Compliance in the Cloud is Crucial\u003c/strong\u003e":""},"title":"Compliance in the Cloud: Understanding Your Responsibilities"},"/utho-docs/docs/blog/customer-centric-cloud-how-human-support-enhances-user-experience/":{"data":{"":"","#":"\nIn the rapidly evolving landscape of cloud computing, businesses are increasingly relying on the cloud to streamline operations, improve efficiency, and drive innovation.¬†As organizations migrate their applications, data, and processes to the cloud, the importance of a customer-centric approach cannot be overstated. In this blog post, we‚Äôll explore the significance of prioritizing the user experience in the cloud and how incorporating human support can elevate customer satisfaction to new heights.\nThe Shift to Customer-Centricity in the Cloud Traditionally, technology solutions were evaluated primarily based on their technical capabilities and cost-effectiveness. However, as the cloud becomes an integral part of business operations, the focus has shifted towards the user experience.¬†Customer-centricity in the cloud involves putting the end-user at the center of decision-making processes, ensuring that their needs and expectations are met seamlessly.\nThe Role of Human Support in the Cloud While advancements in artificial intelligence and automation have undoubtedly enhanced the efficiency of cloud services, the human touch remains irreplaceable when it comes to customer support.¬†Human support plays a crucial role in addressing the unique challenges and concerns of users, providing a personalized and empathetic approach that technology alone cannot replicate.\n1. Personalized Assistance Human support enables users to interact with knowledgeable professionals who can understand their specific requirements and provide tailored solutions. Whether it‚Äôs troubleshooting technical issues, offering guidance on best practices, or addressing unique use cases, human support adds a level of customization that enhances the overall user experience.\n2. Empathetic Communication The cloud may be a virtual environment, but the challenges users face are very real. Human support brings empathy to the table, acknowledging the frustrations and concerns of users and working collaboratively to find resolutions. This empathetic communication fosters a positive relationship between the user and the service provider, building trust and loyalty over time.\n3. Complex Issue Resolution Not all challenges can be resolved through automated responses or self-service tools. Complex technical issues or intricate requirements often demand the expertise of a human support team. Having skilled professionals available ensures that users receive comprehensive solutions, preventing prolonged downtime and minimizing disruptions to their operations.\nThe Integration of Human Support and Technology The ideal customer-centric cloud strategy involves a harmonious integration of human support and advanced technologies. Artificial intelligence and automation can handle routine tasks, provide instant responses to common queries, and facilitate self-service options. Meanwhile, human support steps in for nuanced problem-solving, understanding user emotions and addressing issues that require a human touch.\nConsider a cloud provider that provides human support. We discussed how important it is for you to have access to human support when it comes to cloud computing. So, it is very important for you to choose a cloud provider that offers human support.\nWe at Utho - India‚Äôs first cloud provider - provide you with top-notch customer support. Even our slogan is ‚Äúwe care like a mother‚Äù and if you want a provider with human support who understands your queries like a human and is always there for you 24x7 then Utho is the provider for you.\nConclusion: Elevating the Cloud Experience with Human-Centricity As businesses continue to leverage the power of the cloud, the need for a customer-centric approach becomes paramount. By integrating human support into cloud services, organizations can create a seamless, personalized, and empathetic user experience. The customer-centric cloud is not just a technological evolution; it‚Äôs a cultural shift that prioritizes the human element in an increasingly digital world. As we navigate the future of cloud computing, let‚Äôs remember that technology is at its best when it serves and enhances the lives of its users.\nRead Also: SaaS, PaaS, and IaaS: A Comparison of Business Models"},"title":"Customer-Centric Cloud: How Human Support Enhances User Experience"},"/utho-docs/docs/blog/deploying-and-managing-a-cluster-on-utho-kubernetes-engine-uke/":{"data":{"":" Deploying and Managing a Cluster on Utho Kubernetes Engine (UKE)\nIn this tutorial we will learn how you can deploy and manage a Cluster on Utho Kubernetes Engine (UKE). The Utho Kubernetes Engine (UKE) is a fully-managed container orchestration engine for deploying and managing containerized applications and workloads. UKE combines Utho‚Äôs ease of use and¬†simple pricing¬†with the infrastructure efficiency of Kubernetes. When you deploy an UKE cluster, you receive a Kubernetes Master at no additional cost; you only pay for the Utho‚Äôs (worker nodes),¬†load balancers. Your UKE cluster‚Äôs Master node runs the Kubernetes control plane processes ‚Äì including the API, scheduler, and resource controllers.\nAdditional UKE features:\netcd Backups: A snapshot of your cluster‚Äôs metadata is backed up continuously, so your cluster is automatically restored in the event of a failure. ","before-you-begin--#Before you begin -":"Install kubectl - You need to install the kubectl client to your computer before proceeding. Follow the steps corresponding to your computer‚Äôs operating system.\nmacOS install via Homebrew\nbrew install kubectl Linux Download the latest kubectl release: curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" 2. Make the downloaded file executable.\nchmod +x ./kubectl 3. Move the command into your PATH:\nsudo mv ./kubectl /usr/local/bin/kubectl Windows Visit the¬†Kubernetes documentation¬†for a link to the most recent Windows release.","connect-to-your-uke-cluster-with-kubectl#Connect to your UKE Cluster with kubectl":" After you‚Äôve created your UKE cluster using the Cloud Manager, you can begin interacting with and managing your cluster. You connect to it using the kubectl client on your computer. To configure kubectl, download your cluster‚Äôs¬†kubeconfig¬†file.\nAnytime after your cluster is created you can download its¬†kubeconfig. The kubeconfig is a YAML file that will allow you to use kubectl to communicate with your cluster. Here is an example kubeconfig file:\nChange the kubeconfig.yaml file‚Äôs permissions so that only the current user may access it to increase security: chmod go-r /Downloads/kubeconfig.yaml Launch a shell session at the terminal and add the location of your kubeconfig file to the $KUBECONFIG environment variable. The kubeconfig file can be found in the Downloads folder, as shown in the sample command, but you will need to modify this line to reflect the location of the Downloads folder on your own computer: export KUBECONFIG=~/Downloads/kubeconfig.yaml You may look at the nodes that make up your cluster using kubectl. kubectl get nodes output of the command\nYour cluster is now prepared, and you can start managing it with kubectl. For further details on kubectl usage, refer to the Kubernetes guide titled ‚ÄúOverview of kubectl.‚Äù\nUse the config get-contexts command for kubectl to acquire a list of the available cluster contexts:\nkubectl config get-contexts If the asterisk in the current column does not indicate that your context is already selected, you can switch to it with the config use-context command. Please supply the full name of the cluster, including the authorised user and the cluster itself: kubectl config use-context Utho-k8s-ctx Output:\nSwitched to context ‚ÄúUtho-k8s-ctx‚Äù.\nYou are now ready to use kubectl to talk to your cluster. By getting a list of Pods, you can test how well you can talk to the cluster. To see all pods running in all namespaces, use the get pods command with the -A flag: kubectl get pods -A all node of cluster","create-an-uke-cluster#Create an UKE Cluster":"Step 1: First, We need to login to your Utho Cloud Dashboard.\nStep 2: From the Utho cloud dashboard, click on Kubernete option and then you will get the option to deploy the Cluster as per the screenshot.\nStep 3: While clicking on deploy Cluster, will get the option to create the cluster in our desired location along with the node Configuration option as per the below screenshot.\nStep 4. After clicking on Deploy cluster, a new cluster will be created where we can see the mater and slave node details as per the screenshot.\nStep -5. After the successful creation, we need to download the kubeconfig file from the dashboard. Please go through the screenshot for more details.\nStep 6: After downloading the file on local system, You can manage the Kubernete Cluster through using Kubectl tool.","delete-a-cluster#Delete a Cluster":"Using the Utho Kubernetes Manager, you have the ability to remove a whole cluster. After they have been implemented, these adjustments are irreversible.\nStep 1: To access Kubernetes, use the link located in the sidebar. You will then be brought to the Kubernetes listing page, where each of your clusters will be shown in turn.\nDashboard of k8s\nStep 2: Choose Manage Options next to the cluster you want to remove\nManage section of Kubernetes\nStep 3: Here, click on Destroy option.\nDestroy the cluster\nYou will need a confirmation string to remove the Cluster. Enter the precise string, then confirm by clicking the Delete button.\nDelete the cluster\nAfter deletion, The Kubernetes listing page will load, and when it does, you won‚Äôt be able to find the cluster that you just destroyed.\nHopefully, now you have the understanding of how to deploy and manage a Cluster on Utho Kubernetes Engine (UKE)","in-this-guide--#In this guide -":"In this guide you will learn-\nHow to create a Kubernetes cluster using the Utho Kubernetes Engine.\nHow to modify a cluster\nHow to delete a cluster\nNext steps after deploying cluster","modify-a-clusters-node-pools#Modify a Cluster‚Äôs Node Pools":"You can use the Utho Cloud Manager to modify a cluster‚Äôs existing node pools by adding or removing nodes. You can also recycle your node pools to replace all of their nodes with new ones that are upgraded to the most recent patch of your cluster‚Äôs Kubernetes version, or remove entire node pools from your cluster.\nThe details page of your Cluster Step 1: Click the menu in the sidebar that says ‚ÄúKubernetes.‚Äù When you go to the Kubernetes listing page, all of your clusters are shown.\nDashboard of Mirohost Panel\nStep 2: Select the cluster‚Äôs manage button that you want to change. The information page for the Kubernetes cluster displays.\nManage section of K8s\nScale a Node Pool Step 1: Go to the cluster‚Äôs information page and click the ‚Äúadd a node pool‚Äù option to the right that shows the node pools if you want to add a new node pool to your cluster.\nScale a cluster\nStep 2: Choose the hardware resources that you want to add to your new Node Pool from the menus that appear in the new window that just appeared. To add or remove a node from a node pool one at a time, choose the plus (+) and minus (-) buttons that are located to the right of each plan. Select ‚ÄúAdd Pool‚Äù when you are pleased with the amount of nodes that are included inside a node pool before incorporating it into your setup. After you have deployed your cluster, you always have the option to alter your Node Pool if you later determine that you need a different quantity of hardware resources.\nConfiguration of nodes\nEdit or Remove Existing Node Pools Step 1: On the Node Pools portion of the page that displays information about your cluster, click the Scale Pool option that is shown in the top-right corner of each item.\nScale option of nodes\nStep 2: After clicking on the Scale Pool, you will see the below screen. Here, just decrease the Node Count to your desired number and then clink on update button.\nSimilarly, if you want to delete any Node Pool, you just need to put Node Count to 0 and then click on update\nAdd or delete the node\nCaution\nThe removal of¬†nodes is an inevitable consequence of reducing the size of a node pool. Any local storage that was previously present on deleted nodes will be removed, including ‚ÄúhostPath‚Äù and ‚ÄúemptyDir‚Äù volumes, as well as ‚Äúlocal‚Äù PersistentVolumes."},"title":"Deploying and Managing a Cluster on Utho Kubernetes Engine  (UKE)"},"/utho-docs/docs/blog/edge-computing-a-user-friendly-explanation/":{"data":{"":"","microhost-and-edge-computing#\u003cstrong\u003eMicrohost and Edge Computing\u003c/strong\u003e":"You‚Äôve probably heard the buzz about edge computing lately, but what is it, and how does it differ from traditional cloud computing? In this article, we‚Äôll explain edge computing in plain language, and give you some examples of how it‚Äôs used.\nWhat is Edge Computing? Edge computing is all about processing data as close to the source of that data as possible. Normally, data is sent to a central data center for processing and analysis, but with edge computing, that processing happens at or near the device or sensor that generated the data in the first place.\nWhy is Edge Computing Important? There are a few reasons why edge computing is becoming more popular these days. First, it can help reduce the delay between when data is generated and when it‚Äôs processed. This is really important for things like self-driving cars, where split-second decisions can make a big difference.\nSecond, edge computing can help save bandwidth, which is really helpful when you‚Äôre dealing with expensive or limited connections, like in remote locations or on mobile devices.\nFinally, edge computing can help keep sensitive data more secure, since it‚Äôs not all sent to a central location where it could be at risk of being hacked.\nExamples of Edge Computing Here are a few examples of how edge computing can be used in different industries:\nHealthcare: Real-time processing of patient data could help doctors and nurses make better decisions about patient care.\nTransportation: Data from sensors on self-driving cars could be processed at the edge to help avoid accidents.\nRetail: In-store sensors could be used to process data on inventory and store layout.\nMicrohost and Edge Computing If you‚Äôre interested in exploring edge computing for your business, Microhost can help. We‚Äôre experts in cloud computing, including edge computing, and we can help you take advantage of this exciting technology. To learn more, visit us at https://utho.com/.","what-is-edge-computing#\u003cstrong\u003eWhat is Edge Computing?\u003c/strong\u003e":"","why-is-edge-computing-important#\u003cstrong\u003eWhy is Edge Computing Important?\u003c/strong\u003e":""},"title":"Edge Computing: A User-Friendly Explanation"},"/utho-docs/docs/blog/ethics-and-regulation-of-ai-the-next-frontier-for-artificial-intelligence/":{"data":{"":"","addressing-algorithmic-bias-in-ai#\u003cstrong\u003eAddressing Algorithmic Bias in AI\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction Artificial intelligence (AI) is rapidly transforming our world, with applications in healthcare, finance, transportation, and many other industries. As AI becomes more sophisticated, it is essential to consider the ethical implications of its use. This article explores the ethical challenges of AI, the need for regulation, and the path forward for a responsible and beneficial AI future.\nEthical Dilemmas of AI AI systems are often designed to learn and act autonomously, which raises profound ethical questions. For example, how should AI systems be programmed to make decisions that involve life and death? What are the implications of AI systems that are biased against certain groups of people? These are just some of the ethical dilemmas that must be addressed as AI continues to develop.\nEnsuring Transparency and Explainability of AI One of the primary concerns with AI is the ‚Äúblack box‚Äù problem, where complex algorithms make decisions without offering clear explanations. This lack of transparency can undermine trust and make it difficult to hold AI systems accountable for their actions. Researchers are working to develop techniques that allow AI systems to provide understandable explanations for their decisions, which would help to address this challenge.\nAddressing Algorithmic Bias in AI Another ethical concern with AI is the potential for algorithmic bias. AI systems learn from data, and if that data is biased, the AI system may perpetuate those biases. This can have a significant impact on the way that AI systems make decisions, and it is important to take steps to mitigate algorithmic bias.\nData Privacy and Security Concerns with AI The extensive use of data in AI applications also raises concerns about data privacy and security. It is important to ensure that AI systems are not used to collect or share personal data without people‚Äôs consent. Additionally, AI systems must be designed to be secure, in order to protect against cyberattacks.\nThe Role of Government and Industry in Regulating AI Governments and industry leaders play a crucial role in shaping the ethical development and use of AI. Governments can establish regulations that promote responsible AI, while industry leaders can develop ethical guidelines for their own organizations. It is important for these stakeholders to work together to ensure that AI is used for good and not for harm.\nGlobal Collaboration for Harmonized AI Ethics and Regulation AI is a global technology, so it is important to develop harmonized standards for AI ethics and regulation. This will help to ensure that AI is used in a responsible and consistent manner across different countries and regions. The United Nations and the World Economic Forum are among the organizations that are working to promote global collaboration on AI ethics.\nPreparing for the Employment Impact of AI The development of AI is also likely to have a significant impact on employment. AI could automate many jobs, leading to job displacement for some workers. It is important to prepare for these changes by upskilling workers and providing them with support during periods of transition.\nConclusion The ethical challenges of AI are complex and multifaceted. However, it is essential to address these challenges in order to ensure that AI is used for good and not for harm. By taking steps to ensure transparency, mitigate bias, safeguard data privacy, and collaborate globally, we can build a responsible and beneficial AI future.\nAlso Read: 5 Most Effective Ways to Avoid Cloud Bill Shocks.","data-privacy-and-security-concerns-with-ai#\u003cstrong\u003eData Privacy and Security Concerns with AI\u003c/strong\u003e":"","ensuring-transparency-and-explainability-of-ai#\u003cstrong\u003eEnsuring Transparency and Explainability of AI\u003c/strong\u003e":"","ethical-dilemmas-of-ai#\u003cstrong\u003eEthical Dilemmas of AI\u003c/strong\u003e":"","global-collaboration-for-harmonized-ai-ethics-and-regulation#\u003cstrong\u003eGlobal Collaboration for Harmonized AI Ethics and Regulation\u003c/strong\u003e":"","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","preparing-for-the-employment-impact-of-ai#\u003cstrong\u003ePreparing for the Employment Impact of AI\u003c/strong\u003e":"","the-role-of-government-and-industry-in-regulating-ai#\u003cstrong\u003eThe Role of Government and Industry in Regulating AI\u003c/strong\u003e":""},"title":"Ethics and Regulation of AI: The Next Frontier for Artificial Intelligence"},"/utho-docs/docs/blog/green-cloud-computing-sustainability-in-cloud-usage/":{"data":{"":"","embrace-sustainability-with-utho-a-cloud-solution-for-a-greener-future#\u003cstrong\u003eEmbrace Sustainability with Utho: A Cloud Solution for a Greener Future\u003c/strong\u003e":"\nGreen cloud computing also referred to as green information technology, offers a promising solution to mitigate energy consumption. The incessant rise in energy demand is largely attributed to the rapid expansion of data centers housing thousands of servers and infrastructure components. This pursuit of power has taken a toll on our environment‚Äôs carbon footprint.¬†What are the objectives of Green Computing¬†The objectives of Green Computing are to reduce the utilization of harmful substances, enhance energy efficiency across the entire lifespan of products, and promote the recyclability or biodegradability of outdated goods and production waste. Green computing can be achieved through a multitude of methods are Product Longevity, Algorithm Efficiency, Resource Allocation, Virtualization Techniques, Power Management Strategies\nGreen cloud computing offers a multitude of compelling benefits for businesses Incorporating environmentally friendly practices is a responsible decision that can bring numerous advantages. It would be wise to consider implementing a green platform in your business. Here are the advantages of utilizing green technology for businesses.\n1. Cost Reduction: Through the optimization of energy consumption and resource utilization, green cloud solutions offer significant cost savings. This includes decreased energy expenditures and hardware needs, resulting in reduced operational expenses.\n2. Improved Reputation: Embracing sustainability not only benefits the environment but also boosts a company‚Äôs image. Customers, partners, and investors are increasingly attracted to environmentally responsible organizations.\n3. Scalability: Cloud solutions, particularly those incorporating eco-friendly elements, provide adaptable resources that can cater to a company‚Äôs evolving demands. This versatility enables growth without significant escalation of environmental repercussions.\nWhat are the current challenges and potential opportunities for future advancements? Although significant advancements have been made in the field of green cloud computing, there are still challenges that need to be addressed. It is important to note that certain regions continue to heavily rely on non-renewable energy sources for their data centers, and the energy efficiency of cloud services can also vary.\nNevertheless, there are promising prospects for the future. As the consciousness of environmental concerns continues to expand, enterprises and cloud service providers will most likely increase their investments in sustainable measures. Additionally, with advancements in technology and the emergence of innovative solutions like liquid cooling systems and edge computing, energy efficiency will be further improved.\nEmbrace Sustainability with Utho: A Cloud Solution for a Greener Future Utho is dedicated to delivering eco-friendly cloud solutions to our valued clients. Our data centers are powered by energy-efficient hardware and sustainable energy sources, and we adhere to industry-leading methods to minimize our energy usage.\nSelecting Utho as your cloud provider guarantees both environmental responsibility and dependable, protected cloud services.¬†Also Read: Benefits of Cloud Computing for Small Businesses","green-cloud-computing-offers-a-multitude-of-compelling-benefits-for-businesses#\u003cstrong\u003eGreen cloud computing offers a multitude of compelling benefits for businesses\u003c/strong\u003e":"","what-are-the-current-challenges-and-potential-opportunities-for-future-advancements#\u003cstrong\u003eWhat are the current challenges and potential opportunities for future advancements?\u003c/strong\u003e":"","what-are-the-objectives-of-green-computing#\u003cstrong\u003eWhat are the objectives of Green Computing\u003c/strong\u003e¬†":""},"title":"Green Cloud Computing ‚Äì Sustainability in Cloud Usage"},"/utho-docs/docs/blog/how-cloud-firewall-can-help-you-to-avoid-costly-data-breaches/":{"data":{"":"","1-it-provides-an-extra-layer-of-security#\u003cstrong\u003e1. It provides an extra layer of security.\u003c/strong\u003e":"","2-a-cloud-firewall-can-stop-malicious-traffic-before-it-reaches-your-network#\u003cstrong\u003e2. A cloud firewall can stop malicious traffic before it reaches your network.\u003c/strong\u003e":"","3-a-cloud-firewall-is-more-cost-effective-than-traditional-hardware-firewalls#3. A cloud firewall is more cost-effective than traditional hardware firewalls.":"Data breaches are becoming more and more common, as cybercriminals become more sophisticated in their methods. Small businesses are especially vulnerable to data breaches, as they often lack the resources to invest in robust security measures. However, there are some steps that small businesses can take to protect themselves, and one of the most effective is to use a cloud firewall. Here are three ways that a cloud firewall can help you to prevent data breaches.\nUsing a cloud firewall, you can prevent cyberattacks on your business.\n1. It provides an extra layer of security. Your first line of defense against data breaches should always be a strong firewall. But cybercriminals are becoming increasingly skilled at bypassing firewalls, so it‚Äôs important to have an additional layer of security in place. A cloud firewall acts as a second layer of defense, making it much more difficult for hackers to breach your network.\n2. A cloud firewall can stop malicious traffic before it reaches your network. Another benefit of using a firewall is that it can stop malicious traffic before it even reaches your network. This is because it is situated between your network and the internet, so it can act as a filter, blocking traffic that contains malware or other malicious content.¬†3. A cloud firewall is more cost-effective than traditional hardware firewalls. Finally, another advantage of using a firewall is that it‚Äôs more cost-effective than traditional hardware firewalls. This is because you don‚Äôt need to purchase any additional hardware or software, and you can scale up or down as needed, depending on your current security needs.¬†Cloud firewall is the best way to prevent data breaches. It‚Äôs important to have it in place so that you can protect your company‚Äôs data and ensure that your customers‚Äô information remains confidential. If you‚Äôre not already using, now is the time to get started."},"title":"How Cloud Firewall Can Help You to Avoid Costly Data Breaches."},"/utho-docs/docs/blog/how-ssl-certificates-keep-you-and-your-business-secure-from-cyber-attacks/":{"data":{"":"","how-does-it-work#\u003cstrong\u003eHow does it work?\u003c/strong\u003e¬†":"","how-to-choose-the-right-ssl-for-your-needs#\u003cstrong\u003eHow to choose the right SSL for your needs.\u003c/strong\u003e":"As cyber crimes have become more sophisticated over the past few years, businesses of all sizes must be vigilant about protecting their data. One of the simplest, most effective ways to protect your business from malicious actors is through an SSL certificate. SSL stands for Secure Socket Layer, a protocol used by websites to encrypt data sent between the website‚Äôs server and visitors‚Äô browsers. In this blog post, we will discuss what an SSL Certificate is, how it works, and why it is important for businesses.¬†How SSL Certificates Keep You and Your Business Secure from Cyber Attacks.\nWhat is an SSL Certificate?¬†An SSL Certificate is a digital document issued by an accredited certification authority (CA) that verifies the identity of a website or domain and binds that identity to a cryptographic key pair. When installed on a web server, an SSL certificate activates the padlock icon in your browser‚Äôs address bar and enables HTTPS encryption. This encryption helps keep any information sent between users‚Äô browsers and your web server secure. It also helps prevent malicious actors from accessing sensitive data like passwords or credit card numbers.\nHow does it work?¬†When a user visits your website, their browser sends out a request asking for information. The web server responds by sending back the requested information in plaintext format (that is, without any encryption). With an SSL Certificate installed on the web server, however, the server will respond with encrypted data instead of plaintext data. This encrypted data can only be decrypted by using the corresponding private key stored on your web server‚Äîwhich means that malicious actors won‚Äôt be able to access it.¬†Why it is important?¬†SSL Certificates are becoming increasingly important as cybercrime continues to grow in sophistication and scale. By encrypting the information sent between users‚Äô browsers and your web server with an SSL Certificate, you can help keep sensitive information safe from malicious actors who might try to intercept it during transmission or use it for nefarious purposes such as identity theft or fraud. Additionally, having an active certificate installed on your website may help boost its search engine rankings‚Äîa benefit that cannot be overlooked in today‚Äôs highly competitive online landscape!¬†How to choose the right SSL for your needs. When choosing an SSL certificate for your website, there are several factors to consider:\nValidation level: SSL certificates come in three validation levels: domain validation, organization validation, and extended validation. Domain validation is the most basic and only verifies that you own the domain name. Organization validation verifies your organization‚Äôs information and Extended validation verifies your organization and also authenticates its legal existence.\nBrand recognition: Some SSL certificate providers are more well-known and have more brand recognition than others. Consider whether the provider‚Äôs reputation is important to you and your customers.\nWarranty: Some SSL certificates come with a warranty that covers financial losses if the certificate is found to be invalid. Consider whether this is important to you.\nCompatibility: Make sure the SSL certificate is compatible with your web server and any other software or systems you use.\nPrice: Compare the prices of different SSL certificates and choose one that fits within your budget.\nTechnical support: Consider whether the provider offers good technical support in case you need help with installation or troubleshooting.\nIt‚Äôs important to prioritize the security of your customers‚Äô personal information. One way to do this is by implementing an up-to-date SSL certificate on your website. This ensures that any sensitive data, such as credit card information or personal details, is securely transmitted over the internet which will help to prevent cyber threats. Not only that, but having it can also improve your search engine rankings, which can lead to more traffic and leads for your business. So if you haven‚Äôt already, consider investing in an SSL certificate to protect your customers‚Äô data and give your online presence a boost.\nMust Read : Top 05 Cloud Security Threats in 2023 and Proven Strategies to Mitigate Them\nThank You üòä","what-is-an-ssl-certificate#\u003cstrong\u003eWhat is an SSL Certificate?\u003c/strong\u003e¬†":"","why-it-is-important#\u003cstrong\u003eWhy it is important?\u003c/strong\u003e¬†":""},"title":"How SSL Certificates Keep You and Your Business Secure from Cyber Attacks."},"/utho-docs/docs/blog/how-to-choose-a-best-cloud-hosting-provider/":{"data":{"":"","1-predictable-pricing#\u003cstrong\u003e1) Predictable Pricing\u003c/strong\u003e":"","2-cloud-hosting-provider-with-ease-of-use#\u003cstrong\u003e2) Cloud Hosting Provider with Ease of Use\u003c/strong\u003e":"","3-customer-service#\u003cstrong\u003e3) Customer Service\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion:\u003c/strong\u003e":"Introduction: The following blog will discuss three points you should consider when selecting a cloud hosting provider, A recent study by Gartner found that global public cloud revenue is forecast to reach $331.2 billion in 2022. That represents a compound annual growth rate (CAGR) of 12.6 percent from 2018. With the cloud hosting market expected to double over the next four years, now is the time to migrate your business to the cloud. But with so many providers, how do you know which one is right for you?\nThe following guide will help you select a cloud hosting provider that meets your business needs.\nQualities of a Good Cloud Server Provider.\n1) Predictable Pricing The first factor you must consider when choosing a cloud hosting provider is price. You‚Äôll want to find a provider that charges a monthly fee within your budget. Be sure to read the fine print, though, as some providers charge additional fees for data storage and bandwidth usage.\n2) Cloud Hosting Provider with Ease of Use Another important factor is the ease of use. You‚Äôll want to find a provider that offers an easy-to-use control panel so you can easily manage your server settings and web files. If you ignore this factor, you will have to hire a separate IT staff to keep your server running, which will be very costly. Keeping this factor in mind, we offer you Independent and Flexible cloud servers with which you can do whatever you want and an easy system with no complexity.\n3) Customer Service Customer service is vital when it comes to cloud hosting providers. After all, if something goes wrong with your server, you‚Äôll need someone on hand 24/7/365 to help you resolve the issue quickly and efficiently. Look for a provider that offers 24X7 HUMAN SUPPORT or live chat support so you can get answers to your questions in real time.\nConclusion: Migrating your business to the cloud is a big decision, but it can pay off handsomely in terms of increased productivity and efficiency. By following the tips above, you can be sure you‚Äôre selecting a cloud hosting provider right for your business needs‚Äîand your budget. If you‚Äôre looking for a reliable and affordable cloud hosting provider, MicroHost is the right choice. We offer a wide range of plans to suit your needs. Our customer service team is available 24/7 to help you with any questions or problems you may have. Plus, we offer a 7-day free trial so that you can try us out risk-free. So what are you waiting for? Get started today and see the difference MicroHost makes.","introduction#\u003cstrong\u003eIntroduction:\u003c/strong\u003e":""},"title":"How to Choose a Best Cloud Hosting Provider"},"/utho-docs/docs/blog/how-to-choose-the-right-kubernetes-solution-for-your-business/":{"data":{"":"","assess-community-support-and-ecosystem#\u003cstrong\u003eAssess Community Support and Ecosystem\u003c/strong\u003e":"","assess-your-business-requirements#\u003cstrong\u003eAssess Your Business Requirements\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction Kubernetes has become the de facto standard for container orchestration, offering scalability, flexibility, and automation for managing containerized applications. However, with a wide range of Kubernetes solutions available, choosing the right one for your business can be a daunting task. In this article, we will provide you with practical tips and considerations to help you select the best Kubernetes solution that aligns with your business needs.\nAssess Your Business Requirements Before diving into the world of Kubernetes solutions, it‚Äôs essential to assess your business requirements. Consider factors such as the size of your organization, the complexity of your applications, scalability needs, and resource constraints. Understanding your specific needs will help you determine the features and capabilities required from a Kubernetes solution.\nEvaluate Managed Kubernetes Services Managed Kubernetes services provide a hassle-free way to leverage the power of Kubernetes without the burden of infrastructure management. Evaluate different managed Kubernetes service providers, such as Amazon EKS, Google Kubernetes Engine, and Azure Kubernetes Service. Look for providers that offer seamless integration with your existing cloud infrastructure, robust security features, scalability options, and reliable support.\nConsider On-Premises or Hybrid Options If you have strict data governance requirements or specific regulatory constraints, an on-premises or hybrid Kubernetes solution might be more suitable. Evaluate Kubernetes distributions that can be deployed on your own infrastructure, allowing you to have complete control over your environment. Consider factors such as ease of installation, ongoing maintenance, and integration capabilities with your existing systems.\nAssess Community Support and Ecosystem Kubernetes has a vibrant and active community, which translates into a rich ecosystem of tools, plugins, and resources. Assess the community support surrounding different Kubernetes solutions. Look for solutions with an active developer community, regular updates, and a wide range of compatible tools and extensions. A robust ecosystem ensures that you can leverage the latest innovations and easily integrate with other technologies.\nScalability and Performance As your business grows, so does the demand on your infrastructure. Assess the scalability and performance capabilities of the Kubernetes solution you are considering. Look for features like automatic scaling, load balancing, and resource optimization. A Kubernetes solution that can efficiently handle increased workloads and ensure consistent performance will be crucial for the success of your applications.\nSecurity and Compliance Security is paramount when it comes to managing your applications in a Kubernetes environment. Evaluate the security features offered by different Kubernetes solutions. Look for features such as role-based access control (RBAC), network policies, image scanning, and encryption at rest and in transit. Additionally, ensure that the Kubernetes solution aligns with your compliance requirements, whether it be GDPR, HIPAA, or industry-specific regulations.\nTotal Cost of Ownership (TCO) Consider the total cost of ownership (TCO) associated with the Kubernetes solution. Evaluate not only the upfront costs but also ongoing maintenance, support, and potential hidden expenses. Compare the pricing models of different solutions, including factors such as infrastructure costs, licensing fees, and additional services. Understanding the TCO will help you make an informed decision and avoid any budgetary surprises.\nConclusion Choosing the right Kubernetes solution for your business is a critical decision that can significantly impact your application‚Äôs performance, scalability, and security. Assess your business requirements, evaluate managed services, consider on-premises or hybrid options, assess community support, scalability, and performance, and ensure security and compliance. Additionally, factor in the total cost of ownership. By following these guidelines, you will be able to select a Kubernetes solution that empowers your business to leverage the full potential of container orchestration.\nRead Also: 2 Methods for Re-Running Last Executed Commands in Linux","consider-on-premises-or-hybrid-options#\u003cstrong\u003eConsider On-Premises or Hybrid Options\u003c/strong\u003e":"","evaluate-managed-kubernetes-services#\u003cstrong\u003eEvaluate Managed Kubernetes Services\u003c/strong\u003e":"","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","scalability-and-performance#\u003cstrong\u003eScalability and Performance\u003c/strong\u003e":"","security-and-compliance#\u003cstrong\u003eSecurity and Compliance\u003c/strong\u003e":"","total-cost-of-ownership-tco#\u003cstrong\u003eTotal Cost of Ownership (TCO)\u003c/strong\u003e":""},"title":"How to Choose the Right Kubernetes Solution for Your Business"},"/utho-docs/docs/blog/how-to-choose-the-right-multi-cloud-management-tool-for-your-business/":{"data":{"":"","benefits-of-multi-cloud-management#\u003cstrong\u003eBenefits of Multi-Cloud Management\u003c/strong\u003e":"","choosing-the-right-multi-cloud-management-tool#\u003cstrong\u003eChoosing the Right Multi-Cloud Management Tool\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction Managing multiple cloud platforms can be a challenging task, especially if you are using different providers. Each cloud platform has its own set of tools and processes, making it difficult to manage them all efficiently. However, with the help of multi-cloud management tools, you can streamline the management of your cloud environments and ensure that your business runs smoothly.\nWhat is Multi-Cloud Management? Multi-cloud management refers to the process of managing multiple cloud platforms from a single dashboard. It involves monitoring and controlling cloud resources, optimizing performance, and ensuring the security and compliance of your cloud environments. Multi-cloud management tools provide a centralized platform for managing multiple cloud environments, making it easier for IT teams to manage and control their resources.\nBenefits of Multi-Cloud Management There are several benefits to using a multi-cloud management tool for your business:\n1. Centralized Management One of the biggest benefits of using a multi-cloud management tool is that it provides a centralized platform for managing all of your cloud environments. This makes it easier for IT teams to monitor and control cloud resources from a single dashboard.\n2. Cost Optimization Multi-cloud management tools can help you optimize your cloud costs by identifying areas where you can save money. They can also help you allocate resources more efficiently, ensuring that you are only paying for what you need.\n3. Improved Security Multi-cloud management tools can help you improve the security of your cloud environments by providing a centralized platform for managing security policies and configurations. They can also help you identify potential security threats and vulnerabilities, allowing you to take action to mitigate them.\n4. Increased Flexibility Using multiple cloud providers gives you the flexibility to choose the best services from each provider. A multi-cloud management tool can help you manage these services more efficiently and ensure that they work together seamlessly.\nChoosing the Right Multi-Cloud Management Tool When it comes to choosing the right multi-cloud management tool for your business, there are several factors to consider:\n1. Compatibility The first factor to consider is compatibility. Ensure that the tool you choose is compatible with all of the cloud platforms you are using. This will ensure that you can manage all of your cloud environments from a single platform.\n2. Ease of Use The tool you choose should be user-friendly and easy to use. This will ensure that your IT team can easily navigate and manage the platform without requiring extensive training.\n3. Scalability The tool you choose should be scalable, meaning that it can grow and adapt to your business needs. This will ensure that you can add new cloud platforms and resources as your business grows.\n4. Features and Functionality Make sure that the tool you choose has all of the features and functionality that your business needs. This includes monitoring and optimization tools, security features, and automation capabilities.\n5. Cost Finally, consider the cost of the tool. Look for a tool that provides good value for money and fits within your budget.\nConclusion Managing multiple cloud platforms can be a daunting task, but with the right multi-cloud management tool, you can streamline the process and ensure that your business runs smoothly. When choosing a multi-cloud management tool, consider factors such as compatibility, ease of use, scalability, features and functionality, and cost. By choosing the right tool, you can optimize your cloud resources, improve security, and increase flexibility in your cloud environments.\nIf you‚Äôre looking for a reliable and secure multi-cloud management solution for your business, Microhost offers a range of cloud management tools that can help you manage your cloud environments more efficiently. Visit their website to learn more: https://utho.com\nAlso Read: Best Practices for Managing and Securing Edge Computing Devices","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","what-is-multi-cloud-management#\u003cstrong\u003eWhat is Multi-Cloud Management?\u003c/strong\u003e":""},"title":"How to Choose the Right Multi-Cloud Management Tool for Your Business"},"/utho-docs/docs/blog/how-to-keep-your-business-documents-safe-from-online-threats/":{"data":{"":"As an IT manager or small business owner, you know that protecting your business documents is essential. But with the rise of online threats, how can you be sure that your documents are safe? In this blog post, we‚Äôll show you how to keep your business documents safe from online threats. We‚Äôll also share some tips on how to prevent attacks in the future. So read on to learn more about keeping your business safe!","use-a-password-manager-to-create-strong-unique-passwords-for-all-your-accounts#Use a password manager to create strong, unique passwords for all your accounts":"Cyber security is so important in the online landscape, and it‚Äôs necessary to secure your accounts with strong, unique passwords. In this day and age, having multiple online accounts requires you to remember lots of different passwords. Sure, you can keep track of each one on a piece of paper or in a word document, but wouldn‚Äôt it be great if you didn‚Äôt have to worry about that anymore? Password managers make it easy ‚Äì they store all your passwords safely, letting you just remember one master password. It‚Äôs also possible to set: up two-factor authentication so even if someone manages to guess or crack your password, they won‚Äôt be able to access your account unless they know something else as well. So if cyber security and convenience are important to you, then using a password manager might just be the perfect solution!\nEncrypt your business documents before storing them online Encrypting your business documents should be a top priority when storing files online. Encryption is your best line of defense against hackers trying to steal sensitive data. Encrypting business documents provides an extra layer of security that helps prevent unwanted access to confidential information. Not only will it ensure the files are secure, but it also provides peace of mind that your documents won‚Äôt fall into the wrong hands. Plus, by encrypting, you can create tighter access control so that only authorized personnel have the right to retrieve the data and view what it contains. Encrypting your documents is the smartest way to protect them in this digital age.\nBack up your data regularly in case of an attack or data loss Back up your data regularly and never let yourself be caught off guard in the event of an attack or data loss. It‚Äôs all too easy to overlook it, but proper backup plans are essential to protect the important information that runs our lives. Whether backing up on an external hard drive or by cloud storage, keeping critical information safe ensures that you‚Äôll have access to it again no matter what happens. Backup can eliminate the risk of losing years of work, personal photos, and private documents in an instant‚Äîit‚Äôs a no-brainer! So don‚Äôt wait any longer‚Äîback up your data today!\nKeep your software and security systems up to date to protect against new threats System updates can be annoying, but they‚Äôre also extremely important to keep your digital world safe. New viruses and other security threats pop up all the time, so you must update your software and security systems to protect against them. Without updates, your system is vulnerable and can be taken over in a matter of minutes. Don‚Äôt let this happen‚Äîmake sure to keep your systems updated regularly!\nEducate yourself and your employees on cybersecurity best practices Cyber security is essential these days and it‚Äôs pertinent to ensure that you and your employees are prepared and up-to-date. Knowing the best ways to protect yourself and your organization‚Äôs data systems can save you time, money, and headaches in the long run. Cybersecurity risk management entails educating yourself on the basics of cybersecurity protection. It involves designing policies that address technology risks, both external and internal. Take time to educate yourself and your staff on guidelines like using strong passwords, encrypting data when possible, avoiding clicking links from unknown sources, regularly patching hardware, and applying two-factor authentication wherever possible before logging into accounts - all of which can help prevent scams or hacks into a system. Cyber security is no joke; becoming informed is taking action to secure your business‚Äô future online.\nConclusion Cyber security is a critical part of protecting your business, but it can be overwhelming to try and keep up with the latest threats. Here are five basic tips to help you get started. Are you aware of these things? If not, don‚Äôt worry - we can help you out. Our team of experts is well-versed in all things cyber security and knows how to keep your data safe from harm. Contact us today for more information or to set up a consultation.\nRead Also: Benefits of using Cloud Servers compared to Physical Servers"},"title":"How to keep your Business Documents Safe from Online Threats"},"/utho-docs/docs/blog/impact-of-cloud-server-energy-consumption/":{"data":{"":"","the-impact-of-cloud-serverhttpsuthocomcloud-energy-consumption-on-the-environment-green-solutions-and-best-practices#\u003cstrong\u003eThe Impact of \u003ca href=\"https://utho.com/cloud\"\u003eCloud Server\u003c/a\u003e Energy Consumption on the Environment: Green Solutions and Best Practices\u003c/strong\u003e":"\nThe Impact of Cloud Server Energy Consumption on the Environment: Green Solutions and Best Practices The increasing popularity of cloud computing has led to a significant rise in energy consumption by data centers, which has become a growing concern for the environment. According to a study by the Lawrence Berkeley National Laboratory, the energy consumption of data centers in the US alone is estimated to have reached 70 billion kilowatt-hours in 2014, accounting for 1.8% of the country‚Äôs total electricity consumption.\nThe environmental impact of this energy consumption is significant, with data centers contributing to carbon emissions and global warming. However, there are ways to reduce the environmental impact of cloud computing, including implementing green solutions and best practices.\nGreen Solutions for Cloud Computing Green solutions for cloud computing focus on reducing the amount of energy consumed by data centers, as well as increasing the use of renewable energy sources. One of the most effective ways to reduce energy consumption is through virtualization, which allows multiple virtual servers to run on a single physical server, reducing the need for multiple physical servers.\nOther green solutions for cloud computing include: Energy-efficient hardware: Using energy-efficient hardware can significantly reduce the energy consumption of data centers. This can be achieved by using energy-efficient processors, solid-state drives, and power supplies.\nRenewable energy sources: Increasing the use of renewable energy sources, such as solar, wind, and hydroelectric power, can significantly reduce the carbon footprint of data centers.\nData center design: Optimizing the design of data centers can also reduce their energy consumption. This can include using hot and cold aisle containment, implementing free cooling, and using high-efficiency power distribution systems.\nBest Practices for Cloud Computing In addition to green solutions, there are also best practices that can be implemented to reduce the environmental impact of cloud computing. Some of these best practices include:\nServer consolidation: Consolidating servers can significantly reduce energy consumption by reducing the number of physical servers required to run applications.\nEnergy-efficient cooling: Using energy-efficient cooling systems can reduce the amount of energy required to cool data centers.\nPower management: Implementing power management systems can help to reduce energy consumption by ensuring that servers are only running when they are needed.\nBy implementing green solutions and best practices, cloud computing providers can significantly reduce their impact on the environment, while also reducing their energy costs.\nMicrohost - Your Green Cloud Computing Partner At Microhost, we are committed to providing environmentally friendly cloud computing solutions to our customers. We use energy-efficient hardware and renewable energy sources to power our data centers, and we implement best practices to reduce our energy consumption.\nBy choosing Microhost as your cloud computing partner, you can be sure that you are making a positive impact on the environment, while also enjoying reliable and secure cloud computing services. Visit our website to learn more about our green cloud computing solutions."},"title":"Impact of Cloud Server Energy Consumption"},"/utho-docs/docs/blog/impact-of-cloud-server-location-on-latency/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nThe Impact of Cloud Server Location on Latency and User Experience: Factors to Consider In today‚Äôs fast-paced digital world, users expect fast and reliable access to their favorite websites and applications. As such, the location of cloud servers plays a crucial role in determining the quality of the user experience. In this article, we‚Äôll explore the impact of cloud server location on latency and user experience and the factors to consider when choosing a server location.\nFactors Affecting Latency Several factors can affect latency, including the physical distance between the user and the server, the number of hops required to reach the server, network congestion, and the server‚Äôs processing speed. However, the physical distance between the user and the server is the most significant factor. The farther away the user is from the server, the longer it will take for data to travel back and forth, resulting in higher latency.\nImpact on User Experience Latency can have a significant impact on the user experience, particularly for applications that require real-time interactions, such as online gaming, video conferencing, and financial trading. Even minor delays can be frustrating and disruptive, leading to a poor user experience and lost revenue for businesses.\nChoosing a Server Location When selecting a cloud server location, several factors should be considered, including the location of the target audience, the proximity of other servers in the network, and the reliability of the network infrastructure. For businesses with a global audience, it may be necessary to have servers located in multiple regions to provide optimal performance for users worldwide.\nAdditionally, some cloud service providers offer content delivery networks (CDNs), which distribute content across multiple servers worldwide, reducing latency and improving user experience.\nConclusion In conclusion, the location of cloud servers can have a significant impact on latency and user experience. By strategically choosing a server location, businesses can provide their users with a fast and reliable experience, leading to increased customer satisfaction and revenue.\nAbout Microhost Microhost is a leading cloud service provider in India, offering a wide range of cloud hosting solutions, including VPS, dedicated servers, and cloud storage. With state-of-the-art data centers located in India, Microhost provides businesses with high-speed connectivity and low latency, ensuring fast and reliable access to their applications and websites. To learn more about Microhost‚Äôs cloud hosting solutions, visit https://utho.com/.","the-impact-of-cloud-server-location-on-latency-and-user-experience-factors-to-consider#\u003cstrong\u003eThe Impact of Cloud Server Location on Latency and User Experience: Factors to Consider\u003c/strong\u003e":""},"title":"Impact of Cloud Server Location on Latency"},"/utho-docs/docs/blog/instructions-for-migrating-to-a-utho-cloud-environment/":{"data":{"":"","1-deploy-a-new-utho-cloud-instance#1. Deploy a new Utho Cloud instance":"","2--install-required-software-on-your-cloud-server#2- Install required software on your Cloud server":"","3--identify-your-configuration-files-and-database-files#3- Identify your configuration files and database files":"","4--transfer-you-data-from-cloud-instance-to-your-utho-cloud#4- Transfer you data from Cloud instance to your Utho Cloud":"","5--test-your-new-environment#5- Test your new Environment":"","conclusion#Conclusion":"After completing the aforementioned procedures, your service ought to have been fully moved to Utho Cloud. It is advised that you give your shared hosting service a few more days before terminating it to make sure everything is working properly. Additionally, ensure sure you do not need any additional files to be added from the shared host.","decision-making-in-regards-to-a-migration-plan#Decision-Making in Regards to a Migration Plan":"","steps-to-follow#Steps to follow":"","use-uthos-manage-dns-service#Use Utho\u0026rsquo;s manage DNS service":" Instructions for Migrating to a Utho‚Äôs Cloud Environment\nThis article will guide you through the recommended instructions for migrating to a Utho Cloud Environment from another host. Depending on the type of software you‚Äôre employing, you‚Äôll need to do a variety of distinct tasks. Despite this, the outline of the high-level plan remains the same regardless of the type of service you offer.\nThe most typical method for moving websites from one hosting provider to another is to:\nEven while reinstalling your services might be a time-consuming process, any difficulties that arise when you are configuring your apps are often far simpler to debug than problems with low-level setup. When it comes to migration, this is the preferred technique.\nCreate an instance of your services on Utho Cloud, and then transfer over just the configuration and data that is relevant to those instances. As a consequence, this produces a Linux environment that has a 100% chance of starting up properly on the Utho Cloud platform.\nDecision-Making in Regards to a Migration Plan When transitioning from one hosting provider to another, there are two basic options:\n1. Recommended technique- Installing each service independently . Establish a Utho Cloud instance, deploy a Utho-supplied Linux image to the instance, and then copy only the configuration and data relevant to your services. This builds a Linux environment that has a one hundred percent chance of starting up normally on a Utho platform.\nReinstalling all of your services could take some time. Problems that arise when installing your programs, on the other hand, are typically easier to resolve than low-level configuration issues. When it comes to migration, this is the recommended method.\n2. Full Duplication (Not Suggested) After establishing a Utho Cloud Instance, you must clone your existing discs by transferring them from your primary host to the Utho. This action will result in the creation of an exact copy of your discs on the Utho platform. Due to the fact that low-level system configuration files can vary between hosting providers, it is not recommended that you employ this method.\nDue to these differences, your Utho was unable to boot normally. It is possible to modify these parameters sufficiently for your Utho to function normally. However, obtaining the exact values for these configurations can be difficult, as can debugging when the numbers have been entered incorrectly.\nSteps to follow 1. Deploy a new Utho Cloud instance When building a new Utho Cloud, there are two factors to think about: the first is in which data centre the instnace should be located, and the second is the hardware resource plan the instance should operate under.\nLocation of Data Center:\nWhen determining the bandwidth available between your location and the data centre, it is helpful to compare the speeds of download. These comparisons will indicate the latency between where you are located and the data centre; ideally, the latency should be as low as possible.\nConfiguration of cloud instance:\nChoose a plan that has a minimum amount of storage space that is sufficient for the data that is currently being stored on the current VM instance that you are using.\nFind out the Linux distribution the instance you currently have running on your current server is using, and install it on your new Utho cloud. If your existing deployment utilises an earlier version of a Linux distribution, you should deploy the most recent version of that distribution that is available for your new cloud. This will guarantee that you have access to the most recent security upgrades and applications.\nPlease refer to¬†guide of how to create Utho Cloud server¬†for further information on the deployment of your new Linux image.¬†2- Install required software on your Cloud server Install the exact same software stack onto your new Utho cloud server that is already running on your existing cloud instance. You may get a list of all installed packages by using the package manager that is associated with your cloud instance. For instance, if you are operating with Debian or Ubuntu, you may type in the following command:\n# apt list --installed And if you are operating with Redhat or CentOS machine:\n# yum list --installed You may refer to our Documents and Tutorials to learn how to set up your system‚Äôs software on your new cloud instance once you have determined which software you would want to move to your new Utho Cloud server and after you have decided which software you would like to migrate.\n3- Identify your configuration files and database files Which of the software configuration options have to be kept (e.g. web server, virtual host information, database connection settings, and which files contain these settings, etc.).\nThe location on the disc where your data is stored (e.g. as files in a directory, in a database process, etc.).\nIt is quite probable that you will want a database dump in order to retrieve your data if it is stored in a database. When you do this, a file will be created on the disc that wraps the data from your database and can be moved over the network just like any other file:\n4- Transfer you data from Cloud instance to your Utho Cloud You may use a network transfer programme such as rsync to move your data to the cloud instance you have created with Utho.¬†Execute the following command from inside the cloud VPS you are currently using. Replace instance-user with the Linux user that is logged into your Utho Cloud instance, and replace cloud-ip with the IP address that is assigned to your Utho Cloud instance.\n# rsync -avh /path/to/data_directory insatance-user@cloud-ip:/path/to/destination_directory Uploading files from the current host‚Äôs /path/to/data_directory to the new Utho Cloud instance‚Äôs /path/to/destination_direcotory will occur when the command shown above is executed.\nIf you uploaded a database dump file to your new Utho Cloud instance, you must also restore the dump file in order for your database programme to utilise the data regularly.\n5- Test your new Environment After installing your programme and recovering your data, you should test the installation to check that everything is working properly. Even if you have not yet updated the DNS records to point to your Utho Instance¬†deployment, you can still preview your services without DNS.\nUtilize this opportunity to perform load testing on the newly constructed service. ApacheBench is one of the most commonly used benchmarking tools for web services. After completing all of these load tests, if you discover that your hardware resource plan is insufficient for the workload, you should resize it. After that, testing should proceed.\nAfter completing the testing portion of the migration, proceed to the final step, which is updating your DNS records.\n6- Modifying DNS Records By associating your domain with the IP address of your newly formed instance, you may direct your website visitors to your Utho Cloud Instance. You have two possibilities when it comes to transferring DNS records:\nUse Utho‚Äôs speedy and dependable DNS hosting at no extra cost so long as your Utho account includes at least one active cloud instance.\nContinue to use the nameserver authority you currently possess, and update your DNS records with the IP address of the newly formed cloud instance. You must inquire with your current internet service provider about any costs involved with utilising their DNS services. If you use the nameservers given by your domain name registrar, they are often free to use.\nUse Utho‚Äôs manage DNS service By associating your domain with the IP address of your newly formed instance, you may direct your website visitors to your Utho Cloud Instance. You have two possibilities when it comes to transferring DNS records:\nUse Utho‚Äôs speedy and dependable DNS hosting at no extra cost so long as your Utho account includes at least one active cloud instance.\nContinue to use the nameserver authority you currently possess, and update your DNS records with the IP address of the newly formed cloud instance. You must inquire with your current internet service provider about any costs involved with utilising their DNS services. If you use the nameservers given by your domain name registrar, they are often free to use.\nFollow Utho‚Äôs instructions on how to add a domain zone in order to make DNS records for your domain. You should recreate the DNS entries that are posted on the website of your current nameserver authority, but you should change the IP addresses to your Utho Cloud instance¬†IPs wherever it makes sense to do so.\nFind the company from which you bought your domain name. This company is often called your domain name‚Äôs registrar. If you don‚Äôt know who the registrant of your domain name is, you can use a Whois Search tool to find out.\nEven though it is very probable that the same corporation serves as both your registrant and nameserver authority, this is not always the case. Registrars often provide free DNS services.\nSimply upgrade the authoritative nameservers to Utho‚Äôs nameservers by logging into the domain registrar‚Äôs control panel and making the required changes:\n* ns1.microhost.com\n* ns2.microhost.com\nWhen you have waited the amount of time you chose for your TTL, the domain will start to spread. If you didn‚Äôt shorten your TTL, it could take up to 48 hours for this to happen.\nWhen you‚Äôre ready, use a web browser to go to your domain. It should now show your Utho Cloud website instead of the one hosted by your old provider. If you can‚Äôt tell the difference between the two, use the DIG utility. It should show the IP address of the Utho Cloud instance you are using.\nWhen you‚Äôre ready, use a web browser to go to your domain. It should now show your Utho-hosted website instead of the one on your old host. If you can‚Äôt tell the difference between the two, use the DNS utility. It should show the IP address that your Utho¬†is using.\nYou need to set up the reverse DNS for your domain. If you run a mail server, this is the most important thing for you to know.\nUse Your Current Nameservers as an Alternative All of the DNS entries associated with the IP address of your former host must be changed to use the IP address of your new Utho Cloud instance¬†if you wish to continue utilising your current nameservers. To alter your DNS records, speak with the organisation in charge of running your nameserver."},"title":"Instructions for Migrating to a Utho Cloud Environment"},"/utho-docs/docs/blog/introduction-to-ai-and-machine-learning-in-the-cloud-what-are-they-and-how-do-they-work/":{"data":{"":"\nArtificial intelligence and machine learning (ML) are revolutionizing the way businesses operate, and cloud computing is making it easier than ever to access and utilize these powerful technologies. But what exactly are AI and ML, and how do they work in the cloud?","advantages-of-ai-and-machine-learning-in-the-cloud#Advantages of AI and Machine Learning in the Cloud":"There are several advantages to using AI and machine learning in the cloud:\nCost savings: Cloud-based AI and machine learning platforms eliminate the need for businesses to invest in expensive hardware or hire specialized talent, which can result in significant cost savings.\nScalability: Cloud-based platforms can easily scale up or down to meet the needs of businesses of all sizes.\nEase of use: Cloud-based platforms typically offer user-friendly tools and services that make it easy for businesses to build and deploy AI models.","challenges-of-ai-and-machine-learning-in-the-cloud#Challenges of AI and Machine Learning in the Cloud":"While there are many advantages to using AI and machine learning in the cloud, there are also some challenges to consider:\nSecurity: Cloud-based platforms can be vulnerable to cyber attacks, which can compromise sensitive data and models.\nData privacy: Cloud-based platforms typically require businesses to share data with third-party providers, which can raise concerns about data privacy and compliance.\nIntegration: Integrating AI and machine learning capabilities into existing applications can be challenging and require specialized expertise.","conclusion#Conclusion":"AI and machine learning are powerful technologies that are transforming the way businesses operate, and cloud computing is making it easier than ever to access and utilize these technologies. By understanding the basics of AI and machine learning and the advantages and challenges of using them in the cloud, businesses can make informed decisions about how to best leverage these technologies to drive innovation and growth.\nAt Microhost, we understand the importance of AI and machine learning in the cloud and the potential it has to revolutionize businesses. That‚Äôs why we offer a range of cloud hosting solutions, including managed cloud services and cloud-based AI platforms, to help businesses stay ahead of the curve. Contact us today to learn more about how we can help you leverage the power of AI and machine learning in the cloud for your business.\nAlso Read: Advantages and Challenges of Using AI and Machine Learning in the Cloud","how-does-ai-and-machine-learning-work-in-the-cloud#How Does AI and Machine Learning Work in the Cloud?":"Cloud computing has made it easier than ever to access and utilize Artificial Intelligence and machine learning technologies. With cloud-based AI and machine learning platforms, businesses can quickly and easily build and deploy sophisticated AI models without needing to invest in expensive hardware or hire specialized talent.\nCloud-based Artificial Intelligence and machine learning platforms typically provide a range of tools and services that make it easy for businesses to build and train models, such as pre-built algorithms, data preprocessing tools, and visualization tools. These platforms also typically offer APIs that businesses can use to integrate AI and machine learning capabilities into their existing applications.","what-is-ai#What Is AI?":"AI refers to a broad range of technologies that enable machines to perform tasks that would typically require human intelligence, such as recognizing speech, making decisions, and even driving cars. AI is typically broken down into two categories: narrow AI and general AI.\nNarrow AI refers to systems that are designed to perform a specific task, such as recognizing faces or playing chess. These systems are highly effective at their specific task, but they lack the flexibility to perform other tasks.\nGeneral AI, on the other hand, refers to systems that can perform any intellectual task that a human can. While we are still far from achieving true general AI, many researchers and companies are working towards this goal.","what-is-machine-learning#What Is Machine Learning?":"Machine learning is a subset of AI that refers to systems that can learn and improve without being explicitly programmed. Machine learning algorithms use statistical models to analyze data and identify patterns, which they can then use to make predictions or perform other tasks.\nThere are two main types of machine learning: supervised learning and unsupervised learning. In supervised learning, the machine learning algorithm is trained on a labeled dataset, meaning that the correct answer is provided for each example. In unsupervised learning, the algorithm is not provided with labeled data and must identify patterns on its own."},"title":"Introduction to AI and Machine Learning in the Cloud: What Are They and How Do They Work?"},"/utho-docs/docs/blog/introduction-to-big-data-analytics-in-the-cloud-what-are-the-benefits/":{"data":{"":"\nIn today‚Äôs data-driven world, organizations are faced with the challenge of managing and extracting insights from vast amounts of data. Big data analytics, combined with the power of cloud computing, has emerged as a game-changer in this regard. This article will provide an introduction to big data analytics in the cloud and explore the benefits it offers.","benefits-of-big-data-analytics-in-the-cloud#Benefits of Big Data Analytics in the Cloud":"1. Scalability and Flexibility Cloud-based big data analytics platforms provide on-demand scalability, allowing organizations to scale up or down their computing resources based on their needs. This flexibility ensures that you have the necessary resources to process and analyze data without worrying about infrastructure limitations.\n2. Cost Efficiency Traditional on-premises infrastructure for big data analytics can be expensive to set up and maintain. With cloud-based solutions, organizations can reduce upfront costs by paying only for the resources they consume. Additionally, the cloud‚Äôs pay-as-you-go model enables cost optimization by eliminating the need for overprovisioning.\n3. Enhanced Performance Cloud providers offer high-performance computing capabilities, allowing for faster data processing and analysis. With distributed computing and parallel processing, big data analytics tasks can be completed more efficiently, resulting in quicker insights and actionable outcomes.\n4. Advanced Analytics Capabilities Cloud-based big DA platforms often come equipped with a wide range of tools and technologies for advanced analytics. These include machine learning algorithms, natural language processing, and predictive analytics, enabling organizations to uncover valuable insights and make data-driven decisions.\n5. Data Security and Compliance Cloud providers prioritize data security and compliance, implementing robust measures to protect sensitive data. Encryption, access controls, and regular security audits are some of the security features offered by cloud platforms. Additionally, many cloud providers comply with industry-specific regulations, making it easier for organizations to meet compliance requirements.","conclusion#Conclusion":"Big DA in the cloud presents numerous benefits for organizations looking to extract meaningful insights from their data. The scalability, cost efficiency, enhanced performance, advanced analytics capabilities, and data security provided by cloud-based solutions make it an attractive option for businesses of all sizes.\nMicrohost offers a comprehensive suite of cloud-based big data analytics services and solutions to help organizations harness the power of their data. To learn more about how Microhost can assist you in unlocking the potential of big DA in the cloud, visit https://utho.com/.\nBy leveraging the benefits of big DA in the cloud, organizations can gain valuable insights, drive innovation, and stay ahead in today‚Äôs competitive landscape.\nRead Also: Benefits of using Cloud Servers compared to Physical Servers","what-is-big-data-analytics-in-the-cloud#What is Big Data Analytics in the Cloud?":"Big data analytics in the cloud refers to the practice of analyzing large and complex datasets using cloud-based platforms and tools. By leveraging the scalability and computing power of the cloud, organizations can process, store, and analyze massive volumes of data quickly and cost-effectively."},"title":"Introduction to Big Data Analytics in the Cloud: What are the Benefits?"},"/utho-docs/docs/blog/introduction-to-rsync/":{"data":{"":"Rsync is a command line tool that syncs files and folders between locations. Some workflows, which can be implemented using rsync like, update a production host on a developer or call rsync using a cron job to save the data backup to a storage location regularly. You can even move your server from other providers to Microhost using rsync.\nRsync is incremental, thus successive backup operations completed very quickly once the initial operation is finished. Only the differences are copied between source and target files. This rsync property is ideal for automated operations.","install-rsync-on-linux#Install rsync on linux":"Linux / Unix: Not all * nix systems are default to rsync, but they may be installed from the software repository of your distribution or compiled from source. Use below command to install rsync package.\n[root@Microhost ~]# yum install rsync -y Reasons for considering rsync over cp or scp\nCreates incremental data backups. rsync‚Äôs - -del option deletes files located at the destination which are no longer at the source. rsync can resume failed transfers (as long as they were started with rsync). rsync can compress data with the -z option, so no need to pipe to an archiving utility. Copies from source to destination only the data which is different between the two locations. ","working-with-rsync#Working With rsync":"A wide range of options are available to use with rsync, and many people have their favorite set of option while using this tool. Single rsync options can be aliases from several others as well, so rsync -a, for instance, gives the same result as rsync -rlptgoD.\n[ht_message mstyle=‚Äúsuccess‚Äù title=‚ÄúNote:‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=\"\" style=\"\" ] Rsync should be installed on both sources (locally and remote system),if you‚Äôre synchronizing files over a network.[/ht_message]\nSo rsync is a device for which you ought to be very careful when copying commands from forum posts and other websites on the internet without understanding exactly what they are doing. If you take the time to investigate and experiment before using your information, you get the most out of rsync.\nThe¬†two¬†commands¬†you¬†will¬†need¬†to¬†start¬†familiarizing¬†yourself¬†with¬†rsync¬†are:\n[root@Microhost ~]# man rsync [root@Microhost ~]# rsync -help A¬†rsync¬†command‚Äôs¬†basic¬†structure¬†is¬†identical¬†to¬†cp,¬†and¬†SCP.\n[root@Microhost ~]# rsync -[argument] source destination When you have several destinations, they are connected to the end of the string just as the cp command does:\n[root@Microhost ~]# rsync -zarvh source loation1 location2 location3 Either source or destination may be local or distant, or both. If you are synchronizing files over a network then rsync would need to be enabled on both local and remote machines. Rsync uses SSH to encrypt the data while transmitting across networks, and it operates with SSH keys to easily authenticate remote servers.\nRemote locations such as SSH or SCP commands are formatted. For instance, you would use: to synchronize a local folder with one on a remote server,\n[root@Microhost ~]# rsync -zarvh /path_of _source_folder username@:/path_of_destination_folder To synchronize a remote location folder on local machine:\n[root@Microhost ~]# rsync -zarvh username@:/path_of_destination_folder /path_of _source_folder Thank You :)"},"title":"Introduction to rsync"},"/utho-docs/docs/blog/microservices-vs-monolith-choose-right-choice-for-your-business/":{"data":{"":"","can-utho-cloud-facilitate-a-secure-migration-from-monolithic-to-microservices#\u003cstrong\u003eCan Utho Cloud facilitate a secure migration from monolithic to microservices?\u003c/strong\u003e":"Selecting the right architecture for your new application is crucial for its success. In this article, we‚Äôll compare two popular approaches: monolithic and microservices. As we explore the strengths and weaknesses of both. By the end, you‚Äôll know when to choose one over the other.\nWhether you‚Äôre an experienced architect or a curious developer, let‚Äôs embark on this comparison journey to find the perfect fit for your next project.\nWhat does Monolithic Architecture entail? A monolithic application, or ‚Äúmonolith,‚Äù is built from a single large codebase encompassing all components like backend, frontend, and configuration. While considered an older approach, some businesses still opt for monolithic architecture due to quicker development and deployment. However, it may face challenges in scalability and maintenance as the codebase grows.\nWhat does Microservices Architecture involve? Microservices architecture divides system components into independent chunks, allowing separate development, deployment, and scaling. Also called microservice architecture, it constructs applications as a collection of small, self-contained services focused on specific business features. Widely adopted in today‚Äôs industry, Microservices offer a versatile approach to application development.\nWhat are the main distinctions between Monolithic and Microservices Architecture? The key difference between Monolithic and Microservices Architecture lies in how applications are structured. Monolithic builds the entire application as one tightly connected unit, making it easy to develop and deploy initially. However, it can get complicated to maintain and scale as the app grows. Microservices, on the other hand, break down the app into small, independent modules that can be developed and scaled individually. This approach provides flexibility but demands specialized skills and careful coordination between modules. Choosing between them depends on your project‚Äôs specific needs and goals.\nWhat are the primary scenarios where a monolithic architecture is best suited? Let‚Äôs delve into instances where the monolithic approach is well-suited.\nSmall to Medium-sized Applications: For straightforward applications with limited features and smaller development teams, a monolithic architecture provides a simple and cost-effective solution. The unified codebase and shared data environment streamline development and maintenance processes.\n**\nResource-Constrained Environments:** In environments with constrained infrastructure resources or limited deployment options, a monolithic architecture can be beneficial. It demands fewer resources compared to a distributed microservices setup, making it well-suited for settings with hardware or cost constraints.\nSingle-Function Applications: Monolithic architecture is advantageous for applications with a single, well-defined function, minimal integrations, and limited scalability needs. Operating within a single process ensures efficient performance for focused use cases. Additionally, scaling is straightforward ‚Äì just add more replicas of the monolith behind a load balancer for a simple and effective solution.\nLegacy Systems: Modernizing and migrating legacy systems can be intricate. In certain instances, retaining the current monolithic architecture and progressively refactoring or introducing microservices where needed may be more practical. This approach enables a phased transition, reducing disruptions and capitalizing on the existing codebase.\nWhat are the primary scenarios where microservices architecture is most suitable? Microservices architecture presents various advantages that make it an appealing option for specific use cases. Let‚Äôs delve into instances where microservices excel:\nLarge and Complex Systems: In handling extensive applications with intricate functionality, microservices architecture excels. Decomposing the system into smaller, autonomous services enhances organization, maintainability, and scalability. Each service can concentrate on a distinct business capability, resulting in more manageable development and maintenance.\nHigh Scalability and Elasticity: Microservices offer significant advantages for applications facing variable or unpredictable workloads. The granular scalability enables each service to be independently scaled according to its specific demand. This ensures efficient resource utilization, allowing precise allocation where needed for optimal performance and cost-effectiveness**.\nContinuous Deployment and DevOps Culture:** Microservices seamlessly align with the principles of continuous integration, delivery, and deployment. Each service can undergo independent development, testing, and deployment, facilitating rapid iteration and reducing time-to-market. This approach fosters an agile and DevOps-oriented development culture, encouraging faster and more frequent releases.Domain-Driven Design: Microservices are advantageous for applications with intricate domain models and distinct bounded contexts. Aligning services with specific subdomains enables superior separation of concerns, encapsulation, and maintainability. This encourages a modular approach, where each microservice concentrates on a specific business capability and can evolve independently.\nWhat are the advantages and disadvantages of a monolithic architecture? By consolidating all components and functionalities as self-contained and deploying them as a single unit, a monolithic architecture provides specific advantages. These include:\nSimplicity: Developing and deploying monolithic architectures is straightforward due to their singular codebase and a unified deployment module. This simplifies overall application management and testing, making the initial setup more straightforward. Moreover, deployments are uncomplicated, usually requiring deployment to a single location.\nSpecialist knowledge: As an application expands, the development team usually specializes in one or two aspects. For instance, you may divide the front-end team from the back-end team, enabling technology specialists to apply in-depth technical expertise to their respective domains.\nCertainly, monolithic architectures come with drawbacks, such as:\nScalability: Scaling monolithic applications poses challenges since, as a single deployment, they require vertical scaling. When limited to vertical scaling as the sole option, this inflexibility can result in increased costs.\nFlexibility: Monolithic architectures may lack flexibility as modifications to one component might necessitate changes to the entire application. Moreover, team technology specialization can result in less adaptable teams.\nOperations: As applications expand, maintaining a monolithic architecture can become challenging because changes may impact numerous parts of the application. A single fault can trigger issues across the monolith, and identifying bottlenecks can be time-consuming and challenging.\nWhat are the advantages and disadvantages of a microservices architecture? In a microservice-based architecture, each component service of the application is developed and deployed independently. Some advantages of a microservice-based architecture include:\nScalability: Independently scaling microservices allows for more efficient and flexible resource utilization.\nFlexibility: Microservices exhibit greater flexibility, as services can be developed using different technology stacks, fostering increased agility.\nBusiness fit: Designing microservices to be purpose-fit for each business need enables cross-functional teams to collaborate more closely with each business unit. While a microservice-based architecture may seem appealing, it does come with certain drawbacks, such as:\n**\nComplexity:** Developing, deploying, and maintaining microservices applications can be more complex due to multiple codebases and deployment units. Testing such intricate applications is also challenging, demanding specialized testing environments with proper setup.\nPerformance: Microservices may bring about increased latency and additional network overhead as they necessitate communication with various services. Debugging within a microservices architecture can be demanding, given the complexity of tracing issues across multiple services**.**\nDiscoverability: Managing extensive fleets of microservices can pose challenges in identifying previously written code, potentially resulting in inadvertent duplication of efforts, also known as ‚Äúreinventing the wheel.\nCan Utho Cloud facilitate a secure migration from monolithic to microservices? Utho offers a solution designed to assist enterprises in securely transitioning to a microservices-based architecture. Functioning as a distributed edge and cloud computing platform, Utho ensures security, scalability, and performance optimization for microservices.\nThe Utho platform enables enterprises to construct and deploy microservices-based applications at the edge. Leveraging Utho‚Äôs expansive global network of servers, businesses can deploy microservices securely, experiencing low network latency and high availability across multiple regions.","what-are-the-advantages-and-disadvantages-of-a-microservices-architecture#\u003cstrong\u003eWhat are the advantages and disadvantages of a microservices architecture?\u003c/strong\u003e":"","what-are-the-advantages-and-disadvantages-of-a-monolithic-architecture#\u003cstrong\u003eWhat are the advantages and disadvantages of a monolithic architecture?\u003c/strong\u003e":"","what-are-the-main-distinctions-between-monolithic-and-microservices-architecture#\u003cstrong\u003eWhat are the main distinctions between Monolithic and Microservices Architecture?\u003c/strong\u003e":"","what-are-the-primary-scenarios-where-a-monolithic-architecture-is-best-suited#\u003cstrong\u003eWhat are the primary scenarios where a monolithic architecture is best suited?\u003c/strong\u003e":"","what-are-the-primary-scenarios-where-microservices-architecture-is-most-suitable#\u003cstrong\u003eWhat are the primary scenarios where microservices architecture is most suitable?\u003c/strong\u003e":"","what-does-microservices-architecture-involve#\u003cstrong\u003eWhat does Microservices Architecture involve?\u003c/strong\u003e":"","what-does-monolithic-architecture-entail#\u003cstrong\u003eWhat does Monolithic Architecture entail?\u003c/strong\u003e":""},"title":"Microservices vs. Monolith: Choose Right architecture for Business"},"/utho-docs/docs/blog/multi-cloud-strategy-everything-you-need-to-know/":{"data":{"":"","implementing-a-multi-cloud-strategy#\u003cstrong\u003eImplementing a Multi-Cloud Strategy\u003c/strong\u003e":"","microhost-and-multi-cloud-strategy#\u003cstrong\u003eMicrohost and Multi-Cloud Strategy\u003c/strong\u003e":"\nAs more and more businesses adopt cloud computing, many are realizing that one cloud provider may not be enough. That‚Äôs where a multi-cloud strategy comes in. In this article, we‚Äôll explain what a multi-cloud strategy is, why it‚Äôs important, and how to implement it.\nWhat is Multi-Cloud Strategy? A multi-cloud strategy is simply the practice of using more than one cloud provider for your computing needs. This could mean using one provider for storage, another for compute, and another for machine learning, for example. The goal is to take advantage of the unique strengths of each provider and avoid being locked into a single vendor.\nWhy is Multi-Cloud Strategy Important? There are a few reasons why a multi-cloud strategy might make sense for your business. First, it can help you avoid vendor lock-in, which can be a real concern if you‚Äôre heavily invested in one cloud provider. By using multiple providers, you can spread your risk and avoid being at the mercy of a single vendor‚Äôs pricing or service changes.\nSecond, a multi-cloud strategy can help you take advantage of the strengths of different cloud providers. Maybe one provider has better security features, while another has a more user-friendly interface. By using multiple providers, you can get the best of both worlds.\nFinally, a multi-cloud strategy can help you avoid downtime. If one provider experiences an outage, you can quickly switch to another provider to keep your services up and running.\nImplementing a Multi-Cloud Strategy Implementing a multi-cloud strategy can be challenging, but it doesn‚Äôt have to be overwhelming. Here are a few tips to get started:\nIdentify your business needs: Figure out which cloud providers and services will best meet your business needs.\nDevelop a governance plan: Make sure you have a plan in place for managing multiple cloud providers and keeping your data secure.\nAutomate where possible: Use automation tools to simplify the process of deploying and managing resources across multiple cloud providers.\nMonitor and optimize: Regularly monitor your multi-cloud environment and optimize as needed to ensure you‚Äôre getting the best possible performance and value.\nMicrohost and Multi-Cloud Strategy If you‚Äôre interested in exploring a multi-cloud strategy for your business, Microhost can help. We offer a range of cloud computing solutions, including multi-cloud management tools, to help you take advantage of the benefits of a multi-cloud strategy. To learn more, visit us at https://utho.com/.","what-is-multi-cloud-strategy#\u003cstrong\u003eWhat is Multi-Cloud Strategy?\u003c/strong\u003e":"","why-is-multi-cloud-strategy-important#\u003cstrong\u003eWhy is Multi-Cloud Strategy Important?\u003c/strong\u003e":""},"title":"Multi-Cloud Strategy: Everything You Need to Know"},"/utho-docs/docs/blog/pros-and-cons-of-using-serverless-computing-in-your-business/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction Serverless computing is a relatively new cloud computing model that allows developers to build and run applications without having to manage the underlying infrastructure. Instead of provisioning, scaling, and managing servers, developers can focus on writing code that responds to specific events or triggers. Serverless computing offers many benefits, but it also has some drawbacks that you should be aware of before deciding whether to adopt it for your business.\nPros of Serverless Computing Cost Savings One of the most significant advantages of serverless computing is cost savings. With traditional server-based computing, you typically pay for a fixed amount of resources, even if you don‚Äôt use them all. With serverless computing, you only pay for the resources you use, which can result in significant cost savings over time.\nIncreased Scalability Serverless computing also offers increased scalability. With traditional server-based computing, you need to manually provision and scale your servers to handle increases in traffic. With serverless computing, the infrastructure automatically scales based on the demand, so you don‚Äôt have to worry about overprovisioning or underprovisioning resources.\nFaster Development Serverless computing can also speed up the development process. Because developers don‚Äôt have to worry about infrastructure management, they can focus on writing code and delivering features faster. This can be particularly beneficial for businesses that need to move quickly to stay competitive.\nReduced Maintenance Since serverless computing providers manage the underlying infrastructure, businesses can reduce the maintenance burden on their IT teams. This can free up resources to focus on other critical tasks, such as security, testing, and optimization.\nCons of Serverless Computing Vendor Lock-In One of the significant drawbacks of serverless computing is vendor lock-in. When you adopt a serverless computing model, you become dependent on a specific provider‚Äôs platform, which can make it challenging to switch providers if you need to. This can be particularly problematic if the provider raises prices, changes its service offerings, or goes out of business.\nPerformance Issues Serverless computing may also have performance issues, especially if you have a large number of functions running simultaneously. Each function is executed in its own container, which can lead to latency issues if you have a high volume of requests.\nDebugging Challenges Since serverless computing is event-driven, debugging can be challenging, particularly if there are many functions involved. You may need to use specialized tools to trace and debug your application, which can be time-consuming and expensive.\nConclusion Serverless computing offers many benefits, including cost savings, increased scalability, faster development, and reduced maintenance. However, it also has some drawbacks, such as vendor lock-in, performance issues, and debugging challenges. Before deciding whether to adopt serverless computing for your business, it‚Äôs essential to weigh the pros and cons carefully.\nIf you‚Äôre interested in learning more about serverless computing and how it can benefit your business, visit Microhost‚Äôs website. Their cloud computing experts can help you navigate the serverless landscape and find the right solution for your needs.\nRead Also: serverless computing: What is it and how does it work?","cons-of-serverless-computing#\u003cstrong\u003eCons of Serverless Computing\u003c/strong\u003e":"","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","pros-of-serverless-computing#\u003cstrong\u003ePros of Serverless Computing\u003c/strong\u003e":""},"title":"Pros and Cons of Using Serverless Computing in Your Business"},"/utho-docs/docs/blog/public-vs-private-vs-hybrid-cloud-which-is-right-for-your-business/":{"data":{"":"","hybrid-cloud#\u003cstrong\u003eHybrid Cloud\u003c/strong\u003e":"\nAre you trying to decide which type of cloud computing is right for your business? With so many options available, it can be overwhelming to determine which one is the best fit. In this article, we‚Äôll break down the differences between public, private, and hybrid cloud and help you decide which one is right for your needs.\nPublic Cloud Public cloud is a type of cloud computing that is owned and operated by third-party providers. These providers offer resources such as virtual machines, applications, and storage to customers over the internet. This option is best for businesses that need to scale their operations quickly and want to avoid the costs and complexities of managing their own IT infrastructure. Public cloud also provides greater flexibility, as users can quickly access additional resources when needed and only pay for what they use.\nPrivate Cloud Private cloud is a type of cloud computing that is used exclusively by a single organization. This option is best for businesses that require greater control over their data and infrastructure, as it provides the highest level of security and customization. With a private cloud, companies can create custom infrastructure solutions that are tailored to their specific needs, and have greater control over who has access to their data.\nHybrid Cloud Hybrid cloud is a type of cloud computing that combines elements of both public and private cloud. It allows businesses to leverage the benefits of both cloud environments by creating a single, unified infrastructure. This option is best for businesses that need to balance their need for security and control with their need for scalability and flexibility. With a hybrid cloud, businesses can maintain sensitive data on-premises while using the public cloud for non-sensitive workloads.\nWhich One Is Right For You? The answer to this question ultimately depends on your business‚Äôs unique needs and goals. If you need a scalable and flexible solution that can adapt to changing demands quickly, public cloud may be the best option for you. If you require greater control over your infrastructure and data, private cloud may be the way to go. And if you need to balance both needs, a hybrid cloud could be the perfect fit.\nNo matter which option you choose, it‚Äôs important to select a reputable and reliable cloud provider that can meet your needs. And that‚Äôs where Microhost comes in. As a leading cloud hosting provider, Microhost offers a wide range of cloud solutions to fit your unique needs. Contact us today to learn more about how we can help you take your business to the next level with cloud computing.\nRead Also: 6 Benefits of Deploying a Load Balancer on your server.","private-cloud#\u003cstrong\u003ePrivate Cloud\u003c/strong\u003e":"","public-cloud#\u003cstrong\u003ePublic Cloud\u003c/strong\u003e":""},"title":"Public VS Private VS Hybrid Cloud: Which is Right for Your Business?"},"/utho-docs/docs/blog/real-time-revolution-conquering-cloud-challenges/":{"data":{"":"","how-can-organizations-optimize-cost-effectiveness-while-implementing-real-time-data-processing-solutions-in-the-cloud#\u003cstrong\u003eHow can organizations optimize cost-effectiveness while implementing real-time data processing solutions in the cloud?\u003c/strong\u003e":"","how-is-utho-cloud-overcoming-the-challenges-inherent-in-cloud-computing-and-in-what-ways-does-it-facilitate-business-growth#\u003cstrong\u003eHow is Utho Cloud overcoming the challenges inherent in cloud computing, and in what ways does it facilitate business growth?\u003c/strong\u003e":"As businesses increasingly rely on cloud computing, they face various hurdles like performance optimization and data security. However, there‚Äôs a solution: the real-time revolution. By processing data instantly, companies can tackle cloud challenges more effectively. This shift not only boosts efficiency but also allows for quick and precise problem-solving. In this blog, we‚Äôll explore how the real-time revolution is reshaping modern businesses, paving the way for growth and innovation.\nWhat are the key challenges associated with real-time data processing in cloud environments? Key challenges associated with real-time data processing in cloud environments include:\nLatency: Cloud environments often involve data transfer over networks, leading to latency issues. Processing data in real-time requires minimizing latency to ensure timely analysis and decision-making.\nScalability: Real-time data processing systems must handle varying workloads effectively. Cloud environments need to scale resources dynamically to accommodate fluctuations in data volume and processing demands.\nResource Management: Efficiently managing computing, storage, and networking resources is crucial for real-time data processing. Cloud platforms offer scalability, but optimizing resource allocation and utilization is essential for cost-effectiveness and performance.\nFault Tolerance: Real-time systems in the cloud must be resilient to failures. Ensuring fault tolerance involves mechanisms such as redundancy, data replication, and failover strategies to maintain continuous operation and data integrity.\nData Consistency: Maintaining consistency across distributed data sources is challenging in real-time processing. Cloud environments may involve data replication across multiple regions, requiring synchronization mechanisms to ensure data consistency.\nSecurity: Real-time data processing systems in the cloud are vulnerable to security threats such as data breaches, unauthorized access, and malicious attacks. Implementing robust security measures, including encryption, access controls, and threat detection, is essential to protect sensitive data.\nCompliance: Cloud-based real-time data processing systems must adhere to regulatory compliance requirements, such as GDPR, HIPAA, or PCI DSS. Ensuring compliance involves implementing appropriate data governance policies, auditing mechanisms, and data protection measures.\nCost Management: Real-time data processing in the cloud can incur significant costs, especially with high data volumes and complex processing workflows. Optimizing costs involves selecting cost-effective cloud services, implementing efficient resource utilization strategies, and monitoring usage to avoid unnecessary expenses.\nIntegration Complexity: Integrating real-time data processing systems with existing infrastructure, applications, and external data sources can be complex in cloud environments. Compatibility issues, data format conversions, and interoperability challenges may arise, requiring careful planning and implementation.\nMonitoring and Performance Tuning: Continuous monitoring and performance tuning are essential for optimizing the efficiency and effectiveness of real-time data processing in the cloud. Monitoring metrics such as throughput, latency, and resource utilization help identify bottlenecks and optimize system performance.\nWhat strategies can organizations employ to overcome the challenges of real-time data processing in cloud environments? Certainly! Here are strategies organizations can employ to overcome the challenges of real-time data processing in cloud environments, presented in an easy-to-read format:\nOptimize Network Infrastructure: Invest in high-speed and low-latency network infrastructure to minimize data transfer times and improve real-time data processing performance.\nUtilize Scalable Cloud Services: Leverage scalable cloud services such as serverless computing, auto-scaling instances, and managed databases to dynamically allocate resources based on workload demands.\nImplement Streamlined Data Integration: Use data integration tools and techniques to streamline the ingestion of data from various sources into the cloud, ensuring compatibility, consistency, and reliability.\nDeploy Edge Computing: Employ edge computing technologies to process data closer to its source, reducing latency and network overhead for real-time applications.\nOptimize Data Pipelines: Design efficient data processing pipelines using stream processing frameworks, batch processing systems, and data caching mechanisms to optimize performance and resource utilization.\nImplement Data Compression and Caching: Use data compression techniques and caching mechanisms to minimize data transfer volumes and reduce latency in real-time data processing workflows.\nEnhance Security Measures: Implement robust security measures such as encryption, access controls, and threat detection to protect sensitive data and ensure compliance with regulatory requirements.\nInvest in Training and Skill Development: Provide training and skill development opportunities to employees to enhance their proficiency in cloud technologies, data processing frameworks, and security best practices.\nAdopt Multi-Cloud and Hybrid Solutions: Diversify cloud deployment strategies by adopting multi-cloud or hybrid cloud architectures to mitigate vendor lock-in and enhance flexibility and resilience.\nEstablish Performance Monitoring and Optimization: Implement comprehensive monitoring and performance optimization strategies to identify bottlenecks, fine-tune resource allocation, and continuously improve the efficiency of real-time data processing systems.\nLeverage Serverless and Managed Services: Utilize serverless computing and managed services offered by cloud providers to offload operational overhead and focus on developing and deploying real-time data processing applications.\nEmbrace DevOps Practices: Adopt DevOps practices to automate infrastructure provisioning, deployment, and monitoring, enabling rapid iteration and continuous improvement of real-time data processing solutions.\nCollaborate with Cloud Providers: Work closely with cloud providers to leverage their expertise, support services, and advanced capabilities for optimizing real-time data processing workflows in the cloud.\nFoster a Data-driven Culture: Promote a data-driven culture within the organization by encouraging data-driven decision-making, fostering collaboration between data engineering, analytics, and business teams, and incentivizing innovation in real-time data processing initiatives.\nHow can organizations optimize cost-effectiveness while implementing real-time data processing solutions in the cloud? Certainly! Here are strategies organizations can employ to optimize cost-effectiveness while implementing real-time data processing solutions in the cloud, presented in an easy-to-read format:\nRight-sizing Resources: Analyze workload requirements and select appropriately sized cloud instances, storage options, and other resources to avoid over-provisioning and minimize costs.\nUtilize Spot Instances and Reserved Capacity: Take advantage of spot instances and reserved capacity offerings from cloud providers to access discounted pricing for compute resources, especially for workloads with flexible scheduling requirements.\nImplement Autoscaling: Configure autoscaling policies to automatically adjust the number of instances or resources based on workload demand, scaling up during peak periods and scaling down during off-peak times to optimize cost efficiency.\nOptimize Storage Costs: Opt for cost-effective storage options such as object storage, tiered storage, and data lifecycle management policies to reduce storage costs while ensuring data availability and durability.\nUse Serverless Architectures: Leverage serverless computing services such as AWS Lambda, Azure Functions, or Google Cloud Functions to pay only for the compute resources consumed during real-time data processing tasks, eliminating the need for provisioning and managing infrastructure.\nMonitor and Optimize Usage: Implement comprehensive monitoring and analytics tools to track resource utilization, identify inefficiencies, and optimize usage patterns to minimize unnecessary costs.\nEnable Data Compression and Deduplication: Implement data compression and deduplication techniques to reduce data transfer volumes and storage requirements, lowering costs associated with data ingress, egress, and storage.\nLeverage Cloud Cost Management Tools: Utilize built-in cost management tools and services provided by cloud providers to analyze spending, set budget limits, and receive cost optimization recommendations tailored to specific workloads.\nImplement Cost Allocation and Chargeback: Establish cost allocation mechanisms and chargeback models to attribute cloud costs to individual projects, departments, or teams, promoting accountability and cost awareness across the organization.\nOpt for Pay-as-You-Go Pricing: Choose pay-as-you-go pricing models for cloud services to align costs with usage patterns, avoiding upfront commitments and enabling flexibility to scale resources as needed.\nOptimize Data Transfer Costs: Minimize data transfer costs by strategically selecting cloud regions, optimizing network configurations, and leveraging content delivery networks (CDNs) for caching and content delivery.\nEvaluate Reserved Instances and Savings Plans: Assess the benefits of purchasing reserved instances or savings plans for predictable workloads with steady usage patterns, which can offer significant cost savings compared to on-demand pricing.\nImplement Cost-aware Architectural Design: Design real-time data processing architectures with cost optimization in mind, considering factors such as data partitioning, caching strategies, and resource pooling to maximize efficiency and minimize costs.\nRegularly Review and Adjust Strategies: Continuously monitor cloud costs, review cost optimization strategies, and adjust resource allocation and usage patterns based on changing business requirements and cost-performance trade-offs.\nWhat are the risks associated with vendor lock-in when implementing real-time data processing solutions in the cloud, and how can organizations mitigate these risks? Certainly! Here‚Äôs the answer to your question presented in an easy-to-read format:\nRisks Associated with Vendor Lock-in:\nLimited Flexibility: Dependency on a single cloud provider can limit flexibility in adapting to changing business needs or technological advancements.\nIncreased Costs: Switching cloud providers or migrating to alternative solutions can incur significant costs due to data transfer, retraining, and re-architecture requirements.\nLack of Innovation: Vendor lock-in may restrict access to innovative technologies or services offered by other cloud providers, potentially hindering competitiveness and innovation.\nData Portability Concerns: Challenges related to data portability and interoperability may arise when attempting to migrate data and workloads between cloud environments.\nLoss of Negotiating Power: Over-reliance on a single cloud provider can weaken negotiating leverage, leading to less favorable contract terms, pricing, or service levels.\nMitigation Strategies: Adopt Multi-Cloud or Hybrid Cloud Approaches: Implement multi-cloud or hybrid cloud architectures to distribute workloads across multiple cloud providers or combine cloud and on-premises resources, reducing dependency on any single vendor.\nUse Open Standards and APIs: Prioritize solutions that adhere to open standards and provide interoperable APIs, facilitating portability and reducing reliance on proprietary technologies.\nContainerization and Orchestration: Containerize applications using technologies like Docker and orchestrate deployments with Kubernetes to abstract away infrastructure dependencies and enable portability across cloud environments.\nImplement Data Portability Measures: Design data storage and processing solutions with portability in mind, leveraging standardized data formats, APIs, and migration tools to facilitate seamless data movement between cloud platforms.\nNegotiate Flexible Contracts: Negotiate contracts with cloud providers that include provisions for flexibility, such as exit clauses, pricing discounts for long-term commitments, and assurances of data portability and interoperability support.\nInvest in Cloud-agnostic Tools and Services: Select tools, frameworks, and services that are compatible with multiple cloud providers, minimizing vendor-specific dependencies and enabling interoperability across different cloud environments.\nRegularly Evaluate Vendor Offerings: Continuously monitor the evolving landscape of cloud services and evaluate alternative vendors to ensure alignment with organizational requirements and mitigate dependency risks.\nPlan for Exit Strategies: Develop comprehensive exit strategies and contingency plans for migrating workloads and data from one cloud provider to another in case of vendor-related issues or changing business needs.\nInvest in Employee Skills and Training: Invest in employee training and skill development to ensure proficiency in cloud-agnostic technologies, best practices for multi-cloud deployments, and migration strategies to mitigate vendor lock-in risks.\nEngage with Vendor Partnerships: Engage with cloud provider partnerships, consortia, or industry groups advocating for interoperability and standards compliance to influence vendor practices and promote open ecosystems in the cloud computing market.\nHow is Utho Cloud overcoming the challenges inherent in cloud computing, and in what ways does it facilitate business growth? Utho Cloud is overcoming challenges inherent in cloud computing through several key strategies:\nScalability and Performance: Utho Cloud offers scalable infrastructure and high-performance computing capabilities, allowing businesses to easily scale resources up or down based on demand. This ensures that businesses can handle fluctuating workloads efficiently, supporting growth without compromising performance.\nSecurity and Compliance: Utho Cloud prioritizes security and compliance, providing advanced security features such as encryption, access controls, and identity management. Additionally, Utho Cloud adheres to industry regulations and compliance standards, giving businesses peace of mind regarding data protection and regulatory requirements.\nReliability and Availability: Utho Cloud ensures high reliability and availability through redundant infrastructure, data replication, and disaster recovery capabilities. This minimizes downtime and ensures continuous availability of services, supporting business continuity and reliability.\nAdvanced Technologies: Utho Cloud incorporates advanced technologies such as artificial intelligence (AI), machine learning (ML) empowering businesses to unlock new insights, automate processes, and drive innovation. These technologies enable businesses to stay ahead of the competition and capitalize on emerging opportunities for growth.\nUtho Cloud addresses the challenges inherent in cloud computing by offering scalable, secure, reliable, and integrated solutions powered by advanced technologies. By leveraging Utho Cloud, businesses can accelerate their growth, drive innovation, and stay competitive in today‚Äôs dynamic digital landscape.\nAs companies evolve, using real-time data processing will be crucial to staying ahead. With this approach, businesses can confidently tackle cloud complexities, ensuring ongoing success and seizing new opportunities. Through the real-time revolution, they can conquer cloud challenges and emerge stronger than ever.","mitigation-strategies#\u003cstrong\u003eMitigation Strategies:\u003c/strong\u003e":"","what-are-the-key-challenges-associated-with-real-time-data-processing-in-cloud-environments#\u003cstrong\u003eWhat are the key challenges associated with real-time data processing in cloud environments?\u003c/strong\u003e":"","what-are-the-risks-associated-with-vendor-lock-in-when-implementing-real-time-data-processing-solutions-in-the-cloud-and-how-can-organizations-mitigate-these-risks#\u003cstrong\u003eWhat are the risks associated with vendor lock-in when implementing real-time data processing solutions in the cloud, and how can organizations mitigate these risks?\u003c/strong\u003e":"","what-strategies-can-organizations-employ-to-overcome-the-challenges-of-real-time-data-processing-in-cloud-environments#\u003cstrong\u003eWhat strategies can organizations employ to overcome the challenges of real-time data processing in cloud environments?\u003c/strong\u003e":""},"title":"Real-Time Revolution: Conquering Cloud Challenges"},"/utho-docs/docs/blog/rising-to-new-heights-the-rebranding-story-of-microhost-to-utho/":{"data":{"":"","a-mothers-wake-up-call#\u003cstrong\u003eA Mother\u0026rsquo;s Wake-Up Call\u003c/strong\u003e":"","a-promise-of-exceptional-service#\u003cstrong\u003eA Promise of Exceptional Service\u003c/strong\u003e":"‚ÄúChange is the driving force behind growth and evolution.‚Äù¬†As you know, MicroHost is now Utho, a brand that symbolizes rising up, breaking free from limitations, and embracing a fresh approach to cloud solutions.\nMicrohost is now Utho\nLet‚Äôs take a closer look at the reasons behind Utho‚Äôs rebranding and the journey behind it.\nWhy was the name changed to Utho?\nA Mother‚Äôs Wake-Up Call Take a moment to recall those precious childhood memories when your mother‚Äôs gentle voice nudged you out of bed. Her words had the power to awaken your senses and prepare you for the day ahead. ‚ÄúUtho, school jana hai‚Äù (Get up, it‚Äôs time to go to school) were not just ordinary words; they held the magic of motivation and served as a reminder that we all have a purpose to fulfill.\n‚ÄúOur mother‚Äôs unwavering support has always been our driving force. She believes in us, inspires confidence, and reminds us that obstacles are only opportunities to grow.‚Äù\nThe Deep Meaning of UTHO UTHO holds a deep meaning for all of us as it represents the times when we faced the toughest challenges in our life. It is a voice from our hearts and says, ‚ÄúUTHO, kaam par lag jao‚Äù (Rise up and get to work). It became a call to action, igniting our determination and propelling us forward.\nA vision with a purpose.\nNeed for Change We recognized the struggles businesses face with traditional cloud service providers, and we firmly believed that it was time for a change. Our new name, Utho, reflects our commitment to providing simple, secure, and reliable cloud solutions that empower businesses to take control of their technology. With Utho, you can break free from the limitations of vendor lock-in, slow speed, high cost, and unnecessary complexity.\nEmpowering Your Success Our goal is to empower businesses to achieve new heights of success. We are dedicated to delivering the highest level of service to our clients. Our vision is to be the leading provider of cloud services, breaking down barriers and creating opportunities for businesses of all sizes.\nStanding by Your Side Today, our mission is to stand by you, just as your mother stood by you. UTHO symbolizes our commitment to supporting you unconditionally in every situation you encounter. We understand the challenges you face, and we are here to provide the hands-on assistance you need to elevate your business to new heights.\nUnlocking Your Potential Utho is not just for big corporations; it‚Äôs for businesses of all sizes. We are passionate about democratizing cloud services, creating opportunities for startups and small businesses to thrive. Let us level the playing field and help you unleash your true potential.\nA Promise of Exceptional Service Throughout our transformation, one thing remains unchanged: our unwavering dedication to serving you. Our team is committed to providing an exceptional experience, ensuring you receive the best possible support. With Utho, you‚Äôll find a partner who understands your needs.\nAs we begin this journey, we welcome you to Utho, a platform where empowerment and innovation meet. Embrace a future filled with endless possibilities, and leave behind the obstacles of the past. It is our pleasure to welcome you to this exciting journey, where you possess the tools and expertise to outperform your competition. It‚Äôs time to rise up and unleash your business‚Äôs full potential with Utho by your side. Let‚Äôs rewrite the rules and shape a brighter future together.\n‚ÄúUtho Business utha dega,\nSabkuch easy bana Dega‚Äù\nThe inspiring vision sparked our transformation.\nMr. Manoj Dhanda (Founder and CTO - Utho), strongly believed in empowering businesses and breaking barriers which led him to rebrand MicroHost as Utho.\nClick to See The Video","empowering-your-success#\u003cstrong\u003eEmpowering Your Success\u003c/strong\u003e":"","need-for-change#\u003cstrong\u003eNeed for Change\u003c/strong\u003e":"","standing-by-your-side#\u003cstrong\u003eStanding by Your Side\u003c/strong\u003e":"","the-deep-meaning-of-utho#\u003cstrong\u003eThe Deep Meaning of UTHO\u003c/strong\u003e":"","unlocking-your-potential#\u003cstrong\u003eUnlocking Your Potential\u003c/strong\u003e":""},"title":"Rising to New Heights: The Rebranding Story of MicroHost to Utho"},"/utho-docs/docs/blog/saas-paas-and-iaas-a-comparison-of-business-models/":{"data":{"":"","infrastructure-as-a-service-iaas-offers-unparalleled-control-and-flexibility#\u003cstrong\u003eInfrastructure as a Service (IaaS) offers unparalleled control and flexibility\u003c/strong\u003e":"\nThe cloud service marketplace has implemented various cutting-edge technologies and incorporated emerging trends in response to the dynamic nature of the technology industry. When considering the appropriate cloud service model, it is crucial to have a thorough understanding of each option and assess their capabilities. These innovative models aim to enhance the efficiency, scalability, productivity, and security of users‚Äô digital data and connected devices.\nInfrastructure as a Service (IaaS) offers unparalleled control and flexibility The IaaS model is a fundamental and highly adaptable cloud service. It grants access to virtualized computing assets, including servers, storage, networks, and operating systems, which can be leased and controlled based on your preferences. You have full control over the establishment, growth, security, and maintenance of your infrastructure while only paying for the resources you use.\nBenefits and Limitations of IaaS 1. Fast scaling: IaaS providers offer distinct advantages in terms of rapid scalability to meet the evolving needs of rapidly expanding enterprises\n2. High reliability: IaaS providers distribute workloads across a network of data centers and servers, effectively mitigating potential technical challenges.\n3. Operational resilience: IaaS allows your team to maintain uninterrupted access to hardware and computing resources.¬†There are several disadvantages associated with utilizing this model 1. Legacy system problems: Legacy applications have the potential to function in a cloud environment, however, there may be concerns surrounding their security protocols, configuration preferences, and initial setup.\n2. In-house training: Your team must be equipped to handle the new infrastructure, making internal training a prerequisite.\n3. Update issues: Your system upgrades are dependent on the vendor‚Äôs actions. If your provider does not consistently update the system, there is a risk of decreased productivity and efficiency. So choose a provider that specifically deals in IaaS.\nPlatform as a Service (PaaS) offers the perfect amalgamation of both worlds PaaS is a cloud computing model that bridges the gap between IaaS and SaaS. It offers users a pre-configured platform that includes tools such as web servers, databases, and development environments for building and launching applications.\nWith PaaS, users are relieved of the burden of managing underlying infrastructure, as the provider assumes responsibility for the provisioning, security, and maintenance of servers, networks, and operating systems.\nBenefits and Drawbacks of PaaS When evaluating PaaS, SaaS, and IaaS, one will discover a comprehensive list of advantages.\n1. Reduced operating costs: There is no necessity to create applications from the ground up. This can help you save valuable time and resources in your software development process.\n2. Rapid development: By utilizing prebuilt backend infrastructure, you can effectively streamline your application‚Äôs time-to-market.\n3. Maintenance-free: PaaS providers have the responsibility of constructing, maintaining, and configuring servers.\nAlthough PaaS offers numerous benefits, It is crucial to recognize and handle the limitations that come with it. 1. Runtime errors: Some PaaS solutions may not be tailored to support all programming languages and frameworks.\n2. Update issues: Modifications made by PaaS vendors may result in compatibility issues with the components utilized.\n3. Capability limits: Tailored cloud operations may not be fully compatible with PaaS solutions.\nSoftware as a Service (SaaS) ensures minimal hassle and responsibility SaaS is a widely used and convenient model for cloud services. It offers users access to fully operational software applications such as email, CRM, or ERP that are hosted on the provider‚Äôs cloud infrastructure. With this service, there is no need to install, update, or manage any hardware or software as the provider handles all of it.¬†While this model offers convenience, it also means relying on the provider‚Äôs availability, security, and performance without much influence over data and functionality. SaaS is best suited for projects requiring uncomplicated and uniform solutions while sacrificing some degree of control and flexibility.\nBenefits and drawbacks of using SaaS The SaaS model is a cloud-based approach that offers numerous advantages. Let us explore some of the key benefits of utilizing this model:\n1. Cutting costs: Clients receive Cloud-based infrastructure as an alternative to On-Premises solutions, allowing them to avoid investing in expensive In-House resources and applications. The service model facilitates the creation of management information systems without needing to purchase, Install, or maintain physical products.\n2. End-user-friendly format: The user-friendly format allows for seamless and convenient cloud-hosted access without the need for software downloads on your computer. SaaS solutions are versatile, enabling usage on both computers and mobile devices.\n3. Flexibility: SaaS offers effortless scalability and seamless integration with other SaaS solutions. You will have the freedom to scale your SaaS usage according to your specific requirements without having to purchase additional servers or software.\n4. Software backups: SaaS providers offer protection and backup services to prevent data loss for their customers. Some may choose to store copies on local disk systems, while others may utilize public cloud storage by creating additional copies.\nSome of the key disadvantages of SaaS solutions include 1. Security issues: Security concerns are a significant factor influencing companies‚Äô hesitation to adopt SaaS applications. Emphasizing access control, the selection of a trustworthy provider is crucial.\n2. System integration challenges: Certain SaaS products require integration with your company‚Äôs existing tools and applications to enhance productivity. As the SaaS vendor is unable to offer support, it will be necessary for your organization to allocate internal resources towards managing these integrations.\n3. Vendor lock-in: Switching to a different vendor can present challenges, as it typically entails a substantial commitment of both resources and time. Furthermore, implementing a new SaaS solution may entail programming and necessitate additional support from an external vendor.\nIaaS, PaaS, and SaaS solutions with Utho. The implementation of Cloud SaaS, PaaS, and IaaS solutions equips your business to adapt to evolving industry norms and attain a competitive advantage. This is crucial for enhancing the level of customer service and driving overall business growth. When you‚Äôre prepared to embark on your cloud journey, Utho will assess your specific needs and budget to determine the most suitable option for your enterprise.\nRead Also: Collaborating DevOps and SRE for Efficient Cloud Migration"},"title":"SaaS, PaaS, and IaaS: A Comparison of Business Models"},"/utho-docs/docs/blog/saas-transforming-business-in-the-digital-era/":{"data":{"":"","how-does-this-cloud-based-software-technology-operate#\u003cstrong\u003eHow does this cloud-based software technology operate?\u003c/strong\u003e":"","what-are-the-advantages-of-using-this-cloud-based-software-technology#\u003cstrong\u003eWhat are the advantages of using this cloud-based software technology?\u003c/strong\u003e":"","what-are-the-applications-of-software-as-a-service#\u003cstrong\u003eWhat are the applications of Software-as-a-Service?\u003c/strong\u003e":"","what-challenges-come-with-this-technology-of-cloud-based-software#\u003cstrong\u003eWhat challenges come with this technology of cloud-based software?\u003c/strong\u003e":"","what-does-this-cloud-based-software-technology-entail-in-the-realm-of-cloud-computing#\u003cstrong\u003eWhat does this cloud-based software technology entail in the realm of cloud computing?\u003c/strong\u003e":"","what-makes-customers-choose-uthos-for-this-cloud-based-software-technology#\u003cstrong\u003eWhat makes customers choose Utho\u0026rsquo;s for this cloud-based software technology?\u003c/strong\u003e":"As technology evolves daily, ‚Äúdigital transformation‚Äù has become a ubiquitous term. The convergence of cloud computing and the Software as a Service sector has revolutionized how businesses function in the digital age. This powerful alliance ushers in an era marked by enhanced efficiency, scalability, and accessibility, fundamentally altering software delivery and consumption. The cloud‚Äôs transformative influence significantly redefines the SaaS industry, catalyzing global digital transformation. Let‚Äôs delve into the synergistic bond between digital transformation and the cloud, highlighting how the cloud reshapes the landscape of the SaaS industry.\nWhat does this cloud-based software technology entail in the realm of cloud computing? SaaS (Software as a Service) operates as cloud-based software on a third-party server via the Internet. Unlike on-premise deployment, it is accessible on various mobile devices such as phones, tablets, and laptops, from any location with a robust internet connection.\nHow does this cloud-based software technology operate? SaaS applications and services operate on cloud platforms with a multi-tenant architecture. Through virtualization, cloud providers run software from multiple virtual servers on a single physical server, allowing them to offer cost-effective services to multiple customers. This architecture enables efficient scaling and streamlines maintenance, updates, and troubleshooting.\nWhat are the advantages of using this cloud-based software technology? Benefits associated with a Software-as-a-Service-based application encompass:\nScalable usage: Adjust your subscription based on your requirements. Initially, you might only require 1,000 user accounts. As your company expands, you can easily upgrade to, for instance, 10,000 user access.\nAutomatic updates: Updates for a SaaS application are automated and occur seamlessly in the cloud without any need for your intervention.\nAccessibility and persistence: Access the SaaS-based application on the go; SaaS embraces remote work, allowing users to log in from anywhere in the world.\nCustomization: SaaS applications provide extensive customization options, allowing integration with other SaaS-based applications. For instance, automate specific tasks in your application by partnering with a SaaS vendor offering such services, resulting in a personalized workflow.\nWhat challenges come with this technology of cloud-based software? Despite the benefits, there are still challenges in implementing Software-as-a-Service:\nSecurity: The responsibility for data safety rests entirely with the vendor. Therefore, the security of your data depends on how rigorously the SaaS provider has implemented security principles.\nDifficulty switching vendors: Extensive usage of SaaS apps results in a substantial amount of data stored on the vendor‚Äôs cloud, posing challenges for migration to a different vendor.\nCustomers lose control over versioning: If you find the updated version of the vendor‚Äôs software unsatisfactory, your options are either to accept it or consider switching to a different vendor.\nIssues beyond customer control: Various problems, such as vendor infrastructure failures or errors on the vendor‚Äôs part, can result in damage to your services.\nWhat are the applications of Software-as-a-Service? Here are a few examples of SaaS applications:\nSalesforce: A leading American company offering Software-as-a-Service to assist your sales team. Their software enables the management of customer relationships and sales teams, providing a unified platform for handling leads and prospects.\nZoom: This cloud platform facilitates communication through webstreams, video calls, and more. Its features encompass chat and screen-sharing capabilities.\nGoogle Applications: In addition to its renowned search engine, Google offers a comprehensive suite of applications to empower and streamline your workflow. Its products assist with word processing, presentations, collaboration, and more.\nSlack: Efficiently communicate with your entire team using this cloud-based software application. Slack offers features for seamless collaboration, providing a suitable alternative when emails are cumbersome and messaging apps are too informal.\nHubspot: This CRM provider, which offers a free version, extends its services beyond customer relationship management to accelerate sales through features like social media marketing and content management. Additionally, its software seamlessly integrates with Gmail and Outlook.\nWhat makes customers choose Utho‚Äôs for this cloud-based software technology? Utho provides comprehensive SaaS development services encompassing the entire lifecycle, spanning from initial planning and design to deployment and ongoing maintenance. Engage with our expert SaaS development team to collaboratively create and implement tailored solutions, perfectly aligned with your business‚Äôs unique needs and requirements."},"title":"SaaS: Transforming Business in the Digital Era"},"/utho-docs/docs/blog/scaling-heights-building-scalable-apps-with-cloud-apis/":{"data":{"":"","how-can-cloud-apis-contribute-to-the-scalability-of-modern-applications#\u003cstrong\u003eHow can cloud APIs contribute to the scalability of modern applications?\u003c/strong\u003e":"","how-do-cloud-api-usage-patterns-differ-between-startups-and-established-enterprises-seeking-to-build-scalable-applications#\u003cstrong\u003eHow do cloud API usage patterns differ between startups and established enterprises seeking to build scalable applications?\u003c/strong\u003e":"Scaling applications efficiently is essential in today‚Äôs digital landscape. Utilizing cloud APIs is a fundamental approach to achieving this goal. By leveraging cloud APIs, developers can seamlessly allocate resources, automate scaling processes, and efficiently manage applications. In this article, we‚Äôll delve into the strategies for harnessing cloud APIs to build scalable applications, empowering businesses to innovate and thrive in a dynamic market environment.\nHow can cloud APIs contribute to the scalability of modern applications? Cloud APIs, or Application Programming Interfaces, play a crucial role in enhancing the scalability of modern applications in several ways:\nElasticity: Cloud APIs allow applications to dynamically scale resources up or down based on demand. This means that when there‚Äôs a surge in users or workload, the application can quickly provision additional resources from the cloud provider to handle the increased load, ensuring optimal performance without manual intervention.\nAuto-scaling: With cloud APIs, developers can configure auto-scaling policies based on predefined metrics such as CPU utilization, memory usage, or incoming traffic. When these thresholds are reached, the API triggers the automatic scaling of resources accordingly. This proactive approach ensures that applications remain responsive and available even during peak usage periods.\nResource Provisioning: Cloud APIs provide access to a wide range of infrastructure resources such as virtual machines, containers, databases, and storage. Developers can programmatically provision and manage these resources through APIs, enabling on-demand allocation of computing power and storage capacity as needed, thereby optimizing resource utilization and minimizing costs.\nGlobal Reach: Many cloud providers offer data centers located in various regions worldwide. By leveraging cloud APIs, developers can deploy their applications across multiple geographic locations effortlessly. This not only improves performance by reducing latency for users in different regions but also enhances fault tolerance and disaster recovery capabilities.\nIntegration and Interoperability: Cloud APIs facilitate seamless integration with other cloud services and third-party applications, allowing developers to leverage a wide array of functionalities without reinventing the wheel. This enables rapid development and deployment of feature-rich applications by leveraging existing cloud-based services such as authentication, messaging, analytics, and machine learning.\nDevOps Automation: Cloud APIs enable automation of various DevOps processes such as continuous integration, deployment, monitoring, and management. By integrating cloud APIs with CI/CD pipelines and configuration management tools, developers can automate the deployment and scaling of applications, leading to faster release cycles, improved reliability, and reduced operational overhead.\nCloud APIs empower developers with the tools and capabilities needed to build scalable and resilient applications that can dynamically adapt to changing workloads and user demands, ultimately enhancing the efficiency, performance, and agility of modern software systems.\nWhat are some common challenges developers face when building scalable applications with cloud APIs, and how can they be overcome? Developers encounter several challenges when building scalable applications with cloud APIs. Here are some common ones along with potential solutions:\nManaging Complexity: Cloud environments can be complex, with numerous services, APIs, and configurations to navigate. This complexity can make it challenging for developers to understand and utilize cloud APIs effectively. To overcome this, developers should invest time in learning the cloud provider‚Äôs documentation thoroughly and consider leveraging abstraction layers or SDKs that simplify interaction with cloud services.\nScalability Limitations: While cloud APIs offer scalability features, developers need to design their applications with scalability in mind from the outset. Failure to do so may result in bottlenecks or performance issues that hinder scalability. By adopting architectural patterns like microservices, containerization, and serverless computing, developers can build applications that scale horizontally and vertically to meet growing demands.\nVendor Lock-in: Depending heavily on proprietary cloud APIs can lead to vendor lock-in, limiting flexibility and making it difficult to migrate to alternative cloud providers in the future. To mitigate this risk, developers should adhere to industry standards and best practices, use abstraction layers or middleware to decouple applications from specific cloud services, and architect applications in a modular and interoperable manner.\nPerformance and Latency: Over Reliance on cloud APIs for critical operations can introduce performance bottlenecks and latency, especially in distributed systems spanning multiple regions. Developers can address this challenge by optimizing API usage, implementing caching mechanisms, leveraging content delivery networks (CDNs) for static content, and strategically placing resources closer to end-users to minimize latency.\nSecurity and Compliance: Integrating third-party cloud APIs into applications introduces security risks such as data breaches, unauthorized access, and compliance violations. Developers must implement robust security measures such as encryption, authentication, authorization, and audit logging to protect sensitive data and ensure compliance with regulatory requirements. Additionally, they should stay informed about security best practices and monitor for security vulnerabilities in third-party APIs.\nCost Management: While cloud APIs offer scalability, developers must be mindful of cost implications, as excessive resource usage can lead to unexpected expenses. To optimize costs, developers should leverage cloud provider‚Äôs cost management tools, implement resource tagging and monitoring, utilize cost-effective instance types, and implement auto-scaling policies to dynamically adjust resources based on demand.\nBy addressing these challenges proactively and adopting best practices for designing, deploying, and managing applications in the cloud, developers can build scalable, resilient, and cost-effective solutions that meet the evolving needs of their users and businesses.\nHow does Utho Cloud provide the most extensive APIs for developing scalable applications? Utho Cloud offers an extensive array of APIs for building scalable applications through several key features and capabilities:\nDiverse Services: Utho Cloud provides a wide range of services spanning compute, storage, networking, databases, analytics, AI, and more. Each service comes with its set of APIs, allowing developers to leverage these functionalities programmatically to build scalable applications tailored to their specific requirements.\nComprehensive Documentation: Utho Cloud offers thorough documentation for its APIs, including guides, tutorials, sample code, and reference documentation. This comprehensive documentation helps developers understand how to use the APIs effectively, accelerating the development process and reducing time-to-market for scalable applications.\nStandardized Interfaces: Utho Cloud APIs adhere to industry standards and protocols, ensuring interoperability and ease of integration with existing systems and third-party applications. By following established standards, developers can seamlessly incorporate Utho Cloud services into their applications without the need for extensive custom integration efforts.\nScalability Features: Utho Cloud APIs are designed to support scalability requirements, allowing applications to dynamically scale resources up or down based on demand. Developers can programmatically provision additional compute instances, storage capacity, or other resources using APIs, enabling applications to handle varying workloads effectively while maintaining optimal performance.\nDeveloper Tools: Utho Cloud offers a suite of developer tools and SDKs (Software Development Kits) that simplify the process of working with APIs. These tools provide features such as code generation, debugging, testing, and monitoring, empowering developers to build and manage scalable applications more efficiently.\nIntegration Capabilities: Utho Cloud APIs enable seamless integration with other Utho Cloud services as well as third-party platforms and applications. This integration flexibility allows developers to leverage a wide range of functionalities, such as authentication, messaging, analytics, and machine learning, to enhance the scalability and capabilities of their applications.\nOverall, Utho Cloud‚Äôs extensive APIs, coupled with comprehensive documentation, standardized interfaces, scalability features, developer tools, and integration capabilities, provide developers with the necessary resources and flexibility to build scalable applications effectively and efficiently.\nHow do cloud APIs enable horizontal and vertical scaling in application architecture? Cloud APIs facilitate both horizontal and vertical scaling in application architecture through different mechanisms:\nHorizontal Scaling\nDefinition: Horizontal scaling, also known as scaling out, involves adding more instances or nodes to distribute the workload across multiple machines.\nCloud API Role: Cloud APIs enable horizontal scaling by providing features such as auto-scaling and load balancing. Developers can programmatically configure auto-scaling policies that dynamically provision additional instances or containers based on predefined metrics like CPU utilization or incoming traffic. Load balancing APIs distribute incoming requests evenly across multiple instances, ensuring efficient utilization of resources and improved scalability.\nVertical Scaling\nDefinition: Vertical scaling, also known as scaling up, involves increasing the computing power or capacity of individual instances or nodes.\nCloud API Role: Cloud APIs enable vertical scaling by providing access to scalable resources such as virtual machines and database instances. Developers can programmatically resize or upgrade these resources using APIs to meet growing demands. For example, they can increase the CPU, memory, or storage capacity of virtual machines or scale up database instances to handle larger datasets or higher transaction volumes.\nCloud APIs play a crucial role in enabling both horizontal and vertical scaling in application architecture by providing features for dynamically provisioning resources, distributing workloads, and optimizing resource utilization based on changing demands. This flexibility allows developers to build scalable and resilient applications that can adapt to varying workloads and user demands effectively.\nHow do cloud API usage patterns differ between startups and established enterprises seeking to build scalable applications? Cloud API usage patterns can vary between startups and established enterprises seeking to build scalable applications due to differences in organizational priorities, resources, and development approaches:\nStartups\nAgility and Flexibility: Startups often prioritize agility and speed of development to quickly iterate on their products and gain a competitive edge. As a result, they may favor lightweight and flexible cloud APIs that enable rapid prototyping and experimentation.\nCost Sensitivity: Startups typically have limited budgets and strive to minimize costs while building scalable applications. They may prioritize cloud APIs that offer pay-as-you-go pricing models, free tiers, or generous startup credits to reduce upfront investment.\nFocus on Innovation: Startups may prioritize cloud APIs that provide access to cutting-edge technologies and services, such as AI, machine learning, or serverless computing, to differentiate their products and deliver innovative solutions to market quickly.\nEstablished Enterprises\nScalability and Reliability: Established enterprises prioritize scalability, reliability, and robustness when building scalable applications to support their existing customer base and infrastructure. They may opt for cloud APIs backed by service level agreements (SLAs) and enterprise-grade support to ensure high availability and performance.\nIntegration with Legacy Systems: Enterprises often have legacy systems and existing IT infrastructure that need to be integrated with cloud-native applications. They may require cloud APIs with comprehensive integration capabilities, support for industry standards, and compatibility with on-premises systems to facilitate seamless migration and interoperability.\nSecurity and Compliance: Enterprises place a strong emphasis on security and compliance when selecting cloud APIs for building scalable applications. They may prioritize APIs that offer robust security features, such as encryption, authentication, access controls, and compliance certifications, to protect sensitive data and ensure regulatory compliance.\nWhile both startups and enterprises seek to build scalable applications using cloud APIs, their usage patterns may differ based on factors such as organizational priorities, budget constraints, technological requirements, and risk tolerance. Ultimately, the choice of cloud APIs depends on aligning with the specific needs and objectives of the organization, whether it‚Äôs rapid innovation, cost efficiency, reliability, or regulatory compliance.\n**As technology continues to advance and demands grow, the flexibility and efficiency offered by cloud APIs will become increasingly indispensable. With the right approach and utilization of cloud APIs, businesses can future-proof their applications and position themselves for sustained success in the dynamic digital environment of tomorrow.\n**","how-do-cloud-apis-enable-horizontal-and-vertical-scaling-in-application-architecture#\u003cstrong\u003eHow do cloud APIs enable horizontal and vertical scaling in application architecture?\u003c/strong\u003e":"","how-does-utho-cloud-provide-the-most-extensive-apis-for-developing-scalable-applications#\u003cstrong\u003eHow does Utho Cloud provide the most extensive APIs for developing scalable applications?\u003c/strong\u003e":"","what-are-some-common-challenges-developers-face-when-building-scalable-applications-with-cloud-apis-and-how-can-they-be-overcome#\u003cstrong\u003eWhat are some common challenges developers face when building scalable applications with cloud APIs, and how can they be overcome?\u003c/strong\u003e":""},"title":"Scaling Heights: Building Scalable Apps with Cloud APIs"},"/utho-docs/docs/blog/serverless-computing-what-is-it-and-how-does-it-work/":{"data":{"":"","benefits-of-serverless-computing#\u003cstrong\u003eBenefits of serverless computing\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nAs businesses move towards cloud computing, serverless computing has become increasingly popular. It allows organizations to focus on the core business logic without worrying about the underlying infrastructure. But what exactly is serverless computing, and how does it work?\nIn this article, we will provide an introduction to serverless computing, its benefits, and how it differs from traditional server-based computing.\nWhat is serverless computing? Serverless computing is a cloud-based model that allows developers to run and scale applications without having to manage servers or infrastructure. It is a fully managed service where the cloud provider manages the infrastructure and automatically scales it up or down as required. With serverless computing, you only pay for what you use, making it a cost-effective solution.\nHow does serverless computing work? In serverless computing, a cloud provider such as Amazon Web Services (AWS) or Microsoft Azure runs the server infrastructure on behalf of the customer. Developers write code in the form of functions and upload it to the cloud provider. These functions are then executed on the provider‚Äôs infrastructure, triggered by events such as a user uploading a file or a customer placing an order. The cloud provider automatically allocates resources to run the function and scales it up or down as required.\nBenefits of serverless computing Serverless computing offers several benefits to businesses, including:\nCost-effectiveness: With serverless computing, you only pay for what you use, making it a cost-effective solution.\nScalability: Serverless computing automatically scales up or down based on the demand, ensuring that the application is always available to the end-users.\nHigh availability: Serverless computing ensures high availability by automatically replicating the application across multiple data centers.\nIncreased productivity: Serverless computing allows developers to focus on writing code rather than managing infrastructure.\nDifferences between serverless computing and traditional server-based computing In traditional server-based computing, the organization manages the servers and infrastructure, including the operating system, patches, and updates. The application runs continuously on the server, and the organization pays for the server, regardless of whether the application is being used or not. In serverless computing, the cloud provider manages the infrastructure, and the application runs only when triggered by an event. The organization pays only for the resources used during the execution of the function.\nConclusion Serverless computing is a powerful cloud-based model that offers several benefits to businesses, including cost-effectiveness, scalability, and high availability. It differs significantly from traditional server-based computing, as it allows organizations to focus on the core business logic without worrying about the underlying infrastructure. If you are considering serverless computing for your business, MicroHost can help. Our cloud-based solutions are designed to meet the needs of businesses of all sizes. Contact us today to learn more.\nRead Alos: 5 Best practices for configuring and managing a Load Balancer","differences-between-serverless-computing-and-traditional-server-based-computing#\u003cstrong\u003eDifferences between serverless computing and traditional server-based computing\u003c/strong\u003e":"","how-does-serverless-computing-work#\u003cstrong\u003eHow does serverless computing work?\u003c/strong\u003e":"","what-is-serverless-computing#\u003cstrong\u003eWhat is serverless computing?\u003c/strong\u003e":""},"title":"serverless computing: What is it and how does it work?"},"/utho-docs/docs/blog/the-cat-and-tac-commands-in-linux-a-step-by-step-guide-with-examples/":{"data":{"":"","#":"Description In this article, we will cover some basic usage of the cat command, which is the command that is used the most frequently in Linux, and tac, which is the reverse of the cat command and prints files in reverse order. We will illustrate these concepts with some examples from real life.\nHow Cat Command Is Used One of the most popular commands in *nix operating systems is called ‚Äúcat,‚Äù which is an acronym for ‚Äúconcatenate.‚Äù The most fundamental application of the command is to read files and output their contents to the standard output, which simply means to show the contents of files on your computer‚Äôs terminal.\n#cat micro.txt In addition, the cat command can be used to read or combine the contents of multiple files into a single output, which can then be displayed on a monitor, as shown in the examples that follow.\n#cat micro1 micro2 micro3 Utilizing the ‚Äú\u003e‚Äù Linux redirection operator enables the command to also be used to combine multiple files into a single file that contains all of the combined contents of the individual files.\n#cat micro1 micro2 micro3 \u003e micro-all #cat micro-all The following syntax allows you to append the contents of a new file to the end of the file-all.txt document by making use of the append redirector.\n#cat micro4 \u003e\u003e micro-all #cat micro4 #cat micro4 \u003e\u003e micro-all #cat micro-all With the cat command, you can copy a file‚Äôs contents to a new file. Any name can be given to the new file. Copy the file from where it is now to the /tmp/ directory, for example.\n#cat micro1 \u003e\u003e /mnt/micro1 #cd /mnt/ #ls One of the less common uses of the cat command is to generate a new file using the syntax shown below. After you have finished making changes to the file, press CTRL+D to save and close the modified file.\n#cat \u003e new_file.txt Applying the -n switch to your command line will cause all output lines of a file, including blank lines, to be numbered.\n# cat -n micro-all Use the -b switch to show only the number of each line that isn‚Äôt empty.v\n#cat -b micro-all Discover How to Use the Tac Command On the other hand, the tac command is one that is not as well known and is utilised only occasionally in *nix systems. This command prints each line of a file to your machine‚Äôs standard output, beginning with the line at the bottom of the file and working its way up to the line at the top. Tac is practically the reverse version of the cat command, which is also spelled backwards.\n#tac micro-all The -s switch, which separates the contents of the file based on a string or a keyword from the file, is one of the most important options that the command has to offer. It is represented by the asterisk (*).\n#tac micro-all --separator \"two\" The second and most important use of the tac command is that it can be of great assistance when trying to debug log files by inverting the chronological order of the contents of the log.\n#tac /var/log/messages And if you want the final lines displayed\n#tail /var/log/messages | tac Similar to the cat command, tac is very useful for manipulating text files, but it should be avoided when dealing with other types of files, particularly binary files and files in which the first line specifies the name of the programme that will execute the file.\nThank You "},"title":"The 'cat' and 'tac' Commands in Linux: A Step-by-Step Guide with Examples"},"/utho-docs/docs/blog/the-crucial-role-of-cloud-monitoring-in-business-success/":{"data":{"":"","how-utho-is-the-best-option-for-the-cloud-monitoring-solution#\u003cstrong\u003eHow Utho is the best option for the cloud monitoring solution?\u003c/strong\u003e":"As businesses shift operations to the cloud‚Äîwhether for hosting websites, running applications, or storing data a crucial need arises to monitor performance, security, and costs. Cloud monitoring tools provide real-time insights, empowering administrators to pinpoint issues before they escalate into critical problems.\nWhat does Cloud Monitoring entail? It entails the continuous observation, assessment, and control of the well-being, efficiency, and accessibility of cloud-based applications, architecture, and services. This practice frequently employs both automated and manual methods, leveraging various tools to ensure your cloud infrastructure aligns with performance expectations.\nIntegral to cloud security and management, cloud monitoring involves real-time scrutiny of your cloud environment. This ongoing process is pivotal for promptly identifying and addressing any issues that could impact service availability.\nWhat are the types? Different types of cloud monitoring concentrate on specific aspects of cloud architecture. Keep an eye on the following components and areas:\nWebsite monitoring: Website monitoring enables administrators to monitor diverse facets of websites hosted in the cloud. This includes overseeing traffic patterns, ensuring availability, and tracking resource utilization.\nVirtual network monitoring: The monitoring of virtual networks encompasses the surveillance of operations and elements related to virtual network connections, performance, and devices.\nDatabase monitoring: Database monitoring assesses the integrity, availability, querying, and access of data within your system. It also examines how your application utilizes this data, pinpointing any potential bottlenecks that might impede efficient data transmission.\nVirtual machines monitoring: The monitoring of virtual machines involves overseeing their health, examining traffic logs, and ensuring scalability to adapt to varying workloads.\nCloud storage monitoring: Cloud storage monitoring offers visibility into performance metrics, user activities, storage expenses, bugs, and other crucial indicators of system performance.\nWhat are the reasons for monitoring your cloud environment? In general, cloud monitoring affords engineers an enhanced level of insight into their cloud environment. Additional advantages encompass the capability to:\nSave money by catching and fixing security problems early with cloud monitoring, helping DevOps teams stay on top of risks.\nFix structural issues, like misconfigurations, that can impact customer service.\nExamine the performance of your cloud-based services on various devices to enhance their overall optimization.\nImprove visibility and streamline the management of cloud environments with automation.\nGain a clearer understanding of your application‚Äôs performance. Use the gathered insights to enhance user experiences and prevent customer loss to competitors.\nHow Utho is the best option for the cloud monitoring solution? Utho stands as a powerful cloud-based solution including robust, enhancing business productivity by ensuring heightened availability of contacts, emails, tasks, and calendars. Tailored to meet the diverse needs of organizations, it excels in providing efficient solutions. Specifically designed for businesses requiring cost-effective and straightforward communication and file-sharing through intranets.","what-are-the-reasons-for-monitoring-your-cloud-environment#\u003cstrong\u003eWhat are the reasons for monitoring your cloud environment?\u003c/strong\u003e":"","what-are-the-types#\u003cstrong\u003eWhat are the types?\u003c/strong\u003e":"","what-does-cloud-monitoring-entail#\u003cstrong\u003eWhat does Cloud Monitoring entail?\u003c/strong\u003e":""},"title":"The Crucial Role of Cloud Monitoring in Business Success"},"/utho-docs/docs/blog/the-dark-side-of-cloud-servers-risks-and-threats/":{"data":{"":"","the-dark-side-of-cloud-servers-risks-and-threats-to-data-privacy-and-security-for-business-leaders-and-it-professionals#\u003cstrong\u003eThe Dark Side of Cloud Servers: Risks and Threats to Data Privacy and Security for Business Leaders and IT Professionals\u003c/strong\u003e":"\nThe Dark Side of Cloud Servers: Risks and Threats to Data Privacy and Security for Business Leaders and IT Professionals As a CEO, CTO, VP, Developer, IT Manager or IT Admin, you understand the value of using cloud servers to store and manage data. Cloud servers are cost-effective, scalable, and offer the flexibility to access data from anywhere in the world. However, with these benefits come a host of risks and threats to data privacy and security that business leaders and IT professionals must address.\nIn this article, we will explore the dark side of cloud servers, the risks and threats they pose to data privacy and security, and strategies for mitigating these risks.\nData Privacy Risks for Businesses Cloud servers store data in a shared environment, which means that multiple users share the same physical server infrastructure. While this makes it cost-effective and convenient, it also poses a risk to data privacy. A security breach that affects one user on the server can potentially compromise the data of other users on the same server. This can lead to legal and regulatory consequences, loss of revenue, and damage to brand reputation.\nFurthermore, cloud service providers may collect user data, including personal information and browsing habits, to analyze and monetize. This can pose a significant risk to user privacy and data security, as this information can be shared with third-party companies or exposed to hackers.\nData Security Threats for Businesses In addition to privacy risks, cloud servers also face security threats. The shared nature of the server infrastructure makes it vulnerable to hacking attempts, malware infections, and other security breaches. A successful attack on the server can result in data loss, theft, or exposure.\nFurthermore, cloud service providers may not always have robust security measures in place to protect user data. This can leave businesses vulnerable to data breaches, which can result in severe financial and reputational damage.\nMitigating Risks and Threats for Businesses To mitigate the risks and threats associated with cloud servers, businesses need to take a proactive approach to data security and privacy. This includes:\nCarefully selecting a reputable cloud service provider that offers robust security measures and privacy policies.\nImplementing strong password policies and two-factor authentication to prevent unauthorized access to the cloud account.\nRegularly monitoring the cloud account for suspicious activity, such as unusual logins or data transfers.\nBacking up data regularly to an external storage device or cloud service to minimize data loss in the event of a security breach.\nEducating employees or team members on best practices for data security and privacy.\nConclusion for Business Leaders and IT Professionals Cloud servers offer significant benefits to businesses, but they also come with risks and threats to data privacy and security. Business leaders and IT professionals must take a proactive approach to data security and privacy to protect their organizations and customers from the negative consequences of a security breach. By carefully selecting a reputable cloud service provider, implementing strong security measures, regularly monitoring the cloud account, and educating employees, businesses can ensure that their data remains safe and secure."},"title":"The Dark Side of Cloud Servers: Risks and Threats"},"/utho-docs/docs/blog/the-future-landscape-of-data-storage-cloud-data-management/":{"data":{"":"","what-are-the-advantages-of-data-management-in-cloud-computing#\u003cstrong\u003eWhat are the advantages of data management in cloud computing?\u003c/strong\u003e":"As data volumes and sources grow exponentially, managing data across various locations becomes a formidable challenge for organizations. Modern data managers seek versatile systems that ensure broad employee access without compromising data security. Cloud computing emerges as a solution for companies grappling with these data challenges. Effectively addressing complex business issues through cloud services necessitates a grasp of cloud data management, staying current on best practices, and drawing insights from successful organizations.\nWhat is data management in Cloud Computing technology? Cloud data management is the efficient administration of data across cloud platforms, offering a cost-effective alternative to on-premises storage. Enterprises choose to leverage external services for data storage, utilizing cloud server providers to streamline costs. This approach involves procuring on-demand cloud resources and includes processes like data archiving, tiering, replication, protection, or migration.\nWhat are the advantages of data management in cloud computing? The advantages of a cloud data management platform align with the extensive Benefits provided by the cloud and they are significant.\nSecurity: Contemporary cloud data management frequently provides enhanced data protection compared to on-premises solutions. In fact, 94% of those embracing the cloud note security enhancements. Why? Firstly, cloud data management lowers the risk of data loss from device damage or hardware failure. Secondly, companies specializing in cloud hosting and data management implement more advanced security measures for safeguarding sensitive data than those relying on on-premises solutions.\nScalability and savings: Cloud data management allows users to adjust services based on demand, scaling up or down as necessary. Additional storage or compute power can be incorporated to accommodate fluctuating workloads. After completing a substantial project, companies can scale back to prevent unnecessary expenses on services not currently required.\nAutomated backups and disaster recovery: The vendor for cloud storage takes charge of managing and automating data backups, allowing the company to concentrate on other priorities while ensuring the safety of its data. Maintaining an always-up-to-date backup also expedites the disaster recovery process in emergencies and aids in minimizing the impact of ransomware attacks.\nImproved data quality: A cohesive and well-governed cloud data management solution enables organizations to dismantle data silos, establishing a unified source of truth for each data point. The data stays clean, consistent, up-to-date, and accessible for a range of use cases, including real-time data analytics, advanced machine learning applications, and external sharing through APIs.\nAutomated updates: Cloud data management providers are dedicated to delivering optimal services and capabilities. When applications require updates, cloud providers automatically implement these changes. This eliminates the need for your team to halt work while waiting for IT to update everyone‚Äôs systems.\nSustainability: For companies and brands dedicated to minimizing their environmental footprint, adopting cloud data management is a pivotal measure. It enables organizations to lessen the carbon footprint associated with their own facilities and expand telecommuting options for their teams.\nWhat risks come with the territory of data management in cloud computing? Having covered the advantages, let‚Äôs now delve into the major challenges that businesses should be aware of:\nData Breaches \u0026 Attacks: Effectively managing data on cloud servers poses a significant challenge, particularly concerning data security. Instances of data breaches, DDOS attacks, and account hijacking have become prevalent. Insufficient security measures can leave the platform vulnerable, particularly due to the shared use of resources among multiple users, creating potential security gaps. Addressing this requires the engagement of skilled personnel, especially when handling sensitive data.\nCost Implications: While cloud storage appears cost-effective, recent findings indicate otherwise. Approximately one-third of businesses are surpassing their cloud budgets by up to 40%, as per a recent survey. Effectively managing the costs associated with cloud software licenses and resources poses a substantial challenge. Therefore, the financial dimension can become a risk if not appropriately handled.\nReduced Visibility \u0026 Control: A distinct risk in cloud environments involves diminished visibility and control over assets and operations. This becomes a significant worry when utilizing external cloud services as certain policies and infrastructure fall under the cloud providers‚Äô responsibility. This shift poses challenges in effectively monitoring security measures and ensuring data integrity within a cloud environment.\nRisks In Management APIs: Management APIs provided by cloud providers serve as a means to control resources, but their accessibility over the Internet makes them susceptible to attacks. Compromising these APIs poses significant threats to both your cloud resources and data.\nInsufficient Due Diligence: Insufficient due diligence heightens cybersecurity risks. Businesses occasionally migrate data to the cloud without a comprehensive understanding of the implications, their responsibilities, and the security measures implemented by cloud providers. This lack of awareness can jeopardize the overall strategy for cloud data management.\nWhat does the future hold for data management in cloud computing? Data management has swiftly progressed from outdated, locally hosted storage systems to a more adaptable and dependable cloud data management paradigm. While local data storage prevailed for a considerable period, this inclination is shifting as businesses recognize advancements in cloud storage technology.\nIn the coming years, an escalating number of companies will embark on digital transformation journeys, choosing the cloud as their primary approach to data management. The significance of data in ensuring organizational competitiveness is on the rise. This forecast underscores the imperative for establishing and sustaining an effective data management framework that enables companies to match the pace of a dynamic and ever-changing business environment.\n**What are some use cases of data management in cloud computing? **\n**Cloud data management finds numerous applications for businesses. Typical use cases encompass:Deployment: Simplifying the provisioning of test and development environments, cloud data management allows for the effortless creation of test environments using production data sets.**Sharing data between multi-cloud environments: Facilitating seamless data sharing among multiple cloud applications, cloud data management ensures that a single data set serves as a unified source of truth, eliminating the need for each app to rely on its isolated data set.\nData backup and recovery: Providing a reliable and flexible solution for data backup and recovery, cloud data management allows organizations to securely store their data in the cloud. This ensures data security and enables swift recovery in case of data loss or system failures.\nData governance and compliance: Enabling enterprises to address data governance and compliance requirements, cloud data management aids in establishing data governance frameworks, enforcing data security and privacy rules, and ensuring compliance.\nLong-term storage and data archiving: For data archiving and long-term storage, cloud storage offers a cost-effective solution. Organizations can leverage the cloud to store infrequently accessed data, reducing storage expenses while ensuring data durability and availability.\nHow does Utho facilitate the analysis and management of cloud storage? Utho, as a holistic cloud solutions provider, empowers businesses to streamline operations and reduce costs through effective cloud utilization. Our services cover key areas such as cloud migration, cloud data management, security, and optimization. Our portfolio provides access to a comprehensive set of data management offerings, including backup and recovery, disaster recovery, archiving, file and object services, data governance, and security. All these services are seamlessly integrated into a user-friendly, consolidated environment for your convenience.","what-is-data-management-in-cloud-computing-technology#\u003cstrong\u003eWhat is data management in Cloud Computing technology?\u003c/strong\u003e":""},"title":"The Future Landscape of Data Storage: Cloud Data Management"},"/utho-docs/docs/blog/the-future-of-business-apps-embracing-cloud-based-apis/":{"data":{"":"","cross-platform-api-and-cloud-providers#\u003cstrong\u003eCross-platform API and Cloud Providers:\u003c/strong\u003e":"","how-do-you-incorporate-an-api-into-your-cloud-infrastructure#\u003cstrong\u003eHow do you incorporate an API into your cloud infrastructure?\u003c/strong\u003e":"","what-are-the-advantages-and-benefits-associated-with-the-utilization-of-apis#\u003cstrong\u003eWhat are the advantages and benefits associated with the utilization of APIs?\u003c/strong\u003e":"","what-lies-ahead-for-the-future-of-cloud-application-programming-interfaces-apis#\u003cstrong\u003eWhat lies ahead for the future of Cloud Application Programming Interfaces (APIs)?\u003c/strong\u003e":"","what-measures-does-utho-employ-to-ensure-the-security-of-cloud-apis#\u003cstrong\u003eWhat measures does Utho employ to ensure the security of cloud APIs?\u003c/strong\u003e":"Cloud APIs, short for Application Programming Interface, operates on a set of definitions and protocols designed to integrate systems and streamline communication between software applications through a set of rules.\nWhat‚Äôs the essence of Cloud API? A Cloud API, short for Cloud Application Programming Interface, facilitates the creation of applications and services for managing cloud hardware, software, and platforms. It empowers administrators to seamlessly integrate applications and workloads into the cloud environment.\nThe evolution of the cloud API landscape, designed to enrich the cloud experience and ensure compatibility, empowers administrators to seamlessly integrate applications and diverse workloads into the cloud environment.\nHow do you incorporate an API into your cloud infrastructure? There are various methods to integrate with an infrastructure, each with its own underlying components. Cloud computing integration with another platform (or even another cloud provider) typically involves four main areas.\nPaaS API: These service APIs, also referred to as Platform as a Service (PaaS), offer access and functionality within a cloud environment. This includes integration into databases, messaging systems, portals, and even storage components.\nSaaS API: Also recognized as Software-as-a-Service (SaaS) APIs, these are designed to facilitate the connection between the application layer and both the cloud and the underlying IT infrastructure.\nIaaS API: These APIs, often called Infrastructure as a Service (IaaS), help oversee cloud resources. They can quickly add or remove resources, and are handy for tasks like managing networks and workloads (VM).\nCross-platform API and Cloud Providers: In today‚Äôs setups, we often use different cloud providers and platforms. There‚Äôs a growing need for compatibility across platforms. Providers now offer easy HTTP and HTTPS API integration, making the cloud experience better. Cross-platform APIs let you access resources not just from your main provider but from others too, saving time and effort in development.\nWhat are the advantages and benefits associated with the utilization of APIs? APIs drive growth for data-driven businesses, from customer outreach to social collaboration tools. Here are some primary advantages.\nBuild, deploy, and scale cloud services automatically.\nEnhance service flexibility.\nSimplify content and application integration.\nPersonalize content and services for users.\nStreamline automatic sharing and publishing.\nWhat lies ahead for the future of Cloud Application Programming Interfaces (APIs)? Cloud APIs will keep growing as more companies seek efficient ways to connect their environments. Secure and versatile, they help businesses expand by distributing data and managing resources across different platforms, ensuring greater cloud elasticity.\nWhat measures does Utho employ to ensure the security of cloud APIs? APIs are susceptible to a range of attacks. Safeguarding APIs from misuse demands a multi-layered defense capable of thwarting, identifying, and mitigating incoming attacks. Utho API Gateway aids organizations in uncovering and cataloging shadow APIs, blocking API data exfiltration, and safeguarding APIs from both external and internal threats.","whats-the-essence-of-cloud-api#\u003cstrong\u003eWhat\u0026rsquo;s the essence of Cloud API?\u003c/strong\u003e":""},"title":"The Future of Business Apps: Embracing Cloud-Based APIs"},"/utho-docs/docs/blog/the-future-of-cloud-server-management/":{"data":{"":"","benefits-of-predictive-analytics-and-automation-for-cloud-server-management#\u003cstrong\u003eBenefits of Predictive Analytics and Automation for Cloud Server Management\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nThe Future of Cloud Server Management: Predictive Analytics and Automation Cloud server management is a critical component of modern IT infrastructure, and the industry is evolving at a rapid pace. The future of cloud server management looks promising with the integration of predictive analytics and automation. In this article, we will discuss the latest trends in cloud server management and how predictive analytics and automation are transforming the way businesses manage their cloud infrastructure.\nThe Need for Predictive Analytics and Automation in Cloud Server Management Managing cloud servers manually can be a time-consuming and error-prone process. Traditional approaches to cloud server management struggle to keep up with the scale and speed of modern cloud infrastructure. Predictive analytics and automation can address these challenges by providing real-time insights and automating routine tasks. Predictive analytics can anticipate potential issues and proactively take corrective actions before they impact the cloud infrastructure. Automation can reduce manual efforts and improve efficiency, enabling businesses to focus on higher-value tasks.\nTrends in Predictive Analytics and Automation for Cloud Server Management Machine Learning: Machine learning algorithms can analyze large amounts of data to identify patterns and predict future outcomes. Machine learning can be used to optimize resource allocation, detect anomalies, and provide real-time insights into cloud server performance.\nAutomation: Automation tools can automate routine tasks such as server provisioning, configuration management, and security updates, reducing manual efforts and improving efficiency.\nSelf-Healing Systems: Self-healing systems can detect and resolve issues automatically, ensuring high availability and minimal downtime.\nInfrastructure as Code: Infrastructure as code enables cloud infrastructure to be treated as a software application, allowing businesses to automate the deployment and management of their cloud infrastructure.\nBenefits of Predictive Analytics and Automation for Cloud Server Management Improved Efficiency: Predictive analytics and automation can significantly improve the efficiency of cloud server management by reducing manual efforts and streamlining routine tasks.\nIncreased Scalability: Predictive analytics and automation can enable businesses to scale their cloud infrastructure quickly and efficiently, ensuring that their infrastructure can handle growing workloads.\nEnhanced Security: Predictive analytics and automation can enhance security by automating security updates, monitoring vulnerabilities, and detecting anomalies.\nCost Savings: Predictive analytics and automation can help businesses optimize resource allocation and reduce wastage, leading to cost savings.\nManaged Cloud Services for Predictive Analytics and Automation Managed cloud services such as Microhost provide expert support and management of cloud infrastructure, enabling businesses to leverage the latest trends in predictive analytics and automation for optimal performance and efficiency. Microhost‚Äôs managed cloud services include automated server management, security updates, and performance optimization, ensuring that your cloud infrastructure is always up-to-date and performing at its best.\nConclusion Cloud server management is an essential part of modern IT infrastructure, and the integration of predictive analytics and automation is transforming the industry. By leveraging the latest trends in cloud server management, businesses can improve efficiency, scalability, security, and cost savings. For expert-managed cloud services, consider Microhost, a leading provider of cloud hosting services in India. With Microhost‚Äôs managed cloud services, businesses can leverage the power of predictive analytics and automation for optimal performance and efficiency of their cloud infrastructure.\nVisit: Best Cloud Server","managed-cloud-services-for-predictive-analytics-and-automation#\u003cstrong\u003eManaged Cloud Services for Predictive Analytics and Automation\u003c/strong\u003e":"","the-future-of-cloud-server-management-predictive-analytics-and-automation#\u003cstrong\u003eThe Future of Cloud Server Management: Predictive Analytics and Automation\u003c/strong\u003e":"","the-need-for-predictive-analytics-and-automation-in-cloud-server-management#\u003cstrong\u003eThe Need for Predictive Analytics and Automation in Cloud Server Management\u003c/strong\u003e":"","trends-in-predictive-analytics-and-automation-for-cloud-server-management#\u003cstrong\u003eTrends in Predictive Analytics and Automation for Cloud Server Management\u003c/strong\u003e":""},"title":"The Future of Cloud Server Management"},"/utho-docs/docs/blog/the-future-of-kubernetes-what-to-expect-in-2023/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion:\u003c/strong\u003e¬†":"As one of the most popular and widely-used container orchestration platforms, Kubernetes has a bright future ahead. In 2023, we can expect to see a number of exciting developments and trends in the world of Kubernetes. Some of the most promising predictions include an increased focus on cloud-native application development, the adoption of serverless computing, and the integration of artificial intelligence and machine learning into Kubernetes environments.\nAdditionally, we may see an expansion of the Kubernetes ecosystem, with the introduction of new tools and services that make it even easier for organizations to deploy and manage their containerized applications. Overall, the future of Kubernetes looks bright, and we can expect to see continued growth and innovation in the years ahead.\nKubernetes is an open source container platform designed to automate deployment, scaling, and management of containerized applications.\nIt has quickly become one of the most popular DevOps tools on the market today due to its ability to enable organizations to quickly deploy and manage complex workloads across multiple cloud providers with relative ease. As such, it has become a powerful tool for businesses looking to modernize their IT infrastructure.¬†Increasingly enterprise-grade features are expected to be a trend in Kubernetes‚Äô future**.** This includes things like integration with identity providers (such as Okta) for authentication, improved security measures (such as role-based access control), better scalability options (including multi-cluster deployments), and more robust monitoring capabilities (including metric collection). All of these features will be essential for companies wanting to take advantage of Kubernetes in 2023 and beyond.¬†Another trend that is expected over the next few years is an increase in adoption by smaller businesses and startups. While this technology has traditionally been used by large enterprises due to its complexity, recent advances have made it more accessible than ever before. Companies are now taking advantage of managed services such as Microhost, which make it easier than ever before for businesses of all sizes to take advantage of this powerful technology without needing extensive technical expertise. This increased accessibility will likely lead to increased adoption amongst these types of organizations over time.¬†Conclusion:¬†Kubernetes has already had a significant impact on the way organizations manage their IT infrastructure since its introduction just a few years ago. As we look ahead towards 2023, we can expect this technology to only continue growing in popularity with advances such as enterprise-grade features becoming available and more small businesses taking advantage of managed services that make adoption easier than ever before.\nKeeping an eye out for these trends will help you prepare for using Kubernetes within your organization in the next few years so that you can be sure you are leveraging this powerful technology effectively and efficiently when the time comes!"},"title":"The Future of Kubernetes: What to Expect in 2023¬†?"},"/utho-docs/docs/blog/the-impact-of-cloud-server-downtime-on-business-operations/":{"data":{"":"","the-impact-of-cloud-server-downtime-on-business-operations-mitigation-strategies-and-best-practices#\u003cstrong\u003eThe Impact of Cloud Server Downtime on Business Operations: Mitigation Strategies and Best Practices\u003c/strong\u003e":"\nThe Impact of Cloud Server Downtime on Business Operations: Mitigation Strategies and Best Practices In today‚Äôs world, businesses are relying more and more on technology for their operations. Cloud servers have become an essential part of business infrastructure, providing a reliable and cost-effective solution for data storage and application hosting. However, with the benefits of cloud computing come the risks of cloud server downtime, which can have a significant impact on business operations. In this article, we will discuss the impact of cloud server downtime on business operations and provide mitigation strategies and best practices to prevent or minimize its effects.\nWhat is Cloud Server Downtime? Cloud server downtime is the period during which a cloud server is not accessible to its users. This can happen due to various reasons, such as hardware failure, network issues, software bugs, human error, or cyber-attacks. Cloud server downtime can cause severe disruptions to business operations, resulting in lost revenue, damaged reputation, and decreased productivity.\nImpact of Cloud Server Downtime on Business Operations Lost Revenue: Downtime can lead to lost sales, missed opportunities, and dissatisfied customers. In a highly competitive market, even a few hours of downtime can cause significant revenue loss.\nDamage to Reputation: Customers expect businesses to be available 24/7, and any disruption to their services can damage their reputation. This can result in customer churn, negative reviews, and reduced trust in the brand.\nDecreased Productivity: Employees may be unable to access critical data or applications, resulting in delays and decreased productivity. Downtime can also cause stress and frustration among employees, leading to a demotivated workforce.\nMitigation Strategies and Best Practices Regular Maintenance: Regular maintenance of cloud servers can prevent hardware failure and ensure software is up-to-date. This includes regular backups, security patches, and monitoring for potential issues.\nDisaster Recovery Plan: A disaster recovery plan outlines the steps to take in case of downtime. This includes backup and recovery procedures, testing, and regular updates.\nRedundancy and Failover: Redundancy and failover systems ensure that if one server fails, another can take over seamlessly. This reduces the risk of downtime and ensures business continuity.\nMonitoring and Alerting: Monitoring tools can identify potential issues before they occur and alert IT teams to take action. This includes real-time monitoring of server performance, network connectivity, and security threats.\nCloud Provider Selection: Choosing a reliable cloud provider with a proven track record of uptime and customer support can minimize the risk of downtime. This includes evaluating service level agreements (SLAs), customer reviews, and support options.\nConclusion Cloud server downtime can have a severe impact on business operations, leading to lost revenue, damaged reputation, and decreased productivity. Mitigation strategies and best practices can minimize the risks of downtime and ensure business continuity. Regular maintenance, disaster recovery planning, redundancy and failover, monitoring and alerting, and cloud provider selection are essential components of a robust cloud infrastructure. By implementing these strategies, businesses can minimize the risks of cloud server downtime and ensure the smooth operation of their operations.\nAbout Microhost Microhost is a leading provider of cloud hosting solutions in India. With over ten years of experience in the industry, they offer reliable and cost-effective cloud hosting services for businesses of all sizes. Their services include cloud servers, dedicated servers, VPS hosting, and domain registration. Microhost has a proven track record of uptime and customer support, making them an excellent choice for businesses looking for a reliable cloud hosting provider."},"title":"The Impact of Cloud Server Downtime on Business Operations"},"/utho-docs/docs/blog/the-importance-of-hosting-your-website-on-a-cloud-server/":{"data":{"":"","1-scalability#\u003cstrong\u003e1. Scalability\u003c/strong\u003e":"","2-reliability#\u003cstrong\u003e2. Reliability\u003c/strong\u003e":"","3-security#\u003cstrong\u003e3. Security\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion:\u003c/strong\u003e¬†":"Your website is one of the most important parts of your business. It‚Äôs how you connect with customers, promote your products or services, and make a good impression. That‚Äôs why it‚Äôs so important to make sure your website is always up and running‚Äîand one of the best ways to do that is by hosting it on a cloud server.\nWhat is Cloud Server ?\nA powerful physical or virtual infrastructure that handles application- and information-processing storage is referred to as a cloud server. Virtualization software is used to split up a physical (bare metal) server into numerous virtual servers in order to construct cloud servers.\nHere are 3 Main benefits of hosting your website on a cloud server:¬†3 Main benefits of hosting your website on a cloud server\n1. Scalability One of the biggest advantages of cloud hosting is that it‚Äôs scalable. This means that you can easily increase or decrease your resources as needed, without having to go through the hassle and expense of reconfiguring your server. This is perfect for businesses that are growing quickly and need to be able to scale their resources up or down as needed.\n2. Reliability Cloud servers are also more reliable than traditional servers. This is because they‚Äôre spread out across multiple physical locations, which means that if one server goes down, your website will still be accessible from other servers.¬†3. Security Another benefit of hosting your website on a cloud server is that it‚Äôs more secure. This is because cloud servers are often located in data centers with state-of-the-art security measures, like biometric scanners and CCTV cameras.¬†Conclusion:¬†As you can see, there are many benefits to hosting your website on a cloud server. If you‚Äôre looking for a reliable, scalable, and secure solution for your website, then cloud hosting is the way to go! There is no doubt that Microhost offers the best cloud hosting services. The best part is that you can try it without hesitation for 7 days."},"title":"The Importance of Hosting Your Website on a Cloud Server"},"/utho-docs/docs/blog/the-pros-and-cons-of-multi-cloud-server-strategies/":{"data":{"":"","the-pros-and-cons-of-multi-cloud-server-strategies-is-it-worth-the-complexity#\u003cstrong\u003eThe Pros and Cons of Multi-Cloud Server Strategies: Is it Worth the Complexity?\u003c/strong\u003e":"\nThe Pros and Cons of Multi-Cloud Server Strategies: Is it Worth the Complexity? Multi-cloud computing is a popular approach to managing IT infrastructure where organizations use multiple cloud providers to host their applications, data, and services. While multi-cloud strategies offer many benefits, such as flexibility, reliability, and cost optimization, they also introduce complexities and challenges.\nThe Pros of Multi-Cloud Server strategy: Cloud Provider Selection: With a multi-cloud approach, you can select the best cloud service providers that meet all your specific needs.\nCloud Redundancy: Multi-cloud strategies offer greater redundancy and uptime, as you can distribute your applications and data across multiple cloud providers.\nCloud Security: Multi-cloud approaches can improve your security posture, as you can implement different security controls and measures across different cloud providers.\nCloud Cost Optimization: Multi-cloud strategies can help you optimize your cloud costs by using the most cost-effective providers for your workloads.\nThe Cons of Multi-Cloud Server strategy: Cloud Management Costs: Multi-cloud approaches increase management overhead and complexity, requiring specialized skills and expertise.\nCloud Integration: Multi-cloud strategies require effective integration between your different cloud providers, which can be challenging due to differences in APIs, data formats, and infrastructure.\nCloud Governance and Compliance: Multi-cloud strategies can create governance and compliance challenges, as you need to ensure that your data and services comply with different regulatory requirements and standards across different cloud providers.\nCloud Reliability: Multi-cloud approaches can introduce new points of failure and complexity, impacting the reliability of your services.\nIs it Worth the Complexity? Deciding whether a multi-cloud approach is worth the complexity depends on your organization‚Äôs specific needs and goals. Before adopting a multi-cloud approach, consider the following factors:\nWorkload Requirements: Determine the specific requirements of your workloads, such as performance, scalability, and security. Evaluate whether a multi-cloud approach can help you meet these requirements more effectively than a single-cloud approach.\nProvider Selection: Research and evaluate different cloud service providers based on your specific needs and goals.\nManagement and Integration: Determine whether your organization has the necessary skills, expertise, and tools to manage and integrate multiple cloud providers effectively.\nGovernance and Compliance: Evaluate whether a multi-cloud approach can help you meet your governance and compliance requirements more effectively than a single-cloud approach.\nRisk Tolerance: Evaluate your organization‚Äôs risk tolerance and determine whether a multi-cloud approach aligns with your risk management strategy.\nConclusion Overall, a multi-cloud approach can be beneficial for organizations that require high flexibility, redundancy, and cost optimization, but it can also introduce additional complexity and costs. Before adopting a multi-cloud strategy, it‚Äôs important to evaluate your organization‚Äôs specific needs, goals, and capabilities, and carefully manage the complexities and challenges that come with it.\nIf you‚Äôre looking for a cloud service provider that offers multi-cloud solutions, consider Microhost. They provide a range of cloud services and support multiple cloud providers, such as AWS, Azure, and Google Cloud. With Microhost, you can leverage the benefits of multi-cloud strategies while minimizing the complexities and risks. Visit their website at https://utho.com/ to learn more about their services and how they can help you achieve your cloud goals."},"title":"The Pros and Cons of Multi-Cloud Server strategy"},"/utho-docs/docs/blog/the-role-of-cloud-servers-in-edge-computing/":{"data":{"":"","challenges-of-cloud-servers-in-edge-computing#\u003cstrong\u003eChallenges of Cloud Servers in Edge Computing\u003c/strong\u003e":"\nEdge computing is a paradigm shift in the way we process and store data. Rather than sending all data to centralized cloud servers, edge computing distributes data processing and storage closer to where it is generated, allowing for faster and more efficient data processing. However, edge computing also presents challenges, including the need for reliable and scalable cloud servers to support the distributed computing infrastructure. In this article, we will explore the role of cloud servers in edge computing, including the opportunities and challenges they present.\nOpportunities of Cloud Servers in Edge Computing Scalability: Cloud servers provide the scalability needed to support the dynamic and distributed nature of edge computing. By using cloud servers, organizations can quickly add or remove computing resources as needed, making it easier to scale up or down based on changing demands.\nCost Efficiency: Cloud servers offer cost efficiency by allowing organizations to pay only for the resources they use, rather than investing in expensive hardware that may not be fully utilized. This can be especially beneficial for smaller organizations or those with limited budgets.\nReliability: Cloud servers offer high reliability and availability, ensuring that edge computing applications are always up and running. With built-in redundancy and failover mechanisms, cloud servers can provide a high level of reliability and availability that would be difficult to achieve with traditional on-premise infrastructure.\nChallenges of Cloud Servers in Edge Computing Latency: Cloud servers may introduce latency in edge computing applications, as data needs to be sent to and from the cloud servers for processing. This can be especially problematic for real-time applications that require low latency, such as those used in the Internet of Things (IoT) and autonomous vehicles.\nSecurity: Cloud servers may introduce security risks, as data needs to be transmitted and stored in multiple locations, increasing the potential for data breaches. Organizations need to ensure that their cloud servers are secured and comply with applicable regulations and standards.\nComplexity: Edge computing and cloud server environments can be complex, requiring specialized knowledge and expertise to manage effectively. Organizations need to ensure that they have the right skills and tools to manage their edge computing infrastructure, including cloud servers.","conclusion#Conclusion":"Overall, cloud servers play a critical role in supporting edge computing, providing the scalability, cost efficiency, and reliability needed to power distributed computing infrastructure. However, they also present challenges, including latency, security, and complexity. Organizations need to carefully evaluate their edge computing needs and choose the right cloud server solutions to meet their requirements.\nMicrohost If you‚Äôre looking for a cloud service provider that offers reliable and scalable cloud servers for edge computing, consider Microhost. We provide a range of cloud services, including public cloud, private cloud, and hybrid cloud, with built-in redundancy and failover mechanisms to ensure high availability and reliability. With Microhost, you can take advantage of the opportunities of edge computing while minimizing the challenges and risks. Visit our website at https://utho.com/ to learn more about our services and how we can help you achieve your edge computing goals.","opportunities-of-cloud-servers-in-edge-computing#\u003cstrong\u003eOpportunities of Cloud Servers in Edge Computing\u003c/strong\u003e":""},"title":"The Role of Cloud Servers in Edge Computing"},"/utho-docs/docs/blog/top-05-cloud-security-threats-in-2023-and-proven-strategies-to-mitigate-them/":{"data":{"":"","1-malware-attacks#\u003cstrong\u003e1 Malware Attacks.\u003c/strong\u003e¬†":"","2-ato-attacks#\u003cstrong\u003e2. ATO Attacks.\u003c/strong\u003e":"","3-data-breaches#\u003cstrong\u003e3. Data Breaches\u003c/strong\u003e.":"","4-cloud-service-misconfigurations#\u003cstrong\u003e4. Cloud Service Misconfigurations\u003c/strong\u003e.":"","5-insider-threats#\u003cstrong\u003e5. Insider Threats.\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion:\u003c/strong\u003e":"With the rapid advances in cloud Technology, businesses now have access to a wide range of data and applications. Unfortunately, this also means that cyber criminals have more opportunities than ever before to access sensitive information. As technology continues to evolve, it is important for IT managers, small and medium-sized businesses to stay up to date on the latest cloud security threats and how they can be mitigated. In this blog post, I¬†will provide an overview of the top five cloud security threats in 2023 and proven strategies you can use to tackle them.\nTop 05 Cloud Security Threats in 2023 and Proven Strategies to Mitigate Them\n1 Malware Attacks.¬†Malware attacks are another potential cloud security threat that organizations may face in 2023. Malware, short for malicious software, can take various forms, such as viruses, worms, trojans, and ransomware, and it can be used to steal sensitive information, disrupt business operations, and even hold data hostage.\nSolution To mitigate the risks of malware attacks, organizations can implement the following strategies:\nKeep all software and systems updated with the latest security patches.\nUse anti-malware software to detect and remove malware from computers and servers.\nUse endpoint security software to block malware and unauthorized software from running on user devices.\nImplement email and web filtering to block malicious links and attachments.\nTrain employees on how to identify and avoid phishing and social engineering attacks, which are common vectors for delivering malware.\nRegularly back up important data and maintain offline copies, so that in the event of a ransomware attack, the organization can restore its data without paying the ransom.\nIt‚Äôs important to remember that malware attacks can happen through various vectors and can often evade standard protection, to keep your organization secure, you must continuously monitor your systems and be ready to respond to an incident when it happens.\n2. ATO Attacks. Account takeovers (ATO) are a type of cyber attack where an attacker gains unauthorized access to a user‚Äôs account by obtaining their login credentials, such as a username and password. Once an attacker has taken over an account, they can use it to steal personal information, make unauthorized purchases, or even cause damage to the user‚Äôs reputation.\nSolution Here are some steps that can help prevent account takeovers:\nUse strong and unique passwords for all accounts. Enable two-factor authentication (2FA) or multi-factor authentication (MFA) wherever it is available.\n2FA/MFA adds an extra layer of security, by requiring a second form of authentication such as a fingerprint, or a code sent via SMS or email.\nBe wary of suspicious or unsolicited emails or phone calls that ask for personal information or login credentials.\nKeep your computer, browser and mobile device security software updated.\nKeep an eye on your account activity, and look for unusual login attempts, or unauthorized changes to personal information.\nBe careful where you share your personal information online, especially on social media platforms.\nATO attacks can cause great damage to your personal or professional reputation and your financial losses. By taking the appropriate preventive measures, you can help protect yourself from ATO attacks and prevent unauthorized access to your accounts.\n3. Data Breaches. A data breach is a security incident in which sensitive, confidential, or protected information is accessed, disclosed, or stolen by an unauthorized individual or organization. Data breaches can happen due to a variety of reasons, such as weak passwords, poor access controls, unpatched software vulnerabilities, and phishing attacks.\nSolution Here are some steps that can help prevent data breaches:\nImplement robust access controls, including user authentication and authorization to limit who can access sensitive data.\nUse encryption to protect data both in transit and at rest. This can help ensure that even if a breach does occur, the data will be unreadable to the attacker.\nKeep all software and systems updated with the latest security updates. This can help prevent attackers from exploiting known vulnerabilities.\nTrain employees on security best practices and how to identify and report suspicious activity.\nRegularly monitor your systems and network for unusual activity or signs of compromise\nHave an incident response plan in place and test it regularly, including regular vulnerability assessments, penetration testing and incident management\nIt is important to note that even with all the appropriate security measures in place, breaches can still happen. By preparing yourself with a proactive and comprehensive approach you can minimize the impact of a breach on your organization and its customers.\n4. Cloud Service Misconfigurations. Cloud service misconfigurations are a common security threat in which cloud environments are not properly configured, leaving them vulnerable to attack. This can happen when organizations fail to properly set up security controls, or when changes are made to the cloud environment without proper testing or oversight.\nSolution Here are some steps organizations can take to prevent cloud service misconfigurations:\nImplement cloud security best practices: Use industry standard frameworks such as CIS or NIST to establish security baselines for your cloud environment.\nUse cloud security tools to automatically detect and fix misconfigurations. This can help you quickly identify and remediate issues before they become a security risk.\nUse a least-privilege model: Limit access to cloud resources to only the users and roles that require it.\nUse a change management process: Establish a process for making changes to your cloud environment, which includes testing, approval, and rollback procedures.\nRegularly review your cloud environment for misconfigurations and vulnerabilities.\nUse security groups, virtual private clouds (VPCs), and network segmentation to create a secure and controlled environment in your cloud.\nCloud service misconfigurations are a serious concern, they can lead to data breaches, unauthorized access, or other security incidents. It is important to be proactive in identifying and addressing cloud service misconfigurations, as well as maintaining a secure and compliant cloud environment.\n5. Insider Threats. Insider threats refer to malicious or unintentional actions by employees or contractors with access to an organization‚Äôs sensitive data and systems. Insider threats can be caused by various factors such as disgruntled or financially motivated employees, accidental data leaks or lack of security awareness.\nSolution Here are some steps organizations can take to prevent insider threats:\nImplement strict access controls: Limit access to sensitive data and systems to only those who need it, and use multi-factor authentication (MFA) whenever possible.\nMonitor user activity: Use monitoring tools to track user activity on your network, looking for unusual or suspicious behavior.\nImplement security awareness training: Regularly educate and train employees on security best practices, the risks of insider threats, and the importance of proper data handling.\nRegularly review access permissions: Ensure that employee access is continuously reviewed and that permissions are removed as soon as they are no longer needed.\nKeep track of who is accessing sensitive data: Understand who is accessing your data and when.\nUse Data Loss Prevention (DLP) systems to monitor and protect sensitive data from accidental or malicious exfiltration\nInsider threats can be difficult to detect and prevent, as the individuals involved already have authorized access to your organization‚Äôs sensitive data and systems. It is important to be vigilant and proactively mitigate the risk by implementing security controls, monitoring user activity, and providing regular security awareness training to employees.\nConclusion: In conclusion, cloud security is a critical concern for organizations of all sizes, as more and more companies are moving their operations and data to the cloud. The above-mentioned security threats, such as malware attacks, account takeovers, data breaches, cloud service misconfigurations, and insider threats, are just a few examples of the potential risks that organizations may face when using cloud services.\nTo mitigate these risks, organizations must implement a comprehensive security strategy that includes robust access controls, encryption, security awareness training, monitoring, and incident response planning. Organizations must also stay informed about the latest security threats and best practices for cloud security.","solution#\u003cstrong\u003eSolution\u003c/strong\u003e":"","solution-1#\u003cstrong\u003eSolution\u003c/strong\u003e":"","solution-2#\u003cstrong\u003eSolution\u003c/strong\u003e":"","solution-3#\u003cstrong\u003eSolution\u003c/strong\u003e":"","solution-4#\u003cstrong\u003eSolution\u003c/strong\u003e":""},"title":"Top 05 Cloud Security Threats in 2023 and Proven Strategies to Mitigate Them"},"/utho-docs/docs/blog/top-10-factors-to-consider-when-choosing-a-cloud-server-provider/":{"data":{"":"Choosing a cloud server provider can be overwhelming, especially if you‚Äôre new to cloud hosting. However, it‚Äôs important to choose the right provider to ensure your business runs smoothly.","top-10-factors-to-consider#Top 10 factors to consider":"1. Reliability: The provider should have a track record of providing reliable services with minimal downtime. Look for a provider that offers a Service Level Agreement (SLA) with a guaranteed uptime percentage.\n2. Security: Security is crucial for any online business. Look for a provider that offers robust security measures such as firewalls, intrusion detection systems, and data encryption. They should also have protocols in place to protect against data breaches and unauthorized access.\n3. Scalability: Scalability is important as your website traffic or workload can increase unexpectedly. Look for a provider that offers scalable solutions, allowing you to expand your resources as needed without downtime.\n4. Pricing: Pricing is important for any business. Look for a provider that offers transparent pricing with no hidden fees. Compare pricing plans and features across different providers to ensure you‚Äôre getting the best value for your money.\n5. Support: Support is critical when it comes to cloud server providers. Your provider should offer 24/7 support with a knowledgeable team available to address any issues that may arise.\n6. Flexibility: Flexibility is important as your business needs may change over time. Look for a provider that offers customizable plans that can be tailored to your specific needs. They should allow you to add or remove resources as needed.\n7. Features: Look for a provider that offers features such as load balancing, auto-scaling, and backup and disaster recovery options. These features can help optimize your website‚Äôs performance and improve user experience.\n8. Compliance: Depending on your industry, you may need to comply with specific regulations such as HIPAA, GDPR, or PCI-DSS. Look for a provider that is compliant with the regulations that apply to your business.\n9. Reputation: Reputation is important when choosing a cloud server provider. Look for a provider with a good reputation in the industry and positive customer reviews.\n10. Migration Support: If you‚Äôre switching from another provider, ensure that the new provider offers migration support to minimize downtime and ensure a smooth transition.\nIn conclusion, choosing the right cloud server provider is important for the success of your business. At Microhost, we offer reliable, scalable, and flexible cloud server solutions backed by exceptional support. Contact us today to learn more about our cloud server services and how we can help your business grow.\nVisit here for best price: Best Cloud Server Hosting - Microhost"},"title":"Top 10 Factors to Consider When Choosing a Cloud Server Provider"},"/utho-docs/docs/blog/unleash-the-magic-of-vpn-in-cloud-security/":{"data":{"":"","how-can-utho-ensure-the-security-of-your-data-through-virtual-private-network#\u003cstrong\u003eHow can Utho ensure the security of your data through Virtual Private Network?\u003c/strong\u003e":"In today‚Äôs digital world, where we use the internet for work and important tasks, it‚Äôs crucial to keep our data safe from cyber threats. An effective approach to achieve this is through the use of a VPN. In this blog post, we‚Äôll simplify the concept of a Virtual Private Network and illustrate how it plays a crucial role in safeguarding our information.\nWhat does the term Virtual Private Network mean? A Virtual Private Network (VPN) is a secure, encrypted connection established over the Internet between a device and a network. This encryption safeguards the transmission of sensitive data, preventing unauthorized individuals from intercepting the traffic. VPNs enable users to work remotely while maintaining the confidentiality of their communication. This technology is extensively employed in corporate settings.\nWhat is the functioning mechanism of Virtual Private Network? VPNs operate by directing a device‚Äôs internet connection through a specifically configured remote server network managed by the VPN service. As a result, all data transmitted through a VPN connection is not only encrypted but also concealed behind a virtual IP address, providing the ability to safeguard your identity and location.\nWhat are the advantages of using this private network connection? VPNs offer numerous advantages for organizations seeking to facilitate remote work and connectivity for various sites, such as:\nSecure Connectivity: VPNs establish an encrypted link between a remote user and the enterprise network, guarding against eavesdropping and minimizing the likelihood of the remote user contracting malware.\nSimplified Distributed Networks: Whether it‚Äôs a remote worker or a branch site, a user maintains an encrypted connection with the headquarters network through VPN. These connections offer a user experience closely resembling a direct connection to the headquarters network, simplifying the design and implementation of distributed networks.\nAccess Control: Prior to accessing resources on the corporate network, authentication is mandatory for a VPN user. This measure serves to safeguard against unauthorized access to corporate assets.\nData Throttling Prevention: By restricting outsiders‚Äô visibility into the data transmitted through the encrypted channel, a VPN aids in protecting against the throttling of specific types of network traffic.\nNetwork Scalability: VPNs allow organizations to seamlessly connect dispersed networks over the public Internet through encrypted channels. This facilitates the effortless scaling of the network while treating it as a unified, private network.\nWhat are the Disadvantages of using this private network connection? The appropriate secure remote access solution offers substantial advantages to an organization, yet an ill-suited one can pose a significant liability. Several typical drawbacks of VPN solutions include:\nSlow Connection Speeds: Authentication and connection setup are essential for VPNs, involving interactions between the remote user and the VPN server on the corporate network. This can result in sluggish connection speeds, and frequent session timeouts may necessitate repeated authentication.\nComplicated Setup \u0026 Management: A VPN establishes a point-to-point connection connecting a remote user or site to the corporate network. This can lead to the development of an intricate network infrastructure that proves challenging to deploy, configure, and manage.\nPoor User Experience: Certain VPN solutions pose challenges in terms of configuration and may lack robust support on specific devices and operating systems. This results in a suboptimal user experience, potentially leading users to inadvertently violate corporate policies.\nSecurity Risks: A VPN is crafted to furnish a secure connection and unrestricted access to the enterprise network for a remote user. However, it lacks access controls or security inspection, leaving it without safeguards against compromised accounts, data exfiltration, malware, or other security risks.\nWhat are the different types of these private network connections? VPNs are designed to offer accessible security tailored for smaller-scale requirements. Here are a few instances of VPNs:\nCloud VPN: Deploying VPNs on virtual machines allows for ‚Äúcloud-enabling‚Äù them, leveraging the hardware capabilities of a VPN while artificially incorporating cloud functionality, such as enhanced scalability and endpoint protection. While these may prove more beneficial for extended enterprises compared to a traditional standalone VPN appliance, they might still lack the flexibility to adequately support a remote or hybrid workforce on a larger scale.\n**\nPersonal/Mobile VPN:** Organizations like ExpressVPN and NordVPN provide VPN apps that users can download to ensure the security of their personal devices. This is a prudent precaution, especially when browsing the web on unsecured Wi-Fi networks. While certain free VPNs are accessible for safeguarding your devices, it‚Äôs worth noting that they may transition to paid services over time.\nRemote access VPN: Tailored for users operating remotely in a corporate environment, these are usually implemented within a company‚Äôs data center. They can be extended, albeit at the expense of web and/or app performance, to shield remote users from malware and other threats. Their prevalence surged significantly in the wake of the COVID-19 pandemic.\nHow can Utho ensure the security of your data through Virtual Private Network? In today‚Äôs interconnected world, constant and widespread connectivity is essential. However, this hyperconnectivity introduces novel challenges concerning security, performance, resilience, and privacy. Utho addresses these challenges by assisting in connecting and safeguarding millions of customers globally. Whether it‚Äôs individuals or the world‚Äôs largest enterprises, our unified platform of network security tools, including VPN and VPC, empowers them to thrive in this ubiquitous environment.","what-are-the-advantages-of-using-this-private-network-connection#\u003cstrong\u003eWhat are the advantages of using this private network connection?\u003c/strong\u003e":"","what-are-the-different-types-of-these-private-network-connections#\u003cstrong\u003eWhat are the different types of these private network connections?\u003c/strong\u003e":"","what-are-the-disadvantages-of-using-this-private-network-connection#\u003cstrong\u003eWhat are the Disadvantages of using this private network connection?\u003c/strong\u003e":"","what-does-the-term-virtual-private-network-mean#\u003cstrong\u003eWhat does the term Virtual Private Network mean?\u003c/strong\u003e":"","what-is-the-functioning-mechanism-of-virtual-private-network#\u003cstrong\u003eWhat is the functioning mechanism of Virtual Private Network?\u003c/strong\u003e":""},"title":"Unleash the Magic of VPN in Cloud Security"},"/utho-docs/docs/blog/unleashing-the-power-of-artificial-intelligence-what-ai-can-do-with-utho-cloud/":{"data":{"":"","conclusion#Conclusion":"Artificial Intelligence is a game-changer that can revolutionize industries and transform the way we live and work. From automation and data analysis to personalization and natural language processing, AI‚Äôs capabilities are vast and diverse. By understanding and harnessing the power of AI, businesses can enhance efficiency, drive innovation, and deliver exceptional experiences to their customers. Embrace the potential of AI with Utho Cloud and unlock a future of limitless possibilities.\nRead Also: Can Artificial Intelligence Replace Teachers? The Future of Education with AI","the-versatility-of-artificial-intelligence#The Versatility of Artificial Intelligence":"","unlocking-ais-potential-with-utho-cloud#Unlocking AI\u0026rsquo;s Potential with Utho Cloud":"\nArtificial Intelligence (AI) is revolutionizing the way we live and work. This groundbreaking technology holds immense potential to transform industries and reshape our future. In this article, we will delve into the incredible capabilities of AI and explore the myriad of tasks it can accomplish. Join us as we uncover the possibilities of AI and discover how you can leverage its power with Utho Cloud, a leading AI education provider.\nThe Versatility of Artificial Intelligence Artificial Intelligence encompasses a wide range of applications that can have a profound impact on various sectors. Let‚Äôs explore some key areas where AI can make a significant difference:\nAutomation and Efficiency AI excels in automating repetitive and mundane tasks, freeing up human resources for more complex and creative endeavors. With machine learning algorithms and intelligent automation, AI can streamline processes, enhance productivity, and optimize resource allocation. From data entry and analysis to routine customer service interactions, AI-powered systems can handle these tasks efficiently, reducing errors and saving time.\nData Analysis and Insights The ability of AI to analyze vast amounts of data and derive valuable insights is unparalleled. AI algorithms can process and interpret complex data sets, identify patterns, and make predictions. This capability finds applications in diverse fields, such as finance, marketing, and healthcare. AI-powered analytics tools can help businesses make data-driven decisions, optimize strategies, and uncover hidden opportunities for growth.\nPersonalization and Recommendation Systems AI enables personalized experiences by understanding user preferences and delivering tailored recommendations. Online platforms, such as streaming services and e-commerce websites, leverage AI to analyze user behavior, interests, and previous interactions. This information is then used to provide customized content, product recommendations, and targeted advertisements. By leveraging AI‚Äôs personalization capabilities, businesses can enhance customer satisfaction and drive engagement.\nNatural Language Processing and Chatbots AI‚Äôs advancements in natural language processing have given rise to sophisticated chatbot systems. These AI-powered virtual assistants can understand and respond to human queries, providing instant support and information. Chatbots find applications in customer service, information retrieval, and even virtual companionship. By leveraging AI‚Äôs language processing capabilities, businesses can enhance customer interactions and improve overall user experiences.\nImage and Speech Recognition AI has made remarkable progress in image and speech recognition, enabling machines to understand and interpret visual and auditory data. The applications of AI in the field of image manipulation and editing are equally impressive. Tools like Picsart background changer utilize AI‚Äôs sophisticated image background remover capabilities. Using deep learning algorithms, these tools can identify foreground subjects and separate them from their background, providing users with more flexibility and control over their imagery. This technology is driving change across numerous sectors such as advertising, digital marketing, and social media, making it easier to create compelling visuals with just a few clicks.\nUnlocking AI‚Äôs Potential with Utho Cloud To tap into the full potential of AI and navigate this transformative landscape, education and skill development are crucial. Utho Cloud offers a wide range of AI courses and training programs designed to empower individuals and organizations. With experienced instructors, hands-on projects, and comprehensive resources, Utho Cloud equips you with the knowledge and skills needed to harness the power of AI effectively.\nDiscover Utho Cloud and explore our AI courses to embark on a transformative learning journey."},"title":"Unleashing the Power of Artificial Intelligence: What AI Can Do with Utho Cloud"},"/utho-docs/docs/blog/unlock-growth-opportunities-for-msps-a-collaboration-with-utho/":{"data":{"":"","a-smart-strategic-move-with-utho-partnership#\u003cstrong\u003eA Smart Strategic Move with Utho Partnership\u003c/strong\u003e":"","archive-storage#\u003cstrong\u003eArchive Storage\u003c/strong\u003e":"","backup-services#\u003cstrong\u003eBackup Services\u003c/strong\u003e":"","bare-metal-instances#\u003cstrong\u003eBare Metal Instances\u003c/strong\u003e":"","block-storage#\u003cstrong\u003eBlock Storage\u003c/strong\u003e":"","cloud-firewall#\u003cstrong\u003eCloud Firewall\u003c/strong\u003e":"","ddos-protection#\u003cstrong\u003eDDoS Protection\u003c/strong\u003e":"","dedicated-cpu-instances#\u003cstrong\u003eDedicated CPU Instances\u003c/strong\u003e":"","dns-manager#\u003cstrong\u003eDNS Manager\u003c/strong\u003e":"","driving-market-expansion-and-growth#\u003cstrong\u003eDriving Market Expansion and Growth\u003c/strong\u003e":"","exclusive-partnership-program#\u003cstrong\u003eExclusive Partnership Program\u003c/strong\u003e":"","exploring-the-benefits-of-collaborating-with-utho#\u003cstrong\u003eExploring the Benefits of Collaborating with Utho\u003c/strong\u003e":"","fostering-success-for-msps-through-collaborative-partnership#\u003cstrong\u003eFostering Success for MSPs through Collaborative Partnership\u003c/strong\u003e":"Today, more businesses are looking to managed service providers (MSPs) to make their IT operations smoother, improve efficiency, and spark innovation. Among the plethora of cloud service providers, Utho Cloud stands out as a robust platform offering a comprehensive suite of services tailored to meet the diverse needs of modern enterprises. Recognizing the immense potential for growth and innovation, MSPs are forging strategic partnerships with Utho Cloud to deliver unparalleled value to their clients.\nExploring the Benefits of Collaborating with Utho Partnering with Utho Cloud enables MSPs to harness the full potential of cutting-edge technologies, advanced analytics, and scalable infrastructure. By leveraging Utho‚Äôs extensive portfolio of cloud solutions, MSPs can offer their clients a wide array of services, including but not limited to.\nUtho Cloud offers a comprehensive suite of services tailored to meet various compute requirements, catering to diverse business needs. Let‚Äôs explore how Utho Cloud assists businesses in providing compute services across different configurations:\nDedicated CPU Instances Utho Cloud provides dedicated CPU instances where businesses can have exclusive access to physical CPU cores, ensuring consistent performance without contention from other users. This setup is ideal for applications requiring predictable performance levels, such as high-performance databases or latency-sensitive applications.\nShared CPU Instances For workloads with variable processing needs or where cost optimization is a priority, Utho Cloud offers shared CPU instances. These instances allocate CPU resources dynamically among multiple users, making them cost-effective for applications with sporadic usage patterns or lower compute requirements.\nHigh Memory Instances Some applications, especially data analytics, caching, or in-memory databases, demand large memory capacities to handle extensive datasets or perform complex computations. Utho Cloud offers high-memory instances enabling businesses to efficiently process and analyze large datasets without encountering memory constraints.\nGPU Instances For tasks requiring massive parallel processing power, such as machine learning, scientific simulations, or rendering, Utho Cloud provides GPU instances equipped with powerful graphics processing units (GPUs). These instances accelerate computations, significantly reducing processing times and enhancing performance for GPU-intensive workloads.\nBare Metal Instances Utho Cloud‚Äôs bare metal instances offer direct access to physical servers without virtualization overhead, delivering raw computing power and high performance. Businesses can leverage bare metal instances for performance-sensitive applications, high-performance computing (HPC), or applications that demand direct hardware control.\nKubernetes Services Utho Cloud makes it easy to deploy, manage, and scale containerized applications through its Kubernetes services. By providing managed Kubernetes clusters, businesses can streamline container orchestration, ensuring efficient resource utilization, scalability, and resilience for their microservices-based applications.\nUtho Cloud offers a robust suite of storage services tailored to address various business requirements, providing scalable, secure, and reliable storage solutions. Here‚Äôs how Utho Cloud assists businesses in providing storage services across different categories:\nBlock Storage Utho Cloud‚Äôs block storage service provides high-performance, durable storage volumes that can be attached to compute instances. Businesses benefit from flexible provisioning options, enabling them to adjust storage capacities based on demand, while ensuring low-latency access for critical applications and databases.\nObject Storage With Utho Cloud‚Äôs object storage service, businesses can securely store and manage vast amounts of unstructured data, such as documents, images, videos, and backups, offering seamless scalability for cost-effective storage of petabytes of data while ensuring durability and accessibility.\nOperating System Images Utho Cloud provides a repository of pre-configured operating system images that businesses can use to deploy virtual machines and containers quickly, streamlining the provisioning process and enabling effortless deployment of instances with preferred operating systems, thus reducing deployment times and operational overhead.\nBackup Services Utho Cloud‚Äôs backup services offer automated, scalable, and secure data protection solutions for critical workloads and applications, enabling businesses to create backup policies, schedule backups, and define retention policies to ensure data integrity and compliance with regulatory requirements.\nRemote Backup Utho Cloud facilitates remote backup solutions, allowing businesses to securely replicate data to geographically distributed data centers, thereby enhancing data resilience and disaster recovery capabilities. This redundancy safeguards against data loss due to localized failures or catastrophic events.\nSnapshot Services Businesses can leverage Utho Cloud‚Äôs snapshot services to capture point-in-time copies of their storage volumes or file systems, enabling efficient data protection, versioning, and recovery. Snapshots offer a reliable mechanism for restoring data to a previous state in case of errors or data corruption.\nArchive Storage Utho Cloud‚Äôs archive storage service provides a cost-effective solution for long-term data retention and compliance needs, allowing businesses to offload infrequently accessed data, reduce storage costs, and ensure data durability and regulatory compliance.\nISO Storage Utho Cloud offers ISO storage for storing ISO images of optical discs, simplifying software deployment and system provisioning processes. These images can be mounted to virtual machines for software installation or system recovery purposes, enabling businesses to quickly deploy applications and operating systems.\nCloud Firewall Utho Cloud‚Äôs cloud firewall service enables businesses to define and enforce granular network security policies, protecting their cloud environments by controlling inbound and outbound traffic. These rules safeguard applications and data from unauthorized access and threats.\nLoad Balancer Utho Cloud‚Äôs load balancer service distributes incoming traffic across multiple compute instances or services, ensuring optimal performance, scalability, and reliability. Businesses can dynamically scale their applications and handle fluctuations in traffic while maintaining high availability and responsiveness.\nReserved IP Utho Cloud provides reserved IP addresses that businesses can allocate to their resources for consistent and predictable network addressing, facilitating seamless resource management. These reserved IPs enable businesses to maintain connectivity even when instances are stopped or restarted.\nDDoS Protection Utho Cloud offers DDoS protection services to mitigate and defend against distributed denial-of-service (DDoS) attacks, providing businesses with automatic detection and mitigation of malicious traffic. This ensures the continuous availability and performance of their applications and services.\nVirtual Router With Utho Cloud‚Äôs virtual router capabilities, businesses can create and manage virtual routing instances to route traffic between different networks and subnets, enabling them to design and implement complex network topologies tailored to their specific requirements, such as multi-tier architectures or hybrid cloud environments.\nVirtual Private Cloud (VPC) Utho Cloud‚Äôs virtual private cloud (VPC) service enables businesses to provision logically isolated and customizable network environments within the cloud, allowing them to define their IP address ranges, subnets, route tables, and security policies. This provides a secure and controlled networking environment for their workloads.\nIPv4 and IPv6 Utho Cloud supports both IPv4 and IPv6 addressing schemes, allowing businesses to choose the appropriate protocol for their networking needs, ensuring compatibility with existing infrastructure. Additionally, this future-proofs businesses for the transition to IPv6 as the demand for IP addresses grows.\nIPsec Tunnel Utho Cloud facilitates secure connectivity between on-premises networks and cloud environments through IPsec tunnels, enabling businesses to establish encrypted tunnels over the internet for secure communication. This allows for seamless integration between their on-premises infrastructure and Utho Cloud services.\nDNS Manager Utho Cloud‚Äôs DNS manager provides businesses with a centralized platform to manage domain names and DNS records.\nNAT Gateway Utho Cloud‚Äôs NAT gateway service allows businesses to enable outbound internet connectivity for resources within their private subnets, enabling instances in private subnets to access the internet for software updates, patches, and other external services while maintaining network security and isolation.\nUtho Cloud offers robust managed database services for various popular database technologies, including PostgreSQL, MySQL, and MongoDB. Here‚Äôs how Utho Cloud assists businesses in providing managed database services for each of these platforms:\nPostgreSQL Utho Cloud‚Äôs managed PostgreSQL service offers businesses a scalable, highly available platform with easy deployment via the console or APIs, automated backups, patching, and upgrades for data durability, security, and compliance. It provides high performance, reliability, and flexible scaling options, along with built-in monitoring and management tools for proactive issue resolution.\nMySQL Utho Cloud‚Äôs managed MySQL service provides businesses with a fully managed MySQL database platform boasting enterprise-grade features and capabilities. Leveraging automated provisioning and configuration management, businesses can deploy MySQL databases on Utho Cloud with ease, streamlining deployment and management processes. This managed service includes automated backups, replication, and failover capabilities to ensure data availability, disaster recovery, and business continuity.\nMongoDB Utho Cloud‚Äôs managed MongoDB service provides businesses with a fully managed, scalable, and secure MongoDB database platform for modern application development. Leveraging automated provisioning and configuration management, businesses can deploy MongoDB databases on Utho Cloud with simplicity, accelerating time-to-market for applications. This managed service includes automated backups, monitoring, and performance tuning, enabling businesses to optimize database performance and ensure data availability and reliability.\nA Smart Strategic Move with Utho Partnership Utho strives to simplify technology requirements by offering a user-friendly cloud solution tailored for developers who prioritize their craft over complex technicalities. Through Utho‚Äôs Platinum and Exclusive Partnership Programs, businesses are empowered to maximize revenue potential with access to top-tier cloud services and products.\nPlatinum and Exclusive Partnership Program Utho offers two partnership programs designed to help businesses boost their earnings by selling our top-tier cloud services and products. Whether you choose our Platinum Partnership Program, where you can sell Utho‚Äôs services alongside others, or our Exclusive Signature Partnership Program, solely for Utho‚Äôs offerings, we‚Äôre here to support you every step of the way.\nPlatinum Partnership Program In the Platinum Program, partners enjoy a range of benefits, from technical support and platform guidance to marketing assistance. This includes access to partner support resources, help with selling, marketing materials like brochures and visiting cards, and advice on branding. Plus, partners get the chance to join new product launches and proudly display the partner logo.\nExclusive Partnership Program Partners enrolled in Utho‚Äôs partnership program are entitled to a host of marketing and support benefits. On the marketing front, partners gain access to the vibrant Utho community, opportunities for renewal and upselling, participation in new product launches, networking events, guidance through the selling process, GTM brochures, visibility and branding support, assistance in creating marketing funnels, and insights into the customer journey map. In terms of support, partners receive priority 24x7 support, technical issue resolution assistance, platform guidance and assistance, access to comprehensive partner support documentation, dedicated customer success resources, as well as hiring support services.\nPricing and Resource Management Made Simple Pay-Per-Use Pricing: With Utho Cloud‚Äôs pay-per-use pricing model, businesses only pay for the resources they consume, allowing for cost optimization and efficiency.\nFlexible Pricing Options: Utho Cloud provides flexible pricing options, including monthly and yearly subscriptions, reserved instances, and spot instances, enabling businesses to choose the most cost-effective model based on their needs.\nEconomies of Scale: Utho Cloud benefits from economies of scale, allowing them to offer competitive pricing while maintaining high-quality services.\nResource Optimization: Utho Cloud offers tools and features to optimize resource usage, such as autoscaling, which automatically adjusts resources based on demand, minimizing unnecessary costs.\nCost Management Tools: Utho Cloud provides robust cost management tools and dashboards that enable businesses to monitor, analyze, and optimize their cloud spending effectively.\nOverall, Utho Cloud‚Äôs commitment to cost-effectiveness ensures that businesses can leverage high-quality cloud infrastructure without breaking the bank, enabling them to innovate and grow while maintaining financial sustainability.\nDriving Market Expansion and Growth Utho Cloud has significantly broadened its market outreach on a global scale, boasting a robust customer base exceeding 22,000 businesses. This expansive reach underscores the trust and confidence placed in Utho Cloud‚Äôs cloud solutions by organizations worldwide. Facilitating this global presence are Utho Cloud‚Äôs seven strategically located data centers, strategically positioned to ensure optimal performance, reliability, and data sovereignty for clients across various regions.\nMoreover, Utho Cloud‚Äôs extensive partner network further enhances its ability to meet diverse business requirements, enabling seamless integration, customization, and support. Combined with its comprehensive suite of services, Utho Cloud empowers organizations to innovate, adapt, and scale effectively in today‚Äôs dynamic digital landscape.\nFostering Success for MSPs through Collaborative Partnership Utho is committed to helping Managed Service Providers (MSPs) succeed through a supportive partnership. When MSPs team up with Utho, they get personalized support to make the most of the platform‚Äôs features and boost their visibility through co-marketing initiatives. MSPs also gain access to resources to speed up their development and market efforts. Overall, Utho ensures MSPs have a practical and supportive experience focused on their growth and success in the cloud ecosystem.\n**Utho‚Äôs collaboration with MSPs is aimed at unlocking growth opportunities in the cloud ecosystem. Through tailored support, co-marketing initiatives, and access to resources, Utho empowers MSPs to maximize their potential and drive success. With a focus on practical solutions and supportive experiences, Utho is committed to fostering the growth and prosperity of MSPs in today‚Äôs dynamic digital landscape.**Join Utho today and boost your business with our collaborative partnership program, crafted for mutual growth. We‚Äôre dedicated to building a lasting relationship that enables us to achieve greater success together. Partner with Utho and unlock your business‚Äôs growth potential. Visit https://utho.com/partners for more information.","gpu-instances#\u003cstrong\u003eGPU Instances\u003c/strong\u003e":"","high-memory-instances#\u003cstrong\u003eHigh Memory Instances\u003c/strong\u003e":"","ipsec-tunnel#\u003cstrong\u003eIPsec Tunnel\u003c/strong\u003e":"","ipv4-and-ipv6#\u003cstrong\u003eIPv4 and IPv6\u003c/strong\u003e":"","iso-storage#\u003cstrong\u003eISO Storage\u003c/strong\u003e":"","kubernetes-services#\u003cstrong\u003eKubernetes Services\u003c/strong\u003e":"","load-balancer#\u003cstrong\u003eLoad Balancer\u003c/strong\u003e":"","mongodb#\u003cstrong\u003eMongoDB\u003c/strong\u003e":"","mysql#\u003cstrong\u003eMySQL\u003c/strong\u003e":"","nat-gateway#\u003cstrong\u003eNAT Gateway\u003c/strong\u003e":"","object-storage#\u003cstrong\u003eObject Storage\u003c/strong\u003e":"","operating-system-images#\u003cstrong\u003eOperating System Images\u003c/strong\u003e":"","platinum-and-exclusive-partnership-program#\u003cstrong\u003ePlatinum and Exclusive Partnership Program\u003c/strong\u003e":"","platinum-partnership-program#\u003cstrong\u003ePlatinum Partnership Program\u003c/strong\u003e":"","postgresql#\u003cstrong\u003ePostgreSQL\u003c/strong\u003e":"","pricing-and-resource-management-made-simple#\u003cstrong\u003ePricing and Resource Management Made Simple\u003c/strong\u003e":"","remote-backup#\u003cstrong\u003eRemote Backup\u003c/strong\u003e":"","reserved-ip#\u003cstrong\u003eReserved IP\u003c/strong\u003e":"","shared-cpu-instances#\u003cstrong\u003eShared CPU Instances\u003c/strong\u003e":"","snapshot-services#\u003cstrong\u003eSnapshot Services\u003c/strong\u003e":"","virtual-private-cloud-vpc#\u003cstrong\u003eVirtual Private Cloud (VPC)\u003c/strong\u003e":"","virtual-router#\u003cstrong\u003eVirtual Router\u003c/strong\u003e":""},"title":"Unlock Growth Opportunities for MSPs: A Collaboration with Utho"},"/utho-docs/docs/blog/unlock-machine-learning-potential-with-cuda-cores/":{"data":{"":"","cuda-cores-the-fundamental-elements-of-gpu-performance#\u003cstrong\u003eCUDA Cores: The Fundamental Elements of GPU Performance\u003c/strong\u003e":"","how-do-gpu-cuda-benefit-from-machine-learning#\u003cstrong\u003eHow do GPU CUDA benefit from machine learning\u003c/strong\u003e":"","how-utho-can-assist-you-in-choosing-the-most-suitable-gpus#\u003cstrong\u003eHow Utho Can Assist You in Choosing the Most Suitable GPUs\u003c/strong\u003e":"\nThe field of machine learning is constantly changing, and high-performance computing is increasingly necessary. A crucial figure in this field is the Graphics Processing Unit (GPU), a dominant force that has fundamentally transformed the process of training machine learning models. The central element of the GPU is its essential component known as CUDA cores, which each serve a critical function in accelerating intricate computations.\nCUDA Cores: The Fundamental Elements of GPU Performance NVIDIA developed the parallel computing technology known as Compute Unified Device Architecture (CUDA). The fundamental processing units of a GPU are known as CUDA cores, which allow for efficient parallel processing of data. Unlike Central Processing Units (CPUs), which excel at sequential processing, GPUs are specifically optimized to handle multiple tasks concurrently. This makes them highly suitable for the parallelized nature of various machine learning algorithms.\nHow do GPU CUDA benefit from machine learning Every individual CUDA core serves as a miniature processor with the ability to carry out its own unique set of instructions. By distributing tasks over numerous CUDA cores, GPUs are able to efficiently tackle complex computations that would otherwise be time-consuming for CPUs. This parallel design is the key factor behind the impressive acceleration that GPUs offer in a variety of computational applications, such as machine learning.\n1. Parallel Processing: One of the key benefits of utilizing GPUs in machine learning is their remarkable capability to simultaneously process numerous data points. This is especially useful for challenging tasks that need processing massive volumes of data in parallel, like deep neural network training.\n2. Matrix Operations: Machine learning models frequently require complex matrix operations, such as matrix multiplication. To efficiently handle these operations, CUDA cores are specially designed for rapid execution, making them crucial in the training of neural networks.\n3. Model Training Iterations: Training a machine learning model involves multiple iterations to adjust its parameters based on the training data. The use of GPU‚Äôs parallel processing capabilities greatly accelerates these iterations, resulting in faster convergence and reduced training times.\n4. Specialized Libraries: GPU companies, such as NVIDIA, offer libraries like CUDA Toolkit and cuDNN to improve the execution of machine learning algorithms on GPUs. These libraries utilize the parallel architecture of CUDA cores to maximize the performance of machine learning tasks.\n5. Deep Learning Frameworks: Well-known deep learning frameworks such as TensorFlow and PyTorch now have integrated GPU support, facilitating effortless integration with GPUs. Thus, instead of requiring laborious manual optimization, developers may take advantage of the capabilities of CUDA cores.\nSpearheading innovation and pushing the frontiers of artificial intelligence CUDA cores serve as the unheralded champions that enable GPUs to enhance the speed of machine learning model training. The parallel processing design of CUDA cores, combined with the optimization of libraries and frameworks, has revolutionized GPUs into imperative assets for data scientists and machine learning experts.\nMachine learning will surely continue to emphasize GPUs and CUDA cores because these components are essential to innovation and expanding the capabilities of artificial intelligence.\nHow Utho Can Assist You in Choosing the Most Suitable GPUs At Utho, we provide advanced cloud GPUs powered by NVIDIA for high-performance computing. Our GPUs are optimized to offer the best price-performance ratio in the market. We pride ourselves on maintaining an uptime of 99.9% and offering 24/7 technical support, backed by SLA guarantees, to ensure the seamless functioning of your applications and workloads without any disruptions. Utho provides services based on NVIDIA GPUs, including the NVIDIA HGX H100, NVIDIA A100, NVIDIA A40, and NVIDIA A16. These GPUs are well-suited for AI tasks, including both deep learning training and inference, facilitating quicker model development and execution.\nRead Also: Introduction to AI and Machine Learning in the Cloud: What Are They and How Do They Work?","spearheading-innovation-and-pushing-the-frontiers-of-artificial-intelligence#\u003cstrong\u003eSpearheading innovation and pushing the frontiers of artificial intelligence\u003c/strong\u003e":""},"title":"Unlock Machine Learning Potential with CUDA Cores"},"/utho-docs/docs/blog/unlock-the-vault-rule-putty-for-smooth-server-access/":{"data":{"":"","downloading-putty-a-step-by-step-guide#\u003cstrong\u003eDownloading PuTTY: A Step-by-Step Guide\u003c/strong\u003e":"","establishing-an-ssh-connection-with-putty#\u003cstrong\u003eEstablishing an SSH Connection with PuTTY\u003c/strong\u003e":"","installing-the-public-key-on-the-server#\u003cstrong\u003eInstalling the Public Key on the Server\u003c/strong\u003e":"SSH, which stands for Secure Shell, is a safe way to connect to a server from a distance. To make this connection, you need a special app called an SSH client, such as PuTTY. You‚Äôll also need some specific information: the server‚Äôs IP address, SSH port number, your SSH username, and your SSH password.\nThis guide will show you how to use PuTTY SSH terminal to connect to either your hosting account or a virtual private server. This allows you to control your remote computer by running different commands.\nDownloading PuTTY: A Step-by-Step Guide PuTTY is widely used on Windows, but it‚Äôs also compatible with Linux and Mac. Here‚Äôs how you can download PuTTY on different operating systems.\nWindows To get PuTTY SSH, head to the official website and download the latest version. Make sure to choose the correct bit version for your computer. After downloading, run the installer to begin the setup process.\nLinux While many prefer the preinstalled OpenSSH on Linux, PuTTY on Linux is handy for tasks like debugging and connecting to serial ports and raw sockets.\nFor Debian, use the following command:\nsudo apt install putty Afterwards, execute the following command to install the tools:\nsudo apt install putty-tools MacOS Similar to Linux, macOS already includes a built-in command-line SSH client. However, if you want to download PuTTY, open the terminal window and enter the following commands.\nTo install PuTTY on MacOS:\nbrew install putty Requirements for Setup Before setting up an SSH connection using PuTTY, make sure you have the following information ready:\nServer IP Address\nSSH Port\nSSH Username\nSSH Password\nEstablishing an SSH Connection with PuTTY Once you have all the SSH connection details, follow these steps:\n1. Open the PuTTY SSH client.\n2. Enter your server‚Äôs SSH IP address and SSH port.\n3. Click the ‚ÄúOpen‚Äù button to proceed.\n1. After opening PuTTY, you will be promted to enter your SSH username. For VPS users, this is typically ‚Äúroot‚Äù. You‚Äôll use a specific username like ‚Äúuthouser‚Äù. Once you‚Äôve entered your username, press Enter.\n2. Next, type your SSH password and press Enter again. For security, the screen won‚Äôt display the password as you type, but it will register what you enter.\nYou‚Äôve successfully connected to your account using the PuTTY SSH client. To see a list of all available SSH commands, simply type ‚Äúhelp‚Äù into the terminal.\nUsing PuTTYgen: A Step-by-Step Guide PuTTYgen is a tool used for generating SSH key pairs. PuTTY saves key authentications in .ppk files. Windows users utilize the PuTTYgen.exe graphical tool, while Linux users rely on the command line.\nTo start, find PuTTY and open PuTTYgen.\nTo create a new key pair, click the ‚ÄúGenerate‚Äù button at the bottom and move your mouse around within the window. Keep moving your mouse until the green progress bar is full.\nOnce the process is finished, your public key will appear in the window.\nIt‚Äôs advisable to set a passphrase and save your private key.\nInstalling the Public Key on the Server To allow access to an account, you must add the new public key to the file named ~/.ssh/authorized_keys.\nStart by installing the public key. Then, log into your server and find the authorized_keys file. Edit the file, paste the new public key, save the changes, and test if the login works.\nUnlock seamless server access with PuTTY, a powerful SSH client. Simply have your SSH credentials ready, and you‚Äôre set to manage your server remotely. Whether you‚Äôre a beginner or an expert, PuTTY‚Äôs flexibility and simplicity streamline server operations, empowering you to tackle tasks with confidence. Experience smooth server management and explore PuTTY‚Äôs full potential today.","linux#\u003cstrong\u003eLinux\u003c/strong\u003e":"","macos#\u003cstrong\u003eMacOS\u003c/strong\u003e":"","requirements-for-setup#Requirements for Setup":"","using-puttygen-a-step-by-step-guide#\u003cstrong\u003eUsing PuTTYgen: A Step-by-Step Guide\u003c/strong\u003e":"","windows#\u003cstrong\u003eWindows\u003c/strong\u003e":""},"title":"Unlock the Vault: Rule PuTTY for Smooth Server Access"},"/utho-docs/docs/blog/utho-driving-it-modernization-via-cloud-adoption/":{"data":{"":"","what-does-the-term-cloud-adoption-mean#\u003cstrong\u003eWhat does the term \u0026ldquo;cloud adoption\u0026rdquo; mean?\u003c/strong\u003e":"Cloud adoption presents numerous advantages for organizations across various scales, with smaller companies reaping particularly substantial benefits. Integrating cloud services into your new or small business is crucial for enhancing prospects in the medium and long term through cloud adoption.\nFurthermore, the obstacles and objections commonly encountered by larger businesses hold less significance for smaller and newer enterprises, underscoring the importance of embracing cloud computing for sustained growth and efficiency.\nWhat does the term ‚Äúcloud adoption‚Äù mean? Cloud adoption involves transitioning to or initiating a service in the cloud. This can encompass a complete migration to the cloud or utilizing cloud services in conjunction with on-premises infrastructure.\nWhat are the reasons to consider Utho as the preferred cloud service provider? Selecting a cloud provider requires unique criteria tailored to your organization, with common focus areas for assessment.\nImproved Customer Service: The vitality of a company‚Äôs survival hinges on the quality of its customer service. Utho Delivers exceptional customer service and serves as a key differentiator, providing a competitive edge for businesses. Utho‚Äôs Cloud adoption and its solutions play a pivotal role in facilitating seamless communication between your company and clients. Utho provides Service Level Agreements (SLAs) for system uptime and proactive customer support, enabling customers to reach out for issue resolution or feedback. The real-time communication afforded by cloud adoption and its solutions has the potential to attract and retain customers, contributing to overall business success.\nCost-Efficiency: Several factors contribute to optimize cost linked with on-premises systems, such as hardware expenses, installation costs, and in-house management and maintenance. Utho empowers companies to opt for subscription plans tailored to their budget, eliminating the need for upfront investments in hardware and installation. Moreover, Utho offers a pay-as-you-go model, enabling organizations to pay solely for the services they actively utilize.\nFaster Implementation Cycles: Users of on-premise software often encounter extended installation-to-use timelines, necessitating assistance. In contrast, Utho cloud solutions enable organizations to install products within weeks instead of months. Its cloud technology facilitates remote editing and sharing of data, enhancing collaboration among teams. The integration of cloud-based workflow and file-sharing tools delivers real-time updates, ultimately elevating productivity levels.\nPromotes Scalability: Organizational structures evolve over time with growth, contraction, or seasonal changes in every corporation. Utho Cloud solutions are adept at accommodating and adapting to such fluctuations. Utho cloud eliminates the need to alter software when scaling up or down, fostering corporate scalability and enabling the organization to grow as required.\nUpgrades \u0026 Maintenance: On-premise software can incur substantial costs for maintenance and downtime during upgrades. Additionally, on-premise devices often receive fewer upgrades, heightening the risk of software obsolescence. In contrast, Utho cloud eliminates this risk by providing frequent and seamless upgrades and maintenance. Users who are using Utho cloud-based applications consistently benefit from the latest version, ensuring they stay up-to-date without the concerns of obsolescence.\nBetter Security: Data stored on Utho cloud servers through cloud services benefits from stringent security measures. This offsite storage enhances the security of data compared to on-premises infrastructure. Shifting personal data to the Utho cloud provides a protective shield against potential threats from hackers and other security concerns.\nBetter Document Control: In organizations where information is prolifically generated and shared within the production cycle, effective documentation is crucial. This often results in a multitude of conflicting files with varied formats and titles. However, the Utho cloud allows employees to consolidate files in a centralized location accessible to everyone. Placing apps and infrastructure in the cloud immerses you in a dynamic ecosystem.\nDisaster Recovery: Businesses, regardless of size, allocate substantial resources to catastrophe recovery. Utho Cloud offers small businesses the opportunity to economize, defer significant investments, and leverage data storage on servers owned by other companies.\nQuality Control: One might initially consider the drawbacks of cloud computing services, fearing limited control. However, in reality, Utho empowers users with the capability to monitor their data closely. Utho Cloud furnishes a more detailed level of permissions control and provides monitoring tools to enhance security.\nIncrease Business Agility: Integrating or enhancing hardware and software in a traditional on-premises infrastructure is often both time-consuming and costly. In contrast, the adaptable Utho cloud server capacity in a cloud environment enables swift and effortless provisioning of new resources. This flexibility of Utho cloud empowers businesses to promptly respond to evolving market conditions or capitalize on new opportunities.\nWho should consider adopting cloud technology and what are the reasons behind it? Numerous industries reap the advantages of adopting cloud technology, including:\nHealthcare: Driven by digital and social consumer trends, along with the imperative for secure and accessible electronic health records (EHRs), hospitals, clinics, and other medical organizations are leveraging cloud computing for document storage, marketing, and human resources.\nMarketing and Advertising: In an industry reliant on social media and the swift creation and dissemination of customer-relevant content, agencies are employing hybrid cloud adoption strategies. These approaches enable the seamless delivery of crucial client messages to both local and global audiences.\nRetail: An effective e-commerce strategy necessitates a robust Internet approach. Through the implementation of cloud adoption, Internet-based retail can efficiently market to customers and store product data at a reduced cost.\nFinance: Effective management of expenses, human resources, and customer communications stands as paramount for today‚Äôs financial organizations. In response, financial services institutions are now opting to house their email platforms and marketing tools in the cloud.\nEducation: Online educational opportunities have gained unprecedented popularity. The cloud enables universities, private institutions, and K-12 public schools to offer online learning, homework assignments, and grading systems.\nHow do companies of varying sizes benefit from this technology revolution? Businesses of different scales experience a multitude of advantages amid the ongoing technological revolution.\nLarge Companies and Corporations: Corporate environments demand substantial IT investments. Embracing enterprise cloud adoption yields considerable bottom-line savings by enhancing efficiency, eliminating the necessity for an extensive security and maintenance team, and reducing the cost of server space.\nSmall and Mid-Size Companies: Small and mid-size organizations, experiencing growth in staff, clientele, and projects, frequently find the need to rapidly expand their IT infrastructure. Embracing cloud computing enables efficient and cost-effective scalability, accomplished within minutes rather than days.\nEntrepreneurs and Startups: Opting for the cloud over an expensive IT infrastructure minimizes startup costs and eliminates the need for significant up-front software investments. Many Software-as-a-Service (SaaS) vendors now commonly provide a subscription model with a monthly fee.\nWhat is the global trend towards using cloud computing technology ? Recent findings on cloud adoption from our reliable sources reveal a remarkable 35% increase in global spending on public cloud services, reaching a staggering $415 billion in 2024. This isn‚Äôt just a trend among large corporations; small and medium-sized businesses (SMBs) are actively participating in cloud adoption, with 53% investing over $1.4 million annually. With this rapid pace of cloud adoption, any business not utilizing cloud solutions is likely contemplating a move soon. Those hesitating to migrate from on-premise solutions may face a significant competitive disadvantage in the evolving business landscape."},"title":"Utho: Driving IT Modernization via Cloud Adoption"},"/utho-docs/docs/blog/utho-transforming-cloud-technology-in-india/":{"data":{"":"","data-sovereignty-assurance#\u003cstrong\u003eData sovereignty assurance\u003c/strong\u003e":"","enhance-business-with-cloud-based-solutions-cultivating-efficiency-reliability-and-growth#\u003cstrong\u003eEnhance business with cloud-based solutions, cultivating efficiency, reliability, and growth\u003c/strong\u003e":"","investment-in-cloud-technology-in-india#\u003cstrong\u003eInvestment in cloud technology in India\u003c/strong\u003e":"","transparent-cloud-pricing-for-predictable-costs#\u003cstrong\u003eTransparent cloud pricing for predictable costs\u003c/strong\u003e":"","unite-with-utho-the-premier-cloud-service-provider-in-india#\u003cstrong\u003eUnite with Utho, the premier cloud service provider in India\u003c/strong\u003e":"In the ever-changing world of technology, cloud computing has become a strong driving force, and Utho is a pioneer in cloud computing solutions. Through his innovative contributions, it played a significant role in shaping the technological landscape of countries including India.¬†Let‚Äôs see how Utho has acted as a catalyst for the transformation of cloud technology across the subcontinent.\nInvestment in cloud technology in India People want to invest more in cloud computing because of its greater scalability, security, maintenance and cost predictability. Cloud investments accelerated in India during the pandemic. According to sources, estimates the Indian cloud market at a CAGR of 22.1% by 2025 and could grow to a billion.\nAccording to trusted sources, SaaS is the most interesting of the various cloud service models in India due to its low agility and low liability. Micro, small, and medium-sized businesses are the largest contributors to the SaaS economy.\nTransparent cloud pricing for predictable costs Utho leads the market by providing businesses with a highly efficient and cost-effective infrastructure. It brings together an array of cutting-edge technologies, including Cloud GPUs, Compute resources, Object Storage, Load Balancers, [CDN](https://aws.amazon.com/what-is/cdn/#:~:text=A%20content%20delivery%20network%20(CDN,network%20or%20content%20distribution%20network.), Containers and Block Storage, to empower businesses in building and launching their applications and platforms seamlessly.\nUtho Networks is committed to becoming the trusted and high-performance computing platform for machine learning platforms and applications. Startups, enterprises and institutions across the country rely on Utho reliable and scalable solutions to enable them to embrace the machine learning and artificial intelligence revolution with confidence, efficiency and innovation.\nEnhance business with cloud-based solutions, cultivating efficiency, reliability, and growth Main features and benefits of Utho Network cloud computing solutions:\nUnbeatable Price-Performance Ratio: Experience the best value in the Indian market with Utho‚Äôs GPU and cloud computing solutions that deliver unmatched performance without breaking the bank.\nTested Open-Source platform: Rely on production-proven open-source platform, rigorously tested and used by Utho‚Äôs customers, ensuring application reliability and stability.\nCutting-Edge GPUs and compute resources: Take advantage of our state-of-the-art GPUs and PCs and deliver the latest and most advanced technologies to your applications.\nHuman-Centric support: Utho‚Äôs exceptional 100% ‚Äúhuman‚Äù support teams are ready to help you every step of the way and ensure a smooth build of production platforms.\nData sovereignty assurance Utho Networks prioritizes information security and self-determination. Utho strictly adheres to the mandates of the Indian IT Act and fully complies with all Indian laws and regulations. Utho‚Äôs commitment to maintaining data integrity ensures that companies using the cloud platform are protected from the risks of data sharing, interception or seizure by foreign governments.\nWith Utho Networks as a trusted cloud service provider, businesses can operate with peace of mind knowing that their data remains secure and sovereign within Indian laws and regulations. Utho‚Äôs unwavering commitment to data protection strengthens our customers‚Äô trust in us.\nUnite with Utho, the premier cloud service provider in India Utho‚Äôs performance-driven GPU infrastructure and innovative frameworks allow you to seamlessly build and deploy machine learning models. Trust certified security standards and gain peace of mind because it prioritizes the security and privacy of your data that is the reason why Utho has been selected by over 22,000 users, including 8 leading cloud telephony companies in India.\nLet us be your trusted partner in this journey of change - switch to Utho and succeed in the world of cloud services."},"title":"Utho: Transforming Cloud Technology in India"},"/utho-docs/docs/blog/vps-hosting-a-beginners-guide-to-virtual-private-servers/":{"data":{"":"\nIf you‚Äôre looking for a web hosting solution that provides better performance, security, and scalability than shared hosting, you may want to consider VPS hosting. In this beginner‚Äôs guide, we‚Äôll explain what VPS hosting is, its benefits, and how to choose the right provider.","what-is-vps-hosting#What is VPS Hosting?":"VPS hosting stands for Virtual Private Server hosting. It uses virtualization technology to create a virtual server, which runs its own copy of an operating system. Each virtual server has its own set of dedicated resources, such as CPU, RAM, and storage, which are isolated from other virtual servers on the same physical server.\nBenefits of VPS Hosting Improved Performance: VPS hosting provides dedicated resources, which means that your website can handle higher traffic volumes and perform better than on shared hosting.\nIncreased Security: With VPS hosting, you‚Äôre less vulnerable to security breaches that could affect other websites on the same server, as you have your own isolated environment.\nScalability: VPS hosting allows you to easily scale your resources up or down, depending on your website‚Äôs needs, without the need to migrate to a different hosting provider.\nCustomization: VPS hosting gives you more control over your hosting environment, allowing you to install your own software and configure your server to meet your specific needs.\nHow to Choose the Right VPS Hosting Provider Choosing the right VPS hosting provider is crucial for your website‚Äôs success. Here are some factors to consider:\nPerformance: Look for a VPS hosting provider with fast server performance and reliable uptime.\nScalability: Make sure that the provider offers easy scalability, so you can increase your resources as your website grows.\nSupport: Choose a provider with excellent customer support, available 24/7, and knowledgeable staff to help you with any issues.\nSecurity: Look for a provider that offers robust security features, such as firewalls and DDoS protection.\nPrice: Consider the provider‚Äôs pricing, ensuring it fits within your budget and provides good value for money.\nMicroHost: Your Reliable VPS Hosting Provider If you‚Äôre looking for a reliable VPS hosting provider, consider MicroHost. They offer fast and customizable VPS hosting solutions, with 24/7 customer support and robust security features. To learn more about their VPS hosting services, visit their website at https://utho.com/."},"title":"VPS Hosting: A Beginner‚Äôs Guide to Virtual Private Servers"},"/utho-docs/docs/blog/what-are-runlevels-in-linux-and-its-understanding/":{"data":{"":"","how-to-change-runlevel-in-redhat-centos#How to change runlevel in Redhat/ CentOS":"","init-0--shutdown-the-machine-immediately#init 0 # Shutdown the machine immediately":"","runlevel#runlevel":"","systemctl-isolate-graphicaltarget--for-systemctl-utility#systemctl isolate graphical.target # for systemctl utility":"\nIn this article you will know about Runlevels in Linux, A runlevel is a way that a Unix or Unix-based operating system is set to run. This way of working is already set up on a Linux-based system. The numbers for runlevels go from 0 to 6.\nSome system administrators use run levels to tell which subsystems are working, like if X is running, if the network is up and running, and so on.\nRunlevels decide which programmes can run once the OS is up and running. The runlevel tells what the machine does after it boots up.\nSystem administrators can change a system‚Äôs default runlevel to suit their needs, or they can use the runlevel command to find out what the system‚Äôs current runlevel is so they can evaluate it. For example, the runlevel can show if the system‚Äôs network is up and running or not. Use the /sbin/runlevel command to find out what the current runlevel is and what it was before.\nRunlevels 0‚Äì6 are usually assigned to single-user mode, multi-user mode with and without network services running, shutting down the system, and restarting the system. Different Linux distributions and versions of Unix have different ways of setting up these configurations.\nEach basic level is used for something different. Runlevels 0, 1, and 6 never change. Different Linux distributions have different runlevels 2 through 5. When the system starts up, only one runlevel is used. They are not done one after the other. For instance, either runlevel 4 or runlevel 5 or runlevel 6 is used. It is not 4 then 5 then 6.\nWhen a LINUX system starts up, the first thing it does is start the init process, which is responsible for running other start scripts. These scripts do things like initialise your hardware, start the network, and start the graphical user interface.\nNow, the init first finds out what the system‚Äôs default run level is so that it can run the start scripts for that run level.\nRunlevel 0shuts down the systemRunlevel 1single-user modeRunlevel 2multi-user mode without networkingRunlevel 3multi-user mode with networkingRunlevel 4user-definableRunlevel 5multi-user mode with networkingRunlevel 6reboots the system to restart it Booting a system into different runlevels solves few problems. For instance, if a machine fails to boot due to a wrong configuration file, refuses to allow the root user to log in due to a wrong /etc/passwd file or if you forget your password, you can solve these problems by booting into single-user mode.\nHow to change runlevel in Redhat/ CentOS First you would like to know, in which runlevel your machine is running. You can know this by two methods\nBy runlevel command runlevel 2. By systemctl command ``` # systemctl get-default Output:\nOutput of first command will be- multi-user.target\nOutput of second command will be- 3 5\nPlease note that the above output is according to my machine which is running in init 3. If you switch from runlevel 3 to runlevel 5( graphical runlevel), it will need a restart of your machine. Also to run your machine in graphical user mode will require the necessary GUI packages.\nPlease note that, upto redhat 8, you can change the runlevel by init command and use 0 to 6 runlevels, but you can also change the runlevel by systemctl command.\n# init 3 The above command, in centos or redhat flavor linux will run the system in multiuser mode and networking enabled but without graphical user interface. If you want to switch to the mode in which you do now want to use networking but multiple users can login and with no graphical user interface.\n# init 2 you can restart or shutdown your system by changing the runlevel of your machine to six or zero respectively.\n# init 6 # Restart the machine init 0 # Shutdown the machine immediately You can also the change the runlevel on the latest redhat( RHEL 8) and centos( CentOS 8) machines by systemctl command\n# systemctl isolate multi-user.target # will be equal to init 3 The above command will enable or run your machine in runlevel 3 which is multiuser with networking but without graphical user mode.\nTo run your machine in graphical machine use the below command\n# init 5 # using init utility systemctl isolate graphical.target # for systemctl utility "},"title":"What are Runlevels in Linux and its understanding"},"/utho-docs/docs/blog/what-is-a-bare-metal-server-an-in-depth-overview/":{"data":{"":"","how-to-effectively-manage-a-bare-metal-server#\u003cstrong\u003eHow to Effectively Manage a Bare Metal Server?\u003c/strong\u003e":"\nBare Metal servers play a crucial role in IT infrastructure, providing a solid base for reliable digital connections and operations. However, many users are not fully aware of the top-notch features that come with Bare Metal servers, such as isolation and dedicated resources. Therefore, these servers are in high demand because they can optimize the performance of various applications. ‚Äã‚Äã‚Äã‚Äã‚Äã‚ÄãUtho is the premier provider of Bare Metal servers for businesses and individuals seeking high-performance hosting solutions for their resource-intensive websites and applications.\nThis comprehensive guideline offers valuable insight to aid you in making wise decisions for your business. Consider all benefits, possibilities, and factors before reaching a conclusion.\nWhat exactly is a Bare Metal Server? A bare-metal server is a dedicated hosting service designed for a physical computer to run for a longer period. Only one user or tenant can access it at a time. Therefore, you cannot have multiple users. That is great because the connection and network are predictable, strong, and stable.\nAdditionally, due to limited capacity, this server does not experience high traffic and maintains a stable performance, unlike other servers. As a single tenant occupies it, there is minimal interference and noise, ensuring smooth operation. Furthermore, it offers direct and efficient access for enterprises.\nWhy Opt for a Bare Metal Server? Bare Metal servers are a suitable choice for small and medium-sized industries, offering an affordable solution for businesses. Their cost-effectiveness and flexibility make them an ideal choice as they allow customization of settings according to specific requirements.\nThey are highly competent hosts known for their impressive ability to quickly and efficiently scale up any allocated resource. Their superior performance makes them the preferred choice in media encoding and rendering environments.\nEnsuring data security is a priority for many organizations and businesses. Therefore, when it comes to efficient and secure data operations, Bare Metal remains the optimal choice.\nWhat are the Benefits of Utilizing a Bare Metal Server? In order to enhance the user experience, these servers allow for unrestricted communication. This is made possible by the lack of hypervisors on the servers, eliminating latency typically associated with virtual machines and their networks. In essence, this type of server seamlessly scales your business while also reducing costs.\nBefore selecting servers, it is important to understand the specific needs of your business. If your objectives include achieving high scalability, on-demand availability, and pay-as-you-go usage, then this would be the most suitable choice for you.\nWhich individuals or organizations commonly utilize Bare Metal servers? Organizations or individuals with high resource-demanding applications, those in need of stringent performance consistency, or those requiring strict security and compliance standards that cannot be fulfilled in a shared environment.\nHow to Effectively Manage a Bare Metal Server? Utilizing an external service provider for server management can greatly enhance your company‚Äôs operations. By freeing up your IT team‚Äôs time, they can focus on improving your business. This comprehensive service includes managing servers in the following areas:\nPerforming OS Updates and Monitoring Processes: Regular and timely updates of the operating system and prompt installation of software patches are essential for server maintenance. These proactive measures guarantee the safety and security of your data, protecting it from potential malicious attacks.\nEnhancing Security with Firewalls: Configuring the firewall involves controlling or blocking network traffic based on IP protocols, ports, and addresses. This is done with the goal of eliminating unwanted or suspicious activities within the network.\nOperations Management: Operational management encompasses a variety of key duties, such as maintaining domain name services, overseeing server backup and recovery, conducting data migration, and managing hardware replacements. These essential functions are crucial in ensuring the efficient operation of servers.\nIn today‚Äôs globalized world, speed, availability and scalability are important factors that every company strives to achieve. With Utho Bare Metal server, you can easily make your resources available on a global scale. This solution is undoubtedly the smartest and most efficient choice for companies both in the short and long term.\nRead Also: Customer-Centric Cloud: How Human Support Enhances User Experience","what-are-the-benefits-of-utilizing-a-bare-metal-server#\u003cstrong\u003eWhat are the Benefits of Utilizing a Bare Metal Server?\u003c/strong\u003e":"","what-exactly-is-a-bare-metal-server#\u003cstrong\u003eWhat exactly is a Bare Metal Server?\u003c/strong\u003e":"","which-individuals-or-organizations-commonly-utilize-bare-metal-servers#\u003cstrong\u003eWhich individuals or organizations commonly utilize Bare Metal servers?\u003c/strong\u003e":"","why-opt-for-a-bare-metal-server#\u003cstrong\u003eWhy Opt for a Bare Metal Server?\u003c/strong\u003e":""},"title":"What is a Bare Metal Server? An In-Depth Overview"},"/utho-docs/docs/blog/what-is-a-cloud-server-microhost/":{"data":{"":"","what-is-a-cloud-server-benefits-types-and-the-best-cloud-server-provider#\u003cstrong\u003eWhat is a Cloud Server? Benefits, Types, and the Best Cloud Server Provider\u003c/strong\u003e":"What is a Cloud Server? Benefits, Types, and the Best Cloud Server Provider Are you looking for a reliable and scalable hosting solution? If so, then you must have heard of cloud servers. In this blog, we will discuss what cloud servers are, their benefits, types, and the best cloud server provider.\nWhat is a Cloud Server? A cloud server is a virtual server that runs in a cloud computing environment. Unlike traditional hosting solutions, such as dedicated servers or shared hosting, cloud servers provide on-demand resources and scalability. This means that you can easily scale up or down your resources, such as CPU, RAM, storage, and bandwidth, depending on your needs. Cloud servers are highly available, redundant, and offer better uptime compared to other hosting solutions.\nBenefits of Cloud Servers There are several benefits of using cloud servers, including:\nScalability: Cloud servers provide on-demand resources that can be scaled up or down based on your needs. This means that you can easily handle traffic spikes or seasonal demands without affecting your website‚Äôs performance.\nReliability: Cloud servers are highly available and redundant. This means that your website will remain online even if one of the servers goes down.\nCost-effective: Cloud servers are cost-effective compared to dedicated servers. You only pay for the resources you use, and there are no upfront costs or long-term commitments.\nSecurity: Cloud servers offer better security than traditional hosting solutions. They are isolated from other servers and can be easily backed up and restored.\nTypes of Cloud Servers There are mainly three types of cloud servers:\nPublic cloud servers: Public cloud servers are owned and operated by third-party service providers. They are scalable, cost-effective, and offer on-demand resources.\nPrivate cloud servers: Private cloud servers are owned and operated by organizations. They are highly secure, customizable, and offer better control over resources.\nHybrid cloud servers: Hybrid cloud servers combine the benefits of public and private cloud servers. They allow organizations to use the best of both worlds.\nThe Best Cloud Server Provider When it comes to choosing the best cloud server provider, there are several factors to consider, such as scalability, reliability, security, and cost-effectiveness. MicroHost is one of the best cloud server providers and cloud platform providers that offer on-demand resources, scalability, reliability, and security at an affordable price.\nMicroHost offers various cloud server solutions, such as public cloud servers, private cloud servers, and hybrid cloud servers, to meet your unique hosting needs. Their cloud servers are highly available, redundant, and offer better uptime compared to other hosting solutions. They also offer 24/7 technical support and easy-to-use control panels.\nConclusion In conclusion, a cloud server is a virtual server that provides on-demand resources, scalability, and better uptime compared to traditional hosting solutions. There are several benefits of using cloud servers, such as scalability, reliability, cost-effectiveness, and security. There are mainly three types of cloud servers: public, private, and hybrid. MicroHost is one of the best cloud server providers and cloud platform providers that offer on-demand resources, scalability, reliability, and security at an affordable price.\nAlso Read:"},"title":"What is a Cloud Server ?"},"/utho-docs/docs/blog/what-is-a-cloud-service-provider/":{"data":{"":"","what-is-a-cloud-service-provider#\u003cstrong\u003eWhat is a Cloud Service Provider?\u003c/strong\u003e":"Cloud computing has become a popular and essential tool for businesses in recent years. Cloud service provider play a crucial role in making this technology accessible to businesses of all sizes. In this article, we‚Äôll explain what cloud service providers are, their role in the cloud computing landscape, and how they can benefit your business.\nWhat is a Cloud Service Provider? A cloud service provider (CSP) is a company that offers cloud computing services to businesses and individuals. These services can include infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS). CSPs make it possible for businesses to access and use these services without having to invest in their own IT infrastructure.\nRole of Cloud Service Providers Cloud service providers play a crucial role in the cloud computing landscape. They provide the necessary infrastructure and tools for businesses to run their operations in the cloud. This includes everything from virtual servers and storage to software applications and development platforms.\nCSPs also take on the responsibility of maintaining and securing their infrastructure, which can be a significant burden for businesses. They typically have highly trained and experienced staff who are responsible for ensuring the reliability and security of their services.\nBenefits of Cloud Service Providers Using a cloud service provider can bring many benefits to your business. Here are some of the most significant advantages:\nCost savings - CSPs can offer lower costs than investing in your own IT infrastructure, as they can spread the cost of their infrastructure across many customers.\nScalability - CSPs make it easy to scale your computing resources up or down depending on your needs. This means you only pay for what you use.\nReliability - CSPs typically have redundant systems and backups in place to ensure their services are available when you need them.\nSecurity - CSPs have dedicated security teams and systems in place to protect your data from cyber threats.\nFlexibility - CSPs offer a wide range of services that can be tailored to your business needs. This includes everything from storage and computing to software applications and development platforms.\nConclusion In summary, cloud service providers play a vital role in the cloud computing landscape. They provide the necessary infrastructure and tools for businesses to run their operations in the cloud, saving them time and money. By using a CSP, businesses can benefit from scalability, reliability, security, and flexibility. Whether you‚Äôre a small business or a large enterprise, partnering with a cloud service provider can help you stay competitive and agile in today‚Äôs fast-paced digital world.]\nAlso Read: 7 Reasons Why Cloud Infrastructure is Important for Startups"},"title":"What is a Cloud Service Provider?"},"/utho-docs/docs/blog/what-is-a-hybrid-cloud-and-why-is-it-important/":{"data":{"":"","advantages-of-a-hybrid-cloud#\u003cstrong\u003eAdvantages of a Hybrid Cloud\u003c/strong\u003e":"","challenges-of-a-hybrid-cloud#\u003cstrong\u003eChallenges of a Hybrid Cloud\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction In recent years, cloud computing has become an essential tool for many businesses. However, there are different types of cloud computing models, and each has its advantages and disadvantages. One model that has gained popularity in recent years is the hybrid cloud. In this article, we will explain what a hybrid cloud is and why it is important for businesses.\nWhat is a Hybrid Cloud? A hybrid cloud is a cloud computing model that combines the benefits of public and private clouds. It allows businesses to run their applications and store their data in both private and public cloud environments. For example, a business may use a private cloud to store sensitive data and a public cloud to run less critical applications. The two environments are connected, and data can be moved between them as needed.\nAdvantages of a Hybrid Cloud There are several advantages to using a hybrid cloud:\n1. Flexibility: A hybrid cloud offers businesses more flexibility in terms of where they store their data and how they run their applications. This flexibility allows businesses to take advantage of the benefits of both public and private clouds.\n2. Scalability: A hybrid cloud allows businesses to scale their computing resources up or down as needed. This is particularly important for businesses with fluctuating computing needs.\n3. Security: A hybrid cloud allows businesses to store sensitive data in a private cloud while still taking advantage of the cost savings and scalability of a public cloud. This helps businesses to meet regulatory and compliance requirements.\n4. Cost savings: By using a hybrid cloud, businesses can save money by storing non-sensitive data in a public cloud, which is typically less expensive than a private cloud.\nChallenges of a Hybrid Cloud While there are many benefits to using a hybrid cloud, there are also some challenges:\n1. Complexity: A hybrid cloud is more complex than a single cloud environment. It requires businesses to manage multiple cloud providers and ensure that their data is properly secured and integrated.\n2. Security: While a hybrid cloud can be more secure than a public cloud, it can also be more vulnerable to security breaches if not properly configured.\n3. Management: Managing a hybrid cloud can be challenging, as it requires businesses to coordinate multiple cloud providers and ensure that their data is properly backed up and integrated.\nConclusion In conclusion, a hybrid cloud offers businesses the flexibility, scalability, security, and cost savings they need to succeed in today‚Äôs digital world. However, it also presents some challenges that must be carefully managed. To take advantage of the benefits of a hybrid cloud, businesses should work with a trusted cloud provider like Microhost. Microhost offers a wide range of cloud solutions, including hybrid cloud solutions, to help businesses meet their unique computing needs. To learn more, visit Microhost‚Äôs website today.\nRead Also: 5 Best practices for configuring and managing a Load Balancer","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","what-is-a-hybrid-cloud#\u003cstrong\u003eWhat is a Hybrid Cloud?\u003c/strong\u003e":""},"title":"What is a Hybrid Cloud and why is it Important?"},"/utho-docs/docs/blog/what-is-a-vpn-and-how-can-it-benefit-your-business/":{"data":{"":"","1-secure-data-transfer#\u003cstrong\u003e1. Secure Data Transfer\u003c/strong\u003e¬†":"","2-easy-accessibility#\u003cstrong\u003e2. Easy Accessibility\u003c/strong\u003e¬†":"","3-cost-savings#\u003cstrong\u003e3. Cost Savings\u003c/strong\u003e¬†":"","conclusion#\u003cstrong\u003eConclusion:\u003c/strong\u003e ¬†":"A Virtual Private Network (VPN) is a secure connection between two networks, usually between an internal network (such as your company‚Äôs private network) and an external network (such as the internet). VPNs are used to protect data from being intercepted or accessed by unauthorized users. They also provide added security for remote workers who access the company‚Äôs internal networks. In this article, we‚Äôll discuss how a VPN can benefit your business.¬†How VPN can benefit your business\n1. Secure Data Transfer¬†The main purpose of VPNs is to keep data secure while it is transferred over public networks, such as the internet. By using encryption technology, data transmitted over the VPN is encrypted and can only be decoded by authorized personnel on either end of the connection. This ensures that confidential information remains confidential and cannot be accessed by unauthorized users. Additionally, since the data traffic is routed through a secure tunnel, it is impossible for outsiders to intercept or alter any of the communication.¬†2. Easy Accessibility¬†VPNs make it easy for employees to access their company‚Äôs resources from any location with an internet connection. Employees can securely connect to their company‚Äôs internal networks without having to worry about their data being intercepted or compromised in any way. Additionally, employees can work remotely without losing access to important resources that are stored on the company‚Äôs private network. This allows them to remain productive even when they are away from the office.¬†3. Cost Savings¬†Using a VPN can help businesses save money by eliminating some of the costs associated with setting up physical networks and running cables between locations. Additionally, since VPNs allow remote employees to easily access their company‚Äôs resources without having to travel back and forth between offices, companies can save money on travel expenses as well. Finally, businesses can save money by not having to purchase additional hardware or software needed for traditional networking solutions such as leased lines or dedicated servers. Combined with other cost-saving measures such as cloud computing and telecommuting policies, using a Virtual Private Network can significantly lower overhead costs for businesses of all sizes.¬†Conclusion: A Virtual Private Network (VPN) provides many benefits for businesses looking to improve their digital security while also saving money in overhead expenses. With its ability to securely encrypt data while transferring over public networks, a VPN ensures that critical information stays safe from outside threats such as hackers or malware attacks. It also provides easy accessibility for remote employees who need access to their company‚Äôs resources without having to compromise security in any way. Finally, using a Virtual Private Network (VPN) helps businesses reduce overhead costs associated with traditional networking solutions such as leased lines and dedicated servers which makes it an attractive solution for small-scale businesses looking for cost-effective ways of connecting multiple locations securely and efficiently. All these factors make using a VPN an attractive option for business owners looking for reliable protection against cyber threats while still reducing operational costs."},"title":"What is VPN and how can it benefit your business?"},"/utho-docs/docs/blog/what-is-cloud-native-application-development-and-why-is-it-important/":{"data":{"":"","best-practices-for-cloud-native-development#Best Practices for Cloud-Native Development":"When it comes to cloud-native development, there are several best practices that can help ensure success:\nDesign for failure: Cloud-native applications should be designed with the assumption that failure is inevitable. This means building in redundancy and failover capabilities to ensure that your application remains available even in the event of a failure.\nUse microservices architecture: Cloud-native applications should be built using a microservices architecture. This allows for greater flexibility and scalability, as individual services can be scaled independently of one another.\nAutomate everything: Automation is a key component of cloud-native development. By automating tasks such as deployment and scaling, you can improve efficiency and reduce the risk of human error.","conclusion#Conclusion":"Cloud-native application development is an essential part of the modern business landscape. By leveraging cloud services and building applications that are designed to run natively in the cloud, organizations can achieve greater efficiency, flexibility, and reliability. To ensure success, it is important to follow best practices such as designing for failure, using a microservices architecture, and automating everything.\nAt Microhost, we offer a range of cloud hosting solutions designed to meet the needs of businesses of all sizes. Our cloud hosting solutions are designed to provide the performance, scalability, and reliability that modern businesses demand. To learn more, visit our website at https://utho.com/.\nAlso Read: Best Practices for Managing and Securing Edge Computing Devices","importance-of-cloud-native-development#Importance of Cloud-Native Development":"Cloud-native development is important for several reasons, including:\nScalability: Cloud-native applications can easily scale up or down to meet changing business demands. This ensures that your application can handle increased traffic without any downtime or performance issues.\nEfficiency: Cloud-native applications are designed to be efficient, allowing you to save money on infrastructure costs. By taking advantage of cloud services, you can reduce the need for physical infrastructure and optimize resource utilization.\nFlexibility: Cloud-native development provides a high degree of flexibility, allowing you to choose the services and tools that best meet your business needs. This gives you the freedom to choose the best solution for your organization without being locked into a specific vendor.\nReliability: Cloud-native applications are designed to be highly available and reliable, with built-in redundancy and failover capabilities. This ensures that your application is always up and running, even in the event of a system failure.","introduction#Introduction":"Cloud-native application development has been gaining immense popularity in recent years due to its numerous benefits. In a world where businesses are rapidly adopting cloud technology, cloud-native development is becoming an increasingly important concept. But what exactly is cloud-native development, and why is it so important?","understanding-cloud-native-development#Understanding Cloud-Native Development":"Cloud-native development is a software development approach that is designed to leverage the benefits of cloud computing. It is an architectural approach that is focused on building applications that can run natively in the cloud environment. Cloud-native applications are built using cloud services and are specifically designed to take advantage of cloud computing infrastructure."},"title":"What is Cloud-Native Application Development and Why is it Important?"},"/utho-docs/docs/blog/what-is-iaas-paas-and-saas/":{"data":{"":"","1-what-is-iaas-and-its-benefits#1. What is IAAS and its benefits":"","2-what-is-paas-and-its-benefits#2. What is PAAS and its benefits":"","3-what-is-saas-and-its-benefits#3. What is SAAS and its benefits":" What is IAAS, PAAS and SAAS\nIn this tutorial, we will learn what is IAAS, PAAS and SAAS. Three of the most important categories of cloud services are IaaS, PaaS, and SaaS. They go by the names ‚Äúcloud service models‚Äù and ‚Äúcloud computing service models,‚Äù respectively.\n‚ÄúAs a service‚Äù refers to how IT assets are used in these services, which is the main difference between standard IT and cloud computing. In standard IT, an organisation uses IT assets like hardware, system software, development tools, and apps by buying, putting, controlling, and keeping them in its own on-premises data centre. In cloud computing, the cloud service provider owns, controls, and keeps the assets. The customer uses the assets through an Internet link and pays for them on a contract or pay-as-you-go basis.\nSo, the major benefit of IaaS, PaaS, SaaS, or any other ‚Äúas a service‚Äù option is that it saves money. A customer can access and grow the IT skills it needs at a fixed cost, without having to buy and manage everything in its own data center, which would be expensive and time-consuming. But each of these options has other benefits that aren‚Äôt shared by the others.\n1. What is IAAS and its benefits Hardware as a Service is another name for IaaS. (HaaS). It is a system of computers that is run over the internet. Systems as a Service (IAAS) is a way to offer on-demand services for IT systems. It is one of the three main service types for the cloud. The user buys computers, software, data centre room, or network equipment, and then rents those resources through a fully outsourced, on-demand service model. It can grow and shrink on the fly, and the resources are shared as a service. It usually means that more than one person can use the same piece of gear.\nIt‚Äôs up to the customer to choose its tools carefully and according to what it needs. It also lets you handle your bills.\nBenefits: Availability: The company can put the tools into the surroundings of a customer at any time. Economic: It could be used by a very large number of people.It saves a lot of money and is easy to grow. Companies can pay for the very high costs of putting in place new technologies. Scalability: When delivering resources, such as virtual machines, apps, storage, and networks, the service has several choices. Its users can change the size of their business based on their needs. Flexibility: The design is provided by the cloud. Better scale and a lot of flexibility. There is support for dynamic tasks. Disaster recovery: Instead of putting up additional computers in different places, IaaS can add its disaster recovery option to the cloud provider‚Äôs already-existing infrastructure, which is spread out geographically. 2. What is PAAS and its benefits PaaS, Platform as a service, is a cloud computing tool that lets programmers build, test, run, and control apps.\nPaaS gives you a tool in the cloud that you can use to build, run, and manage apps. The cloud services provider hosts, manages, and maintains all the hardware and software on the platform. This includes servers (for development, testing, and deployment), operating system (OS) software, storage, networking, databases, middleware, runtimes, frameworks, and development tools, as well as services for security, operating system and software upgrades, backups, and more.\nThe PaaS is accessed by users through a graphical user interface (GUI), where development or DevOps teams can work together on all aspects of their work, such as writing, merging, testing, delivery, release, and feedback.\nBenefits: Programmers don‚Äôt have to worry about what database or programming language was used to make an app. Tension free: It gives developers the chance to build apps without having to worry about the operating system or infrastructure underneath. Focused: Allows writers to focus on the design of the application while the platform handles the language and database. Manageable: It can be changed and moved around. Economic: It‚Äôs not too pricey. Hassle free: It does a great job of managing application development steps in the cloud. 3. What is SAAS and its benefits On-demand software is another name for SaaS. It is a type of software in which the apps are stored by a cloud service company. Users can use a computer browser and an internet connection to get to these apps.\nSaaS is ready-to-use application software that is stored in the cloud. It is sometimes called ‚Äúcloud application services.‚Äù Users pay a monthly or annual fee to use a full programme in an online browser, PC client, or mobile app. The SaaS provider hosts and manages the application and all of the hardware that is needed to run it, such as servers, storage, networking, tools, application software, and data storage.\nThe software provider takes care of all updates and fixes, which customers usually don‚Äôt notice. As part of a service level agreement, the provider usually guarantees a certain level of uptime, speed, and safety. (SLA). Customers can pay extra to add more people and data storage when they want to.\nSaaS is used by almost everyone who has a computer or a cell phone today. People use SaaS apps like email, social media, and file storing in the cloud (like Dropbox or Box) every day in their daily lives. Salesforce (customer relationship management software), HubSpot (marketing software), Trello (workflow management), Slack (sharing and messaging), and Canva are all popular business or workplace SaaS options. (graphics). Adobe Creative Suite and other apps that were originally built for the PC are now offered as SaaS. (e.g., Adobe Creative Cloud).\nBenefits: Computeable: It is a type of cloud computing service that offers a wide range of skills and services that are stored. These can be used to build and run software apps that run on the web. Economic: It has a lower cost of ownership than software that is installed on-site. This is because you don‚Äôt have to buy or install any tools or licences. Easy to use: It is easy to get to by using a browser on a thin computer. Free Use: The first setting doesn‚Äôt cost anything. Low or zero maintenance: Low cost to keep up. No Installation: Less time is needed for installation, so time is used well. And this is all you will learn in this tutorial- What is IAAS, PAAS and SAAS"},"title":"What is IAAS, PAAS and SAAS"},"/utho-docs/docs/blog/what-is-iostat-command-and-how-to-use-it/":{"data":{"":"","how-to-install-iostat-command-in-linux#\u003cstrong\u003eHow to install iostat command in Linux\u003c/strong\u003e":"","how-to-use-iostat-command#\u003cstrong\u003eHow to use IOSTAT command:\u003c/strong\u003e":" What is IOSTAT command and how to use it\nDescription: In this tutorial, we will learn what the IOSTAT command and how to use it. With the iostat command, you can see how busy your system‚Äôs input/output devices are by comparing the amount of time they are active to their average transfer rates.\nThe iostat command makes reports that can be used to change how the system is set up so that the input/output load is more evenly spread across the physical discs.\nHow to install iostat command in Linux Prerequisites: To install the iostat command you must be either super user or a normal user with SUDO privileges. The iostat command does not come pre-installed with Linux distributions, but it is part of the default package. This means that the package manager of the Linux distribution can be used to install it.\nIn Fedora/ Redhat/ Centos:\n# yum install sysstat -y In Ubuntu/ Debian:\n# apt install sysstat -y How to use IOSTAT command: To generate the default or basic report using iostat, you can simply type iostat in your terminal and hit enter\n# iostat output of iostat comand\nThe iostat command makes reports that are divided into two sections: the CPU Utilization report and the Device Utilization report.\nCPU Utilization report: The CPU Utilization Report shows how well the CPU is working based on different parameters. Here‚Äôs what these parameters mean:\n%user: CPU utilization in percentage that was used while running user processes.\n%nice: CPU utilization in percentage that was used while running user processes with nice priorities.\n%system: CPU Utilization in percentage that was used while running kernel.\n%iowait: Show the percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.\n%steal: Show the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hypervisor was servicing another virtual processor.\n%idle: Show the percentage of time that the CPU or CPUs were idle and the system did not have an outstanding disk I/O request.\nDisk Utilization Report:\nThe Device Utilization Report is the second report that is made by the iostat command. The device report gives information about statistics for each physical device or partition. On the command line, you can enter statistics to be shown for block devices and partitions.\nIf you don‚Äôt enter a device or partition, statistics are shown for every device the system uses, as long as the kernel keeps statistics for it.\nIn this report too, it show the report on different parameters. Here‚Äôs what these parameters mean:\nDevice: This column gives the device (or partition) name as listed in the /dev directory.\ntps: The number of transfers per second that were sent to the device. If tps is high, it means the processor is working hard.\nKB_read/s: Indicate the amount of data read from the device\nkB_wrtn/s: Indicate the amount of data written to the device\nkB_dscd/s: It displays the rate of data discarded by the CPU per second\nkB_read: It displays the amount the data read\nkB_wrtn: It displays the amount the data written\nKB_dscd: It display the amount the data discarded\nYou can find the other useful paramethers you can find in the output related to iostat command.\nrps: Indicates the number of read transfers per second.\nKrps: here, K represent is kilo which means 1000. So it defines as above but of value of 1000.\nTo display the report only for CPU Utilization:\n# iostat -c To display the report only for Disk Utilization:\n# iostat -d To display a continuous device report at two-second intervals.\n# iostat -d 2 If you want to generate n reports in a given time, use the below format to execute the command\niostat interval-time numbers_of_time For example, to generate 6 report for every 2 seconds, but for only devices\n# iostat -d 2 6 To generate a report for a specific device.\n# iostat sda # or any other device To generate a report for a specific device with all its partition.\n# iostat -p sda # or any other device output some advance\nIn conclusion, you have learned that what is IOSTAT command and how to use it."},"title":"What is IOSTAT command and how to use it"},"/utho-docs/docs/blog/what-is-kubernetes-and-why-is-it-important/":{"data":{"":"","conclusion#Conclusion:¬†":"K8s has emerged as one of the most popular tools for managing cloud-based applications thanks to its portability, scalability, and security features. If you‚Äôre looking for a flexible solution for running your app in the cloud, Kubernetes should definitely be on your shortlist.","kubernetes-is-portable#\u003cstrong\u003eKubernetes Is Portable\u003c/strong\u003e¬†":"","kubernetes-is-scalable#\u003cstrong\u003eKubernetes Is Scalable\u003c/strong\u003e¬†":"","kubernetes-is-secure#\u003cstrong\u003eKubernetes Is Secure\u003c/strong\u003e¬†":"In this blog post, we‚Äôll explore what Kubernetes is and why it‚Äôs become such an important tool for managing cloud applications. We‚Äôll also touch on some of the features that make Kubernetes so powerful and popular. By the end of this post, you should have a good understanding of what Kubernetes is and why it might be a good fit for your own cloud computing needs.¬†Kubernetes also known as k8s is a system for managing containerized applications across a cluster of servers. It provides a platform for automating deployment, scaling, and operations of application containers across clusters of hosts. Basically, with Kubernetes you can take a bunch of software applications, store them in ‚Äúcontainers,‚Äù and then spread those containers across multiple servers.¬†The idea behind containerization is that it makes it easy to move applications around because everything the application needs to run is packaged up nicely in the container. That way, you don‚Äôt have to worry about dependencies or configuration issues when you move the application from one server to another\nKubernetes was originally designed by Google, who used it to run some of their most popular services like Gmail and YouTube.\nKubernetes Is Portable¬†One of the great things about Kubernetes is that it‚Äôs portable. This means that you can run Kubernetes on any public or private cloud provider. You‚Äôre not locked into using a specific provider‚Äôs services. For example, if you‚Äôre using Amazon Web Services (AWS) today but want to switch to Microhost tomorrow, you can do that without having to rewrite your entire application or infrastructure.¬†Kubernetes Is Scalable¬†Kubernetes was designed from the ground up to be scalable. This means that you can start small with just a few servers and then easily add more servers as your needs grow. You can also selectively remove servers from the cluster if you need to save money or reduce capacity. This scalability makes K8s an ideal solution for businesses that are growing quickly or that have unpredictable workloads.¬†Kubernetes Is Secure¬†Security is always a top concern when it comes to managing critical business applications. With K8s, you can choose from a variety of authentication methods to control who has access to your cluster. You can also use role-based access control (RBAC) to granularly control what individual users can do within the cluster. Finally, Kubernetes integrates with many existing security solutions like network firewalls and intrusion detection/prevention systems (IDS/IPS). These features make Kubernetes an appealing option for businesses with strict security requirements.¬†"},"title":"What is Kubernetes and Why is it important?"},"/utho-docs/docs/blog/what-is-nginx/":{"data":{"":"NGINX is an open source web server with high load balancing capabilities, reverse proxy and cache features. It was planned initially to solve problems of scaling and rivalry with existing web servers. It is one of today‚Äôs most popular web servers, due to its event-based asynchronous architecture.","add-a-basic-site#Add a Basic Site":"1. Build your site‚Äôs new tab. Substitute abc.com domain name for your article.\nsudo mkdir -p /var/www/abc.com 2. Use SELinux‚Äôs chcon command to change the file security context for web content:\nsudo chcon -t httpd_sys_content_t /var/www/abc.com -R sudo chcon -t httpd_sys_rw_content_t /var/www/abc.com -R 3. In the /var/www/abc.com directory you may add your site‚Äôs files. Creates an index file with a basic example of \"Microhost Cloud.\" Build a new /var/www/abc.com/index.html file with your preferred text editor. Substitute abc.com by the name of your website or the public IP address of your cloud server.\nMy Basic Website Microhost Cloud!","before-you-begin#Before You Begin":"1. In the Start and Securing your Application Guides, set up your cloud application.\n2. You can customize this using our DNS Manager guide if you want your site‚Äôs custom domain name.\n3. You should not forget to update your file /etc/hosts with the public IP address and the full domain name of your site, as explained in the hosts file section of the Getting Started Update Guide your system your network.","configure-nginx#Configure NGINX":"In /etc/nginx/sites-available NGINX site-specific settings files are stored and symlinked to a /etc/nginx /sites-enabled/. Typically, for each domain or sub-domain you host, you generate a new server block file in the directory you have accessed . Then, a connection to your files will be set up in the website-enabled directory.\n1. Make your configuration files directories:\nsudo mkdir -p /etc/nginx/{sites- available,sites-enabled} 2. In the text editor of your choosing, build your site configuration file. Replace abc.com with your site‚Äôs domain name or IP address in the server name directive and /var/www/abc.com with your own root address in the root directive.\nserver { listen 80; listen [::]:80; server_name abc.com; root /var/www/abc.com; index index.html; location / { try_files $uri $uri/ =404; } } 3. To allow your configuration, set a new symlink for the directory /etc/nginx/sites-enabled/\nsudo ln -s /etc/nginx/sites-available/abc.com /etc/nginx/sites-enabled/ 4. To include a directive in a /etc/nginx/sites-enabled/* directory, update NGINX configuration file /etc/nginx/nginx.conf. It must be part of the http block of your settings files¬†include /etc/nginx/conf.d/*.conf; line to list the inclusive directives.\n‚Ä¶ http { ‚Ä¶ include /etc/nginx/conf.d/_.conf; include /etc/nginx/sites-enabled/_; ‚Ä¶ } 5. Enable the traffic firewall:\nsudo firewall-cmd --zone=public --permanent --add-service=http sudo firewall-cmd --zone=public --permanent --add-service=https sudo firewall-cmd --reload ","install-nginx#Install NGINX":"Now, it is safer to use the version included in the CentOS repositories to install NGINX on CentOS 8:\nsudo yum update sudo dnf install @nginx ","test-and-enable-nginx#Test and Enable NGINX":"1. This command helps you to check your NGINX setup:\n``` sudo nginx -t\n2\\. Start the service with the commands below: sudo systemctl enable nginx sudo systemctl start nginx\n3\\. Check that it‚Äôs running: sudo systemctl status nginx\n4\\. Browse the domain name or IP address of your cloud server in the browser. Your easy page should be seen. "},"title":"What is NGINX?"},"/utho-docs/docs/blog/when-was-artificial-intelligence-invented/":{"data":{"":"","ais-impact-today#AI\u0026rsquo;s Impact Today":"\nIntroduction Artificial Intelligence (AI) has revolutionized the modern world, transforming industries and reshaping our lives. In this article, we will explore the timeline of AI‚Äôs invention, uncovering its origins and key milestones. Join us on a captivating journey as we trace the evolution of AI and its profound impact on society.\nThe Birth of AI The concept of artificial intelligence emerged in the 1950s when visionary researchers began envisioning intelligent machines. However, AI, as we know it today, has evolved over the years through remarkable advancements in computing power and algorithms.\nThe Dartmouth Conference: AI Takes Shape (1956) A significant milestone in AI‚Äôs history was the Dartmouth Conference of 1956. This groundbreaking event brought together leading researchers who shared a common vision: creating machines capable of replicating human intelligence. It marked the birth of AI as a field of study.\nEarly AI Breakthroughs In the subsequent years, researchers achieved notable breakthroughs in AI, including the development of expert systems and rule-based reasoning. These early AI systems focused on specific domains and tasks, such as chess-playing or mathematical problem-solving.\nThe AI Winter: Challenges and Setbacks (1970s-1980s) The 1970s and 1980s presented challenges and setbacks for the field of AI, leading to what became known as the ‚ÄúAI winter.‚Äù Sky-high expectations were not fully met, resulting in decreased funding and waning interest in AI research. Progress slowed down during this period.\nRise of Machine Learning: A Turning Point (1990s) The 1990s witnessed a resurgence of interest in AI with the advent of machine learning. Machine learning algorithms empowered computers to learn from data and improve their performance over time. This breakthrough opened doors to practical applications of AI across various domains.\nDeep Learning Revolutionizes AI (2000s) The 2000s saw the emergence of deep learning, a subfield of machine learning that revolutionized AI. Deep learning involves training artificial neural networks with multiple layers to recognize patterns and make complex decisions. It has led to significant advancements in image recognition, natural language processing, and autonomous vehicles.\nAI‚Äôs Impact Today Today, AI is seamlessly integrated into our daily lives. It powers voice assistants, recommendation systems, personalized advertisements, and fraud detection mechanisms, among others. The healthcare, finance, transportation, and other industries have experienced transformative changes driven by AI, revolutionizing the way we live and work.","conclusion#Conclusion":"The journey of artificial intelligence has been a captivating one, spanning decades of research, breakthroughs, and challenges. From its humble beginnings in the 1950s to the AI revolution of today, AI has come a long way. By embracing the transformative potential of AI, organizations can shape a future powered by intelligent machines.\nRead Also: Best Cloud Platform for Your Business","deep-learning-revolutionizes-ai-2000s#Deep Learning Revolutionizes AI (2000s)":"","early-ai-breakthroughs#Early AI Breakthroughs":"","introduction#Introduction":"","rise-of-machine-learning-a-turning-point-1990s#Rise of Machine Learning: A Turning Point (1990s)":"","the-ai-winter-challenges-and-setbacks-1970s-1980s#The AI Winter: Challenges and Setbacks (1970s-1980s)":"","the-birth-of-ai#The Birth of AI":"","the-dartmouth-conference-ai-takes-shape-1956#The Dartmouth Conference: AI Takes Shape (1956)":"","utho-cloud-harnessing-the-power-of-ai#Utho Cloud: Harnessing the Power of AI":"\nAI Solutions for Businesses Utho Cloud is a leading provider of cutting-edge AI solutions that empower businesses to leverage the full potential of AI. Their advanced technologies enable organizations to harness the power of data, automate processes, and gain invaluable insights.\nInnovation Across Industries Utho Cloud‚Äôs AI solutions are revolutionizing industries such as healthcare, finance, retail, and manufacturing. By harnessing AI-powered tools, businesses can drive operational efficiency, enhance customer experiences, and make data-driven decisions, gaining a competitive edge.\nExpert Guidance and Support Utho Cloud offers expert guidance and support to organizations embarking on their AI journey. Their team of AI specialists provides personalized assistance, ensuring a seamless integration process and optimal results."},"title":"When Was Artificial Intelligence Invented?"},"/utho-docs/docs/blog/why-artificial-intelligence-is-important/":{"data":{"":"","conclusion#Conclusion":"Artificial Intelligence is a critical component of our future. Its ability to enhance efficiency, drive innovation, and enable data-driven decision-making is transforming industries. Embracing AI allows businesses to stay competitive, improve customer experiences, and unlock new opportunities. With Utho Cloud‚Äôs expertise in AI solutions, organizations can harness AI‚Äôs power and shape a brighter future.\nRead Also: 6 Cloud Computing Myths, Busted!","introduction#Introduction":"Artificial Intelligence (AI) has become a transformative force in our world, revolutionizing industries and shaping our future. In this article, we will explore why AI is so important in a way that is easy to understand. We‚Äôll discuss the benefits and applications of AI and how it impacts our lives.","the-importance-of-artificial-intelligence#The Importance of Artificial Intelligence":"\nEnhancing Efficiency and Productivity AI automates repetitive tasks, saving time and allowing humans to focus on more complex activities. This boosts efficiency and helps organizations accomplish more in less time.\nEnabling Data-Driven Insights AI algorithms process vast amounts of data, uncovering meaningful patterns and insights. This helps businesses make data-driven decisions, identify trends, and gain a competitive edge.\nPersonalizing User Experiences AI analyzes user behavior, preferences, and feedback to provide personalized experiences. It powers recommendation systems, chatbots, and virtual assistants, enhancing customer satisfaction.\nAdvancing Healthcare and Medicine AI is transforming healthcare by speeding up disease diagnosis, predicting outcomes, and aiding in drug discovery. Medical devices and virtual health assistants powered by AI improve patient care and outcomes.\nDriving Innovation and Automation AI fuels innovation in autonomous vehicles, robotics, and advanced manufacturing. It streamlines processes, optimizes resource allocation, and drives automation, increasing productivity and cost savings.\nImproving Decision-Making AI systems analyze complex data sets and provide insights to support decision-making. By considering multiple factors and scenarios, AI helps make informed choices, reducing biases and improving accuracy.","understanding-artificial-intelligence#Understanding Artificial Intelligence":"Artificial Intelligence refers to the development of computer systems that can perform tasks requiring human intelligence. It includes machine learning, natural language processing, computer vision, and robotics. AI systems analyze data, learn from patterns, make decisions, and adapt to changing situations.","utho-cloud-harnessing-the-power-of-ai#Utho Cloud: Harnessing the Power of AI":"\nEmpowering Businesses with AI Solutions Utho Cloud is a leading AI innovator, empowering businesses to leverage the transformative power of AI. Their advanced AI technologies help organizations unlock the value of data, automate processes, and gain valuable insights.\nRevolutionizing Industries Utho Cloud‚Äôs AI solutions optimize operations, enhance customer experiences, and drive innovation across sectors like finance, healthcare, manufacturing, and retail. Their AI-powered tools extract actionable insights, improve decision-making, and provide a competitive edge.\nPersonalized Support and Expertise Utho Cloud‚Äôs team of AI experts offers personalized support, guiding businesses in adopting and implementing AI solutions tailored to their specific needs. They understand the unique challenges organizations face and deliver exceptional results."},"title":"Why Artificial Intelligence is Important"},"/utho-docs/docs/blog/why-firewalls-are-important-for-your-business/":{"data":{"":"","1-blocks-viruses-and-malware#\u003cstrong\u003e1) Blocks Viruses and Malware:\u003c/strong\u003e":"","2-prevents-hackers-from-gaining-access-to-your-network#\u003cstrong\u003e2) Prevents Hackers from Gaining Access to Your Network:\u003c/strong\u003e":"","3-saves-bandwidth#\u003cstrong\u003e3) Saves Bandwidth:\u003c/strong\u003e":"","4-reduces-spam-emails#\u003cstrong\u003e4) Reduces Spam Emails:\u003c/strong\u003e":"","5-protecting-against-malware#\u003cstrong\u003e5) Protecting Against Malware:\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion:\u003c/strong\u003e":"Introduction: A firewall is a network security system that monitors and controls the incoming and outgoing traffic of your network. It is basically a barrier between your computer network and the outside world. Firewalls are important because they can block viruses, malware, and other malicious software from infecting your computer systems.\nFirewalls are essential for defending your company from online threats.¬†In simple words a Cloud Based Firewall is a barrier that helps to prevent unauthorized access to your network. You can prevent hackers from accessing your systems and data by installing a trustworthy firewall.\nThe benefits of using a Cloud Based Firewall include:\n1) Blocks Viruses and Malware: Viruses and malware can infect your computer system through email attachments, downloads, and websites. By installing a firewall, you can block these malicious programs from entering your network.\n2) Prevents Hackers from Gaining Access to Your Network: Hackers can use different techniques to gain access to your computer network. Firewalls can prevent hackers from gaining access to your network by blocking their IP address.\n3) Saves Bandwidth: It is a great benefit, it can also save bandwidth by blocking unwanted traffic from entering your network.\n4) Reduces Spam Emails: Spammers often use public IP addresses to send spam emails. By installing it, you can block these IP addresses and reduce the amount of spam email you receive.\n5) Protecting Against Malware: Malicious software, sometimes known as malware, is any software that is designed to infect or disable computers or computer systems.¬†Viruses, Trojan horses, worms, spyware, adware, and ransomware are all examples of malware. On your computer system, malware can create havoc by causing data loss, damage, and even system failure. By preventing harmful traffic from reaching your network, it can aid in malware defence. It serves as the first line of defence against an attack by accomplishing this.\nConclusion: Firewalls are important because they can block viruses, malware, and other malicious software from infecting your computer systems. There are different types of it, but the most common ones are hardware firewalls, software firewalls, and cloud-based firewalls. The benefits of using it include saving bandwidth, reducing spam emails, increasing productivity, preventing hackers from gaining access to your network, and blocking viruses and malware.\nIf you don‚Äôt have a firewall in place already, now is the time to get! and MicroHost is a leading Cloud Service provider in India which can help you to secure your servers.","introduction#\u003cstrong\u003eIntroduction:\u003c/strong\u003e":""},"title":"Why Firewalls Are Important For Your Business || Benefits of Using a Firewall"},"/utho-docs/docs/blog/why-less-is-faster-than-more-command-for-effective-file-navigation/":{"data":{"":"","learn-linux-less-command#Learn Linux \u0026rsquo;less\u0026rsquo; Command":" Why less is Faster Than more Command for Effective File Navigation\nDescription\nIn this article we will learn Why less is Faster Than more Command for Effective File Navigation..\n‚ÄúMore‚Äù is a command line tool in Unix that is used to display the contents of a file in the console. The most fundamental way to use the more command is to execute it in opposition to a file, as demonstrated below: c\nFollow the below steps to learn Why less is Faster Than more Command for Effective File Navigation‚Ä¶\nLearn ‚Äúmore‚Äù Linux commands. # more /var/log/boot.log One other approach to employ additional commands in conjunction (pipe) with other ones, such as the cat command, as seen in the sample that follows:\n# cat /var/log/boot.log | more Press the Enter key to move through the file one line at a time, or press the Spacebar key to move through the file one page at a time, where one page corresponds to the screen size of your current terminal. Simply pressing the q key will cause the command to terminate.\nThe -number switch, which is part of the more command, is a helpful option since it allows you to determine how many lines each page should have. As an illustration, show one page of the auth.log file with the following 10 lines:\n# more -10 /var/log/boot.log # more +14 /var/log/boot.log Learn Linux ‚Äôless‚Äô Command In a manner analogous to that of more, the less command enables you to inspect the contents of a file and browse through that file. The most significant distinction between more and less is that the less command is significantly quicker. Unlike more, less does not load the entire file at once and makes it possible to navigate through the file by using the page up and page down keys.\nIn can be used as a solo command that is issued against a file, or it can be used with pipes with a wide variety of other Linux commands in order to narrow the screen output of those commands, allowing you to scroll over the results.\n# less /var/log/boot.log or\n# ls /etc | less You can go through the file line by line by pressing the Enter key. The spacebar key can be used to navigate the page. Your current terminal screen size represents the page size. To quit the command, use the q key, just as you did for the more command.\nThe /word-to-seach option of the less command is a handy feature. For example, by entering the /sshd string interactively, you can search and match all sshd messages from a log file.\nTo display a file starting at a certain line number, use the following syntax:\n# less +5 /var/log/boot.log If you need to count the number of lines in less, use the -N option.\n# less -N /var/log/yum.log By default, hitting the q key is the only way to leave the less command. Use the -e or -E option to change this behaviour and automatically end the file when it reaches the end:\n# less -e /var/log/boot.log # less -E /var/log/boot.log I really hope you understand all the steps carefully. Why less is Faster Than the more Command for Effective File Navigation.\nMust read :- https://utho.com/docs/tutorial/cheat-sheet-for-15-nmcli-commands-in-linux-rhel-centos/\nThank You","learn-more-linux-commands#Learn \u0026ldquo;more\u0026rdquo; Linux commands.":""},"title":"Why less is Faster Than more Command for Effective File Navigation"},"/utho-docs/docs/blog/why-ransomware-attacks-are-rising-and-how-you-can-protect-your-business/":{"data":{"":"","having-an-incident-response-plan#\u003cstrong\u003eHaving an Incident Response Plan\u003c/strong\u003e¬†":"Ransomware attacks are becoming increasingly common as cybercriminals look for new ways to steal data and extort money. Ransomware is a type of malicious software (malware) that encrypts your data and holds it hostage until you pay a ransom fee. This type of attack has become more prevalent in recent years, due to the fact that it can be so lucrative for hackers. In this article, we‚Äôll discuss why ransomware attacks are on the rise and how you can protect yourself from them.¬†Why Are Ransomware Attacks On The Rise?¬†Ransomware attacks are on the rise due to several factors, including increased computing power, more sophisticated hacking techniques, and a greater number of targets. The threat is especially concerning because even small businesses are now being targeted by hackers with ransomware. A recent report found that nearly 60% of ransomware victims were small businesses with fewer than 250 employees; however, larger companies with more resources still accounted for 40%. With the increasing availability of digital currency such as Bitcoin, it‚Äôs now easier than ever for criminals to accept payment without being traced.¬†How Can You Prevent A Ransomware Attack?¬†The best way to protect yourself from a ransomware attack is by taking preventive measures such as regularly updating your software and using reliable anti-virus/anti-malware protection on all your devices. Additionally, you should always back up your data on an external hard drive or cloud storage system so that if you do get attacked, you won‚Äôt lose any information or files. It‚Äôs also important to be aware of phishing emails or links sent via email or social media ‚Äì these are often used as a way for attackers to gain access to your computer or network.\nLet‚Äôs take a look at some key steps you can take to ensure your business is secure.¬†Avoid ransomware attacks with these tips\nPassword Protection¬†One of the most important ways to protect your business from ransomware attacks is by ensuring that all of your passwords are secure. All staff should be required to use strong passwords, and they should never share their passwords with anyone else. Additionally, you should consider using two-factor authentication (2FA) for added security. This ensures that even if someone were able to gain access to your system, they wouldn‚Äôt be able to do any damage without entering an additional code or password sent directly from you.¬†Regular Software Updates¬†Another key step you can take is installing regular software updates on all devices used by your staff. Software updates often contain security patches designed to help protect against malicious threats such as malware and ransomware attacks. If a device isn‚Äôt being used anymore, make sure it‚Äôs wiped clean and any data stored on it is securely destroyed before it‚Äôs disposed of so that no one can access it.¬†Train Your Staff¬†It‚Äôs also important that all staff members receive training on cybersecurity best practices, such as not clicking on suspicious links or attachments in emails or downloading applications from unknown sources. Regular training sessions will help ensure that everyone knows the proper procedures for handling sensitive data and staying safe when using the internet.¬†Having an Incident Response Plan¬†Finally, every business should have an incident response plan in place in case of a cyberattack or data breach. This plan should include detailed steps outlining who needs to be contacted and what steps need to be taken if there is ever a suspected breach or attack on the company‚Äôs systems or data. Having this plan in place ahead of time will help reduce the impact and duration of any potential attack so that normal operations can resume quickly and safely.¬†There are many steps you can take to ensure your business stays safe from cyberattacks like ransomware attacks. From password protection and regular software updates, to training staff and having an incident response plan ready for any potential breaches, these measures will help keep your business protected against malicious threats online at all times. Taking the necessary precautions now could save you from severe damage down the line, so make sure you‚Äôre doing everything possible to keep your organization safe!","how-can-you-prevent-a-ransomware-attack#\u003cstrong\u003eHow Can You Prevent A Ransomware Attack?\u003c/strong\u003e¬†":"","password-protection#\u003cstrong\u003ePassword Protection\u003c/strong\u003e¬†":"","regular-software-updates#\u003cstrong\u003eRegular Software Updates\u003c/strong\u003e¬†":"","train-your-staff#\u003cstrong\u003eTrain Your Staff\u003c/strong\u003e¬†":"","why-are-ransomware-attacks-on-the-rise#\u003cstrong\u003eWhy Are Ransomware Attacks On The Rise?\u003c/strong\u003e¬†":""},"title":"Why Ransomware Attacks Are Rising and How You Can Protect Your Business."},"/utho-docs/docs/blog/will-artificial-intelligence-replace-humans/":{"data":{"":"","introduction#Introduction":"In the ever-evolving world of technology, the emergence of artificial intelligence (AI) has sparked intriguing discussions about its potential to replace human workers. This article aims to shed light on this thought-provoking question, exploring both the advantages and limitations of AI technology. We will delve into this topic in a user-friendly manner, providing valuable insights for better understanding.","the-advantages-of-artificial-intelligence#The Advantages of Artificial Intelligence":"\nIncreased Efficiency AI technologies have the potential to automate repetitive and mundane tasks, freeing up human workers to focus on more complex and strategic activities. This enhances overall productivity and efficiency across industries.\nImproved Accuracy AI algorithms excel at processing vast amounts of data with precision and accuracy, minimizing human errors and providing more reliable results. This is particularly beneficial in fields such as healthcare, finance, and manufacturing, where accuracy is crucial.\nEnhanced Decision-Making AI systems have the capability to analyze extensive data sets and provide valuable insights to support decision-making. By considering numerous factors and patterns, AI can assist humans in making informed choices, leading to better outcomes.\nHandling Complex Tasks AI is adept at tackling complex tasks that involve processing massive datasets and performing intricate calculations. This capability has significant implications in scientific research, data analysis, and problem-solving.","the-future-of-ai-and-human-collaboration#The Future of AI and Human Collaboration":"Rather than entirely replacing humans, AI is more likely to augment human capabilities and reshape the nature of work. Collaborative efforts between humans and AI systems can unlock new possibilities and drive innovation across industries. By leveraging AI to automate routine tasks and enhance decision-making, humans can focus on higher-value activities that require creativity, empathy, and complex problem-solving.","the-limitations-of-artificial-intelligence#The Limitations of Artificial Intelligence":"\nLack of Human Creativity While AI can process data and generate insights, it currently lacks the creative and innovative thinking that humans possess. Tasks that require intuition, emotional intelligence, and artistic expression are still better suited to human capabilities.\nEthical Considerations The adoption of AI raises ethical concerns regarding data privacy, bias in decision-making algorithms, and potential job displacement. Addressing these ethical implications is vital to ensure fairness, transparency, and accountability in AI development and deployment.\nContextual Understanding AI systems struggle to comprehend complex human emotions, nuances of language, and cultural contexts. Professions that involve customer service, counseling, and interpersonal communication rely on human interaction and empathy.\nLimitations in Unstructured Environments While AI excels in structured environments with well-defined rules, it can face challenges when confronted with unstructured situations or unexpected events. Humans possess adaptability and critical thinking skills necessary for such scenarios.","understanding-artificial-intelligence#Understanding Artificial Intelligence":"Artificial intelligence refers to the development of computer systems capable of performing tasks that typically require human intelligence. AI systems are designed to analyze data, recognize patterns, make decisions, and even learn from experience. With advancements in machine learning and deep learning algorithms, AI has made significant strides in various domains.","utho-cloud-embracing-the-power-of-ai#Utho Cloud: Embracing the Power of AI":"Utho Cloud recognizes the transformative potential of AI and provides innovative solutions to help businesses harness its benefits. As a leading provider of AI-driven technologies, Utho Cloud empowers organizations to optimize their operations, make data-driven decisions, and achieve their business objectives. To learn more about Utho Cloud and their AI offerings, visit their website at utho.com.\nIn conclusion, while artificial intelligence has the potential to automate certain tasks and improve efficiency, it is unlikely to completely replace humans. The collaboration between AI and human intelligence will shape the future of work, leading to new opportunities and advancements across industries.\nRead Aslo: How to Configure FTP Server on Windows Server 2019"},"title":"Will Artificial Intelligence Replace Humans?"},"/utho-docs/docs/change-and-update-password-of-cpanel-account/":{"data":{"":"\nStep 1: Login into you Cpanel by opening ‚Äú server-ip:2087 ‚Äù in the URL of the browser\nStep 2: login into the WHM using root credentials\nStep 3: Open List Accounts\nStep 4: Open the account of cpanel by clicking on the Cpanel Icon\nStep 5: Open the password and security Option\nStep 6: Enter old and new password and click on the change your password now\nPassword has been changed/Updated\nThank you :)"},"title":"CHANGE AND UPDATE PASSWORD OF CPANEL ACCOUNT"},"/utho-docs/docs/check-disk-utilization-in-details-in-windows/":{"data":{"":" Check Disk-utilisation in details on Windows\nIn this article, you will learn how to check Disk Utilization in details in Windows. WinDirStat is a free and open-source graphical disk usage analyzer for Microsoft Windows. It presents a sub-tree view with disk-use percentage alongside a usage-sorted list of file extensions that is interactively integrated with a colorful graphical display (a treemap). Created as an open-source project released under the GNU GPL, it was developed using Visual C++/MFC 7.0 and distributed using SourceForge. The project was inspired by SequoiaView, an application based on research done by the Visualization Section of the Faculty of Mathematics and Computer Science at the Technische Universiteit Eindhoven.","1--steps-to-install-windirstat-on-windows#1- Steps to install WinDirStat on Windows.":"Step 1: Download the WInDirStat on your windows server. You can use this link.\nDownload the WinDirStat page\nStep 2: Once the application downloaded, go to your downloaded folder and click on the .exe to run the application.\nS\nStep 3 Once you opened the application, you will see the screen like below. Click on Next button.\nStep 4: After clicking on Next button on above step, you will now see the screen as below.Click on Next Button.\nStep 5: Now, the application should be installing on the path you have selected in the previous step. Click on Next Button.\nStep 6: Now, you just need to click on Close button.","2--step-to-check-the-disk-utilisation-on-your-machine#2- Step To check the disk utilisation on your machine":"Step 2.1: Now, after opening, you will see the windows just like to below windows. Select the ‚ÄúAll Local Drives‚Äù and then OK button\nStep 2.2: After doing this, you will see the below windows.\nNow, you have learnt check Disk Utilization in details in Windows","prerequisites#Prerequisites":" Administrator level user to download and install new package\nInternet enabled on server to download the WINDIRSTAT. If you want to have a fully functional server at minimul cost, check this out."},"title":"Check Disk Utilization in details in Windows"},"/utho-docs/docs/cloud-automation-empowering-business-dynamics/":{"data":{"":"","how-can-utho-assist-in-expanding-your-cloud-automation-strategy#\u003cstrong\u003eHow can Utho assist in expanding your cloud automation strategy?\u003c/strong\u003e":"In the dynamic digital landscape, businesses are actively pursuing innovative solutions to enhance efficiency, boost productivity, and expedite growth. The advent of cloud automation stands out as a revolutionary force, reshaping organizational operations and unlocking unprecedented potential. Explore the transformative impact of this technology and its pivotal role in empowering businesses to flourish in the digital era.\nWhat is automation technology in cloud computing? Automation in cloud computing streamlines tasks in cloud computing, leveraging technology to reduce manual operations and enhance efficiency. Using scripting, APIs, and other tools, it creates automated workflows for seamless management of cloud resources in diverse environments.\nWhat are the advantages of implementing cloud automation in business? Introducing cloud automation in businesses yields substantial advantages, greatly improving operational efficiency. Some of these benefits include:\nIncreased efficiency and productivity: Cloud automation markedly enhances both efficiency and productivity. By automating repetitive and time-consuming tasks, it liberates your team to concentrate on strategic initiatives. Additionally, it expedites the delivery of IT services, empowering your business to respond swiftly to changes in the marketplace.**\nImproved reliability and consistency:** Guarantees consistent task execution every time. This uniformity minimizes the risk of errors and enhances the reliability of your business operations.\nEnhanced security and compliance: Empowered by inherent governance and security protocols. The technology aids your business in adhering to industry standards and regulations. Automated tasks inherently pose lower security risks compared to manual execution, further fortifying your business against potential breaches.\nScalability and flexibility: Facilitates seamless scalability of operations aligned with business growth. Additionally, it provides the flexibility to adapt to evolving technologies and changing business requirements.\nWhat are the emerging trends in cloud automation? In the ever-evolving landscape of technology, the domain of automation in cloud computing witnesses the rise of various impactful trends that are shaping the future of cloud computing and operations. Notable trends include:\nAI-enabled automation: Incorporating artificial intelligence (AI) and machine learning (ML) into cloud automation tools enhances the intelligence and adaptability of automated processes.\nServerless automation: Utilizing serverless computing platforms for executing scalable and cost-effective automation scripts, eliminating the necessity for manual infrastructure management.\nMulti-cloud automation: Streamlining the management and orchestration of resources across various cloud providers to ensure seamless operations in a multi-cloud environment.\nSecurity and compliance automation: Incorporating cloud automation with security compliance frameworks to conduct automated security checks and audits, thereby enhancing overall cloud security.\nThese trends open up new opportunities for organizations to optimize their cloud operations and harness the advantages of cutting-edge technologies such as AI and serverless computing.\nIn which domains is cloud automation used? Several critical domains consistently feature automation for efficient cloud environment management. Examples of these use cases include:\nProvisioning Resources: Global cloud applications often spread across multiple cloud platforms to address geographic and jurisdictional considerations. SaaS providers hosting worldwide leverage automation to ensure performance, compliance with local regulations, and seamless updates through streamlined code and data backups.\nOrchestrating Workloads: Automation and orchestration differ; automation strings tasks into workflows for controlling repetitive tasks, while orchestration directs the automation flow to manage broader strategies. For instance, automate local cloud resource provisioning while orchestrating data flows between that instance and a public cloud storage service provider in a multi-cloud setup.\nOngoing Monitoring: Monitoring plays a pivotal role in both security and optimization. Hence, automation is integral to conducting monitoring operations, collecting and analyzing real-time data, and delivering insights to administrators.\nHow can Utho assist in expanding your cloud automation strategy? Unlock business potential with Utho‚Äôs cloud automation services aim to streamline the intricacies of cloud management. With features such as automated backups, scaling, floating IPs, and team accounts, Utho automates diverse tasks, enhancing the efficiency and simplicity of cloud operations. Offering a range of tools and services, Utho provides solutions for automating various aspects of cloud operations.\nOpting for automation in cloud computing with Utho can yield time and cost savings, enhance efficiency, and mitigate the risk of human errors for your business. If you‚Äôre contemplating the integration of cloud automation into your business, explore the array of automation possibilities in cloud computing with Utho.","in-which-domains-is-cloud-automation-used#\u003cstrong\u003eIn which domains is cloud automation used?\u003c/strong\u003e":"","what-are-the-advantages-of-implementing-cloud-automation-in-business#\u003cstrong\u003eWhat are the advantages of implementing cloud automation in business?\u003c/strong\u003e":"","what-are-the-emerging-trends-in-cloud-automation#\u003cstrong\u003eWhat are the emerging trends in cloud automation?\u003c/strong\u003e":"","what-is-automation-technology-in-cloud-computing#\u003cstrong\u003eWhat is automation technology in cloud computing?\u003c/strong\u003e":""},"title":"Cloud Automation: Empowering Business Dynamics"},"/utho-docs/docs/cloud-cost-optimization-maximizing-efficiency-and-saving/":{"data":{"":"","how-can-one-define-cloud-cost-optimization#\u003cstrong\u003eHow can one define cloud cost optimization?\u003c/strong\u003e":"","how-did-utho-implement-strategies-to-reduce-the-clients-annual-cloud-expenses#\u003cstrong\u003eHow did Utho implement strategies to reduce the client\u0026rsquo;s annual cloud expenses?\u003c/strong\u003e":"Migrating operations to the cloud presents numerous advantages, extending access to enterprise-grade infrastructure and services beyond exclusively large corporations with substantial IT budgets. However, regardless of the cloud service provider users must eventually grapple with the importance of cloud cost optimization. The growth potential for small businesses should not be underestimated, and adopting a forward-thinking approach involves strategic planning, offering distinctive perspectives, and potentially reaping substantial rewards.\nHow can one define cloud cost optimization? Cloud cost optimization involves identifying methods to operate applications in the cloud, carrying out tasks or delivering value to the business with minimal expenses, and utilizing cloud providers in a cost-effective manner. The optimization process spans from basic business management to intricate disciplines such as operations research, decision science, analytics, and modeling and forecasting in scientific and engineering domains.\nWhat makes prioritizing cloud cost optimization essential? In the age of digital transformation, businesses widely adopt cloud computing for its flexibility and scalability. Yet, managing and optimizing costs in the cloud poses challenges. The solution lies in cloud cost optimization, offering various benefits.\nMaximizing return on investment (ROI): Cloud providers provide scalable resources, but cloud cost optimization enables businesses to pay precisely for their usage. Effectively managing cloud expenses ensures companies derive optimal value from each service, enhancing their return on investment (ROI).\nEnsuring financial predictability: Variable prices in cloud bills create confusion for businesses, as operating within a fixed budget becomes challenging. Emphasizing cloud cost optimization introduces predictability to cloud expenses, enabling more effective financial planning and mitigating the risks of unforeseen budget overruns.\nEncouraging efficient resource use: Cloud cost optimization is closely linked with resource efficiency. By actively monitoring costs, businesses become more attuned to idle resources, paving the way for enhanced performance and more efficient resource utilization.\nEnhancing competitive advantage: In fiercely competitive markets such as IoT, the capacity to regulate costs while maximizing output stands as a crucial factor for success. Organizations adept at managing their cloud expenses can reinvest their savings, gaining a competitive edge over rivals grappling with escalating costs. Cloud cost optimization transcends mere spending control; it emerges as a strategic initiative capable of propelling an organization forward across various dimensions.\nWhat challenges are associated with cloud optimization? Lack of visibility: A significant hurdle in cloud cost optimization is the absence of clear cost visibility. Without adequate visibility, accurately assessing cloud spending data becomes impossible. Nevertheless, investing in a dedicated tool can facilitate effective monitoring of costs.\nLack of an accurate budget: Another prevalent challenge is the absence of precise budget controls. Incorrectly set budgets pose heightened risks for organizations. While optimizing cloud expenditures is paramount, it is a complex undertaking. Opting for a cloud management tool to analyze cloud resources stands as the ideal choice.\nComplex billing: Cloud billings are frequently complicated by technical specifications, rendering them difficult to understand. The use of intricate billing systems exacerbates the situation. Choosing the appropriate tool enables effective navigation through bill costs, eliminating any unnecessary expenses.\nLack of cost awareness: Developers may not always possess a complete awareness of costs, potentially resulting in higher expenses and overspending. Employing the right tool can assist in identifying and addressing these issues while establishing budgeting alerts for enhanced cost control.\nHow does cloud cost management differ from cloud cost optimization? Cloud cost management involves tracking, analyzing, and allocating cloud spend, whereas cloud cost optimization strives to eliminate unnecessary cloud expenses by strategically selecting, provisioning, and right-sizing resources**.**\nHow did Utho implement strategies to reduce the client‚Äôs annual cloud expenses? Utho is dedicated to assisting you in unlocking the full potential of your cloud investments. Our suite of managed services is crafted to provide customized solutions tailored to your specific business requirements.\nLeveraging our proficiency in cloud cost management, our team is poised to provide you with the necessary tools and strategies for cloud cost optimization, enhance operational efficiency, and generate tangible business value.\nFor further insights into how Utho can elevate your technical investments through managed services, reach out to our team today. Together, we can convert the complexities of cloud cost management into a strategic advantage for your organization.","how-does-cloud-cost-management-differ-from-cloud-cost-optimization#\u003cstrong\u003eHow does cloud cost management differ from cloud cost optimization?\u003c/strong\u003e":"","what-challenges-are-associated-with-cloud-optimization#\u003cstrong\u003eWhat challenges are associated with cloud optimization?\u003c/strong\u003e":"","what-makes-prioritizing-cloud-cost-optimization-essential#\u003cstrong\u003eWhat makes prioritizing cloud cost optimization essential?\u003c/strong\u003e":""},"title":"Cloud Cost Optimization: Maximizing Efficiency and Saving"},"/utho-docs/docs/cloud-disaster-recovery-empowering-business-continuity/":{"data":{"":"","what-does-the-term-cloud-disaster-recovery-entail#\u003cstrong\u003eWhat does the term \u0026ldquo;cloud disaster recovery\u0026rdquo; entail?\u003c/strong\u003e":"","what-makes-cloud-disaster-recovery-a-crucial-element#\u003cstrong\u003eWhat makes cloud disaster recovery a crucial element?\u003c/strong\u003e":"What does the term ‚Äúcloud disaster recovery‚Äù entail? Cloud disaster recovery (CDR) is a proactive approach to securely storing and preserving electronic records in a cloud environment. Its primary objective is to furnish organizations with the means to retrieve data and uphold seamless business operations in the face of a disaster. Diverging from conventional disaster recovery techniques, cloud disaster recovery stands out for its flexibility and cost-efficiency. The decentralized structure of CDR facilitates expedited data recovery, empowering businesses to swiftly resume activities following a data loss incident. This strategy empowers businesses to protect vital data, guaranteeing its accessibility even during instances of local server or network failures.\nWhat makes cloud disaster recovery a crucial element? It is essential for safeguarding businesses against disruptions such as natural disasters, cyber-attacks, or technical failures. It ensures swift restoration of access to applications and data from cloud backup sites, minimizing downtime and preventing costly operational disruptions.\nIn today‚Äôs digital era, prolonged downtime poses significant financial, operational, and reputational risks. Adhering to data privacy regulations is imperative, making a disaster recovery strategy a necessity for organizations to avoid compliance issues and regulatory fines. Implementing cloud disaster recovery showcases a commitment to resilience and rapid recovery, benefiting customers, employees, and stakeholders.\nWhat factors should be considered when selecting a cloud disaster recovery provider? Here are the key considerations when selecting a Cloud DR provider.\nDistance: Choose a provider whose data centers are located at a significant distance from your primary site to reduce the risk of simultaneous disasters affecting both locations.\nReliability: Seek a provider with a demonstrated history of consistent uptime and reliability.\nFlexibility and scalability: Select a provider that provides versatile solutions capable of adapting to your requirements and accommodating future growth.\nSecurity and compliance: Verify that the provider has strong security measures in place and adheres to applicable industry regulations to safeguard your data and applications.\nArchitecture: Choose a provider with an architecture tailored to meet your disaster recovery needs, particularly if high-performance recovery is crucial for your specific use case.\nIntegrations: Choose a provider that integrates seamlessly with your current IT systems. Compatibility simplifies the implementation and management of your disaster recovery solution.\nWhat is the functioning mechanism of Cloud Disaster Recovery? Cloud Disaster Recovery entails storing vital data and applications off-site, activating a virtual host or secondary site during crises for swift business recovery. Vendors ensure regular patching and updates for systems and applications. Automated cloud DR functions minimize errors, requiring minimal user involvement.\nCloud Disaster Recovery typically utilizes pay-as-you-go services, allowing businesses to pay for the specific amount of storage and software licenses used. It offers users the flexibility to scale up services according to their business requirements.\nIn what ways does Utho contribute to disaster recovery assistance? Utho simplifies cloud disaster recovery, making it hassle-free. Our services minimize the cost and complexity of safeguarding critical workloads from ransomware, natural disasters, infrastructure failures, and other common threats.\nUtho ensures swift deployment of essential systems and applications in the Cloud at a cost much lower than establishing and maintaining an off-site disaster recovery. It is crafted to ease the workload of IT professionals by handling all the necessary services for installation, management, failover, and recovery of crucial business operations during a disaster."},"title":"Cloud Disaster Recovery: Empowering Business Continuity"},"/utho-docs/docs/cloud-operating-system-next-frontier-of-technological-evolution/":{"data":{"":"","heading#**":"","how-does-the-cloud-operating-system-function-in-a-cloud-environment#\u003cstrong\u003eHow does the cloud operating system function in a cloud environment?\u003c/strong\u003e":"","what-are-the-initial-steps-to-dive-into-cloud-os#\u003cstrong\u003eWhat are the initial steps to dive into cloud OS?\u003c/strong\u003e":"Operating system designed to store and analyze data for web-based applications, offering access to the server‚Äôs hardware and software. It is programmed to execute and manage programs for evaluating user reactions. Cloud operating system tailored for running virtual servers, this platform allows users to explore pre-installed applications and their features online.\nWhat exactly is meant by a Cloud OS? A cloud operating system is specifically crafted for functionality within cloud computing and virtualization settings. It oversees the operation, execution, and processes of virtual machines, servers, and infrastructure. Additionally, it efficiently manages the underlying hardware and software resources on the backend.\nHow does the cloud operating system function in a cloud environment? Unlike traditional operating systems that rely on a computer‚Äôs hard drive, a Cloud OS, also known as a Web OS, operates from a remote server. On the computer itself, there‚Äôs essentially just an interface, comparable to a basic web browser. All data is stored on the remote server, reducing the need for substantial RAM and a large hard disk to run various applications seamlessly. In essence, your Cloud OS only requires a functional interface, and your tasks are accomplished through a browser, offering a transformative experience for on-the-go productivity.\n** What are the benefits and challenges of cloud OS?**\nCloud-based operating systems offer a key advantage by simplifying device management, reducing costs, and eliminating concerns about OS installation and maintenance. Compatibility, performance, and security worries are alleviated as the OS and applications run on a server maintained by professional service providers. This approach enhances mobility and productivity, enabling access to the OS and applications from any device, anytime, anywhere. Seamless data syncing and online collaboration further amplify the benefits of cloud-based operating systems.\nWhile cloud-based operating systems offer advantages, they come with challenges. Dependency on internet connection and server availability poses a risk ‚Äì loss of connection or server downtime may hinder OS and application access. Control and privacy concerns arise as data and settings are stored on external servers, subject to different policies. Additionally, customization and flexibility may be limited, dictated by the service provider.\nWhat are the ways to utilize cloud operating systems? Whether for personal or professional use‚Äîsuch as web browsing, email, document work, or media streaming‚Äîeducation, entertainment, or specific tasks like software testing, simulations, or data analysis, these systems offer versatility. You can select the one that aligns with your goals, easily switching between them as needed.\nWhat are the initial steps to dive into cloud OS? For those eager to explore cloud-based operating systems, several options await. Invest in a device with a pre-loaded system. Alternatively, install a cloud-based OS on your current device using a bootable USB drive or virtual machine. Accessing through a web browser is also an option using services or apps. For the adventurous, consider creating a personalized cloud-based OS using platforms like Utho.","what-are-the-ways-to-utilize-cloud-operating-systems#\u003cstrong\u003eWhat are the ways to utilize cloud operating systems?\u003c/strong\u003e":"","what-exactly-is-meant-by-a-cloud-os#\u003cstrong\u003eWhat exactly is meant by a Cloud OS?\u003c/strong\u003e":""},"title":"Cloud Operating System: Next Frontier of Technological Evolution"},"/utho-docs/docs/cloud-server-management/":{"data":{"":"","about-microhost#\u003cstrong\u003eAbout Microhost\u003c/strong\u003e":"\nCloud Server Management: How to Ensure Maximum Efficiency and Security Introduction As businesses increasingly rely on cloud-based infrastructure, effective cloud server management is essential for maximizing efficiency and ensuring security. Cloud server management involves overseeing the cloud servers that host and run applications and data, ensuring their availability, performance, and security. In this article, we will discuss the key aspects of cloud server management, its benefits and risks, and how to ensure that your business gets the most out of its cloud servers.\nBenefits of Cloud Server Management Cloud server management offers numerous benefits to businesses of all sizes, including:\nScalability One of the most significant benefits of cloud server management is its scalability. Cloud servers can easily be scaled up or down to accommodate changing business needs and traffic patterns. This ensures that businesses can quickly respond to demand spikes and avoid wasting resources during periods of low demand.\nCost Savings Cloud server management can also lead to significant cost savings compared to traditional on-premise infrastructure. By outsourcing infrastructure management to a cloud provider, businesses can avoid the capital expenses associated with purchasing and maintaining hardware, as well as the operational costs of managing the infrastructure.\nImproved Security Cloud server management also offers improved security compared to on-premise infrastructure. Cloud providers typically offer robust security measures, such as firewalls, intrusion detection and prevention systems, and data encryption, to protect data and applications from cyber threats.\nRisks of Cloud Server Management While cloud server management offers numerous benefits, it also carries some risks that businesses should be aware of, including:\nDependence on a Third-Party Provider Cloud server management requires businesses to rely on a third-party provider for their infrastructure needs. This can create a dependency on the provider and potentially lead to vendor lock-in, where businesses become trapped in a particular provider‚Äôs ecosystem, making it difficult to switch providers.\nData Security Risks Cloud server management also carries the risk of data security breaches. While cloud providers typically offer robust security measures, businesses must still take steps to secure their data, such as implementing access controls and encrypting sensitive data.\nHow to Ensure Effective Cloud Server Management To ensure effective cloud server management, businesses should take the following steps:\nChoose the Right Provider Choosing the right cloud provider is critical to ensuring effective cloud server management. Businesses should look for a provider that offers the features and services they need, as well as robust security measures, reliability, and scalability.\nOptimize Performance To optimize performance, businesses should monitor and tune their cloud servers regularly. This involves identifying bottlenecks, optimizing configurations, and scaling resources up or down as needed to ensure maximum efficiency.\nImplement Robust Security Measures To ensure the security of their data and applications, businesses must implement robust security measures, such as access controls, encryption, and monitoring tools. This helps protect against cyber threats and ensures compliance with regulatory requirements.\nConclusion Cloud server management is critical to ensuring the efficiency and security of cloud-based infrastructure. By choosing the right provider, optimizing performance, and implementing robust security measures, businesses can maximize the benefits of cloud server management while minimizing its risks.\nAbout Microhost Microhost is a leading cloud server management provider that offers a range of cloud-based solutions, including virtual private servers, dedicated servers, and cloud storage. With over 10 years of experience in the industry, Microhost provides reliable, scalable, and secure cloud solutions to businesses of all sizes. To learn more about Microhost‚Äôs cloud server management solutions, visit https://utho.com/.","benefits-of-cloud-server-management#\u003cstrong\u003eBenefits of Cloud Server Management\u003c/strong\u003e":"","cloud-server-management-how-to-ensure-maximum-efficiency-and-security#\u003cstrong\u003eCloud Server Management: How to Ensure Maximum Efficiency and Security\u003c/strong\u003e":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"","how-to-ensure-effective-cloud-server-management#\u003cstrong\u003eHow to Ensure Effective Cloud Server Management\u003c/strong\u003e":"","introduction#\u003cstrong\u003eIntroduction\u003c/strong\u003e":"","risks-of-cloud-server-management#\u003cstrong\u003eRisks of Cloud Server Management\u003c/strong\u003e":""},"title":"Cloud Server Management"},"/utho-docs/docs/cloud-snapshot-your-shield-in-the-cloud-server-realm/":{"data":{"":"","how-does-utho-cloud-ensure-complete-data-protection-using-snapshot-technology#\u003cstrong\u003eHow does Utho Cloud ensure complete data protection using snapshot technology?\u003c/strong\u003e":"","what-are-cloud-snapshots-and-why-are-they-essential-in-navigating-the-digital-skyline#\u003cstrong\u003eWhat are cloud snapshots, and why are they essential in navigating the digital skyline?\u003c/strong\u003e":"","what-are-some-noteworthy-trends-or-advancements-in-cloud-snapshot-technology-that-readers-should-be-informed-about#\u003cstrong\u003eWhat are some noteworthy trends or advancements in cloud snapshot technology that readers should be informed about?\u003c/strong\u003e":"In today‚Äôs digital world, keeping your data safe is crucial, especially in cloud server setups where important business information is stored. That‚Äôs where cloud snapshots come in ‚Äì they act like a shield, protecting your data from getting lost or damaged. In this blog, we‚Äôll take a closer look at cloud snapshots and why they‚Äôre so important for keeping your cloud servers secure. Let‚Äôs dive in and learn how cloud snapshots can be your trusted guardian in the world of digital technology.\nWhat are cloud snapshots, and why are they essential in navigating the digital skyline? Cloud snapshots are essentially point-in-time copies of data stored in cloud environments, capturing the exact state of a cloud-based system or application at a specific moment. They are essential tools for managing and protecting data in the cloud.\nHere‚Äôs why cloud snapshots are crucial in navigating the digital skyline:\nData Protection: Cloud snapshots provide a means of backing up data stored in the cloud, ensuring that in the event of data loss or corruption, organizations can quickly restore their systems to a previous known-good state.\nDisaster Recovery: By creating regular snapshots, organizations can establish a robust disaster recovery strategy. In the event of a disaster or system failure, snapshots enable rapid recovery, minimizing downtime and potential losses.\nData Consistency: Snapshots ensure data consistency by capturing all changes to the data at a specific moment, providing a reliable point of reference for data recovery or rollback purposes.\nEfficient Testing and Development: Cloud snapshots enable developers to create replicas of production environments for testing and development purposes without impacting live systems. This facilitates innovation and accelerates the development lifecycle.\nRegulatory Compliance: Many industries are subject to strict data retention and compliance regulations. Cloud snapshots offer a mechanism for organizations to maintain historical data records in accordance with regulatory requirements.\nCost Optimization: Cloud snapshots can also contribute to cost optimization by providing a more economical alternative to traditional backup methods. They often require less storage space and can be automated to reduce manual intervention.\nCloud snapshots play a vital role in safeguarding data, enabling efficient recovery, supporting development initiatives, ensuring compliance, and optimizing costs in cloud environments. They are essential tools for organizations looking to navigate the complexities of the digital landscape securely and effectively.\nWhat challenges or limitations may arise from depending on cloud snapshots for data management? While cloud snapshots offer numerous benefits for data management, several challenges and limitations should be considered:\nCosts: Depending on the cloud provider and storage requirements, frequent snapshotting can lead to increased storage costs. Organizations need to carefully manage snapshot retention policies to avoid unnecessary expenses.\nStorage Capacity: Cloud snapshots consume storage space, and over time, they can accumulate, potentially exceeding allocated storage limits. This necessitates monitoring and management to prevent unexpected storage overages.\nPerformance Impact: Creating and managing snapshots can sometimes impact system performance, especially in environments with high I/O operations. Organizations must balance the frequency of snapshots with system performance requirements.\nComplexity: Managing multiple snapshots across various cloud services or regions can become complex, particularly in large-scale environments. Organizations need robust management tools and processes to ensure snapshot consistency and accessibility.\nData Retention: Cloud snapshot retention policies may not align with regulatory or compliance requirements. Organizations must ensure that snapshot retention periods meet legal obligations and data governance standards.\nVendor Lock-In: Switching between cloud providers or migrating snapshot data to on-premises systems can be challenging due to proprietary snapshot formats and compatibility issues. This can potentially limit flexibility and increase dependency on specific cloud vendors.\nData Security: While cloud providers implement robust security measures, snapshots containing sensitive data may still pose security risks if not adequately protected. Organizations must implement encryption and access controls to safeguard snapshot data from unauthorized access or breaches.\nData Recovery Complexity: Restoring data from snapshots may require familiarity with cloud provider-specific tools and processes, leading to potential complexity and delays in recovery efforts.\n**What is the operational mechanism behind snapshots? **\nThe operational mechanism behind snapshots involves capturing the state of a system or data at a specific point in time, preserving it for later use or recovery. Here‚Äôs how it typically works:\nInitial Snapshot Creation: When a snapshot is initiated, the system identifies the data or resources to be captured. This can include virtual machine disks, file systems, or database volumes. The system then creates a read-only copy of the data, capturing its current state.\nCopy-on-Write Technique: Most snapshot mechanisms use a copy-on-write (CoW) technique to minimize storage overhead. Instead of duplicating all data immediately, only changes made after the snapshot creation are stored separately. The original data remains untouched, while new changes are written to different storage blocks.\nIncremental Updates: As changes occur to the original data, such as file modifications or database updates, only the modified blocks are written to new storage locations. This incremental approach reduces the amount of storage space required for each snapshot and minimizes performance impact.\nPoint-in-Time Reference: Each snapshot serves as a point-in-time reference, allowing users to revert to a specific state of the data at any time. This provides flexibility for data recovery, rollback, or testing purposes without affecting ongoing operations.\nSnapshot Management: Administrators can manage snapshots by setting retention policies, specifying how long snapshots should be retained before being deleted or consolidated. This helps optimize storage usage and ensures compliance with data retention requirements.\nSnapshot Deletion and Consolidation: When snapshots are no longer needed, they can be deleted or consolidated to reclaim storage space. Consolidation involves merging incremental changes back into the original data, effectively removing redundant snapshot copies.\nIntegration with Backup and Recovery: Snapshots are often integrated into broader backup and recovery strategies, complementing traditional backup methods. They provide additional layers of protection and flexibility for data management, particularly in virtualized or cloud environments.\nThe operational mechanism behind snapshots involves capturing the state of data or resources at a specific moment, using techniques such as copy-on-write and incremental updates to minimize storage overhead and performance impact. Snapshots serve as point-in-time references, enabling efficient data recovery, rollback, and management within complex IT environments.\nWhat sets snapshots apart from traditional backups? Snapshots offer several distinct advantages over traditional backups, setting them apart in terms of efficiency, speed, and flexibility:\nInstantaneous Recovery: Snapshots provide near-instantaneous recovery capabilities by capturing the state of data at a specific moment in time. This enables rapid restoration of systems and data to a known-good state, minimizing downtime and ensuring business continuity.\nIncremental Backup: Unlike traditional backups that typically involve copying entire datasets, snapshots employ an incremental approach. They only capture changes made since the last snapshot, reducing storage requirements and backup times significantly.\nGranularity: Snapshots provide detailed recovery choices, enabling users to restore data to precise moments in time. This flexibility enables targeted recovery of individual files, folders, or system configurations, providing greater control and efficiency.\nLow Overhead: Snapshots incur minimal overhead compared to traditional backups. They leverage copy-on-write or redirect-on-write techniques to capture changes efficiently, mitigating performance impact and storage consumption.\nIntegration with Virtualization: Snapshots are tightly integrated with virtualization platforms, such as VMware, Hyper-V, and cloud environments. They leverage hypervisor-level functionality to create and manage snapshots seamlessly, simplifying backup processes and enhancing scalability.\nAutomation and Orchestration: Many snapshot solutions offer automation and orchestration capabilities, enabling scheduled snapshot creation, retention management, and recovery workflows. This automation streamlines backup operations and ensures consistency across environments.\nSpace-Efficiency: Snapshots optimize storage utilization by sharing data blocks between snapshots and the original dataset. They eliminate the need to store redundant copies of unchanged data, resulting in efficient use of storage resources.\nApplication Consistency: Snapshots ensure application-consistent backups by coordinating with applications and databases to capture data in a consistent state. This ensures data integrity and recoverability, particularly for mission-critical applications.\nwhat sets snapshots apart from traditional backups is their ability to deliver instantaneous recovery, incremental backup, granular recovery options, low overhead, integration with virtualization, automation, space-efficiency, and application consistency. These characteristics make snapshots an essential tool for modern data protection and disaster recovery strategies, offering speed, efficiency, and flexibility in managing and safeguarding critical data and systems.\nHow does Utho Cloud ensure complete data protection using snapshot technology? Utho Cloud ensures complete data protection through snapshot technology by employing several key features and practices:\nFrequent Snapshot Creation: Utho Cloud regularly creates snapshots of data to capture its state at different points in time. These snapshots serve as backups that can be used for data recovery in case of accidental deletion, corruption, or other data loss events.\nIncremental Backup: Utho Cloud employs an incremental backup approach, where only the changes made to the data since the last snapshot are saved. This minimizes storage space and reduces backup time, making the process more efficient.\nSecure Storage: Snapshots in Utho Cloud are stored securely using encryption and access control mechanisms. This ensures that the data remains protected from unauthorized access or tampering.\nData Replication: Utho Cloud may replicate snapshots across multiple data centers or regions to ensure redundancy and high availability. This replication strategy helps prevent data loss in the event of a disaster or outage in one location.\nAutomation and Scheduling: Utho Cloud provides automation and scheduling capabilities for snapshot creation, allowing users to define policies for when and how often snapshots are taken. This helps ensure that data is regularly backed up according to business requirements.\nUtho Cloud ensures complete data protection using snapshot technology by implementing features such as frequent snapshot creation, incremental backup, secure storage, data replication, integration with Utho Database, and automation and scheduling capabilities. These practices help organizations safeguard their data and ensure business continuity in the cloud.\nWhat are some noteworthy trends or advancements in cloud snapshot technology that readers should be informed about? Certainly! Here are some notable trends and advancements in cloud snapshot technology:\nAutomated Snapshot Management: Increasing automation capabilities enable cloud snapshot creation, scheduling, and lifecycle management to be more streamlined and efficient. Automated policies and tools allow for hands-off management, reducing the need for manual intervention and improving overall reliability.\nIncremental Snapshotting: The adoption of incremental snapshot techniques is gaining traction, allowing for more efficient use of storage resources. Incremental snapshots capture only changes made since the last snapshot, reducing storage overhead and minimizing backup windows.\nIntegration with Cloud-Native Services: Cloud snapshot technology is becoming increasingly integrated with other cloud-native services and platforms. This integration allows for seamless snapshot management within broader cloud ecosystems, facilitating data protection and management across multiple cloud environments.\nApplication-Consistent Snapshots: Advancements in snapshot technology now enable the creation of application-consistent snapshots, ensuring data integrity and consistency across databases, applications, and virtualized environments. Application-consistent snapshots are essential for maintaining data integrity and supporting reliable recovery processes.\nSnapshot Orchestration and Management Platforms: Dedicated snapshot orchestration and management platforms are emerging to address the complexities of managing snapshots at scale. These platforms offer centralized control, automation, and monitoring capabilities, making it easier for organizations to manage large volumes of snapshots across diverse cloud environments.\nBy staying informed about these trends and advancements in cloud snapshot technology, readers can make informed decisions about implementing and optimizing snapshot strategies to meet their data protection and management requirements in cloud environments.\nAs technology continues to advance, embracing cloud snapshots as a reliable shield in the cloud server realm will remain essential for protecting against the ever-present threats of data loss and disruption. So, harness the power of cloud snapshots today and fortify your digital fortress for the challenges of tomorrow.","what-challenges-or-limitations-may-arise-from-depending-on-cloud-snapshots-for-data-management#\u003cstrong\u003eWhat challenges or limitations may arise from depending on cloud snapshots for data management?\u003c/strong\u003e":"","what-is-the-operational-mechanism-behind-snapshots#**What is the operational mechanism behind snapshots?":"","what-sets-snapshots-apart-from-traditional-backups#\u003cstrong\u003eWhat sets snapshots apart from traditional backups?\u003c/strong\u003e":""},"title":"Cloud Snapshot: Your Shield in the Cloud Server Realm"},"/utho-docs/docs/configure-lets-encrypt-ssl-on-ubuntu-with-certbot/":{"data":{"":"","generate-an-ssl-certificate-using-certbot#\u003cstrong\u003eGenerate an SSL certificate using Certbot\u003c/strong\u003e":"","step-1-initially-install-pip#\u003cstrong\u003eStep 1: Initially, install PIP:\u003c/strong\u003e":"","step-1-select-the-most-suitable-option-based-on-your-requirements#\u003cstrong\u003eStep 1:\u003c/strong\u003e Select the most suitable option based on your requirements.":"","step-2-establish-a-virtual-environment#\u003cstrong\u003eStep 2: Establish a virtual environment:\u003c/strong\u003e":"","step-2-provide-an-email-address-for-renewal-and-security-notifications#\u003cstrong\u003eStep 2:\u003c/strong\u003e Provide an email address for renewal and security notifications.¬†":"","step-3-accept-the-terms-of-service#\u003cstrong\u003eStep 3:\u003c/strong\u003e Accept the terms of service.¬†":"","step-3-install-certbot-for-utho#\u003cstrong\u003eStep 3: Install Certbot for Utho\u003c/strong\u003e":"","step-4-create-a-symbolic-link-to-ensure-certbot-operates-smoothly#\u003cstrong\u003eStep 4: Create a symbolic link to ensure Certbot operates smoothly:\u003c/strong\u003e":"","step-4-decide-if-you-wish-to-receive-emails-from-eff#\u003cstrong\u003eStep 4:\u003c/strong\u003e Decide if you wish to receive emails from EFF.¬†":"","step-5-if-prompted-select-whether-to-redirect-http-traffic-to-https-option-1-for-no-redirect-and-no-additional-server-changes-or-option-2-to-redirect-all-http-requests-to-https#\u003cstrong\u003eStep 5:\u003c/strong\u003e If prompted, select whether to redirect HTTP traffic to HTTPS: Option 1 for no redirect and no additional server changes, or Option 2 to redirect all HTTP requests to HTTPS.":"Let‚Äôs Encrypt offers SSL certificates at no cost, enabling secure connections for your websites. Certbot, a free and open-source tool, simplifies the process of generating Let‚Äôs Encrypt SSL certificates on your unmanaged Linux server. To get started, log into SSH as root.\nInstall Certbot in Ubuntu 20.04\nCertbot now suggests using the snapd package manager for installing on Ubuntu, Instead Python Installs Packages (PIP) is a suitable alternative.\n**Install Certbot in Ubuntu with PIP\nUbuntu users of cloud servers have the option to install Certbot using PIP**\nStep 1: Initially, install PIP: sudo apt install python3 python3-venv libaugeas0 Step 2: Establish a virtual environment: sudo python3 -m venv /opt/certbot/ sudo /opt/certbot/bin/pip install --upgrade pip Step 3: Install Certbot for Utho sudo /opt/certbot/bin/pip install certbot certbot-utho sudo /opt/certbot/bin/pip install certbot certbot-nginx Step 4: Create a symbolic link to ensure Certbot operates smoothly: sudo ln -s /opt/certbot/bin/certbot /usr/bin/certbot **To install Certbot on Ubuntu, utilize snapd\n**\nSnapd is available for use by Dedicated Server Hosting users\nSet up snapd:\nsudo apt install snapd Verify that you have the latest version of snapd installed:\nsudo snap install core; sudo snap refresh core Installing Certbot using snapd:\nsudo snap install --classic certbot Establish a symlink to guarantee Certbot‚Äôs operation:\nsudo ln -s /snap/bin/certbot /usr/bin/certbot Generate an SSL certificate using Certbot Execute Certbot to generate SSL certificates and adjust your web server configuration file to redirect HTTP requests to HTTPS automatically. Alternatively, include ‚Äúcertonly‚Äù to create SSL certificates without altering system files, which is recommended for staging sites not intended for forced SSL usage.\nStep 1: Select the most suitable option based on your requirements. Generate SSL certificates for all domains and set up redirects in the web server configuration.\nsudo certbot --utho sudo certbot --nginx Generate SSL certificates for a specified domain, which is recommended if you‚Äôre utilizing your system hostname\nsudo certbot --utho -d example.com -d www.example.com Only install SSL certs:\nsudo certbot certonly --utho sudo certbot certonly --nginx Step 2: Provide an email address for renewal and security notifications.¬†Step 3: Accept the terms of service.¬†Step 4: Decide if you wish to receive emails from EFF.¬†Step 5: If prompted, select whether to redirect HTTP traffic to HTTPS: Option 1 for no redirect and no additional server changes, or Option 2 to redirect all HTTP requests to HTTPS. SSL Maintenance and TroubleshootingOnce you‚Äôve installed a Let‚Äôs Encrypt certificate on your Ubuntu Certbot setup, you can check your website‚Äôs SSL status at https://WhyNoPadlock.com. This will help you detect any mixed content errors.\nThe certificate files for each domain are stored in:\ncd /etc/letsencrypt/live Let‚Äôs Encrypt certificates have a lifespan of 90 days. To avoid expiration, Certbot automatically monitors SSL status twice daily and renews certificates expiring within thirty days. You can review settings using Systemd or cron.d.\nsystemctl show certbot.timer cat /etc/cron.d/certbot Verify that the renewal process functions correctly:\nsudo certbot renew --dry-run Simply having an SSL certificate and implementing 301 redirects to enforce HTTPS may not always suffice to thwart hacks. Cyber attackers have devised methods to circumvent both security measures, potentially compromising server communications.\nHTTP Strict Transport Security (HSTS) is a security HTTP header designed to counteract this by instructing web browsers to serve your website only when a valid SSL certificate is received. If the browser encounters an insecure connection, it outright rejects the data, safeguarding the user.\nConfiguring HSTS within your web server, is straightforward and enhances security significantly."},"title":"Configure Let's Encrypt SSL on Ubuntu with Certbot"},"/utho-docs/docs/connecting-uthos-object-storage-to-your-phone-a-step-by-step-guide/":{"data":{"":"","configuring-connection-details-and-uploading-files#\u003cstrong\u003eConfiguring Connection Details and Uploading Files\u003c/strong\u003e":"The ability to seamlessly access and manage data across multiple devices is essential. Utho‚Äôs Object Storage provides a dependable solution for storing and organizing your files in the cloud. With the convenience of accessing these files directly from your smartphone, you can stay productive on the go. Follow this guide to link Utho‚Äôs Object Storage to your phone, making file access and management easy from anywhere.\nCreating a Bucket on Utho‚Äôs Platform Step 1: Begin by creating a bucket on Utho‚Äôs platform.\nStep 2: Then, proceed to create a bucket and select the ‚ÄúCreate Bucket‚Äù option.\nDuring the creation process, choose the Delhi/Noida data center and assign a name to your bucket as per your preference.\nFollowing that, you‚Äôll have a bucket at your disposal.\nGenerating Access Keys for Bucket Access Step 5: Subsequently, return to the object storage section and generate access keys to enable access to your bucket.\nStep 6: Then, provide a name and proceed to create the access key.\nAfter creating the access keys, you will have two keys: a secret key and an access key. Please ensure to copy both keys securely, as they will not be visible again.\nManaging Access Control and Permissions Step 3: Then, proceed to click on the ‚ÄúManage‚Äù option.\nStep 4: Next, navigate to the ‚ÄúAccess Control‚Äù section and grant permissions for uploading as either public or private according to your preference. Choose the ‚ÄúUpload‚Äù option accordingly.\n**Updating Permissions for Object Storage Access **\nStep 7: Next, navigate to the ‚ÄúManage‚Äù option under Object Storage, select ‚ÄúPermissions,‚Äù and proceed to update the permissions as necessary.\nStep 8: Proceed by selecting the access keys, then update the read/write permissions accordingly.\nInstalling and Adding ‚ÄúBucket Anywhere‚Äù Application Step 9: Get the ‚ÄúBucket Anywhere‚Äù app on your phone from the Android Play Store.\nStep 10: Open the application and proceed to click on the ‚ÄúAdd‚Äù option.\nConfiguring Connection Details and Uploading Files Step 11: Fill in the following information: S3 URL - ‚Äúhttps://innoida.utho.io/‚Äù, access key, secret access key, and ensure the bucket URL aligns with the details provided when creating Access keys.\nStep 12: Click on ‚ÄúSave‚Äù and proceed to upload the files and folders. Select the files you from wish to upload, then initiate the upload process\nStep: 14¬†We will connect it to the connect option.\nFinally, You‚Äôve successfully connected to your Utho‚Äôs bucket. Now, you can effortlessly access and manage your files from anywhere. If you have any questions, feel free to ask. Enjoy easy access to your files wherever you are.","creating-a-bucket-on-uthos-platform#\u003cstrong\u003eCreating a Bucket on Utho\u0026rsquo;s Platform\u003c/strong\u003e":"","generating-access-keys-for-bucket-access#\u003cstrong\u003eGenerating Access Keys for Bucket Access\u003c/strong\u003e":"","installing-and-adding-bucket-anywhere-application#\u003cstrong\u003eInstalling and Adding ‚ÄúBucket Anywhere‚Äù Application\u003c/strong\u003e":"","managing-access-control-and-permissions#\u003cstrong\u003eManaging Access Control and Permissions\u003c/strong\u003e":"","updating-permissions-for-object-storage-access#**Updating Permissions for Object Storage Access":""},"title":"Quick Guide to Utho's Object Storage on Mobile"},"/utho-docs/docs/create-and-deploy-your-own-stack-with-utho-cloud/":{"data":{"":"A ‚Äústack‚Äù is like a digital toolkit that bundles together a collection of software goodies needed to build and run your online creations. Imagine it as a carefully curated package containing an operating system, a web server, a database, and all the special software friends your application loves to hang out with. In the exciting realm of cloud computing, this stack becomes the magic potion that transforms a bunch of code into a fully functional and smoothly running application. So, when you‚Äôre creating your custom stack, you‚Äôre basically crafting a personalized universe for your digital brainchild to thrive in ‚Äì a unique and tailored environment designed just the way you like it!\nIn order to create your own Stack on the Utho Cloud Dashboard, follow these simple steps:\nLogin to your Utho Cloud Dashboard using the Link\nFrom the Left side menu, navigate to the Stacks option.\nTo create your very own custom Stack, click on the Create Stacks button which will redirect you to the page to create your own stack using the Utho Dashboard\nHere, you can add a Stack Label to identify your stack among multiple others. Similarly, you can use the description field to provide details about the stack.\nAdditionally, you have the flexibility to choose the operating system that aligns with your preferences for configuring the stack. Feel free to select and add multiple operating systems on which you‚Äôd like the stack to operate seamlessly. Now, in the script section below, you have the opportunity to incorporate a bash script that outlines all the desired stacks, complete with their version names and specific configurations to be automatically set each time this stack is deployed. Once you‚Äôve finished customizing your script, save the stack by clicking on Create Stack. You will be redirected to the Stacks page, where you can view all the Stacks that you have defined. In order to deploy the stack click on the Deploy button as shown in the image below. Now, you will be directed to a new window where you can choose the Data Center (DC) Location and switch between the various operating systems you listed during Stack creation. Once you‚Äôve selected the desired configuration for your cloud instance, click on Deploy Now. Congratulations! An instance with a custom stack has been deployed successfully. You can either set the login credentials at the time of deployment, or you will receive secure credentials after your server has been provisioned, sent to your email ID"},"title":"Create and Deploy your own Stack with Utho Cloud"},"/utho-docs/docs/create-your-custom-stack-on-utho-cloud-dashboard-with-these-easy-steps/":{"data":{"":"","heading#":"","heading-1#":"","step-1-access-your-utho-cloud-dashboard-by-logging-in-through-the-provided-linkhttpsconsoleuthocom#\u003cstrong\u003eStep 1:\u003c/strong\u003e Access your Utho Cloud Dashboard by logging in through the provided \u003ca href=\"https://console.utho.com/\"\u003eLink\u003c/a\u003e.":"","step-2-navigate-to-the-stacks-option-from-the-left-side-menu#\u003cstrong\u003eStep 2:\u003c/strong\u003e Navigate to the \u0026ldquo;Stacks\u0026rdquo; option from the left-side menu.":"","step-3-initiate-the-creation-of-your-customized-stack-by-selecting-the-stacks-button-this-action-will-redirect-you-to-the-page-within-the-utho-dashboard-dedicated-to-crafting-your-own-stack#\u003cstrong\u003eStep 3:\u003c/strong\u003e Initiate the creation of your customized Stack by selecting the \u0026ldquo;Stacks\u0026rdquo; button. This action will redirect you to the page within the Utho Dashboard dedicated to crafting your own stack.":"","step-4-add-a-stack-label-for-identification-and-utilize-the-description-field-to-provide-details-for-your-stack#\u003cstrong\u003eStep 4:\u003c/strong\u003e Add a Stack Label for identification and utilize the description field to provide details for your stack.":"","step-5-choose-the-preferred-operating-system-for-configuring-the-stack-you-can-add-multiple-operating-systems-to-ensure-seamless-operation-as-per-your-preferences#\u003cstrong\u003eStep 5:\u003c/strong\u003e Choose the preferred operating system for configuring the stack. You can add multiple operating systems to ensure seamless operation as per your preferences.":"","step-6-in-the-script-section-below-integrate-a-bash-script-detailing-the-desired-stacks-including-version-names-and-specific-configurations-once-customization-is-complete-save-the-stack-by-clicking-on-create-stack#\u003cstrong\u003eStep 6:\u003c/strong\u003e In the script section below, integrate a bash script detailing the desired stacks, including version names and specific configurations. Once customization is complete, save the stack by clicking on \u0026lsquo;Create Stack\u0026rsquo;.":"","step-7-after-saving-the-stack-youll-be-redirected-to-the-stacks-page-to-deploy-the-stack-click-the-deploy-button-as-illustrated-in-the-image-below#\u003cstrong\u003eStep 7:\u003c/strong\u003e After saving the stack, you\u0026rsquo;ll be redirected to the Stacks page. To deploy the stack, click the \u0026ldquo;Deploy\u0026rdquo; button, as illustrated in the image below.":"","step-8-navigate-to-a-new-window-where-you-can-choose-the-data-center-dc-location-and-switch-between-the-listed-operating-systems-after-configuring-your-preferences-for-the-cloud-instance-click-deploy-now#\u003cstrong\u003eStep 8:\u003c/strong\u003e Navigate to a new window where you can choose the Data Center (DC) Location and switch between the listed operating systems. After configuring your preferences for the cloud instance, click \u0026lsquo;Deploy Now\u0026rsquo;.":"A cloud stack denotes the mix of various cloud computing services and elements employed for constructing and overseeing cloud infrastructure. It commonly includes a blend of software, platforms, and infrastructure that collaborate to establish a comprehensive cloud computing environment. Additionally, the term ‚Äúcloud stack‚Äù can also signify a particular software platform designed to facilitate the management and deployment of cloud applications and services.\nTo establish your custom Stack on the Utho cloud dashboard, adhere to the following standardized procedure:\nStep 1: Access your Utho Cloud Dashboard by logging in through the provided Link. Step 2: Navigate to the ‚ÄúStacks‚Äù option from the left-side menu. Step 3: Initiate the creation of your customized Stack by selecting the ‚ÄúStacks‚Äù button. This action will redirect you to the page within the Utho Dashboard dedicated to crafting your own stack. Step 4: Add a Stack Label for identification and utilize the description field to provide details for your stack. Step 5: Choose the preferred operating system for configuring the stack. You can add multiple operating systems to ensure seamless operation as per your preferences. Step 6: In the script section below, integrate a bash script detailing the desired stacks, including version names and specific configurations. Once customization is complete, save the stack by clicking on ‚ÄòCreate Stack‚Äô. #!/bin/bash\napt-get install apache2 -y\nsystemctl enable ‚Äìnow apache2\nStep 7: After saving the stack, you‚Äôll be redirected to the Stacks page. To deploy the stack, click the ‚ÄúDeploy‚Äù button, as illustrated in the image below. Step 8: Navigate to a new window where you can choose the Data Center (DC) Location and switch between the listed operating systems. After configuring your preferences for the cloud instance, click ‚ÄòDeploy Now‚Äô. Congratulations! Your custom stack instance has been deployed successfully. You can set the login credentials during deployment or expect secure credentials to be sent to your email after server provisioning."},"title":"Create Custom Stack on Utho Cloud with These Easy Steps"},"/utho-docs/docs/data-guardian-vpc-elevate-cloud-security-to-new-heights/":{"data":{"":"","define-virtual-private-clouds-or-private-cloud-computing-model#\u003cstrong\u003eDefine Virtual private clouds or private cloud computing model?\u003c/strong\u003e":"","how-does-this-private-cloud-computing-provide-benefits-in-your-setup#\u003cstrong\u003eHow does this private cloud computing provide benefits in your setup?\u003c/strong\u003e":"","how-does-utho-deliver-a-seamless-virtual-private-cloud-infrastructure#\u003cstrong\u003eHow does Utho deliver a seamless Virtual Private Cloud infrastructure?\u003c/strong\u003e":"Modern businesses rely on cloud computing, and while many seek private cloud infrastructure for dedicated resources, the costs can be prohibitive. Virtual private clouds offer a cost-effective solution, providing privacy and dedicated resources akin to private clouds while leveraging the pricing advantages and additional services of public cloud providers.\nDefine Virtual private clouds or private cloud computing model? VPC stands for Virtual Private Cloud, which is a dedicated and private virtual network space hosted within a public cloud environment. Each VPC is secure and logically isolated from other virtual networks in the same public cloud, providing users with complete control for the customization and configuration of their data resources. Within a Virtual private cloud, users can deploy various cloud infrastructure resources, including compute, storage, and networking.\nWhat is the functioning mechanism of this private cloud computing model? A Virtual Private Cloud operates by offering a distinct segment of the cloud, allowing users to launch resources within a user-defined virtual network.\nUsers have authority over their virtual networking environment, enabling them to choose their IP address range, establish subnets, and configure route tables and network gateways. Additionally, users can utilize both IPv4 and IPv6 in their VPC for secure and convenient access to resources and applications.\nHow does this private cloud computing provide benefits in your setup? Leveraging a VPC provides a range of benefits that blend the strengths of both public and private cloud environments, catering to diverse business requirements.\nMinimize downtime: While achieving 100% uptime isn‚Äôt always feasible, customers demand it and tolerate minimal downtime, even as little as ten minutes. Virtual private cloud environments offer the necessary redundancy and features to approach near-100% uptime expectations.\nWith almost constant uptime, your customers will enjoy a heightened level of reliability, fostering loyalty and trust in your brand.\nReduced risk: A Virtual Private Cloud ensures heightened security at both the instance and subnet levels.\nHybrid Cloud Deployment: Virtual Private Cloud simplifies the integration of public clouds with on-premises setups, enabling smooth hybrid strategies and operational streamlining.\nFlexibility: Whether your business is expanding or undergoing transformations, VPCs offer the flexibility to evolve with your requirements. The dynamic deployment of cloud infrastructure resources makes it effortless to adjust a VPC to accommodate the changing needs of your business.\nCost savings: Due to the elastic characteristics of public clouds, you only incur charges for the resources you consume. With a VPC, there‚Äôs no need to cover expenses for hardware or software upgrades, and maintenance costs are eliminated.\nWhat are the disadvantages of the private cloud computing for your infrastructure? Similar to any technology, cloud solutions come with drawbacks that require careful consideration before determining the optimal deployment solution. Here are some of the primary disadvantages of VPC:\nCost: Cost is a significant drawback of using VPC. Despite its potential benefits, the setup and maintenance of VPC can be expensive, particularly for small businesses or startups. Users may incur additional expenses such as data transfer, IP address usage, and other fees on top of the costs associated with running resources within the VPC.\nAdditionally, effective management of a Virtual Private Cloud often demands a high level of technical expertise, which can be both costly to acquire and maintain. Therefore, users should thoroughly assess the associated expenses before opting for VPC as a part of their infrastructure.\nComplexity: Complexity represents another possible drawback of utilizing VPC. The setup and configuration of a VPC can be intricate, demanding a considerable level of technical expertise. Users may find themselves dealing with multiple components, including subnets, routing tables, security groups, and network ACLs, posing challenges for those less experienced.\nFurthermore, troubleshooting issues within a Virtual Private Cloud can be intricate due to the numerous potential points of failure that can impact the network. Hence, users should thoroughly evaluate their technical proficiency and resources before opting for VPC implementation.\nDependency on the Internet: Lastly, Virtual Private Cloud relies on the Internet, introducing vulnerabilities like network outages and cyber-attacks. If a user‚Äôs Internet connection is interrupted, their VPC may experience downtime, resulting in lost productivity. Similarly, a cyber-attack targeting a user‚Äôs VPC could compromise resources and data.\nTherefore, users should thoughtfully weigh the risks associated with depending on the Internet before choosing to implement Virtual Private Cloud. Additionally, considering additional security measures such as VPNs or firewall rules is advisable to mitigate these risks.\nHow does Utho deliver a seamless Virtual Private Cloud infrastructure? Utho, a pioneer and authority in data center security, presents an extensive array of virtual appliances. These appliances offer profound visibility and control over virtual network traffic, ensuring scalability, optimal performance, and value. Additionally, VPC brings elasticity, automation, and orchestration, providing comprehensive security solutions for private cloud, SDN, and VM environments.","what-are-the-disadvantages-of-the-private-cloud-computing-for-your-infrastructure#\u003cstrong\u003eWhat are the disadvantages of the private cloud computing for your infrastructure?\u003c/strong\u003e":"","what-is-the-functioning-mechanism-of-this-private-cloud-computing-model#\u003cstrong\u003eWhat is the functioning mechanism of this private cloud computing model?\u003c/strong\u003e":""},"title":"Data Guardian: VPC Elevate Cloud Security to New Heights"},"/utho-docs/docs/decoding-ddos-safeguarding-your-network/":{"data":{"":"","what-are-the-consequences-of-a-distributed-denial-of-service-ddos-attack#\u003cstrong\u003eWhat are the consequences of a Distributed Denial of Service (DDoS) attack?\u003c/strong\u003e":"","what-are-the-various-types-of-distributed-denial-of-service-attacks#\u003cstrong\u003eWhat are the various types of Distributed Denial of Service attacks?\u003c/strong\u003e":"","what-is-distributed-denial-of-service#\u003cstrong\u003eWhat is Distributed Denial of Service?\u003c/strong\u003e":"","what-is-the-operational-mechanism-of-ddos-attacks#\u003cstrong\u003eWhat is the operational mechanism of DDoS attacks?\u003c/strong\u003e":"","what-kind-of-support-can-utho-provide-for-you#\u003cstrong\u003eWhat kind of support can Utho provide for you?\u003c/strong\u003e":"In the current era, remote work has become the norm, leading people to spend significant time on the internet without always taking specific measures to ensure a secure session. Beyond individuals, organizations globally, which host data and conduct business online, are constantly exposed to the risk of DDoS attacks.\nWhat is Distributed Denial of Service? DDoS, or Distributed Denial of Service, refers to a cyber attack where a server or network resource is made unavailable for legitimate user traffic. The attack involves intentional disruptions by an attacker, causing a denial of service for the target host connected to the internet.\nWhat is the operational mechanism of DDoS attacks? A distributed denial-of-service (DDoS) attack seeks to impede server performance, aiming to either slow it down or cause a complete crash. The objective is to sever user connections from a server or network resource by overwhelming it with an influx of service requests. Unlike a simple denial-of-service, which involves a single attacking computer and one victim, DDoS relies on numerous infected or bot computers capable of executing tasks simultaneously.\nWhat are the various types of Distributed Denial of Service attacks? Numerous DDoS attack types fall into three primary categories:\nVolumetric Attacks: Volumetric attacks, commonly known as ‚Äúflood attacks,‚Äù are the most prevalent form of DDoS. The objective is to overwhelm the target with massive traffic, excluding legitimate requests and causing congestion.\nProtocol DDoS Attacks: Protocol DDoS attacks target OSI Layers 3 and 4, exploiting network protocols to deny service. Some rely on normal protocol behavior, exhausting resources, while others leverage inherent weaknesses in communication architecture. Initial signs include unusually high processor utilization and depleted computing resources. Similar to low-level volumetric attacks, identifying low-level protocol DDoS attacks can be challenging and costly.\nApplication Layer Attacks: Application layer attacks (OSI Layer 7) exploit weaknesses in web servers, applications, and platforms, often known as web application vulnerability attacks. By targeting specific code vulnerabilities, these attacks invoke aberrant behavior, leading to reduced performance or outright crashing. Detecting these attacks is challenging, as they typically involve low traffic volumes, making it difficult to pinpoint the source of the problem.\nWhat are the consequences of a Distributed Denial of Service (DDoS) attack? A DDoS primarily floods a site with an overwhelming volume of traffic, causing temporary disruptions. However, beyond crashing the site, it can serve as a distraction for the site owner, potentially leading to hacking attempts, system weakening, or malware downloads. In some cases, the server may be manipulated as a slave by the attacker.\nCertain DDoS attacks are orchestrated to intimidate owners with ransom demands. After distracting staff with a DDoS, attackers may hack the system and threaten a full-scale attack unless a specified amount is paid.\nWhat kind of support can Utho provide for you? DDoS attacks are on the rise, becoming more sophisticated and utilizing various cybercrime tactics, including phishing, social engineering, and botnets. Utho employs transparent detection and dynamic attack response to identify attackers early without impacting user experience. It aggregates real-time device, network, and behavioral signals to uncover hidden signs of bot and human-driven attacks, such as phishing and device/location spoofing. When suspicious signals arise, Utho‚Äôs proprietary challenge-response technology distinguishes legitimate users from malicious bots."},"title":"Decoding DDoS: Safeguarding Your Network"},"/utho-docs/docs/disable-ssh-root-login-in-opensuse/":{"data":{"":" Disable the SSH root login in OpenSUSE\nIn this article, we will learn how to disable SSH Root login in OpenSUSE server. Today, everyone knows that Linux computers come with root user access, and that by default, the outside world can use root access. For protection, it‚Äôs not a good idea to let people who shouldn‚Äôt have it use ssh root access. Any hacker can try to guess your password and get into your system this way.\nSo, it‚Äôs better to have another account that you use often and then use the ‚Äòsu ‚Äì‚Äô command to switch to root user when you need to. Before we start, make sure you have a regular user account. With that, you can use su or sudo to get root access. In Linux, it‚Äôs easy to make a different account. Just log in as the root user and run the ‚Äòadduser‚Äô command to make a new user. Follow the steps below to stop root from logging in via SSH once the user has been made.","prerequisites#Prerequisites":" vim, vi or any other text editor installed on machine\nSuper user or any normal user with root user privileges.","steps-to-disable-the-ssh-root-login#Steps to disable the SSH Root login":"Step 1: To disable root login, open the main ssh configuration file /etc/ssh/sshd_config with your choice of editor.\nvi /etc/ssh/sshd_config Step 2: Find the keywork- ‚ÄúPermitRootLogin‚Äù and identify the line as shown in below screenshot.\nSearch the keywork PermitRootLogin\nStep 3: Now, add one entry in next line which disable the ssh root login on your machine.\nPermitRootLogin no Disable the SSH Root Login in Configuration file\nStep 4: To reflect the changes, restart the ssh service of your server.\nsystemctl restart sshd Now need to check the server with new ssh connection, that actually root access is working or not. And this is how you have learnt how to disable SSH Root login in OpenSUSE server."},"title":"Disable SSH Root login in OpenSUSE"},"/utho-docs/docs/dns-manager-enhancing-security-and-performance/":{"data":{"":"","how-does-dns-management-enhance-website-resilience-amid-natural-disasters-and-network-disruptions#\u003cstrong\u003eHow does DNS management enhance website resilience amid natural disasters and network disruptions?\u003c/strong\u003e":"","how-does-utho-cloud-deliver-dns-services-to-businesses-and-what-features-distinguish-its-offerings-in-the-market#\u003cstrong\u003eHow does Utho Cloud deliver DNS services to businesses, and what features distinguish its offerings in the market?\u003c/strong\u003e":"Have you ever stopped to wonder how websites and applications maintain their security and performance on the vast expanse of the internet? The answer lies in DNS (Domain Name System) management, the behind-the-scenes hero that ensures your online presence runs smoothly. In this blog, we‚Äôll embark on a journey to demystify DNS management, shedding light on its crucial role in fortifying the security and optimizing the performance of your digital assets.¬†What is DNS management, and why is it crucial for online security and performance? DNS management is the process of overseeing and controlling Domain Name System (DNS) settings for a website or network. DNS translates human-readable domain names (like example.com) into machine-readable IP addresses (like 192.0.2.1), allowing users to access websites using familiar names instead of numerical addresses.\nHere‚Äôs why DNS management is crucial for online security and performance:\nSecurity: DNS management helps prevent various cyber threats like DNS hijacking, where attackers redirect traffic from legitimate websites to malicious ones. Properly configured DNS settings can detect and block such attacks.\nIt enables the implementation of security measures such as DNSSEC (DNS Security Extensions) to authenticate DNS responses, preventing DNS spoofing and ensuring data integrity.\nPerformance: Efficient DNS management enhances website performance by reducing DNS lookup times. Faster DNS resolution means quicker website loading times, improving user experience and search engine rankings.\nUtilizing features like DNS caching and load balancing distributes traffic across multiple servers, optimizing performance and reducing downtime.\nAvailability: Effective DNS management ensures high availability of services by employing redundancy and failover mechanisms. Multiple DNS servers and geographically distributed DNS infrastructure help maintain service availability even during server outages or network issues.\nContent Delivery: DNS management facilitates content delivery network (CDN) integration, directing users to the nearest server location for faster content delivery. This enhances performance by minimizing latency and improving load times for global audiences.\nControl and Flexibility: With proper DNS management, administrators have control over domain settings, enabling rapid updates and changes to DNS records as needed. This flexibility is essential for scaling infrastructure, implementing new services, or responding to security incidents promptly.\nDNS management plays a critical role in ensuring the security, performance, availability, and flexibility of online services. By properly configuring and maintaining DNS settings, organizations can enhance their cybersecurity posture, deliver optimal user experiences, and maintain reliable online operations.\nHow does DNS management enhance website resilience amid natural disasters and network disruptions? A DNS manager plays a crucial role in enhancing the resilience of websites and online services against natural disasters, network outages, and other unforeseen events in several ways:\nLoad Balancing and Failover: DNS managers can distribute traffic across multiple servers or data centers through load balancing. In the event of a server or data center failure, the DNS manager can redirect traffic to available resources, minimizing downtime and maintaining service continuity.\nGeographic Redundancy: DNS managers can configure geographic redundancy by assigning multiple IP addresses to a single domain name, each pointing to servers located in different geographic regions. This ensures that users can access the website or service even if one region experiences a natural disaster or network outage.\nTTL Adjustment: Time-to-Live (TTL) is a setting in DNS records that determines how long DNS information is cached by resolvers. DNS managers can adjust TTL values to control how quickly changes propagate across the DNS infrastructure. Lowering TTL values can expedite failover processes during emergencies.\nMonitoring and Alerting: DNS managers often include monitoring and alerting features that notify administrators of DNS-related issues in real-time. By promptly identifying and responding to problems, administrators can mitigate the impact of natural disasters, network outages, or other unforeseen events on website availability.\nDisaster Recovery Planning: DNS managers play a role in disaster recovery planning by providing tools for backing up DNS configurations and implementing recovery procedures. These measures ensure that DNS settings can be quickly restored in the event of data loss or system failures caused by natural disasters or other emergencies.\nGlobal Traffic Management: Advanced DNS management platforms offer global traffic management capabilities, allowing administrators to dynamically route traffic based on performance metrics, availability, and user location. This enables efficient traffic distribution and load balancing across multiple data centers, improving resilience against network disruptions.\nOverall, DNS managers contribute to enhancing the resilience of websites and online services by implementing strategies such as load balancing, geographic redundancy, TTL adjustment, monitoring, disaster recovery planning, and global traffic management. These measures help ensure continuous availability and reliability, even in the face of natural disasters, network outages, and other unforeseen events.\nWhat are some common challenges that organizations face in managing DNS, and how does a DNS manager help address these challenges? Managing DNS can pose several challenges for organizations, but a DNS manager can help address these issues effectively. Here are some common challenges and how a DNS manager helps mitigate them:\nComplexity of Configuration\nChallenge: Configuring DNS settings, including adding or updating records, can be complex and prone to errors, especially in large-scale environments with numerous domains and subdomains.\nSolution: A DNS manager provides a centralized interface for managing DNS configurations. It simplifies the process by offering intuitive tools for adding, editing, and deleting DNS records, reducing the likelihood of configuration errors.\nDNS Security Vulnerabilities\nChallenge: DNS is susceptible to various security threats, such as DNS hijacking, DDoS attacks, and cache poisoning, which can compromise the integrity and availability of DNS services.\nSolution: A DNS manager incorporates security features like DNSSEC (DNS Security Extensions), which digitally sign DNS records to prevent tampering and ensure data authenticity. It also facilitates the implementation of DNS firewalling and threat intelligence integration to detect and mitigate security threats effectively.\nAvailability and Redundancy\nChallenge: Ensuring high availability and redundancy of DNS services is crucial for maintaining continuous access to websites and online services, especially during network outages or server failures.\nSolution: A DNS manager offers features such as load balancing, geographic redundancy, and failover mechanisms. It distributes traffic across multiple servers or data centers, redirects users to alternative IP addresses during outages, and ensures service continuity.\nPerformance Optimization\nChallenge: Slow DNS resolution times can lead to poor website performance and user experience. Inefficient DNS management practices may result in longer DNS lookup times.\nSolution: A DNS manager optimizes DNS performance through features like DNS caching, which stores previously resolved DNS queries to reduce lookup times. It also implements techniques like Anycast routing and CDN integration to improve DNS response times and enhance overall website performance.\nCompliance and Policy Enforcement\nChallenge: Organizations must comply with industry regulations and internal policies governing DNS management practices, such as data privacy regulations and security policies.\nSolution: A DNS manager includes compliance features for enforcing DNS-related policies and regulatory requirements. It provides audit logs, role-based access control, and policy enforcement mechanisms to ensure adherence to standards and guidelines.\nA DNS manager helps organizations overcome common challenges in managing DNS by simplifying configuration, enhancing security, ensuring availability and redundancy, optimizing performance, and facilitating compliance with regulatory requirements. By leveraging the capabilities of a DNS manager, organizations can effectively manage their DNS infrastructure and mitigate potential risks and issues.\nWhich industries predominantly utilize DNS manager services to enhance their online presence, security, and performance? Several industries predominantly utilize DNS manager services to enhance their online presence, security, and performance. These industries include:\nE-commerce: E-commerce companies rely heavily on DNS manager services to ensure fast and secure access to their online stores. They utilize DNS management for load balancing, content delivery optimization, and DDoS protection to provide a seamless shopping experience for customers.\nFinancial Services: Financial services organizations prioritize security and compliance in their online operations. They utilize DNS manager services for DNSSEC implementation, threat detection and mitigation, and compliance with regulatory requirements such as PCI DSS and GDPR.\nTechnology: Technology companies often operate large-scale online platforms and services that require robust DNS management. They leverage DNS manager services for scalability, reliability, and performance optimization to support their digital products and applications.\nHealthcare: Healthcare organizations increasingly rely on online services for patient care, telemedicine, and administrative functions. They utilize DNS manager services for HIPAA-compliant security measures, high availability, and data privacy to ensure the confidentiality and integrity of patient information.\nMedia and Entertainment: Media and entertainment companies deliver content to global audiences through online platforms and streaming services. They utilize DNS manager services for global traffic management, CDN integration, and load balancing to optimize content delivery and enhance user experience.\nGaming: Gaming companies require low-latency, high-performance online infrastructure to support multiplayer gaming experiences. They utilize DNS manager services for traffic routing, latency-based routing, and DDoS protection to ensure smooth gameplay and minimize disruptions.\nEducation: Educational institutions rely on online learning platforms, student portals, and administrative systems for remote learning and campus operations. They utilize DNS manager services for reliability, scalability, and security to support uninterrupted access to educational resources and services.\nOverall, industries across various sectors utilize DNS manager services to strengthen their online presence, enhance security measures, and optimize performance, enabling them to deliver seamless and secure online experiences to their customers, clients, and users.\nWhat are some anticipated future trends in DNS management that could potentially enhance security and performance for online services? In the realm of DNS management, several anticipated future trends have the potential to enhance both security and performance for online services:\nDNS over HTTPS (DoH): This trend encrypts DNS queries over HTTPS, enhancing privacy and security by preventing eavesdropping and tampering. It helps protect users‚Äô DNS queries from interception and manipulation by malicious actors.\nDNS over TLS (DoT): Similar to DoH, DoT encrypts DNS queries, but it operates over the Transport Layer Security (TLS) protocol. It provides another layer of security for DNS communications, improving privacy and thwarting DNS-related attacks.\nExtended DNS (EDNS): EDNS introduces new features and extensions to the DNS protocol, enabling enhanced functionality such as larger packet sizes, improved security mechanisms, and better support for modern DNS use cases. These enhancements contribute to both security and performance improvements.\nAdvanced DNS Security Features: Future DNS management solutions are likely to incorporate more advanced security features, such as improved DNS filtering capabilities to block malicious domains, enhanced threat intelligence integration for real-time threat detection, and better mitigation techniques against DNS-based attacks like DDoS and DNS spoofing.\nDNS Firewalling and Threat Intelligence: DNS management platforms may integrate advanced firewalling capabilities and threat intelligence feeds to proactively block access to malicious domains and prevent DNS-based attacks. This helps enhance security by identifying and mitigating threats at the DNS level.\nAI and Machine Learning in DNS Security: Integration of artificial intelligence (AI) and machine learning (ML) algorithms into DNS management platforms can enable more intelligent threat detection and response mechanisms. These technologies can analyze DNS traffic patterns to identify anomalies and potential security threats in real-time, improving overall security posture.\nThese anticipated future trends in DNS management have the potential to significantly enhance both security and performance for online services, providing better protection against cyber threats and improving user experiences.\nHow does Utho Cloud deliver DNS services to businesses, and what features distinguish its offerings in the market? Enterprise Capabilities: Utho Cloud DNS offers enterprise-grade features for enhanced security and data integrity, as well as support for advanced DNS configurations.\nScalability and Resilience: Businesses can scale their DNS infrastructure seamlessly with Utho Cloud DNS, thanks to its elastic scalability and built-in redundancy. This ensures that DNS services remain robust and reliable even during periods of high demand or unexpected traffic spikes.\nSecurity Features: Utho Cloud DNS incorporates security features such as DNS firewalling, which helps protect against DNS-based attacks like DDoS (Distributed Denial of Service) and DNS cache poisoning. Additionally, Utho Cloud Infrastructure adheres to industry-leading security standards and compliance certifications to ensure data privacy and regulatory compliance.\nOverall, Utho Cloud‚Äôs DNS service stands out in the market due to its enterprise capabilities, scalability and robust security features. These attributes make it a compelling choice for businesses seeking a reliable and feature-rich DNS solution to support their online operations.\nBy understanding how DNS functions and using its features effectively, you can boost security and ensure smooth operation for your digital assets. Always remember, maintaining a well-handled DNS is vital for a secure and high-performing online environment. Keep exploring and utilizing DNS to enhance your online presence further.","what-are-some-anticipated-future-trends-in-dns-management-that-could-potentially-enhance-security-and-performance-for-online-services#\u003cstrong\u003eWhat are some anticipated future trends in DNS management that could potentially enhance security and performance for online services?\u003c/strong\u003e":"","what-are-some-common-challenges-that-organizations-face-in-managing-dns-and-how-does-a-dns-manager-help-address-these-challenges#\u003cstrong\u003eWhat are some common challenges that organizations face in managing DNS, and how does a DNS manager help address these challenges?\u003c/strong\u003e":"","what-is-dns-management-and-why-is-it-crucial-for-online-security-and-performance#\u003cstrong\u003eWhat is DNS management, and why is it crucial for online security and performance?\u003c/strong\u003e":"","which-industries-predominantly-utilize-dns-manager-services-to-enhance-their-online-presence-security-and-performance#\u003cstrong\u003eWhich industries predominantly utilize DNS manager services to enhance their online presence, security, and performance?\u003c/strong\u003e":""},"title":"DNS Manager: Enhancing Security and Performance"},"/utho-docs/docs/dns/how-to-configure-bind-as-a-private-network-dns-server-on-centos-7/":{"data":{"":"A DNS or Domain Name System is a distributed database, allows the association of zone records, for example, IP addresses with domain names. If a computer, like your laptop or phone, has to communicate over the internet, they use each other IP addresses with a remote computer, like a web server. People don‚Äôt remember IP addresses very well, but they remember the words and phrases in domain names well. Domain names can be used by the DNS system when interfacing with the computer, but computers can still use IP addresses when communicating with each other.\nIn this guide we examine how a DNS server that is the authoritative DNS server for your domain names can be installed and configured. This allows you to fully control your DNS and immediately modify your DNS records whenever you need to do so.","configuration-of-zone-file#Configuration of Zone file":" [root@Microhost]# vi /var/named/forward.example.com You can use the below content as the template.\n$TTL 1d @ IN SOA dns1.example.com. hostmaster.example.com. ( 1 ; serial 6h ; refresh after 6 hours 1h ; retry after 1 hour 1w ; expire after 1 week 1d ) ; minimum TTL of 1 day ; ; ;Name Server Information @ IN NS ns1.example.com. ns1 IN A 198.51.100.10 ; ; ;Mail Server Information example.com. IN MX 10 mail.example.com. mail IN A 198.51.100.20 ; ; ;Additional A Records: www IN A 198.51.100.30 site IN A 198.51.100.30 ; ; ;Additional CNAME Records: slave IN CNAME www.example.com. [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]Whenever you will use the domain name in the zone file always use . dot sign at the end of domain name.[/ht_message]\nYou can modify or add the records according to your requirements using the above template.\nThis line means:\n@ ‚Äì The domain of the file named.conf.local, i.e. example.com, will replace this. IN ‚Äì INTERNET type records in this case. SOA ‚Äì The Start Of Authority record is the record. For this domain this is the authoritative record. dns1.example.com. ‚Äì the DNS record nameserver. ‚Äì The name server. hostmaster.example.com. ‚Äì the name server manager‚Äôs email address. The @ symbol is substituted by a dot. A reverse zone file must be created. Open the text editor and create the file:\n[root@Microhost]# vi /var/named/reverse.example.com $TTL 1d @ IN SOA dns1.example.com. hostmaster.example.com. ( 1 ; serial 6h ; refresh after 6 hours 1h ; retry after 1 hour 1w ; expire after 1 week 1d ) ; minimum TTL of 1 day ; ; ;Name Server Information @ IN NS ns1.example.com. ns1 IN A 198.51.100.10 ; ; ;Reverse IP Information 10.100.51.198.in-addr.arpa. IN PTR ns1.example.com. 20.100.51.198.in-addr.arpa. IN PTR mail.example.com. 30.100.51.198.in-addr.arpa. IN PTR www.example.com. You can use the above file content as the template according to your requirement .\nWe can check the zone file configuration error using the following command.\nBIND¬†provides¬†two¬†tools¬†to¬†ensure¬†that¬†its¬†configuration¬†files¬†have¬†no¬†errors preventing¬†BIND¬†from¬†starting.¬†The¬†first¬†checks¬†the¬†global¬†settings¬†files¬†and¬†uses¬†the¬†following:\n[root@Microhost]# named-checkconf /etc/named.conf For the second tool use the below command.\n[root@Microhost]# named-checkzone (DOMAIN-NAME) (ZONE-FILE) ","configure-systemd-to-keep-dns-server-running#Configure Systemd To Keep Dns Server Running":"Make¬†a¬†copy¬†of¬†the¬†system¬†service¬†BIND¬†file which¬†we¬†are going to¬†edit.\n[root@Microhost]#cp /lib/systemd/system/named.service /etc/systemd/system/ In future system updates, this guarantees that the edits are not lost. Then, in an editor, open the file:\n[root@Microhost]#vi /etc/systemd/system/named.service Add the given lines into the file.\nRestart=always RestartSec=3 Reload the system daemon file and restart the Dns server.\n[root@Microhost]#systemctl daemon-reload [root@Microhost]#systemctl restart named.service We have completed the installation of BIND DNS SERVER.\nThank You :)","global-bind-settings#Global BIND Settings":"BIND functioning as a DNS server is divided into two sections. The first is to define the global parameters to make BIND work as we want. The second step is to create the domain-based DNS data to be used by BIND. This is called ‚Äúarea information‚Äù or ‚Äúarea records.‚Äù\nThe global parameters are configured in this section.\nIn /etc/named.conf we are going to edit the first configuration file and configure how bind works. Open your favorite text editor for this file.\n[root@Microhost]# vi /etc/named.conf Add the below line while editing the domain name according to your requirement at the bottom of the file.\nzone \"exmaple.com\" { type master; file \"/var/named/forward.example.com\"; }; zone \"10.100.51.198.in-addr.arpa\" { type master; file \"/var/named/everse.example.com\"; }; The following lines are used in this file:\nzone ‚Äì this is the domain name or IP address for which BIND responds to requests. Type master ‚Äì BIND reads the local storage zone information and provides the relevant domain information for the zone line. File ‚Äì the zone information file. Two sections have the same syntax as you can see in this file. The first section contains the domain name example.com known as the forward DNS record. This means that domain information is converted to IP addresses.\nThe second latter is the server IP address‚Äô reverse dns or PTR record. This turns the other way round, i.e. Domain name to IP addresses. The reverse record zone line looks a bit odd, since the IP address is in reverse. The IP address of 198.51.100.10, which forms the reverse record.\nInverse records are important as many safety systems, such as spam filters, are less likely to accept e-mails sent from a non-reverse IP address.\nNow that the global setup of BIND is set, the area files can be created that hold the DNS information forward and reverse.","installation-of-dns-server#Installation of DNS Server":"This guide is used for BIND‚Äôs DNS server. BIND is one of the most used and oldest online DNS servers.\nYou should make sure your server updates the latest packages before installing BIND:\n[root@Microhost]# yum update -y The Debian repositories default for BIND are available and will be installed as follows:\n[root@Microhost]# yum install bind bind-utils Bind is now installed on the server.","prerequisites#Prerequisites":"Before starting please follow the below instructions.\nA CentOS 7 server. A domain name. Root or sudo enabled user on the server. "},"title":"How To Configure BIND as a Private Network DNS Server on CentOS 7"},"/utho-docs/docs/dns/how-to-install-and-configure-powerdns-on-centos-7-using-mariadb/":{"data":{"":"PowerDNS is a DNS server that works on many derivatives from Linux / Unix. It can be configured with various backends, including zone files of the BIND style, relational databases and failover / load balancing. The DNS recurrent can also be set up as part of a separate server process.\nPowerDNS Authoritative server‚Äôs latest version is 3.4.4 but 3.4.3 is currently available in the EPEL. In order to test this version for CentOS and Fedora, I would recommend installing the version for the EPEL repository. You can also update PowerDNS easily in the future in this way.\nThis article will present you with the MariaDB backend and PowerAdmin-a usable PowerDNS web interface management tool-how you can install and configure the master PowerDNS-Server.","installing-powerdns-with-mariadb-backend#Installing PowerDNS with MariaDB Backend":"You¬†must¬†first¬†allow¬†your¬†server¬†to simply use¬†the¬†EPEL¬†repository¬†:\n[root@Microhost]# yum install epel-release.noarch Installing the MariaDB Server is the next step. The following command can be easily executed:\n[root@Microhost]# yum -y install mariadb-server mariadb We will configure Mysql as it should be up at system boot time.\n[root@Microhost]# systemctl enable mariadb.service [root@Microhost]# systemctl start mariadb.service We will execute the secure installation for MariaDB password configuration as following:\n[root@Microhost]# mysql_secure_installation /bin/mysql_secure_installation: line 379: find_mysql_client: In order to log into MariaDB to secure it, we'll need the current password for the root user. If you've just installed MariaDB, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): Press ENTER OK, successfully used password, moving on‚Ä¶ Setting the root password ensures that nobody can log into the MariaDB root user without the proper authorisation. Set root password? [Y/n] y New password: ‚Üê Set New Password Re-enter new password: ‚Üê Repeat Above Password Password updated successfully! Reloading privilege tables.. ‚Ä¶ Success! By default, a MariaDB installation has an anonymous user, allowing anyone to log into MariaDB without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] y ‚Üê Enter ‚Äúy‚Äù to disable that user ‚Ä¶ Success! Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] n ‚Üê Enter ‚Äún‚Äù for no ‚Ä¶ skipping. By default, MariaDB comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] y ‚Üê Enter ‚Äúy‚Äù for yes - Dropping test database‚Ä¶ ‚Ä¶ Success! - Removing privileges on test database‚Ä¶ ‚Ä¶ Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] y ‚Üê Enter ‚Äúy‚Äù for yes ‚Ä¶ Success! Cleaning up‚Ä¶ All done! If you've completed all of the above steps, your MariaDB installation should now be secure. Thanks for using MariaDB! Now MariaDB configuration has been completed. We will proceed for the installation of PowerDNS:\n[root@Microhost]# yum -y install pdns pdns-backend-mysql The PowerDNS configuration file lists /etc/pdns/pdns, but a MySQL database for PowerDNS service is configured before editing. We first log on to the MySQL server and build a powerdns named database:\n[root@Microhost]# # mysql -u root -p MariaDB [(none)]\u003e CREATE DATABASE powerdns; Now we create the database user as following:\nMariaDB [(none)]\u003e GRANT ALL ON powerdns.* TO 'powerdns'@'localhost' IDENTIFIED BY 'COMPLX_Password'; MariaDB [(none)]\u003e GRANT ALL ON powerdns.* TO 'powerdns'@'centos7.localdomain' IDENTIFIED BY 'COMPLX_Password'; MariaDB [(none)]\u003e FLUSH PRIVILEGES; Replace COMPLEX_Password with your password:\nWe will create database tables which will be used in future by powerdns:\nMMariaDB [(none)]\u003e USE powerdns; MariaDB [(none)]\u003e CREATE TABLE domains ( id INT auto_increment, name VARCHAR(255) NOT NULL, master VARCHAR(128) DEFAULT NULL, last_check INT DEFAULT NULL, type VARCHAR(6) NOT NULL, notified_serial INT DEFAULT NULL, account VARCHAR(40) DEFAULT NULL, primary key (id) ); MariaDB [(none)]\u003e CREATE UNIQUE INDEX name_index ON domains(name); MariaDB [(none)]\u003e CREATE TABLE records ( id INT auto_increment, domain_id INT DEFAULT NULL, name VARCHAR(255) DEFAULT NULL, type VARCHAR(6) DEFAULT NULL, content VARCHAR(255) DEFAULT NULL, ttl INT DEFAULT NULL, prio INT DEFAULT NULL, change_date INT DEFAULT NULL, primary key(id) ); MariaDB [(none)]\u003e CREATE INDEX rec_name_index ON records(name); MariaDB [(none)]\u003e CREATE INDEX nametype_index ON records(name,type); MariaDB [(none)]\u003e CREATE INDEX domain_id ON records(domain_id); MariaDB [(none)]\u003e CREATE TABLE supermasters ( ip VARCHAR(25) NOT NULL, nameserver VARCHAR(255) NOT NULL, account VARCHAR(40) DEFAULT NULL ); Now exit from Mysql:\nMariaDB [(none)]\u003e quit; We¬†can¬†finally¬†configure¬†our¬†PowerDNS¬†so¬†that¬†it¬†uses¬†MySQL¬†as¬†a¬†backend.\nOpen¬†the¬†configuration¬†file¬†of¬†PowerDNS¬†located¬†at:\nvi /etc/pdns/pdns.conf It will look like:\n################################# launch Which backends to launch and order to query them in\nlaunch=\nCopy the below codes and put at the end of the file: launch=gmysql gmysql-host=localhost gmysql-user=powerdns gmysql-password=COMPLEX_Password gmysql-dbname=powerdns\nThe password \"COMPLEX\\_Password\" should be the same as previous one. Save the file with **:wq** and exit from the text editor. Now we will start and enable the service of Powerdns as following: [root@Microhost]# systemctl enable pdns.service\n[root@Microhost]# systemctl start pdns.service\n## Installing PowerAdmin console to Manage PowerDNS PowerAdmin ‚Äì a friendly Web interface designed to manage PowerDNS servers, is now being installed. We have to install PHP and a web server (Apache) because it is written in PHP. [root@Microhost]# yum install httpd php php-devel php-gd php-imap php-ldap php-mysql php-odbc php-pear php-xml php-xmlrpc php-mbstring php-mcrypt php-mhash gettext\nTwo¬†PEAR¬†packages¬†are¬†also¬†required¬†for¬†PowerAdmin: [root@Microhost]# yum -y install php-pear-DB php-pear-MDB2-Driver-mysql\nAfter the installation has been completed, Apache is started and set to start when the system boots: [root@Microhost]# systemctl enable httpd.service\n[root@Microhost]# systemctl start httpd.service\nWe can proceed and download the package of poweradmin console now that all the system requirements for PowerAdmin are done. Since /var/www/html is Apache's default web directory, the package will be downloaded there. [root@Microhost]# cd /var/www/html/\nNow we will download the packages using Wget as following [root@Microhost]# wget http://downloads.sourceforge.net/project/poweradmin/poweradmin-2.1.7.tgz\nZip file has been downloaded now. We have to extract the zip file as following: [root@Microhost]# tar xfv poweradmin-2.1.7.tgz\nNow we will rename the directory name as poweradmin as following: [root@Microhost]# mv poweradmin-2.1.7.tgz poweradmin\nWe can now launch PowerAdmin's web installer while accessing the URL: http://IP\\_address/poweradmin/install/ The output will be shown as below: ![](images/power-1.png) The above page requires you to select the PowerAdmin language. Choose the one you want and then click on the 'Go To Step 2'. The output will be shown as below: The installer expects that you have already created the PowerDNS . ![](images/power2.png) We can move on to the next step since we have already created one. The database details you set up earlier, will be requested. Poweradmin administrator password is also necessary: ![](images/power3.png) Go to step 4 once you've entered them. You are going to create a new user with limited Poweradmin rights. You will have to enter the following fields: - Username - PowerAdmin username. - Password ‚Äì The user's password. - Hostmaster ‚Äì This value will be used when you create SOA records and do not have the hostmaster specified. - Primary name server- The value will be used to create new DNS zones as primary name server. - Secondary name server ‚Äì The value is used to generate new DNS zones as the primary name server. ![](images/power-4.png) The next step will be to ask Poweradmin to create a new user database with limited rights in database tables. The code you need to put in a MySQL console is given below: ![](images/power-6.png) Now login to MySQL database on server using below command: [root@Microhost]# mysql -u root -p\nNow execute the code provided by the Poweadin console: [root@Microhost]# MariaDB [(none)]\u003e GRANT SELECT, INSERT, UPDATE, DELETE ON powerdns.* TO ‚Äòpowermarin‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äò123qweasd‚Äô;\n![](images/power-5.png) Now go to redirect to the path **/var/www/html/poweradmin/inc**. We have to create a file named **config.inc.php** using below command: [root@Microhost]# vi /var/www/html/poweradmin/inc/config.inc.php\n![](images/power-7.png) Now copy all the content of above mentioned image and paste it to file **config.inc.php** Go to the last page where the installation has been finished and you will receive information on accessing the installation of your Poweradmin. ![](images/power-8.png) URLs¬†of¬†other¬†DNS¬†providers¬†can¬†be¬†activated¬†by¬†execution. [root@Microhost]# cp install/htaccess.dist .htaccess\nIt is important to rename or remove the install directory from the poweradmin folder: [root@Microhost]# mv /var/www/html/poweradmin/install/ /var/www/html/poweradmin/donotinstall/\nNow configuration has been successfully completed. We can access the Poweradmin using Url: http://IP_address/poweradmin/\n![](images/power9.png) After login, you can see the main page as following ![](images/mainpage.png) Now you can proceed to add Master Zone as per your requirement. Thank You :) "},"title":"How to Install and Configure PowerDNS on centos 7 using MariaDB."},"/utho-docs/docs/docker-vs-kubernetes-containerization-solution-for-businesses/":{"data":{"":"","heading#**":"","heading-1#":"","heading-2#":"The emergence of containerization has provided developers with a potent tool to enhance the efficiency of deploying applications and services. Leading the way in this containerization realm are Kubernetes and Docker, each presenting unique solutions to a common challenge. This article investigates the intriguing narrative surrounding Kubernetes versus Docker, examining their individual strengths and weaknesses. It offers data-driven insights to empower you in making well-informed decisions.\nWhat does Docker entail? Docker is a software platform expediting application development, testing, and deployment. It enables running containerization on various PCs or servers, organizing software into self-contained units known as containers. These containers encapsulate all essential components, such as libraries, tools, code, and runtime, promoting seamless portability across operating systems and enhancing developer productivity.\nWhat are the advantages of Docker? The advantages of Docker containerization manifest in various contexts. Here, we outline some of the key benefits of Docker.\nEnhanced portability: Docker containerization can be deployed in any data center, cloud environment, or endpoint without requiring any modifications.\nModular architecture: The Docker framework enables users to integrate multiple processes into a single containerization. This facilitates the creation of applications that can continue to operate seamlessly even during the maintenance or update of specific components.\nContainerization templatization: Docker facilitates the utilization of existing containers as base images, acting as templates for generating new containers.\nAutomated provisioning: Docker has the capability to automatically configure containerization using the application source code.\nVersioning: Docker has the capability to monitor containerization image versions, facilitate version rollbacks when needed, and maintain a record of version creation. Additionally, it supports delta uploading between existing and new versions. Programmers using Docker can develop containerization in various language versions without impacting other lines of code.\nWhat are the Disadvantages of Docker containerization? Like any technology, Docker presents its own set of challenges:\nUpskilling duration and effort: Becoming proficient in Docker is a time-consuming process, and beginners encounter a steep learning curve. Additionally, knowledge of Linux is essential for customizing or maintaining the Docker Engine.\nCross-platform communication: Although Docker containerization communicates smoothly with each other, transferring data between Docker containers and containers from competing companies may not always be seamless. This can pose challenges in environments where developers need to integrate more than one container.\nLack of persistence: Detractors may highlight that Docker‚Äôs notable portability and modularity occasionally result in challenges with persistent storage. Without setting up volumes for data storage in the Docker Engine, containerization completing its assigned process will shut down, rendering all processed data inaccessible. Presently, there is no automated process in place to address this issue.\nCLI reliance: Docker operations heavily depend on proficiency in command-line interface (CLI) usage, and the framework is designed for applications that primarily function with terminal commands. This may pose challenges for users working with applications that demand a graphical user interface (GUI).\nWhat are the use cases for Docker containerization? Docker finds diverse applications across various scenarios, including:\nMicroservices: Docker enables the breakdown of applications into smaller, manageable components, facilitating the deployment and development of architectures based on microservices.\nDevOps Adoption: Docker streamlines the software delivery process by fostering the adoption of DevOps principles within organizations. This is achieved by promoting collaboration between operations and development teams.\nContinuous Deployment: Continuous delivery and integration become easily achievable with Docker, as it facilitates automated and swift deployment of applications.\nLegacy App Migration: Docker facilitates the migration of legacy applications to containerized environments, enhancing the scalability, portability, and ease of management for these legacy applications.\n** What does Kubernetes containerization involve?**\nEffectively handling and scaling applications with multiple containers can be challenging. Kubernetes, or K8s, is an open-source platform that simplifies the management, scaling, and automation of containerization deployment. Its flexibility allows it to collaborate with any containerization runtime, making it a powerful tool for automating essential tasks like load balancing and self-healing configurations in containerized applications.\nWhat are the advantages of Kubernetes containerization?\nKubernetes and containerization offer myriad advantages to organizations. Below are some of the principal benefits of Kubernetes:\nDeployment: Users can select, configure, and alter the states for containerization deployment through Kubernetes. This encompasses tasks such as creating new container instances, migrating existing containers, and removing outdated containers**.**Equitable traffic distribution: The platform has the capability to execute load-balancing operations, ensuring that traffic is evenly distributed among multiple container instances.\nSupervision: Kubernetes enables users to continuously monitor the health of containers. If a container malfunctions, users have the option to either restart it for troubleshooting or, if necessary, remove it.\nData storage: The solution facilitates the storage of container data across various storage types, encompassing local, cloud, and hybrid storage.\nCybersecurity: Ultimately, Kubernetes possesses the ability to securely manage passwords, SSH keys, tokens, and other crucial data.\nWhat are the Disadvantages of Kubernetes containerization? Although Kubernetes is undeniably an exceptional tool for container-enabled enterprise architectures, it does come with some drawbacks, including:\nComplexity of operations: The distributed nature of containerization management via Kubernetes is beneficial for improving scalability and flexibility. Nevertheless, the introduction of extensive containerization at a large scale frequently results in heightened complexity of IT operations, potentially affecting availability in instances of misconfiguration.\nScaling under load: Certain container applications may exhibit varying scalability or even encounter challenges in scaling under high loads. Users need to be attentive to the approaches employed for node and pod balancing.Limited observability: As Kubernetes oversees extensive containerization deployments, the task of human supervision for all production workloads becomes increasingly challenging with the scaling of the architecture. Ensuring optimal security and performance by monitoring the various layers of the Kubernetes stack becomes a challenge in widespread deployments.\nSecurity concerns: Implementing container deployment in a production environment necessitates heightened cybersecurity and compliance measures. This involves the incorporation of multi-factor authentication, scrutiny of code vulnerabilities, and concurrent management of numerous stateless configuration requests. Alleviating concerns regarding Kubernetes security can be achieved through accurate configuration and the establishment of proper access controls.\n**What are the use cases for Kubernetes containerization?\n**\nKubernetes finds extensive applications across the IT industry, business, and science. Explore below some of the most compelling use cases for Kubernetes in contemporary scenarios.\nLarge-scale app deployment: Engineered for managing large applications through automation and a declarative configuration approach, Kubernetes offers features like horizontal pod scaling and load balancing. This allows developers to establish systems with minimal downtime, ensuring continuous operation even during unpredictable events in an application‚Äôs lifecycle, such as traffic surges and hardware issues.\nEffectively managing the environment, including IPs, networks, and resources, is a challenge faced by developers of large-scale applications. Platforms like Glimpse have embraced Kubernetes to address this challenge.\nManaging Microservices: Contemporary applications commonly employ microservice architecture for streamlined and accelerated code management. Microservices act as applications within applications, constituting services with distinct functions that can interact with each other.\nDevelopers transitioning to this architecture often encounter challenges in microservice-to-microservice communication. Kubernetes frequently emerges as the optimal solution for overseeing communication between application components. It not only manages component behavior in the event of a failure but also facilitates authentication processes and resource distribution across microservices.\nEnabling Serverless Computing: Serverless computing denotes a cloud-native model wherein backend server-related services are abstracted from the development process. Cloud providers manage server provisioning and maintenance, while developers design and containerize the application.\nDespite the availability of serverless models from major cloud providers, Kubernetes offers the opportunity to establish an autonomous serverless platform with enhanced control over backend processes. Building a serverless environment driven by Kubernetes empowers developers to concentrate on the product while maintaining control over the infrastructure.\n**\nHybrid and Multi-Cloud Deployments:** In hybrid and multi-cloud environments, Kubernetes plays a pivotal role in facilitating application portability for developers. Its environment-agnostic approach eliminates the necessity for platform-specific application dependencies.¬†Abstraction from the underlying infrastructure is made possible by Kubernetes concepts like services, ingress controllers, and volumes. Additionally, Kubernetes serves as an excellent solution for addressing scaling challenges in a multi-cloud environment, thanks to its built-in auto-healing and fault tolerance features.","what-are-the-advantages-of-docker#\u003cstrong\u003eWhat are the advantages of Docker?\u003c/strong\u003e":"","what-are-the-disadvantages-of-docker-containerization#\u003cstrong\u003eWhat are the Disadvantages of Docker containerization?\u003c/strong\u003e":"","what-are-the-disadvantages-of-kubernetes-containerization#\u003cstrong\u003eWhat are the Disadvantages of Kubernetes containerization?\u003c/strong\u003e":"","what-are-the-use-cases-for-docker-containerization#\u003cstrong\u003eWhat are the use cases for Docker containerization?\u003c/strong\u003e":"","what-does-docker-entail#\u003cstrong\u003eWhat does Docker entail?\u003c/strong\u003e":""},"title":"Docker vs. Kubernetes: Containerization Solution for Businesses"},"/utho-docs/docs/empowering-business-success-through-strategic-data-backup/":{"data":{"":"","how-are-data-backup-and-recovery-interconnected#\u003cstrong\u003eHow are data backup and recovery interconnected?\u003c/strong\u003e":"","in-what-ways-can-utho-assist-in-providing-it-services-for-backup-and-recovery#\u003cstrong\u003eIn what ways can Utho assist in providing IT services for backup and recovery?\u003c/strong\u003e":"The rise in cyber attacks highlights the critical need for data security. Small and medium-sized businesses are increasingly targeted. As data dependency grows, so does the demand for robust security measures. Beyond external threats, data recovery is crucial. While data loss is unpredictable, ensuring your business has effective recovery solutions is essential. IT managers play a key role in implementing proper data backup and disaster recovery procedures.\nWhat does the term ‚ÄúData Backup‚Äù mean? Data Backup involves duplicating your digital data and essential business information to guard against potential damage, deletion, or loss. The replicated data serves as a means to recover or restore your information, ensuring business continuity and facilitating disaster recovery. Often, IT organizations create multiple data backup copies, maintaining one on-premises for quick recovery and storing a second copy offsite or in the cloud to mitigate risks associated with on-premises damage, such as those caused by natural or man-made disasters.\nWhat are its various types? Various methods are available to safeguard your personal data.\nFull backups: Performing a full backup involves backing up every file on your device. The duration of this process may vary, potentially taking several hours depending on the size of the data. A full backup is particularly recommended for initial data backup operations.\nDifferential backups: Differential backups exclusively capture files that have changed or been added since the last full backup, allowing for a faster backup process compared to a full backup.\nIncremental backups: Similar to a differential backup, incremental backups selectively capture data changes or additions since the last backup. However, the distinction lies in the fact that incremental backups encompass changes since the latest backup, regardless of whether it was a full, differential, or incremental backup. Backup software often utilizes these backups due to their smaller size and quick backup times, allowing for frequent execution.\nMirror Backup: A mirror backup duplicates the entire set of data, encompassing all files and folders, providing an exact replica of the backed-up information. This backup type proves beneficial when the goal is to generate an identical copy of a system or device.\nSnapshot Backup: A snapshot backup freezes the state of a system or device at a specific moment in time, making it valuable for dynamic systems or devices undergoing constant changes, such as databases or virtual machines.\nCloud Backup: A cloud backup entails storing data on a remote server via the internet, offering a dependable off-site backup solution accessible from any location with an internet connection.\nHybrid Backup: A hybrid backup integrates on-premises backups with cloud backups, forming a comprehensive backup solution that amalgamates the advantages of both local and cloud backups. This approach ensures swift backup and restore times, along with off-site data protection.\nWhat are the common causes of data loss? It‚Äôs crucial to identify the causes of data loss to create an effective backup and recovery strategy. The main factors include:\nHardware Failure: Hard drives, servers, and storage devices have a tendency to fail unexpectedly, frequently resulting in data loss.\nHuman Error: In business settings, accidental data deletion or overwriting is a common occurrence.\nCyberattacks: Data security and availability can be compromised by ransomware, malware, and phishing attacks.\nSoftware Glitches: In an instant, software bugs or crashes have the potential to corrupt or erase data.\nHow are data backup and recovery interconnected? Data backup and recovery are two interconnected domains in data management. While they may exhibit differences, they complement each other in their respective roles.\nIn the realm of data management, data backup plays a crucial role in salvaging damaged data during recovery. Conversely, recovery is fundamental to the purpose of why backups are created initially. A comprehensive understanding of both fields is essential for grasping the concepts of data backup and recovery.\nDespite their interconnectedness, these fields exhibit differing processes. Data backup emphasizes the efficient use of data storage during the backup process, while data recovery leans toward minimizing damages.\nGiven the serious threat of data loss, especially when dealing with sensitive information, having a robust data backup and recovery plan is imperative. Protect your data to safeguard against potential losses or breaches that could jeopardize your business.\nIn what ways can Utho assist in providing IT services for backup and recovery? When it comes to data backup and recovery, choosing the right tool is crucial. Utho unified data backup is designed for IT professionals, offering a mix of innovation, simplicity, and efficiency. It‚Äôs more than just a service ‚Äì it‚Äôs a comprehensive solution that understands the ins and outs of data backup and recovery. With Utho, you get a reliable partner for protecting your data, ensuring peace of mind from potential loss and facilitating quick recovery.\nFeel free to reach out to us to discover more about how our services can positively impact you and your organization. We stand ready to support you on your path toward heightened data security and enhanced business resilience.","what-are-its-various-types#\u003cstrong\u003eWhat are its various types?\u003c/strong\u003e":"","what-are-the-common-causes-of-data-loss#\u003cstrong\u003eWhat are the common causes of data loss?\u003c/strong\u003e":"","what-does-the-term-data-backup-mean#\u003cstrong\u003eWhat does the term \u0026ldquo;Data Backup\u0026rdquo; mean?\u003c/strong\u003e":""},"title":"Empowering Business Success through Strategic Data Backup"},"/utho-docs/docs/exploring-cloud-computing-scalability-an-in-depth-analysis/":{"data":{"":"","how-does-utho-contribute#\u003cstrong\u003eHow does Utho contribute?\u003c/strong\u003e":"Cloud computing has transformed how businesses and individuals use computing resources, with scalability being a key benefit. Scalability in the cloud involves the ability to adjust computing power or resources as needed. This article delves into the importance and definition of scalability in cloud computing.\nWhat does scalability mean in the context of cloud computing? Cloud computing system to seamlessly respond to fluctuating computing needs, dynamically adjusting resources like computing power, storage, or network capacity on demand. This adaptive capability ensures the system aligns its resources with the workload, consistently meeting necessary performance benchmarks. Scalability in the cloud often entails flexibly scaling the number of servers, storage, or other computing resources.\nWhat are the different types of scalabilities in cloud computing? Horizontal scalability: Horizontal scalability, also known as scale-out, involves adding more instances of the same resource to manage a growing workload. For instance, in a high-traffic scenario for a web application, extra servers can be introduced to distribute the load and maintain responsive performance.\nVertical scalability: Vertical scalability, or scale-up, entails enhancing the computing power of an existing server or resource. This can be achieved by augmenting CPU or memory in a virtual machine. Vertical scalability is commonly employed for applications that demand increased processing power or memory to operate at peak efficiency.\nDiagonal scalability: Diagonal scalability, a hybrid approach, merges elements of both horizontal and vertical scalability. It includes adding more instances of resources while simultaneously increasing the computing power of individual resources. This approach provides a highly flexible and customizable way to address varying workload demands.\nWhat is the functioning mechanism? Using the cloud for scalability: It works by using the virtual nature of cloud computing. Businesses can easily adjust their applications and services by adding or removing virtual instances as needed, without relying on physical hardware. This allows for quick and flexible resource management.\nCloud scalability and elasticity in practice: cloud scalability and elasticity are enforced through automation and monitoring systems. Businesses can establish scaling rules or policies that specify when and how resources should be added or removed. These rules may be determined by factors such as CPU usage, memory utilization, or network traffic.\nResources needed for cloud scalability: To achieve, businesses need a cloud provider with the right tools. Utho providers come with built-in features like auto-scaling groups and load balancers. Additionally, it‚Äôs essential for applications to be designed with scalability in mind, using distributed architectures and horizontal scaling techniques.\nHow does Utho contribute? Utho, a prominent cloud computing services provider, delivers a variety of tools and services for enhancing scalability. Businesses can utilize the Utho Load Balancer to automate resource allocation, guaranteeing optimal performance.","what-are-the-different-types-of-scalabilities-in-cloud-computing#\u003cstrong\u003eWhat are the different types of scalabilities in cloud computing?\u003c/strong\u003e":"","what-does-scalability-mean-in-the-context-of-cloud-computing#\u003cstrong\u003eWhat does scalability mean in the context of cloud computing?\u003c/strong\u003e":"","what-is-the-functioning-mechanism#\u003cstrong\u003eWhat is the functioning mechanism?\u003c/strong\u003e":""},"title":"Exploring Cloud Computing Scalability: An In-Depth Analysis"},"/utho-docs/docs/getting_started/":{"data":{"explore-more#Explore More":"Dive deeper into the features and functionalities of Utho by exploring our documentation. You‚Äôll find detailed articles on a wide range of topics, including:\nUser Guide: Learn how to maximize your use of Utho with this comprehensive guide. API Reference: Developers can find detailed information about our API endpoints, including request examples and response formats. FAQ: Find answers to frequently asked questions about Utho. ","getting-started#Getting Started":"For those new to Utho, we recommend starting with our Getting Started guide, which covers the basics of setting up your account and navigating the platform.","stay-updated#Stay Updated":"Our documentation is regularly updated with the latest information on new features, improvements, and fixes. Be sure to check back often to stay informed about the latest developments.\nExplore the Documentation\nThank you for choosing Utho. We‚Äôre excited to support you on your journey!","welcome-to-uthos-documentation#Welcome to Utho\u0026rsquo;s Documentation":"Welcome to Utho‚Äôs DocumentationWelcome to the official documentation for Utho, your go-to resource for all information related to our platform. Here, you‚Äôll find comprehensive guides, tutorials, and reference materials to help you get the most out of Utho.\nWhether you‚Äôre a new user looking to get started or an experienced user seeking advanced features, our documentation is designed to provide you with the information you need."},"title":"getting_started"},"/utho-docs/docs/how-to-add-a-record-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to add A record in Plesk.\nAlso read: How to add MX record in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to add A record in Plesk.\nThis is the most basic form of DNS record, and it is denoted by the letter ‚ÄúA,‚Äù which stands for the word ‚Äúaddress.‚Äù It provides the IP address of a certain domain.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to Hosting and DNS under the menu of websites and domains, then click on DNS settings.\nStep 3. Click on ‚ÄúAdd Record.‚Äù\nStep 4. Select ‚ÄúA record‚Äù from the record type drop-down menu.\nBecause we do not have any subdomains, we leave the domain name field empty for the main domain, micro.com.\nIn the TTL field, you can assign any value in seconds; we have entered 3600.\nIn the ‚ÄúIP Address‚Äù field, enter the IP address of your server, then click ‚ÄúOK.‚Äù\nNOTE: Use your main domain instead of micro.com."},"title":"How to add A record in Plesk"},"/utho-docs/docs/how-to-add-cname-record-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to add CNAME record in Plesk.\nAlso read: How to add MX record in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to add CNAME record in Plesk.\nInside the Domain Name System (DNS), there is a specific kind of resource record known as a Canonical Name (CNAME) record. This record transfers one domain name (an alias) to another (the canonical name).\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to¬†Hosting and DNS under the menu of websites and domains, then click on DNS settings.\nStep 3. Click on ‚ÄúAdd Record.‚Äù\nStep 4. Select CNAME record from the record type drop-down menu.\nEnter the subdomain name in the domain name field; in our case, we used www.\nIn the TTL field, you can assign any value in seconds; we have entered 3600.\nIn the canonical name field, I entered the canonical name as micro.com for the main domain. then select OK.¬†NOTE: Use your main domain instead of micro.com."},"title":"How to add CNAME record in Plesk"},"/utho-docs/docs/how-to-add-mx-record-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to add MX record in Plesk.\nAlso read: How to add A record in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to add MX record in Plesk.\nA mail exchanger record, also known as an MX record, details the email server that should be contacted to receive email messages on behalf of a certain domain name. In the Domain Name System, it is referred to as a resource record (DNS). It is possible to setup many MX records, each of which would normally refer to a different mail server in order to provide load balancing and redundancy.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to¬†Hosting and DNS under the menu of websites and domains, then click on DNS settings.\nStep 3. Click on ‚ÄúAdd Record.‚Äù\nStep 4. In the record type drop-down menu, select MX record. For the mail domain, we are not using any subdomains, so for the main domain, which is micro.com, we are keeping this field empty.\nIn the TTL field, you can assign any value in seconds; we have entered 3600.\nIn the mail exchange server field, enter the mail exchange server name, in our case, mail.micro.com.\nThese drop-downs were used to specify the priority, with 0 being very high. then select OK.¬†NOTE: Use your main domain instead of micro.com."},"title":"How to add MX record in Plesk"},"/utho-docs/docs/how-to-add-or-remove-components-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to add components in Plesk.\nAlso Read: How to assign permissions to Files and Folders in Plesk\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to add components in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Click on Tools and Settings, which is on the left side of the screen.\nStep 3. Under the PLESK menu, select Updates.¬†Step 4. click on add/remove components.\nStep 5. Select any component that you want to install, then click on continue.\nStep 6. Now that your selected component is installed, click on OK."},"title":"How to add components in Plesk"},"/utho-docs/docs/how-to-allow-a-specific-ip-address-in-firewall-in-windows-server/":{"data":{"":"Step.1 press windows button and open windows firewall with advance security.\nStep.2 \u003e Next, go to the inbound rules section.\nStep 3: Select the inbound rules.\nStep 4 \u003e Right-click on the rdp port 3330 and select properties from the context menu.\nStep 5: Select the Scope option.And select these IP options and give the IP you want."},"title":"How to Allow a Specific IP Address in \"firewall in windows server"},"/utho-docs/docs/how-to-assign-permissions-to-files-and-folders-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to assign permissions to Files and Folders in Plesk.\nAlso Read: How to add components in Plesk\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to assign permissions to Files and Folders in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Step 2. On the left side of the screen, select Files.¬†Step 3. Select any file on which you want to change permissions and click the right-side drop-down menu, then click on change permission.\nStep 4. Change the file permissions to your liking, then save.¬†Step 5. Now you will get a success message."},"title":"How to assign permissions to Files and Folders in Plesk"},"/utho-docs/docs/how-to-blacklist-a-domain-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to do Server-wide blacklist in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Server-wide blacklist in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to tools and settings on the left side of the screen, then select mail server settings from the mail menu.¬†Step 3. Click on the Black list. Then click on ‚Äúadd domain.‚Äù\nStep 4. You will need to provide the name of the domain that you do not wish to receive email from, and then press OK."},"title":"How to do Server-wide blacklist in Plesk"},"/utho-docs/docs/how-to-change-computer-name-in-windows-server-via-powershell/":{"data":{"":"","introduction-change-computer-name#INTRODUCTION Change Computer Name":"Changing name of the computer/system is just a basic step to set a custom tag for your system. In this tutorial, we will learn how to Change Computer Name in Windows Server 2016, 2019 and 2022 via PowerShell.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to [rtestserver]\nRename-Computer -NewName rtestserver -Force -PassThru Step 4. Run the following command to restart server and apply changes Change Computer Name\nRestart-Computer -Force Server name changed.\nThank You!"},"title":"How to Change Computer Name in Windows Server via PowerShell"},"/utho-docs/docs/how-to-change-nginx-port-in-linux/":{"data":{"":"\nStep 1: Check the default port by accessing the server IP address in the browser .\nStep 2:¬†Login into the server using root credentials through putty .\nStep 3: Open Nginx configuration file with a text editor .\n# vi /etc/nginx/nginx.conf Press ‚Äò I ‚Äô for the insert/modification mode .\nStep 4: Change the default port of the nginx to the custom port\nBefore change : port is 80 as shown below in screetshot\nAfter change : Port has been changed to 8081 (here) as shown in screenshot\nStep 5 Restart the nginx and network service .\n# systemctl status nginx # systemctl status network Step 6: Now check the webserver while accessing the server IP_address:8081¬†in the browser.¬†URL: - http://server_domain_name_or_IP:8081\nThank You :)"},"title":"How to change NGINX port in Linux"},"/utho-docs/docs/how-to-change-php-parameter-manually-through-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to change PHP parameter manually through Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to change PHP parameter manually through Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. select website and domains¬†Step 3. Select PHP settings.¬†Step 4. Change the default setting beneath the Performance and Security settings to your liking.¬†Step 5. Scroll down the page and press the ‚Äúapply‚Äù button to save the changes.\nStep 6. And then you will get a success message."},"title":"How to change PHP parameter manually through Plesk"},"/utho-docs/docs/how-to-change-ssh-default-port-on-opensuse/":{"data":{"":" How to change SSH default port on OpenSUSE\nIn this article, you will learn how to change SSH default port on OpenSUSE. A network protocol called Secure Shell (SSH) enables remote server access for users. Additionally, it secures communication between a client and private server by encrypting it.\nNevertheless, utilising the default Transmission Control Protocol (TCP) port 22 for SSH might be dangerous since it is exposed to a number of online dangers, including brute-force assaults, a hacking technique used to obtain encrypted sensitive data.\nTherefore, one of the greatest methods to safeguard your SSH server is to change your default port.","prerequisites#Prerequisites":" vim, vi or any other text editor installed on server to modify config files\nSuper user or SUDO user privileges.","steps-to-change-the-default-port-of-ssh#Steps to change the default port of SSH":"Step 1: Open SSH configuration file using below command in your favorite editor.\nvim /etc/ssh/sshd_config Content of SSH configuration file\nNow, on line number 13, you can see that the line is commented. to set a custom port for ssh service to listen on, follow the below step.\nStep 2: Add ‚ÄúPort 2222‚Äù in the next line then save and exit the file.\nAdd the custom port in configuration file\nNote:: This process is done when the SeLinux policy was in disabled mode. If you are using Enforce SeLinux policy, you need to follow the Step 7 of this article\nStep 3: Restart ssh service using below command\nsystemctl restart sshd Step 4: Now, to ensure that you ssh server is listening on your custom defined port, use netstat commad.\nnetstat -tunlp Ensure to check the custom ssh port\nStep 5: Enable the port in OS firewall\nfirewall-cmd --permanent --add-port=2222/tcp firewall-cmd --reload Now, you have successfully learnt how to change SSH default port on OpenSUSE."},"title":"How to change SSH default port on OpenSUSE"},"/utho-docs/docs/how-to-change-the-php-version-on-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to change the PHP version on Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to change the PHP version on Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. select website and domains¬†Step 3. Select PHP settings.¬†Step 4. Drop down the menu and select the PHP version you want to update for the domain.\nStep 5. Scroll down the page and press the ‚Äúapply‚Äù button to save the changes.¬†Step 6. And then you will get a success message."},"title":"How to change the PHP version on Plesk"},"/utho-docs/docs/how-to-change-your-plesk-password/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to change your Plesk password.\nAlso read: How to create a MySQL/MariaDB Database and Database User in Plesk\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to change your Plesk password.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to the ‚ÄúMy Profile‚Äù option, which is on the left side of the screen.\nStep 3. Click on ‚Äúgenerate,‚Äù copy and paste the new password into Notepad, and then click on ‚Äúapply‚Äù to use the new password.¬†Step 4. Then you will get a success message, and now you are able to login to your Plesk with a new password."},"title":"How to change your Plesk password"},"/utho-docs/docs/how-to-check-current-disk-space-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to check current Disk Space in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to check current Disk Space in Plesk.\nStep 1. Enter your server password to get into your Plesk account, which can be found by searching your browser for server ip:8880.\nStep 2. Choose Statistics from the menu that appears on the left side of the screen.\nStep 3. Now you can see how many people have reached your website in a month, et cetera."},"title":"How to check current Disk Space in Plesk"},"/utho-docs/docs/how-to-check-the-internet-speed-on-opensuse/":{"data":{"":" How to check the internet speed on OpenSUSE\nIn this article, you will learn how to check the internet speed on OpenSUSE. It is not unusual for users to inquire about the bandwidth of their Internet connection. When using the desktop version of the operating systems, all you have to do to check the speed is type the relevant query into a search engine and observe how long it takes for the results to load on any of the websites that come up in the search. This is the only thing you need to do in order to check the speed. In contrast to this, the process will be different while utilising the server-based alternative. You will learn how to check the speed of your connection using Ubuntu 20.04 by following the steps in this guide. When typing commands, it is imperative that one always do it as the root user.\nISPs have reported seeing traffic loads that are greater than they have ever been due to an increase in the number of individuals staying at home and spending more time on the internet. If you have observed that your network speed has slowed down at certain periods, the cause is likely a worldwide overflow.","prerequisites#Prerequisites":" Normal user with SUDO privileges or Super user\nZypper repolist enabled to install packages","steps-to-test-internet-speed-test-on-opensuse#Steps to test internet speed test on OpenSUSE":"Step 1: First check whether python is installed on your machine or not.\npython --version Step 2: Install python-pip to install speedtest command line interface to test the internet on your OpenSUSE server.\nzypper install python-pip install python-pip\nStep 3: Now, after successfully installing the python-pip on your machine, install the speedtest-cli command line interface on your server.\npip install speedtest-cli install Speedtest\nStep 4: Check the internet speed test on your server using the newly install command line interface.\nspeedtest-cli Acutal speed test"},"title":"How to check the internet speed on OpenSUSE"},"/utho-docs/docs/how-to-configure-modsecurity-in-apache/":{"data":{"":"","centos#CentOS":"","debian#Debian":"","default-debian-dir-for-modsecuritys-persistent-data-secdatadir-varcachemodsecurity#Default Debian dir for modsecurity\u0026rsquo;s persistent data SecDataDir /var/cache/modsecurity":"Introduction ModSecurity is a web firewall application for an Apache web server. In addition to providing logging capabilities, ModSecurity can monitor HTTP traffic in real time to detect attacks. ModSecurity also operates as an intrusion detection tool that allows you to respond to suspicious events taking place on your web systems.\nInstall ModSecurity You need Apache installed on your Microhost cloud before you install ModSecurity. The LAMP stack is used in this guide; see LAMP guidelines for installation.\nDebian sudo apt install libapache2-modsecurity Restart Apache:\n/etc/init.d/apache2 restart Check the ModSecurity version is 2.8.0 or later:\napt-cache show libapache2-modsecurity [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]When you list all mods using apachectl -M, ModSecurity is listed under the name security2_module.[/ht_message]\nUbuntu sudo apt-get install libapache2-mod-security2 Restart Apache:\n/etc/init.d/apache2 restart Check the version of ModSecurity is 2.8.0 or higher:\napt-cache show libapache2-mod-security2 CentOS yum install mod_security Restart Apache by entering the below command:\n/etc/init.d/httpd restart Check the version of ModSecurity is 2.8.0 or higher:\nyum info mod_security OWASP ModSecurity Core Rule Set The following steps are for distributions based on Debian. The paths and commands for RHEL will differ slightly.\n1. Move and update the default ModSecurity file name:\nmv /etc/modsecurity/modsecurity.conf-recommended modsecurity.conf 2. If needed, install git:\nsudo apt install git 3. OWASP ModSecurity CRS can be downloaded from Github:\ngit clone https://github.com/SpiderLabs/owasp-modsecurity-crs.git 4. Navigate into the directory you are downloading. Switch to crs-setup.conf.example, and rename crs-setup.conf.¬†Then pass the rules/¬†likewise.\ncd owasp-modsecurity-crs mv crs-setup.conf.example /etc/modsecurity/crs-setup.conf mv rules/ /etc/modsecurity/ 5. The config file should match the above path as specified in the IncludeOptional¬†directive. Add a further Guideline that refers to the collection of rules:\nDefault Debian dir for modsecurity‚Äôs persistent data SecDataDir /var/cache/modsecurity # Include all the *.conf files in /etc/modsecurity. # Keeping your local configuration in that directory # will allow for an easy upgrade of THIS file and # make your life easier IncludeOptional /etc/modsecurity/*.conf Include /etc/modsecurity/rules/*.conf 6. Restart Apache to give effect to changes:\n/etc/init.d/apache2 restart ","install-modsecurity#Install ModSecurity":"","introduction#Introduction":"","modsecurity-test#ModSecurity Test":"OWASP CRS builds on top of ModSecurity in order to extend existing rules.\n1. Navigate to the default Apache configuration and use the default configuration as an example to add two additional directives:\nServerAdmin webmaster@localhost DocumentRoot /var/www/html SecRuleEngine On SecRule ARGS:testparam ‚Äú@contains test‚Äù ‚Äúid:1234,deny,status:403,msg:‚ÄòOur test rule has triggered‚Äô‚Äù\n2. Restart Apache and then curl the index page to intentionally trigger the alarms:\ncurl localhost/index.html?testparam=test The response code is set to be 403. A message that shows the given ModSecurity rule worked should be in the logs. Use :¬†sudo tail -f /var/log/apache2/error.log\nModSecurity: Access denied with code 403 (phase 2). String match ‚Äútest‚Äù at ARGS:testparam. [file ‚Äú/etc/apache2/sites-enabled/000-default.conf‚Äù] [line ‚Äú24‚Äù] [id ‚Äú1234‚Äù] [msg ‚ÄúOur test rule has triggered‚Äù] [hostname ‚Äúlocalhost‚Äù] [uri ‚Äú/index.html‚Äù] [unique_id ‚ÄúWfnEd38AAAEAAEnQyBAAAAAB‚Äù] 3. Verify the OWASP CRS is valid:\ncurl localhost/index.html?exec=/bin/bash Check the error logs again: attempted execution of an arbitrary bash script has been captured by statute.\nModSecurity: Warning. Matched phrase ‚Äúbin/bash‚Äù at ARGS:. [file ‚Äú/etc/modsecurity/rules/REQUEST-932-APPLICATION-ATTACK-RCE.conf‚Äù] [line ‚Äú448‚Äù] [id ‚Äú932160‚Äù] [rev ‚Äú1‚Äù] [msg ‚ÄúRemote Command Execution: Unix Shell Code Found‚Äù] [data ‚ÄúMatched Data: bin/bash found within ARGS:: exec/bin/bash‚Äù] [severity ‚ÄúCRITICAL‚Äù] [ver ‚ÄúOWASP_CRS/3.0.0‚Äù] [maturity ‚Äú1‚Äù] [accuracy ‚Äú8‚Äù] [tag ‚Äúapplication-multi‚Äù] [tag ‚Äúlanguage-shell‚Äù] [tag ‚Äúplatform-unix‚Äù] [tag ‚Äúattack-rce‚Äù] [tag ‚ÄúOWASP_CRS/WEB_ATTACK/COMMAND_INJECTION‚Äù] [tag ‚ÄúWASCTC/WASC-31‚Äù] [tag ‚ÄúOWASP_TOP_10/A1‚Äù] [tag ‚ÄúPCI/6.5.2‚Äù] [hostname ‚Äúlocalhost‚Äù] [uri ‚Äú/index.html‚Äù] [unique_id ‚ÄúWfnVf38AAAEAAEqya3YAAAAC‚Äù] Thankyou..","owasp-modsecurity-core-rule-set#OWASP ModSecurity Core Rule Set":"","ubuntu#Ubuntu":""},"title":"How to Configure ModSecurity in Apache"},"/utho-docs/docs/how-to-configure-mx-record-in-mailenable/":{"data":{"":"\nIn the configuration section, we will see the creation of a webmail URL for Mailenable in IIS. After creation, we would be able to access the Mailenable using a URL from outside the localhost.\nStep 1: While installing the Mailenable, IIS default sites for Mailenable have already been created. Please see the screenshot below.\nStep 2: Now we have to edit the binding of the MailEnable WebMail site. Please see the screenshot for your reference.\nStep 3: While clicking on bindings, a new window will appear where we have to add the binding of our domain.¬†While clicking on ‚ÄùAdd‚Äù, a new window will appear as below:¬†Please see the screenshot below.\nStep 4: Now we have to point the webmail URL to the server‚Äôs IP. For example, if my webmail URL is mail.microhostcloud.com and my server IP is 103.209.144.155, then we have to add ‚ÄúA record‚Äù for mail.microhostcloud.com¬†to 103.209.144.155.\nNOTE: DNS propagation will take 4 to 8 hours.\nStep 5: We have to setup a default mail domain, which will be used for email communication with external domains. First we have to open the Mailenable console by searching for ‚ÄúMailenable‚Äù in the Windows search bar. Afterward, navigate to Servers-\u003elocalhost-\u003eServices and Connectors-\u003eSMTP.\nStep 6: Now in the properties section, we have to give the default mail domain and click on ‚Äúok‚Äù as per the screenshot.\nWe have completed the configuration section of Mailenable.\nThank you :)"},"title":"How to configure MX record in MAILENABLE"},"/utho-docs/docs/how-to-configure-or-change-the-system-hostname-in-linux/":{"data":{"":"","#":"Description A device or system hostname is a name given to a machine so that it may be identified inside a network using a format that is readable by humans. It shouldn‚Äôt come as much of a surprise, but the hostname on a Linux system may be changed quickly and simply by using a simple command such as ‚Äúhostname.‚Äù\n*If you only run hostname by itself, without any other options, it will return the current hostname of your Linux system in the format shown here:\n#hostname To modify or alter the hostname of your Linux system, just type in:\n#hostname dbserver Naturally, you will need to change ‚Äúnew hostname‚Äù to the exact hostname that you want to configure once you have done so. This will instantly result in a change to the hostname of your system; however, there is a catch: the old hostname will be reinstated if you reboot your computer.\n*There is a different, permanent¬†method available for altering the hostname of your machine. You could have already worked out that this would need changes to be made in certain configuration files, and if you did, you will be accurate in your assumption.\nSet System Hostname Permanently Systemd is a system and service manager that offers a hostnamectl command for managing hostnames in Linux. Newer versions of various Linux distributions, such as the most recent Ubuntu, Debian, CentOS, Fedora, and RedHat, come with systemd.\n*We will use the hostnamectl command to set the system hostname on distributions based on SystemD, as demonstrated in the example.\n#sudo hostnamectl set-hostname any_name The hostnames of older Linux distributions, such as those that utilise SysVinit instead of the shorter form init, may be altered by making a simple update to the hostname file, which is stored in:\n#vi /etc/hostname After that, you will need to add an additional entry for the hostname in:\n#vi /etc/hosts *Change the value next to ‚ÄúHOSTNAME‚Äù to your hostname if you want to maintain a permanent hostname.\nThank You "},"title":"How to Configure or Change the System Hostname in Linux"},"/utho-docs/docs/how-to-connect-node-js-application-with-mongodb-on-centos-2/":{"data":{"":"","#":"\nTable of contents This article will help you to connect Node.js application with MongoDB. Also, configure MongoDB drive for nodejs using Mongoose node application on CentOS and Redhat systems\nPrerequsities We assume that Node.js and MongoDB are already installed on your server. If not installed first follow our guide below to complete the installation.\nInstall MongoDB on Centos Install node.js on Centos Install mongoose Module Mongoose offers a simple schema-based approach for modeling data about the program which includes built-in typecasting, validation and much more.\n#npm install mongoose Connect Nodejs with MongoDB Create a file test server.js, and add content to the file below. For the more details about working with Node.js and MongoDB with mongoose read¬†this tutorial.\n\u003c/ Sample script of Node.js with MongoDB Connection // This code requires mongoose node module var mongoose = require('mongoose'); // Connecting local mongodb database named test var db = mongoose.connect('mongodb://127.0.0.1:27017/test'); // testing connectivity mongoose.connection.once('connected', function() { console.log(\"Database connected successfully\") }); Now we execute the test_server.js using node. If we get message ‚ÄúDatabase connected successfully‚Äù, It means our node.js app is successfully connecting database.\n#node test_server.js Output¬†‚Äì database connected successfully\nWe have connected Node.js Application with MongoDB successfully on CentOS\nThank you :)"},"title":"How to Connect Node.js Application with MongoDB on CentOS"},"/utho-docs/docs/how-to-craete-an-account-in-cpanel-with-whm/":{"data":{"":"\nStep 1: Login into you Cpanel by opening ‚Äú server-ip:2087 ‚Äù in the URL of the browser\nStep 2: login into the WHM using root credentials\nStep 3: Click on the Create a New Account button\nStep 4: Enter all the mandatory fields and choose a default of customized package if it is present\nStep 5: After filling all the details , click on the create button to create the account\nThnak you :)"},"title":"How To Create an Account in CPanel with WHM"},"/utho-docs/docs/how-to-create-a-backup-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to create a backup in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to create a backup in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Click on Tools and Settings, which is on the left side of the screen, then click on Backup Manager.\nStep 3. Click on ‚ÄúBack up.‚Äù\nStep 4. Select the options as per your own, then click on ‚ÄúOK‚Äù to complete the backup process. Please wait for the backup process to complete.\nStep 5. Now you are able to see your backup and the creation date of the backup."},"title":"How to create a backup in Plesk"},"/utho-docs/docs/how-to-create-a-mysql-mariadb-database-and-database-user-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to create a MySQL/MariaDB Database and Database User in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to create a MySQL/MariaDB Database and Database User in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. On the left side of the screen, select ‚ÄúDatabase,‚Äù then select ‚ÄúAdd Database.‚Äù¬†Step 3. Fill in the details for the database user name and generated password for the database user, then select your preferred options from the list. Finally, press the OK button.¬†Step 4. Now you can see your database user."},"title":"How to create a MySQL/MariaDB Database and Database User in Plesk"},"/utho-docs/docs/how-to-create-a-self-signed-certificate-in-windows-server/":{"data":{"":"","#":"\nINTRODUCTION In cryptography and computer security, self-signed certificates are public key certificates that are not issued by a certificate authority. These self-signed certificates are easy to make and do not cost money. However, they do not provide any trust value. On modern Windows versions (Windows Server 2022/2019/2016/2012R2) you can create a self-signed certificate using the built-in PowerShell cmdlet¬†[New-SelfSignedCertificate](https://learn.microsoft.com/en-us/powershell/module/pki/new-selfsignedcertificate?view=windowsserver2022-ps)¬†without using additional tools. In this tutorial, we will learn how to Create a Self-Signed Certificate in Windows Server.\nPrerequisites Windows Server\nPowerShell with Administrator\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell with Administrator\nStep 3. Run the following command to generate a self-signed certificate\nStep 4. We will edit the domain name and certificate location according to our preference.\nStep 5. Self-signed certificate generated.\nStep 6. Check our recently generated self-signed certificate by running certlm.msc in run.\nThank You!"},"title":"How to Create a Self-Signed Certificate in Windows Server"},"/utho-docs/docs/how-to-create-a-user-role-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to create a user role in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to create a user role in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Choose user from the menu on the left side of the screen.¬†Step 3. Click on user roles then click on create user roles.\nStep 4. Fill your favorite user role name.\nStep 5. Press ok.\nStep 6. And then you will get a success message."},"title":"How to create a user role in Plesk"},"/utho-docs/docs/how-to-create-and-connect-an-ftp-account-in-cpanel/":{"data":{"":"\nStep 1: Login into you Cpanel by opening ‚Äú server-ip:2087 ‚Äù in the URL of the browser\nStep 2: login into the WHM using root credentials\nStep 3: Open List Accounts\nStep 4: Open the account of cpanel by clicking on the Cpanel Icon\nStep 5: Open FTP Accounts option\nStep 6: Enter the required details .\nAccount has been created\nStep 7: Click on the configure FTP client for the username , server and port of FTP .\nStep 8: Open file zilla site manager and add the following protocol , host (IP) , Encryption ,Login type and username password for the connection .\nClick on connect\nStep 9: FTP has been connected successfully , drag and drop to share the files and folders .\nThank you :)"},"title":"How To create and connect an FTP Account in cPanel"},"/utho-docs/docs/how-to-create-dfs-namespaces-via-powershell/":{"data":{"":"INTRODUCTION\nDFS is a set of client and server services that allow an organization using Microsoft Windows servers to organize many scattered SMB file shares into a distributed file system. DFS has two components to its service: Location transparency and Redundancy. Basically a distributed file system (DFS) is¬†a file system that spans across multiple file servers or multiple locations, such as file servers that are situated in different physical places. Files are accessible just as if they were stored locally, from any device and from anywhere on the network. In this tutorial, we will learn how to create DFS NameSpaces via PowerShell.\nPrerequisites\nWindows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to create a folder for DFS Namespace share\nmkdir C:\\NEWFOLDER\\Share Step 4. Run the following command to set smbshare for DFS Namespace\nNew-SmbShare ‚ÄìName 'Share' ‚ÄìPath 'C:\\NEWFOLDER\\Share' -FullAccess 'Everyone' Step 5. Run the following command to create DFS namespace\n-Path (path of share)\n-Type (standalone , Domain1 , Domain2)\n-TargetPath (target share path : the one created above)\nNew-DfsnRoot -Path '\\\\NEWFOLDER\\Share' -Type DomainV2 -TargetPath '\\\\NEWFOLDER2\\Share' Step 6. Run the following command to confirm path\nGet-DfsnRoot -Path '\\\\NEWFOLDER\\Share' | Format-List Thank You!"},"title":"How to create DFS NameSpaces via PowerShell"},"/utho-docs/docs/how-to-create-redirects-with-nginx/":{"data":{"":"","analyzing-redirect-methods#Analyzing Redirect Methods":"Redirects can be used in many ways. If you already have a website and want to change your domain, you shouldn‚Äôt just give up on your old domain. If the content on your site disappears without telling the browser where it is now, bookmarks and links to your site from other pages on the internet will stop working. If you change domains without redirecting, you‚Äôll lose traffic from people who used to visit your site and all the credibility you‚Äôve worked hard to build.\nOften, it‚Äôs a good idea to register multiple versions of a name so that people who type in addresses that look like your main domain can find you. For example, if you have a domain called myspiders.com, you could also buy the domain names for myspiders.net and myspiders.org and point them both to your myspiders.com site. This lets you catch people who might be typing in the wrong address to get to your site. It can also stop another site from using a similar domain name and making money off of your online presence.\nSometimes, you need to change the names of pages on your site that have already been published and gotten traffic. In most cases, this would lead to a 404 Not Found error or, depending on your security settings, another error. You can avoid these by sending your visitors to a different page that has the right information they were looking for. There are a few different kinds of URL redirects, and each one tells the client browser something different. 302 temporary redirects and 301 permanent redirects are the most common.\nTemporary Redirects Temporary redirects are useful if you need to serve the web content for a certain URL from a different place for a short time. For example, if you are doing maintenance on your site, you may need to use a temporary redirect to send all of your domain‚Äôs pages to a page explaining that you will be back soon.\nTemporary redirects tell the browser that the content is temporarily at a different location, but that the original URL should still be tried.\nPermanent Redirects Permanent redirects are useful when your content has been moved to a new location forever.\nThis is useful for when you need to change domains or when the URL needs to change for other reasons and the old location will no longer be used. This redirect informs the browser that it should no longer request the old URL and should update its information to point to the new URL\nForcing SSL One common way to use redirects is to tell all traffic to a site to use SSL instead of HTTP.\nYou can make all requests for http://www.example.com go to https://www.example.com by using redirects.\nHow to Redirect in Nginx Redirects are an important part of Nginx and use directives that are often used. Most of the time, redirects can be set up by making a new server block for every URL.\nNginx server blocks can be saved in the main Nginx settings file in /etc/nginx/nginx.conf, but it‚Äôs easier to keep track of them if you make a new file for each configuration in /etc/nginx/sites-available/. As you turn files on or off, Nginx creates symbolic links, which are like short-cuts, from files in sites-available/ to another folder called sites-available/. By default, Nginx is installed with a single site-specific configuration in /etc/nginx/sites-available/default that is enabled and linked to /etc/nginx/sites-enabled/default.\nvi /etc/nginx/sites-available/default By default, it contains a standard web server configuration that will listen on port 80 and look for an¬†index.html¬†file located in¬†/var/www/html¬†on your system.\nIf you instead needed to redirect requests for¬†example1.com¬†to¬†example2.com you could remove the existing directives from this server block and replace them with a permanent redirect:\nvi /etc/nginx/sites-available/default server { listen 80; server_name example1.com; return 301 $scheme://example2.com$request_uri; } The return directive runs a URL substitution and then sends back the redirection URL and the status code that was given to it.\nIn this case, it uses the $scheme variable to use whichever scheme was used in the original request (http or https). Then it gives back the 301 permanent redirect code and the new URL.\nUsing the rewrite directive, you can do something similar to the Apache folder redirection to send a folder to a different subdomain. When this directive is put in a server block, requests inside the images directory will be temporarily sent to the subdomain images.example.com:\n/etc/nginx/sites-available/default server { ‚Ä¶ rewrite ^/images/(.*)$ http://images.example.com/$1 redirect; ‚Ä¶ } You could change redirect to permanent at the end of the statement to make the redirection last forever.\nYou‚Äôll need to restart Nginx for your changes to take effect. This can be done with systemctl on a new Ubuntu server:\nsystemctl restart nginx ","introduction#Introduction":"HTTP redirection, also called URL redirection, is a way to send a domain or address to a different one. There are many ways to use redirection, and there are a few different kinds to think about. Redirects are used when a website needs to send people who type in one address to a different one.\nAs you make content and manage servers, you will often need to send traffic from one place to another. This guide will talk about how these techniques can be used and how to do them in Apache and Nginx, which are the two most popular web servers.","prerequisites#Prerequisites":" An Ubuntu 20.04 server set up A super user or any normal user with SUDO privileges. Nginx server installed on your Ubuntu server "},"title":"How To Create Redirects with Nginx"},"/utho-docs/docs/how-to-delete-an-email-account-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to delete an Email account in Plesk.\nAlso Read: How to assign permissions to Files and Folders in Plesk\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to delete an Email account in Plesk.\nStep 1. Enter your server password to get into your Plesk account, which can be found by searching your browser for server ip:8880.\nStep 2. To see a list of the email accounts you have access to in your panel, choose Mail from the menu on the left side of the screen.\nStep 3. Select the email account or accounts that you want to delete by checking the box next to them and clicking the button marked ‚Äúremove.‚Äù.\nStep 4. After that, choose ‚Äúyes, delete‚Äù from the menu. The deletion of an email account on Plesk is completed in this manner.\nStep 5. You should immediately get a message congratulating you on your success."},"title":"How to delete an Email account in Plesk"},"/utho-docs/docs/how-to-disable-selinux-temporarily-or-permanently/":{"data":{"":"","prerequisiteshttpslinuxizecomposthow-to-disable-selinux-on-centos-7prerequisites#Prerequisites¬†\u003ca href=\"https://linuxize.com/post/how-to-disable-selinux-on-centos-7/#prerequisites\"\u003e#\u003c/a\u003e":"In this article you will learn How to Disable SELinux Temporarily or Permanently, SELinux, which stands for ‚ÄúSecurity Enhanced Linux,‚Äù is a security module for the Linux kernel that gives users and administrators more control over access controls. It lets people in based on the rules of SELinux policy.\nSELinux policy rules say how processes and users interact with each other and with files.\nAccess is denied when no SELinux policy rule explicitly allows it, like when a process tries to open a file.\nSELinux has three modes:\nEnforcing: SELinux allows access based on SELinux policy rules. Permissive: SELinux only logs actions that would have been denied if running in enforcing mode. Disabled: No SELinux policy is loaded. By default, in CentOS 7, SELinux is enabled and in enforcing mode\nIt is best to keep SELinux in enforcing mode, but you may need to switch it to permissive mode or turn it off completely in some situations.\nIn this tutorial, we will show you how to disable SELinux on CentOS 7 systems.\nPrerequisites¬†# Before starting with the tutorial, make sure you are logged in as a¬†root user or sudo privileges\nCheck the SELinux Status¬†sestatus After enter the the command output will be like below given.\nNow, for disabled the Selinux we need to use below given command\nvi /etc/selinux/config After enter the command need to change on the place of enabled to disabled like below given.\nAfter that we need to restart the server hit the below given command.\nsestatus After enter the command outpur will show like below.\nConclusion¬†#\nIn this tutorial, you learned how to permanently disable SELinux on CentOS 7 systems.\nThank you :)"},"title":"How to Disable SELinux Temporarily or Permanently"},"/utho-docs/docs/how-to-do-server-wide-whitelist-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to do Server-wide whitelist in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to do Server-wide whitelist in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to tools and settings on the left side of the screen, then select mail server settings from the mail menu.¬†Step 3. Click on the White list. Then¬†click on ‚Äúadd network.‚Äù\nStep 4. Specify an IP address or range of IP addresses from which mail must always be accepted, and then press ok."},"title":"How to do Server-wide whitelist in Plesk"},"/utho-docs/docs/how-to-download-mssql-server-express-edition-via-powershell/":{"data":{"":"INTRODUCTION\nMicrosoft SQL Server Express is a version of Microsoft‚Äôs SQL Server relational database management system that is free to download, distribute and use. It comprises a database specifically targeted for embedded and smaller-scale applications. In this tutorial, we will learn how to download MSSQL Server Express Edition via PowerShell.\nPrerequisites download MSSQL Server Express\nWindows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Write the following script to download SQL server Express Edition\nfunction Install-SQLServerExpress2019 { Write-Host \"Downloading SQL Server Express 2019...\" $Path = $env:TEMP $Installer = \"SQL2019-SSEI-Expr.exe\" $URL = \"https://go.microsoft.com/fwlink/?linkid=866658\" Invoke-WebRequest $URL -OutFile $Path\\$Installer Write-Host \"Installing SQL Server Express...\" Start-Process -FilePath $Path\\$Installer -Args \"/ACTION=INSTALL /IACCEPTSQLSERVERLICENSETERMS /QUIET\" -Verb RunAs -Wait Remove-Item $Path\\$Installer } Step 4. After writing the script, type ‚ÄúInstall-SQLServerExpress2019‚Äù and hit ENTER\nSQL Server Express Edition downloading and installation has started.\nThank You!"},"title":"How to download MSSQL Server Express Edition via PowerShell"},"/utho-docs/docs/how-to-download-ssms-via-powershell/":{"data":{"":"INTRODUCTION\nMicrosoft SQL Server Management Studio is a software application developed by Microsoft that is used for configuring, managing, and administering all components within Microsoft SQL Server. First launched with Microsoft SQL Server 2005, it is the successor to the Enterprise Manager in SQL 2000 or before. In this tutorial, we will learn how to download SSMS on Windows Server via PowerShell.\nPrerequisites\nWindows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Write the following script to download SSMS\nfunction Install-SQLServerManagementStudio { Write-Host \"Downloading SQL Server Management Studio...\" $Path = $env:TEMP $Installer = \"SSMS-Setup-ENU.exe\" $URL = \"https://aka.ms/ssmsfullsetup\" Invoke-WebRequest $URL -OutFile $Path\\$Installer Write-Host \"Installing SQL Server Management Studio...\" Start-Process -FilePath $Path\\$Installer -Args \"/install /quiet\" -Verb RunAs -Wait Remove-Item $Path\\$Installer } Step 4. After writing the script, type ‚ÄúInstall-SQLServerManagementStudio‚Äù and hit ENTER download SSMS via PowerShell\nSSMS downloading and installation has started.\nSSMS downloading and installation has completed.\nThank You!"},"title":"How to download SSMS via PowerShell"},"/utho-docs/docs/how-to-enable-hsts-hypertext-strict-transport-security-for-web-sites-in-windows-servers-via-powershell/":{"data":{"":"","introduction#INTRODUCTION":"HTTP Strict Transport Security¬†(HSTS) is a policy mechanism that helps to protect websites against¬†man-in-the-middle attacks¬†such as¬†protocol downgrade attacks¬†and¬†cookie hijacking. It allows¬†web servers¬†to declare that¬†web browsers¬†(or other complying¬†user agents) should automatically interact with it using only¬†HTTPS¬†connections, which provide¬†Transport Layer Security¬†(TLS/SSL), unlike the insecure¬†HTTP¬†used alone. HSTS is an¬†IETF¬†standards¬†track protocol. In this tutorial, we will learn, how to enable HSTS (Hypertext Strict Transport Security) for Web sites in Windows Servers via PowerShell.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to get site collection\n$sitesCollection = Get-IISConfigSection -SectionPath \"system.applicationHost/sites\" | Get-IISConfigCollection Step 4. Run the following command to get website you‚Äôd like to set HSTS\nSpecify the name of the site for ‚Äúname‚Äù=\"***\"\n$siteElement = Get-IISConfigCollectionElement -ConfigCollection $sitesCollection -ConfigAttribute @{\"name\"=\"yourdomain.com\"} Step 5. Run the following command to get setting of HSTS for target site\n$hstsElement = Get-IISConfigElement -ConfigElement $siteElement -ChildElementName \"hsts\" Step 6. Run the following command to enable HSTS for target site\nSet-IISConfigAttributeValue -ConfigElement $hstsElement -AttributeName \"enabled\" -AttributeValue $true set [max-age] of HSTS as 31536000 sec (365 days)\nset [max-age], refer to https://hstspreload.org/\nSet-IISConfigAttributeValue -ConfigElement $hstsElement -AttributeName \"max-age\" -AttributeValue 31536000 Step 7. Run the following command to set [includeSubDomains] of HSTS as enabled\nSet-IISConfigAttributeValue -ConfigElement $hstsElement -AttributeName \"includeSubDomains\" -AttributeValue $true NOTE: this option applies to all sub-domains\nStep 8. Run the following command to set [redirectHttpToHttps] of HSTS as enabled\nSet-IISConfigAttributeValue -ConfigElement $hstsElement -AttributeName \"redirectHttpToHttps\" -AttributeValue $true Thank You!"},"title":"How to enable HSTS (Hypertext Strict Transport Security) for Web sites in Windows Servers via PowerShell"},"/utho-docs/docs/how-to-enable-ioncube-loader-in-plesk/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to enable IonCube Loader in Plesk.\nThe ionCube PHP Encoder package is responsible for encoding PHP scripts, while the ionCube Loader PHP extension is responsible for decoding such scripts.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Click on tools and settings, which is located on the left side of the screen, and then choose php settings from the menu of general settings that appears after clicking on tools and settings.\nStep 3. Due to the fact that the ioncube loader is now standard in all Plesk PHP packages, the extension must be enabled for versions of PHP prior to 12.5.\nClick on any lesser PHP version other than 12.5; for example, we have clicked on the 7.4.33 FastCGI application, then open the extension tab.\nStep 4. Then click on the ‚Äúioncube‚Äù option to enable it.\nThank You üôÇ"},"title":"How to enable IonCube Loader in Plesk"},"/utho-docs/docs/how-to-enable-windows-subsystem-for-linux-wsl-feature-to-use-linux-on-windows-server/":{"data":{"":"","introduction#INTRODUCTION":"Windows Subsystem for Linux¬†(WSL) is a feature of Windows that allows developers to run a Linux environment without the need for a separate¬†virtual machine¬†or¬†dual booting. There are two versions of WSL: WSL 1 and WSL 2. WSL 1 was first released on August 2, 2016, and acts as a¬†compatibility layer¬†for running¬†Linux binary executables¬†(in¬†ELF¬†format) by implementing Linux¬†system calls¬†on the¬†Windows kernel.¬†It is available on¬†Windows Server 2016, 2019¬†and¬†2022. In this tutorial, we will learn how to enable Windows Subsystem for Linux (WSL) feature to use Linux on Windows Server 2016, 2019¬†and¬†2022.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to enable Windows Subsystem for Linux\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Step 4. In this tutorial, we will be creating the subsystem for Ubuntu.\nRun the following command to download Ubuntu 18.04\nInvoke-WebRequest -Uri \"https://aka.ms/wsl-ubuntu-1804\" -OutFile \"ubuntu-1804.zip\" Expand-Archive ubuntu-1804.zip cd ubuntu-1804 .\\ubuntu1804.exe Step 5. Enter new UNIX username: (to create a new user)\nEnter the password for the user.\nInstallation successful.\nStep 6. Windows resources are mounted on [/mnt/c] which can be accessed by\ndf -h ls -l /mnt/c Step 7. To access Linux resources with root priviledge, us sudo command\nThank You!"},"title":"How to enable Windows Subsystem for Linux (WSL) feature to use Linux on Windows Server."},"/utho-docs/docs/how-to-export-and-import-database-dumps-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to Export and Import Database Dumps in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Export and Import Database Dumps in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. On the left side of the screen, select ‚ÄúDatabase,‚Äù then select ‚ÄúExport Dump.‚Äù\nStep 3. Now click on ‚ÄúAutomatically download dump after creation‚Äù and then click on ‚ÄúOK.‚Äù\nStep 4. On the left side of the screen, select ‚ÄúDatabase,‚Äù then select ‚ÄúImport Dump.‚Äù\nStep 5. Now, select Import Dump and then Choose File.\nStep 6. Your dump will now be imported."},"title":"How to Export and Import Database Dumps in Plesk"},"/utho-docs/docs/how-to-extract-the-certificate-and-keys-from-a-pfx-file/":{"data":{"":"INTRODUCTION\nA Personal Information Exchange (. pfx) Files, is¬†password protected file certificate commonly used for code signing your application. It derives from the PKCS 12 archive file format certificate, and it stores multiple cryptographic objects within a single file: X. 509 public key certificates. In this tutorial, we will learn how to extract the certificate and keys from a¬†.pfx¬†in Windows Servers.\nPrerequisites\nWindows Server\n.pfx file\nInternet connectivity\nStep 1. Open the command prompt and go to the folder that contains your¬†.pfx¬†file.\nStep 2. Run the following command to extract the private key:\nopenssl pkcs12 -in [yourfile.pfx] -nocerts -out [drlive.key]\nStep 3. Run the following command to extract the certificate:\nopenssl pkcs12 -in [yourfile.pfx] -clcerts -nokeys -out [drlive.crt]\nStep 4. Run the following command to decrypt the private key:\nopenssl rsa -in [drlive.key] -out [drlive-decrypted.key]\nThank You!"},"title":"How to extract the certificate and keys from a¬†.pfx¬†file"},"/utho-docs/docs/how-to-gain-access-to-uthos-partner-portal/":{"data":{"":"In this easy-to-follow document, we‚Äôll walk you through the steps to get started with Utho‚Äôs Partner Portal.\nStep 1: Sign Up for an Account\nThe journey begins with signing up for your Utho Partner Portal account.\nJust follow the link to start your adventure: https://console.utho.com/partner-signup.\nStep 2: Explore the Cloud Platform\nAfter you‚Äôve successfully signed up, you‚Äôll be directed to our Cloud Platform.\nHere, you have the power to test and deploy your cloud infrastructure.\nIt‚Äôs like having your very own playground for innovation!\nWithin 24 hours of signing up, our dedicated team will review and approve your partner portal request.\nStep 3: Login to Partner Portal.\nOnce our team approves the partner portal, you are now an official Utho partner.\nYou‚Äôll receive an Email and WhatsApp message with a login link that will take you straight to your Partner Portal.\nYou can login to partner portal by this link.\nAfter logging in using this link, you will gain access to your partner portal.\nWithin this portal, you can efficiently manage orders, customers, and all aspects that enable the efficient management of your customer base.\nStep 4: Switch to Partner Mode\nIf the partner login link doesn‚Äôt directly redirect you to the partner portal, you can easily switch to the partner mode from within the portal.\nThese simple steps will assist you in gaining access to Utho‚Äôs partner portal.\nFacing any issue in getting started? Let our support team help you. Send us an email at support@utho.com and we‚Äôll take care of it for you."},"title":"How to Gain Access to Utho's Partner Portal"},"/utho-docs/docs/how-to-generate-csr-certificate-for-ssl-installation-in-windows-server/":{"data":{"":"Step 1: Login into your server using RDP using Administrator login details.\nStep 2: Open Information Services (IIS) Manager\nStep 3: Select the required server and open the ‚ÄúServer Certificates‚Äù option.\nStep 4: On the right-hand side options, click on the ‚ÄúCreate Certificate Request‚Äù button.\nStep 5: Enter all the required information for the CSR and then click on Next.\nStep 6: Choose a bit length of 2048 and go to Step 7.\nStep 7: Choose the location where you want to save your CSR and click ‚ÄúFinish.‚Äù The CSR.txt file will be saved at your preferred location.\nA CSR.txt file has been generated."},"title":"How to Generate CSR Certificate for SSL Installation in Windows Server"},"/utho-docs/docs/how-to-install-ansible-on-centos-server/":{"data":{"":" How to install Ansible on CentOS\nIntroduction In this article, you will learn how to install ansible on centos. Administrators and operations teams can easily manage a large number of servers thanks to configuration management systems. They give you the ability to automate the control of numerous systems from a single central place. Although there are numerous well-liked configuration management tools for Linux systems, like Chef and Puppet, these are frequently more complicated than many people need or want. Due to its lower startup costs than these alternatives, Ansible is a fantastic substitute.\nAnsible operates by setting up client machines from a computer with installed and set up Ansible components. It connects using standard SSH channels to send commands, copy files, and retrieve data from distant machines. Because of this, installing additional software on the client machines is not necessary for an Ansible system. This is one method by which Ansible makes server administration simpler. No matter where a server is in its life cycle or whether it has an open SSH port, Ansible may be used to configure it.\nBecause Ansible has a modular approach, it is simple to expand it to utilise the functionalities of the primary system to handle particular cases. Any language can be used to write modules, and they communicate using standard JSON. Due to its expressiveness and resemblance to well-known markup languages, YAML data serialisation format is typically used for configuration files. Ansible may communicate with clients using either command-line tools or its Playbooks, or configuration scripts.","prerequisites#Prerequisites":" Yum repository configured to install packages. You can have you CentOS server here.\nAny normal user with SUDO privileges or Super user","steps-to-install-ansible-on-centos#Steps to install Ansible on CentOS":"Step 1: Ensure that your reposlists are configured and working file.\nyum repolist All the repositories\nStep 2: Install the Extra Packages for Enterprise Linux, EPEL repolist on your machine. As you can see in the above screenshot, I have already installed the EPEL repolist. You can install the same using the below command\nyum install epel-release -y Step 3: Now, install the Ansible on your CentOS machine.\nyum install ansible -y Install Ansible on centos\nAnd, this is how you have learnt how to install ansible on centos server"},"title":"How to install Ansible on Centos Server"},"/utho-docs/docs/how-to-install-ansible-on-debian-server/":{"data":{"":" How to install Ansible on Debian server\nIntroduction In this tutorial, we will learn how to install Ansible on Ubuntu. Administrators and operations teams can easily manage a large number of servers thanks to configuration management systems, Ansible. They give you the ability to automate the control of numerous systems from a single central place. Although there are numerous well-liked configuration management tools for Linux systems, like Chef and Puppet, these are frequently more complicated than many people need or want. Due to its lower startup costs than these alternatives, Ansible is a fantastic substitute.\nAnsible operates by setting up client machines from a computer with installed and set up Ansible components. It connects using standard SSH channels to send commands, copy files, and retrieve data from distant machines. Because of this, installing additional software on the client machines is not necessary for an Ansible system. This is one method by which Ansible makes server administration simpler. No matter where a server is in its life cycle or whether it has an open SSH port, Ansible may be used to configure it.\nBecause Ansible has a modular approach, it is simple to expand it to utilise the functionalities of the primary system to handle particular cases. Any language can be used to write modules, and they communicate using standard JSON. Due to its expressiveness and resemblance to well-known markup languages, YAML data serialisation format is typically used for configuration files. Ansible may communicate with clients using either command-line tools or its Playbooks, or configuration scripts.","prerequisites#Prerequisites":" apt repository configured to install packages\nAny normal user with SUDO privileges or Super user","steps-to-install-ansible-on-debian-server#Steps to install Ansible on Debian server":"step 1: Update your apt repolist before start installing the Ansible on your Debian server.\napt-get update Update the apt repo\nStep 2: Now, just install the ansible on your server.\napt install ansible Install the ansible\nStep 3: Verify the installation of the ansible on your server by check the installed version of Ansible.\nansible --version Check the ansible version\nAnd this is how you have learnt how to install Ansible on Debian server"},"title":"How to install Ansible on Debian server"},"/utho-docs/docs/how-to-install-ansible-on-ubuntu/":{"data":{"":" How to install Ansible on Ubuntu\nIntroduction In this tutorial, we will learn how to install Ansible on Ubuntu. Administrators and operations teams can easily manage a large number of servers thanks to configuration management systems, Ansible. They give you the ability to automate the control of numerous systems from a single central place. Although there are numerous well-liked configuration management tools for Linux systems, like Chef and Puppet, these are frequently more complicated than many people need or want. Due to its lower startup costs than these alternatives, Ansible is a fantastic substitute.\nAnsible operates by setting up client machines from a computer with installed and set up Ansible components. It connects using standard SSH channels to send commands, copy files, and retrieve data from distant machines. Because of this, installing additional software on the client machines is not necessary for an Ansible system. This is one method by which Ansible makes server administration simpler. No matter where a server is in its life cycle or whether it has an open SSH port, Ansible may be used to configure it.\nBecause Ansible has a modular approach, it is simple to expand it to utilise the functionalities of the primary system to handle particular cases. Any language can be used to write modules, and they communicate using standard JSON. Due to its expressiveness and resemblance to well-known markup languages, YAML data serialisation format is typically used for configuration files. Ansible may communicate with clients using either command-line tools or its Playbooks, or configuration scripts.","prerequisites#Prerequisites":" apt repository configured to install packages\nAny normal user with SUDO privileges or Super user","steps-to-install-ansible-on-ubuntu-server#Steps to install Ansible on Ubuntu server":"Step 1: Update your apt repolist before start installing the Ansible on your Ubuntu server.\napt-get update Update your repolist\nStep 2: Now,Configure the PPA on your system\napt install software-properties-common add-apt-repository --yes --update ppa:ansible/ansible Step 3: Again update the apt repositories\napt install ansible -y Install Ansible\nStep 4: Verify the installation of the ansible on your server by checking the installed version of Ansible.\nansible --version Check the installed version of Ansible\nAnd this is how you have learnt how to install Ansible on Ubuntu server"},"title":"How to install Ansible on Ubuntu"},"/utho-docs/docs/how-to-install-apache-server-on-opensuse/":{"data":{"":" How to install Apache server on OpenSUSE\nIn this article, you will learn how to install Apache server on OpenSUSE server. Apache is a well-known HTTP Server that is free, open source, and runs on Unix-like operating systems like Linux and Windows OS. Since it came out 20 years ago, it has been the most popular web server. Many sites on the Internet use it to run. It is easy to install and set up so that a Linux or Windows server can host one or more websites.\nIn this article, we‚Äôll show you how to use the command line to install, set up, and manage Apache HTTP web server on OpenSUSE server.","prerequisites#Prerequisites":" Internet accessible on server\nSuper user or any normal user with SUDO privileges\nInstall Apache Web Server Step 1: Check your system release.\ncat /etc/os-release OpenSUSE 15.0\nStep 2: Now, update the system software packages to the latest version.\nzypper update -y Step 3: Next, do the following with the zypper package manager to install Apache HTTP server from the default software repositories.\nzypper install apache2 -y Install apache on Opensuse server","start-apache-server-on-opensuse#Start Apache Server on OpenSUSE":"Step 4: After installing Apache web server, you can start it for the first time and set it to start automatically when the system starts up.\nsystemctl enable --now apache2 Step 5: You can confirm the status of Apache server by using following command\nsystemctl status apache2 Start and enable the Apache2 services\nConfigure firewalld to Allow Apache Traffic Step 6: The firewall that comes with CentOS 7 is set up to block Apache traffic by default. To let web traffic through on Apache, change the system firewall rules to allow HTTP and HTTPS packets to come in.\nfirewall-cmd --permanent --add-service http firewall-cmd --permanent --add-service https firewall-cmd --reload Add http port in firewalld\nTest Apache HTTP Server on CentOS 7 Step 7: Now, edit or create you index page to test your installation.\necho \"\u003ch1\u003e Greetings from Microhost Cloud \u003c/h1\u003e \u003e\u003e /srv/www/htdocs/index.html If you now go to the following URL, a default Apache page will be shown.\nhttp://server_ip Successfully installed apache2 on OpenSUSE\nAnd this is how you have learnt how to install Apache server on OpenSUSE"},"title":"How to install Apache server on OpenSUSE"},"/utho-docs/docs/how-to-install-asp-net-in-windows-servers-via-powershell/":{"data":{"":"","introduction-install-aspnet-in-windows-servers#INTRODUCTION install ASP.NET in Windows Servers":"ASP.NET¬†is an¬†open-source, server-side web-application framework¬†designed for¬†web development¬†to produce¬†dynamic web pages. It was developed by¬†Microsoft¬†to allow¬†programmers¬†to build dynamic¬†web sites,¬†applications¬†and¬†services. The name stands for Active Server Pages Network Enabled Technologies. In this tutorial, we will learn how to install ASP.NET in Windows Servers via PowerShell.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to install ASP.NET 4.7\nInstall-WindowsFeature Web-Asp-Net45 Step 4. Run the following command to restart the server\nRestart-Computer -Force ASP.NET installed.\nThank You!"},"title":"How to install ASP.NET in Windows Servers via PowerShell"},"/utho-docs/docs/how-to-install-dfs-replication-via-powershell/":{"data":{"":"INTRODUCTION\nDistributed File System Replication, or DFS Replication, is a role service in Windows Server that enables you to efficiently replicate folders across multiple servers and sites. You can replicate all types of folders, including folders referred to by a DFS namespace path. DFS Replication is an efficient, multiple-master replication engine that you can use to keep folders synchronized between servers across limited bandwidth network connections. The service replaces the File Replication Service (FRS) as the replication engine for DFS namespaces. In this tutorial, we will learn how to install DFS Replication via PowerShell.\nPrerequisites\nWindows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to\nInstall-WindowsFeature FS-DFS-Replication -IncludeManagementTools install DFS Replication\nThank You!"},"title":"How to install DFS Replication via Powershell"},"/utho-docs/docs/how-to-install-distributed-file-system-namespace-feature-via-powershell/":{"data":{"":"INTRODUCTION\nDFS is a set of client and server services that allow an organization using Microsoft Windows servers to organize many scattered SMB file shares into a distributed file system. DFS has two components to its service: Location transparency and Redundancy. Basically a distributed file system (DFS) is¬†a file system that spans across multiple file servers or multiple locations, such as file servers that are situated in different physical places. Files are accessible just as if they were stored locally, from any device and from anywhere on the network. In this tutorial, we will learn how to install Distributed File System NameSpace feature via PowerShell.\nPrerequisites\nWindows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to install DFS NameSpace with Admin tools\nInstall-WindowsFeature FS-DFS-Namespace -IncludeManagementTools install Distributed File System\n-‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì\nDFS installed."},"title":"How to install Distributed File System NameSpace feature via PowerShell"},"/utho-docs/docs/how-to-install-django-on-alma-linux/":{"data":{"":"","1-global-install-from-packages#1. Global Install from Packages":"","2-install-django-with-pip-in-a-virtual-environment#2. Install Django with pip in a Virtual Environment":"","3-creating-a-sample-project-with-newly-installed-django#3. Creating a Sample Project with newly installed Django":"","4-changing-the-allowed-hosts-setting-in-django#4. Changing the ALLOWED HOSTS setting in Django":"","5-testing-out-the-development-server#5. Testing out the Development Server":" How to install Django on Alma Linux\nIn this document, we will learn how to install Django on Alma Linux. Django is a web framework written in Python that lets you make interactive websites and apps. With Django, it‚Äôs easy to make Python web apps, and the framework takes care of a lot of the hard work for you.\nIt works quickly and is made to help developers get their apps up and running as soon as possible. Django helps developers avoid common security mistakes like SQL injection, XSS, CSRF, and clickjacking.\nThis guide will show you how to put Django on a server running Alma Linux. After you‚Äôve installed it, you‚Äôll make a new project that will be the basis for your website.\nPostgreSQL, MariaDB, MySQL, Oracle, and SQLite are the four main databases that Django supports. Other popular SQL and NoSQL databases are supported at different levels by community libraries. We recommend that you choose the same database for production and development, even though Django‚Äôs Object-Relational Mapper brings out many of the differences in the database (ORM).\nMethods to install Django There are different ways to install Django, depending on your needs and how you want to set up your development environment. There are pros and cons to each of these, and one might work better for you than the others.\nSome of the different ways are:\nGlobal install from packages: The standard yum package manager in Alma Linux can be used to install Django packages from the official repositories. This is simple, but not very adaptable. It‚Äôs also possible that the version in the repositories is behind the official project version.\nIn a virtual environment, you can install with pip:¬†Using tools like venv and virtualenv, you can give each of your projects its own environment. With a virtual environment, you can put Django and other customizations and packages for the project in the project directory. Most of the time, this is the best and most practical way to work with Django.\nTo install the development version with git:¬†If you want to install the latest development version, you can¬†get the code from the Git repository¬†instead of the stable release. This needs to be done in your virtual environment so that you can get the latest fixes and features. But development models aren‚Äôt as stable as versions that have been tested more. (Not covered by this document)\nPrerequisites: Any browser installed on your machine\nSuper user or any normal user with SUDO privileges.\nInstallation Steps: 1. Global Install from Packages If you want to use the yum repositories to set up Django, the process is very easy.\nStep 1:¬†First step is to install Epel-release:\nyum install epel-release -y Step 2:¬†Now check the version of¬†Python you installed on your Alma Linux server. You can see if it‚Äôs true by:\npython3 ‚ÄìV Step 3:¬†You can now put Django on your computer:\nyum install python3-django Step 4:¬†You can make sure the installation is right by:\ndjango-admin version Django version\nThis means that the software will be installed correctly. You might also find that the version of Django you have is not the latest stable version.\n2. Install Django with pip in a Virtual Environment Django can be installed on your system in a virtual environment in a flexible and easy way. We‚Äôll show you how to install Django in a virtual environment, which will be made with the venv module in the standard Python 3 library. With this tool, you can create a virtual Python environment and install Python packages without affecting the rest of the system. So, you can choose Python packages for each project, even if they conflict with the needs of other projects.\nStep 2.1:¬†First list the local package index:\nyum repolist Step 2.2:¬†Check the version of Python that is installed:\npython3 -V Step 2.3:¬†Now, install pip using the yum repositories:\nyum install python3-pip python3-venv -y Step 2.4:¬†Now, whenever you start a new project, you can set up a virtual environment for it. Start by making a new project directory and moving it to the new one:\nmkdir ~/testProject cd ~/testProject Step 2.5:¬†Next, use the python command in the project directory to set up a virtual environment for your version of Python. We‚Äôll call my env as our virtual environment, but you should call it descriptive:\npython3.8 - venv my_env Step 2.6:¬†This will install standalone versions of Python and pip in your project directory into a separate directory structure. The file hierarchy for where your packages will be installed will be put in a directory with the name you give it.\nPackages can be install into the isolated environment by:\nsource my_env/bin/activate Prompt after executing above command\nStep 2.7:¬†Your prompt should now say that your virtual environment is up and running. It will look like (my env)username@hostnamenewProject$.\nYou can add Django to your new environment with pip. No matter what version of Python you have, you should only call pip when you are in your virtual environment. Also, because you are installing locally, you won‚Äôt need sudo:\npip install django Keep in mind that this version may be different.\nStep 2.8:¬†To leave your virtual environment, you must send the deactivate command from anywhere on the system:\ndeactivate 3. Creating a Sample Project with newly installed Django With Django, you can start building your project. We will talk about how to make a project and test it on your development server using a virtual environment.\nStep 3.1:¬†After following the above steps, you jest need to follow the below steps to create a sample project with your newly installed Django\nWith startproject, you can use the¬†django-admin¬†command to build your project. You could call this something else, but we‚Äôre going to call our project djangoproject. In the working directory you already have, startproject will create a directory with:\n(myEnv) $ django-admin startproject Step 3.2:¬†Using the manage.py command to migrate the database. Migration takes into account any changes you have made to your database schema in your Django models.\nTo move the database, you¬†can:\n(myEnv) $ python3.8 manage.py migrate Migrate the database of Django project\nLastly, let‚Äôs make an administrative user so you can use the Djano admin interface. Using the createsuperuser command, let‚Äôs do this:\n(myEnv) # python3.8 manage.py createsuperuser You will be asked to give your user a user name, an email address, and a password.\n4. Changing the ALLOWED HOSTS setting in Django You will need to change one of the directives in the Django settings in order to test your app.\nStep 4.1:¬†Type: to open the settings file.\nvi ~/django-test/djangoproject/settings.py Find the ALLOWED HOSTS directive inside. This sets up a list of addresses or domain names that can be used to connect to the Django instance. If a request comes in with a Host header that is not on this list, an exception will be thrown. Django needs you to set this so that a certain type of security hole doesn‚Äôt happen.\nSave the file and close the editor when you are done.\nStep 4.2:¬†Put the IP addresses or domain names that are connected to your Django server in the square brackets. Each item should be written in quotation marks, with a comma between each entry. Add a period to the beginning of the entry to get requests for the whole domain and any subdomains:\nALLOWED_HOSTS = ['your_server_ip_or_domain', 'your_second_ip_or_domain', . . .] Step 4.3:¬†Save the file and close the editor when you are done.\n5. Testing out the Development Server Once you have a user, you can start up the Django development server to see what a new Django project looks like. You should only use this to build something. When you are ready to deploy, make sure to carefully follow Django‚Äôs deployment instructions.\nStep 5.1:¬†Make sure that the right port is open in your firewall before you try the development server. If you set up your server according to the initial setup guide and are using UFW, you can open port 8000 by typing:\nfirewall-cmd --permanent --allow-port=8000/tcp firewall-cmd --reload Step 5.2:¬†Start the development server:\n(myEnv) # python3.8 manage.py migrate Running Django project after installing Django successfully\nVisit your server‚Äôs IP address followed by¬†:8000¬†in your favourite web browser:\nYou should see something that looks like the below screenshot:\nIn this article, your have learnt how to install Django on Alma Linux","installation-steps#Installation Steps:":"","methods-to-install-django#Methods to install Django":"","prerequisites#Prerequisites:":""},"title":"How to install Django on Alma Linux"},"/utho-docs/docs/how-to-install-docker-on-opensuse-15/":{"data":{"":" How to install Docker on Opensuse\nIn this article, you wil learn how to install Docker on OpenSUSE. Docker is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly. Developers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.","prerequisites#Prerequisites":" Super user privileges or any normal user with SUDO privileges\nInternet enabled OpenSUSE server to install packages.","steps-to-install-docker-on-opensuse#Steps to install Docker on OpenSUSE":"Step 1: The first step you need to do is get the repositories up to date. To accomplish this, use the following command:\nzypper update Step 2: After you have completed all of the necessary preparations, you may at long last go on to installing Docker on OpenSUSE by executing the following command.\nzypper install -y docker Step 3: Now, start the Docker services and enable to start the service automatically after every reboot.\nsystemctl enable --now docker Step 4: Confirm the installation of the docker service on your machine.\ndocker info Step 5: Run a test image on the docker to test the successfully installation.\ndocker run hello-world "},"title":"How to install Docker on OpenSUSE 15"},"/utho-docs/docs/how-to-install-docker-on-windows-server-via-powershell/":{"data":{"":"","#":"\nINTRODUCTION induct Docker on Windows Server Docker¬†is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. The service has both free and premium tiers. The software that hosts the containers is called Docker Engine. It was first started in 2013 and is developed by Docker, Inc. In this tutorial, we will learn how to install Docker in a Windows Server via PowerShell.\nPrerequisites install Docker on Windows Server Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to install Containers feature\nEnable-WindowsOptionalFeature -Online -FeatureName Containers Step 4. Run the following command to restart the server after installing Containers\nRestart-Computer -Force Step 5. Run the following command to install Docker Repo\nInstall-Module -Name DockerMsftProvider -Repository PSGallery -Force Step 6. Run the following command to install Docker\nInstall-Package -Name docker -ProviderName DockerMsftProvider Step 7. Run the following command to restart the server after installing Docker\nRestart-Computer -Force Step 8. Run the following command to check Docker version\ndocker version Thank You!"},"title":"How to install Docker on Windows Server via PowerShell"},"/utho-docs/docs/how-to-install-flutter-on-ubuntu-server/":{"data":{"":" How to install Flutter on Ubuntu server\nIn this article, you will learn how to install Flutter on Ubuntu server. Snaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nInternet enabled server with updated security patches\nAlthough, we have covered the installation of Snap on Ubuntu server. but if you face any issue, you can follow this guide","steps-to-install-snap-on-ubuntu-server#Steps to install Snap on Ubuntu server":"Step 1: Update the APT repository to install the latest packages.\napt update Step 2: Install the snap package using the below command.\napt install snapd Step 3: Now, verify the installation of your snap package.\nsnap version Snap version\nStep 4: Install The flutter gdk on your server.\nsnap install flutter --classic Step 5: Add the flutter tool to your path:\nexport PATH=\"$PATH:`pwd`/flutter/bin\" Step 6: Install the pre-defined binaries of flutter\nflutter precache Flutter installed on server\nAnd this is how you will install Flutter SDK on your Ubuntu server"},"title":"How to install Flutter on Ubuntu server"},"/utho-docs/docs/how-to-install-flutter-sdk-on-centos-server/":{"data":{"":" How to install Flutter on CentOS server\nIn this article, you will learn how to install Flutter SDK on CentOS server. The Google Flutter Software Development Kit (SDK) is a free and open-source tool for creating cross-platform mobile applications. Flutter enables programmers to create high-performance, scalable applications for Android or iOS that have aesthetically pleasing and useful user interfaces using a single platform-independent codebase. With the help of a library of pre-made widgets, Flutter enables even non-programmers and non-developers to swiftly launch their own mobile applications.\nFlutter, which was developed by Google in 2015 and formally released in 2018, has swiftly taken over as the preferred toolkit for developers. Flutter recently eclipsed React Native to take the top spot among mobile app development frameworks, according to Statista.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nyum repositories configured with CentOS server.","steps-to-install-snap-on-centos-server#Steps to install Snap on CentOS server":"Step 1: Before installing the Snap on your server, you need to install the extra packages for enterprises linux( EPEL) repsitories\nyum install epel-release Installing EPEL repo\nStep 2: Install the Snap by executing the below command\nyum install snapd Install the Snap on Centos\nStep 3: Start and enable the snapd socket to start working with snapd\nsystemctl enable --now snapd.socket Output- Created symlink from /etc/systemd/system/sockets.target.wants/snapd.socket to /usr/lib/systemd/system/snapd.socket. Step 4: Now, you must enter the following to establish a symbolic link between /var/lib/snapd/snap and /snap in order to enable support for traditional snaps: ln -s /var/lib/snapd/snap /snap Step 5: Now, either set the PATH varialbe using the below command or restart another terminal\necho export PATH=$PATH:/snap/bin \u003e\u003e ~/.bashrc export .bashrc Step 6: Check the snapd version.\nsnap version Version of Snap\nStep 7: Install flutter on your server .\nsnap install flutter --classic Step 8: Add the flutter tool to your path:\nexport PATH=\"$PATH:`pwd`/flutter/bin\" Step 9: Install the pre-defined binaries of flutter\nflutter precache And this is how you will install Flutter SDK on your CentOS server."},"title":"How to install Flutter SDK on CentOS server"},"/utho-docs/docs/how-to-install-go-on-opensuse/":{"data":{"":" How to install GO on OpenSUSE\nIn this article, you will learn how to install GO on OpenSUSE 15 server. Golang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.\nThis is, in my opinion, one of the most undervalued aspects of Go. Compiling to a single executable binary signifies:\nThere is no requirement for a runtime interpreter, so a binary can be significantly smaller than a project‚Äôs subdirectories. This is advantageous for the efficacy of containerisation and orchestration.\nAs machine code requires no additional runtime for execution, a binary executable can execute and recuperate effectively.\nIf you are familiar with the fundamentals of Go, you will observe that the language does not attempt to be excessively complex or remarkable. It is just enough to complete the task.\nGo is a high-level programming language with autonomous, hands-free memory management. So that you can concentrate on the more important aspects without sacrificing too much performance. Not everyone appreciates the concept of automatic waste collection, but productivity is the focus of this passage.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges\nZypper repolist enabled on OPENSUSE to install packages.","steps-to-install-opensuse#Steps to install OPENSUSE":"Step 1: Update outdated software packages\nzypper update -y Step 2: Install the GO package on your OPENSUSE server\nzypper install go Install GO on OPENSUSE\nStep 3: Check the installed version of GO on your OPENSUSE 15 server\ngo version Check the installed version of GO\nIn this article, you have learnt how to install GO on your OpenSUSE server. You can install Flutter on your CentOS or Ubuntu server."},"title":"How to install Go on OpenSUSE"},"/utho-docs/docs/how-to-install-hyper-v-on-windows-server-via-powershell/":{"data":{"":"","introduction-install-hyper-v-on-windows#INTRODUCTION install Hyper-V on Windows":"Microsoft Hyper-V, codenamed¬†Viridian,¬†and briefly known before its release as¬†Windows Server Virtualization, is a¬†native hypervisor; it can create¬†virtual machines¬†on¬†x86-64¬†systems running¬†Windows.¬†Starting with¬†Windows 8, Hyper-V superseded¬†Windows Virtual PC¬†as the¬†hardware virtualization¬†component of the client editions of¬†Windows NT. A¬†server computer¬†running Hyper-V can be configured to expose individual virtual machines to one or more networks. Hyper-V was first released with¬†Windows Server 2008, and has been available without additional charge since¬†Windows Server 2012¬†and Windows 8. A standalone Windows Hyper-V Server is free, but has a¬†command-line interface¬†only. The last version of free Hyper-V Server is Hyper-V Server 2019, which is based on¬†Windows Server 2019. In this tutorial, we will earn, how to install Hyper-V on Windows Server.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to install Hyper-V with Windows admin tools\nInstall-WindowsFeature Hyper-V -IncludeManagementTools Step 4. Run the following command to restart the server after installing Containers\nRestart-Computer -Force Thank You!"},"title":"How to install Hyper-V on Windows Server via PowerShell"},"/utho-docs/docs/how-to-install-java-jdk-on-opensuse/":{"data":{"":" How to install JAVA JDK on OpenSUSE\nIn this article, you will learn how to install Java JDK on OpenSUSE. The Java Development Kit is a distribution of Java Technology by Oracle Corporation. It implements the Java Language Specification and the Java Virtual Machine Specification and provides the Standard Edition of the Java Application Programming Interface. You can also have you first Cloud on Utho Cloud.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges.\nInternet enabled on server.","steps-to-install-java-on-opensuse#Steps to install Java on OpenSUSE.":"Step 1: First refresh the Zypper repolist on your server.\nzypper refresh Search for packet OpenJDK\nStep 2: Search for the available OpenJDK version available on your server.\nzypper search openjdk-devel Step 3: Now, Install the desired version of java on your server. Here, in this example, we have installed OpenJDK 8.\nzypper --non-interactive install java-1_8_0-openjdk-devel Installing OpenJDK on OpenSUSE\nThanks You !!!"},"title":"How to install JAVA JDK on OpenSUSE"},"/utho-docs/docs/how-to-install-joomla-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Joomla in Plesk.\nAlso read: How to install WordPress in Plesk\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Joomla in Plesk.\nJoomla, often known as Joomla! and frequently abbreviated as J!, is a free and open-source content management system. Joomla is a platform for publishing online content on websites. Web content applications are made up of a broad range of web-based programmers. Online user communities, online photo galleries, online discussion forums, and online commerce are some instances of these programmers.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to the applications¬†option, which is on the left side of the screen.\nStep 3. Click on the arrow to the right of install and you will be given two options for installing Joomla: one for custom installation and the other for Joomla versions.¬†Step 4. Pick the alternative that best suits your needs, and then start the installation.\nStep 5. You have successfully installed Joomla."},"title":"How to install Joomla in Plesk"},"/utho-docs/docs/how-to-install-mariadb-server-on-opensuse/":{"data":{"":" How to install Mariadb server on OpenSUSE\nIn this article, we will learn how to install mariadb server on OpenSUSE. MariaDB is a database management system that is a fork of MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.","prerequisites#Prerequisites":" Internet enabled OpenSUSE server. If you want a server with best speed with the most affordable services and price, just visit here.\nAny normal user with SUDO privileges or Super user","steps-to-install-mariadb-on-your-opensuse-server#Steps to install Mariadb on your OpenSUSE server":"Step 1: Refresh your zypper repolist and the install the mariadb server\nzypper refresh \u0026\u0026 zypper install mariadb-server -y Install MariaDb on your OpenSUSE server\nWhile, installing, you will see a message to view the notification from the MariaDB package. Press y to view it.\nView the message from Mariadb package\nStep 2: Press q to exit from the message\nMessage from Mariadb\nStep 3: Now, start and enabled the service to start using and configuring the mariadb server\nsystemctl enable --now mariadb Step 4: Start Configuring the mariadb server on your opensuse server.\nmysql_secure_installation And, now read the message carefully and choose the steps accordingly. You can take the reference, of the below screenshot.\nStart setup the of Mariadb server\nStep 5: Test your setup by login to mariadb using below command and by using the password you have setup in the above step.\nmysql -u root -p Log in to mariadb server\nNow, if you want to create a user or grant privileges to any user, use this guide. And this is how you have learnt how to install Mariadb server on OpenSUSE."},"title":"How to install Mariadb server on OpenSUSE"},"/utho-docs/docs/how-to-install-maven-on-opensuse/":{"data":{"":" How to install Maven on Opensuse\nIn this article, you will learn how to install Maven on Debian 11 and Debian 10.\nThe Maven lesson explains both the basics and more advanced ideas of the Apache Maven technology. Our maven lesson is made for both newbies and experts. Maven is a strong tool for managing projects. It is built on POM, which stands for ‚Äúproject object model.‚Äù It is used to make, keep track of, and record projects. Like ANT, it makes the building process easier. But it‚Äôs too far ahead of ANT.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges.\nAn updated APT repositories to install java and maven.\nJava installed on server. If you have did not installed Java on your server, you can follow this guide- How to install java.","steps-to-install-maven-on-opensuse#Steps to install Maven on OpenSUSE":"Step 1: Add the required repo that is important to install the latest version of maven.\nzypper addrepo https://download.opensuse.org/repositories/home:alvistack/openSUSE_Tumbleweed/home:alvistack.repo Add the repositories on server\nStep 2: Refresh the zypper repositories to update the changes.\nzypper refresh Trust the signing key\nStep 3: Install the Maven on your server using the below command.\nzypper install maven -y Install the maven on opensuse\nAnd this is how you have learnt how to install Maven on OpenSUSE."},"title":"How to install Maven on OpenSUSE"},"/utho-docs/docs/how-to-install-nginx-on-opensuse/":{"data":{"":" How to install NGINX on OpenSUSE\nIn this tutorial, you will learn how to install Nginx on OpenSUSE. Free, open-source, high-performance HTTP and reverse proxy server Nginx‚Äîpronounced ‚Äúengine X‚Äù‚Äîis in charge of managing the traffic on some of the busiest websites on the Internet. Both as a standalone web server and as a reverse proxy for Apache and other web servers, Nginx has several uses.\nNginx has a reduced memory footprint per connection and can manage many more concurrent connections than Apache.","prerequisites#Prerequisites":" A super user or any normal user with SUDO privileges.\nInternet enabled OpenSUSE server","steps-to-install-nginx-on-opensuse#Steps to install Nginx on OpenSUSE":"Step 1: Update your server to install the latest patches.\nzypper update -y Step 2: Install the Nginx on your server.\nzypper install nginx -y Installing Nginx on Opensuse\nStep 3: Start and enable the server to get start using the Nginx server\nsystemctl enable --now nginx Enabling the server\nStep 4: Allow the http and https port on your OS firewall\nfirewall-cmd --permanent --add-service=http firewall-cmd --permanent --add-service=https firewall-cmd --reload Add http port in firewalld\nStep 5: Now, create a test page to test your installation of nginx webserver.\necho \"\u003ch1\u003e Greetings from Microhost Cloud \u003c/h1\u003e \u003e\u003e /srv/www/htdocs/index.html Step 6: Open your browser and access your index.html page.\nhttp://\u003cserver-ip\u003e Successfully installed apache2 on OpenSUSE\nAnd this is how, you have learnt how to install Nginx on OpenSUSE."},"title":"How to install Nginx on OpenSUSE"},"/utho-docs/docs/how-to-install-nginx-web-server-on-ubuntu-22-04-lts/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install NGINX Web Server on Ubuntu 22.04 LTS.\nAlso Read: How to host a domain on centos 7\nThank You üôÇ","install-nginx#Install NGINX":"1. Install NGINX with the use of the package manager.\n# apt install nginx -y 2. The NGINX service immediately begins its normal operation. You can use the following command to check the current state of it:\n# systemctl status nginx 3. You can start using NGINX another time with the following command:\n# systemctl enable nginx 4. Your system‚Äôs firewall must be configured to allow traffic over port 80. UFW, which stands for Ubuntu Firewall, is the frontend that is most commonly used to administer firewall rules on Ubuntu. You can open port 80 with ufw by using the example commands, and then reload the rules to make the changes you made take effect.\n# ufw allow http # ufw reload 5. To check out how well your installation is working, go to the default NGINX website. You‚Äôll locate it by going to the server‚Äôs IP address in your browser.\n# http://server\\_ip Use NGINX Through the use of NGINX, this section will guide you through the process of putting up your own website. This explains not only how to set up an NGINX proxy to deliver static content but also how to do so.","introduction#Introduction":"In this article, you will learn how to install NGINX Web Server on Ubuntu 22.04 LTS.\nNGINX is free software that can be used for serving web pages, reverse proxying, caching, balancing the load, streaming media, and more. It began out as a web server meant to be as fast and stable as possible. In addition to being an HTTP server, NGINX may also act as a proxy server for email (IMAP, POP3, and SMTP) and as a reverse proxy and load balancer for HTTP, TCP, and UDP servers.","nginx-configuration#NGINX Configuration":"1. Disable the NGINX configuration file that is provided by default.\n# unlink /etc/nginx/sites-enabled/default 2. Generate an NGINX configuration file for the website you‚Äôre working on. In this example, replace both the filename and the contents of the file with your site‚Äôs domain. Do the same thing from now on anytime you see example.com.\n# vi /etc/nginx/sites-available/example.com Paste the following content in this file:\nserver { listen 80; listen [::]:80; server_name example.com; root /var/www/example.com; index index.html; location / { try_files $uri $uri/ =404; } } 3. Bring your NGINX site live.\n# ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ 4. Run the following command to check if the NGINX configuration file is correct or not:\n# nginx -t 5. In order for changes to take effect, you will need to restart NGINX.\n# systemctl restart nginx ","set-up-the-website#Set Up the Website":"1. Make a directory to store the content of your NGINX website.\n# mkdir /var/www/example.com 2. In the new NGINX site directory, you must create an index.html page.\n# vi /var/www/example.com/index.html Paste the following content in this file:\n\u003c!doctype html\u003e \u003chtml\u003e \u003cbody\u003e \u003ch1\u003eHello, Guys!\u003c/h1\u003e \u003cp\u003eThis is an example website running on NGINX.\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 3. Just type your site‚Äôs domain name into a browser to check it out. There should be a ‚ÄúHello, Guys!‚Äù page shown on your domain."},"title":"How to Install NGINX Web Server on Ubuntu 22.04 LTS"},"/utho-docs/docs/how-to-install-snap-on-ubuntu-server/":{"data":{"":" How to install Snap on ubuntu server\nIn this tutorial, we will learn how to install Snap on Ubuntu server. Snaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nInternet enabled server with updated security patches. You can have you fully featured Ubuntu server on Utho Cloud.","steps-to-install-snap-on-ubuntu#Steps to install Snap on Ubuntu":"Step 1: Update the APT repository to install the latest packages.\napt update Step 2: Install the snap package using the below command.\napt install snapd Step 3: Now, verify the installation of your snap package.\nsnap version Snap version\nAnd, this is how you have learnt how to install snap on Ubuntu server"},"title":"How to install Snap on Ubuntu server"},"/utho-docs/docs/how-to-install-snapd-on-centos-server/":{"data":{"":"\nSnaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nyum repositories configured with CentOS server.","steps-to-install-snap-on-centos-server#Steps to install Snap on CentOS server":"Step 1: Before installing the Snap on your server, you need to install the extra packages for enterprises linux( EPEL) repsitories\nyum install epel-release Installing EPEL repo\nStep 2: Install the Snap by executing the below command\nyum install snapd Install the Snap on Centos\nStep 3: Start and enable the snapd socket to start working with snapd\nsystemctl enable --now snapd.socket Output- Created symlink from /etc/systemd/system/sockets.target.wants/snapd.socket to /usr/lib/systemd/system/snapd.socket. Step 4: Now, you must enter the following to establish a symbolic link between /var/lib/snapd/snap and /snap in order to enable support for traditional snaps: ln -s /var/lib/snapd/snap /snap Step 5: Now, either set the PATH varialbe using the below command or restart another terminal\necho export PATH=$PATH:/snap/bin \u003e\u003e ~/.bashrc export .bashrc Step 6: Check the snapd version.\nsnap version Version of Snap\nAnd this is how you will install the Snap on CentOS server"},"title":"How to install Snapd on CentOS server"},"/utho-docs/docs/how-to-install-tree-command-on-ubuntu-20-04/":{"data":{"":"Description\nIn this article, we will acquire new knowledge how to Install tree command on Ubuntu 20.04\nTree is a free and open-source command-line programme for recursively listing directory contents in a depth-indented format. It enables us to view all files and directories recursively without entering the specified path.\nThis makes it sometimes very convenient for developers and programmers to view all project files and their paths using a simple command-line terminal utility instead of an integrated development environment (IDE). Clearly, this eliminates the need to install an IDE on the system. The tree utility is also very simple to install on nearly all Linux and Unix systems. Here are the instructions for installing the tree utility on Ubuntu 20.04 LTS-based machines. Explore information about the tree command.\nFollow the below steps to How to Install tree command on Ubuntu 20.04","step-1-update-server#Step 1: Update Server":"first running sudo apt update, and then upgrading packages to the newest version by running sudo apt upgrade, as demonstrated below.\napt update \u0026\u0026 sudo apt upgrade ","step-2-install-command#Step 2: Install command":"Depending on your needs and tool availability, you can install Tree Command on your system.\nInstall the tree package from the default Ubuntu repository, and then execute sudo apt-get install tree as demonstrated below.\napt install tree ","step-3-verify-installation#Step 3: Verify Installation":"If you loaded the tree utility from the default Ubuntu repository, you can use the dpkg -L tree command, as shown below, to check the installed file path.\ndpkg -L tree ","step-4-check-version#Step 4: Check Version":"Using the tree ‚Äìversion command, as shown below, you can determine the current installed version.\ntree --version ","step-5-use-of-tree-command#Step 5: Use of tree command":"We can test it by using the tree microhost command to show a list of all the files and folders in the microhost directory, as shown below. The results that are put underlined can be seen in the output. Tree will show all the files in the current working directory if you don‚Äôt give it any other information. You can use the tree ‚Äìhelp command to see all of the choices that the tree utility has.","step-6-remove-tree-command#Step 6: remove tree command":"Depending on how you installed the tree command, you can also use one of the ways below to remove it from your system when you‚Äôre done using it.\napt remove tree I hope you carefully followed all of the steps to how to Install tree command on Ubuntu 20.04 and please read the article below as well.\nMust Read :- https://utho.com/docs/tutorial/how-to-install-gawk-on-ubuntu-20-04/"},"title":"How to Install tree command on Ubuntu 20.04"},"/utho-docs/docs/how-to-install-wds-windows-deployment-services-on-windows-server/":{"data":{"":"","#":"\nINTRODUCTION Windows Deployment Services¬†(WDS) is a deprecated component of the¬†Windows¬†operating system that enables centralized, network-based deploy of operating systems to bare-metal computers. It is the successor to¬†Remote Installation Services¬†(RIS).¬†WDS officially supports remote deployment of¬†Windows Vista¬†and later, as well as¬†Windows Server 2008¬†and later. However, because WDS uses¬†disk imaging, in particular the¬†Windows Imaging Format¬†(WIM), it could deploy virtually any operating system. This is in contrast with its predecessor, RIS, which was a method of automating the installation process.\nPrerequisites Windows Deployment Services on Windows Windows Server\nInternet connectivity\nRun Server Manager and Click Add roles and features.\nClick Next button.\nSelect Role-based or feature-based installation.\nSelect a Host which you‚Äôd like to add services.\nCheck a box [Windows Deployment Services].\nAdditional features are required to add WDS. Click [Add Features] button and then Click [Next] button.\nClick Next button.\nClick Next button.\nCheck boxes you‚Äôd like to install role services.\nClick Install button.\nAfter finishing Installation, click [Close] button.\nThank You!"},"title":"How to install WDS (Windows Deployment Services) on Windows Server"},"/utho-docs/docs/how-to-install-wds-windows-deployment-services-via-powershell/":{"data":{"":"","#":"\nINTRODUCTION Windows Deployment Services¬†(WDS) is a deprecated component of the¬†Windows¬†operating system that enables centralized, network-based deploy of operating systems to bare-metal computers. It is the successor to¬†Remote Installation Services¬†(RIS).¬†WDS officially supports remote deployment of¬†Windows Vista¬†and later, as well as¬†Windows Server 2008¬†and later. However, because WDS uses¬†disk imaging, in particular the¬†Windows Imaging Format¬†(WIM), it could deploy virtually any operating system. This is in contrast with its predecessor, RIS, which was a method of automating the installation process. We can also install WDS via Server Manager. But in this tutorial, we will learn how to install WDS (Windows Deployment Services) on Windows Server 2012, 2016 and 2019.\nPrerequisites Windows Deployment Services on Windows Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to install WDS\nStep 4. Installation begins\nStep 5. Installation completed. As the command suggests a system restart s not needed.\nThank You!"},"title":"How to install WDS (Windows Deployment Services) via PowerShell"},"/utho-docs/docs/how-to-install-wine-on-centos-7/":{"data":{"":" How to Install Wine on CentOS 7\nDescription\nIn this article we will learn how to install wine on centos 7 I will walk you through the process of installing wine on CentOS 7, starting with the very first step. Wine is a compatibility layer that was developed for multiple POSIX-based operating systems, such as Linux, Macintosh, and BSD, to enable these systems to run Windows-based software. Wine is available for free and is open source. Wine is a programme that, in its most basic form, automatically translates Windows API calls into POSIX calls. This eliminates the speed and memory penalties that are associated with using other methods and enables you to integrate Windows applications onto your desktop in an uncluttered manner. It is simple to understand and not too difficult to put into practise. Also, the installation process is relatively straightforward in virtually all of the well-known Linux variants. In this section, we will go over the steps necessary to install Wine on computers running CentOS 7.\nFollow the below steps How to Install Wine on CentOS 7‚Ä¶","step-1-update-server#Step 1: Update Server":" yum update ","step-2-install-epel-release-repository#Step 2: Install EPEL Release Repository":"You will only be able to obtain the wine package through the EPEL repository. In order to install and activate this repository, you will need to use the yum install epel-release command, as demonstrated further below.\nyum install epel-release ","step-3-install-wine-package#Step 3: Install Wine package":"With the yum install wine command, which will be demonstrated further down, you will be able to install the wine package from the EPEL repository. The package, as well as all of its dependencies, will be downloaded and installed as a result of this action.\nyum install wine ","step-4-verify-installation-of-package#Step 4: Verify Installation of package":"After the installation has been completed successfully, you will be able to validate all of the wine-related packages that have been installed by querying the rpm database using the rpm -qa | grep -i wine command, as will be demonstrated below.\nrpm -qa | grep -i wine ","step-5-check-version-of-package#Step 5: Check Version of package":"Using the wine ‚Äîversion command, as seen below, is another option for determining the currently installed version of wine.\nwine --version I really hope that you‚Äôve got all of those steps down for how to install wine on CentOS 7.\nMust Read :- https://utho.com/docs/tutorial/add-user-and-give-limited-permission-to-the-host-in-zabbix/"},"title":"How to Install Wine on CentOS 7"},"/utho-docs/docs/how-to-install-wine-on-debian-10/":{"data":{"":" How to install Wine on Debian 11","description#Description":"In this article we will learn How to Install Wine on¬†Debian¬†10 Users have frequently had the requirement to execute Windows applications on POSIX-based operating systems such as Linux, MacOS, and BSD. In order to fulfil this requirement, an interoperability layer was developed, which enables Windows programmes to execute on various flavours of Linux. This compatibility layer is referred to as Wine in the industry. A significant number of people from all around the world use it. Wine is a POSIX-compatible software package that is both free and open-source, and it can be rapidly installed on any computer running a POSIX-based operating system. The steps necessary to install wine on a computer are as follows: running¬†Debian 10.\nPlease proceed by following the steps below. How to Install Wine on Debian.","step-1-update-server#Step 1: Update Server":" sudo apt update Execute the sudo apt upgrade command in order to check if any of the tools that have been installed require an update.","step-2-install-wine#Step 2: Install Wine":"You can choose to install wine using any of the following methods, depending on the architecture of your computer.\nsudo apt install wine You have the option of installing wine on a 64-bit architecture from the standard Debian repository by using the command sudo apt install wine64, which is displayed down below..\nsudo apt install wine64 ","step-3-verify-version#Step 3: Verify Version":"Use the wine ‚Äìversion command, as demonstrated further down, to determine which version of Wine is installed on a 32-bit operating system.\nwine --version ","step-4-uninstall-wine#Step 4: Uninstall Wine":"When you are finished utilising Wine, you can remove it from your system using any of the following methods; which method you use will depend on the architecture of your machine and how you installed Wine in the first place.\nsudo apt remove wine I really hope that you have a good grasp on everything that has been discussed in this essay that how to Install Wine on Debian 10"},"title":"How to install Wine on Debian 10"},"/utho-docs/docs/how-to-install-wordpress-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install WordPress in Plesk.\nAlso read: How to install Joomla in Plesk\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install WordPress in Plesk.\nWordPress is a content management system that is both open-source and free to use. It is developed in the hypertext preprocessor language and can be combined with either a MySQL or MariaDB database. It also supports HTTPS. A plugin architecture and a template system, which are together referred to as ‚ÄúThemes‚Äù inside the WordPress platform, are among the features.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to the applications option, which is on the left side of the screen.\nStep 3. Click on the arrow to the right of install and you will be given two options for installing WordPress: one for custom installation and the other for wordpress versions.¬†Step 4. Pick the alternative that best suits your needs, and then start the installation.\nStep 5. You have successfully installed WordPress."},"title":"How to install WordPress in Plesk"},"/utho-docs/docs/how-to-install-zabbix-agent-on-debian-10/":{"data":{"":" How to install Zabbix agent on Debia\nIntroduction In this article, you will learn how to install Zabbix agent on Debian 10. When a native Zabbix agent is built in the C programming language, it has the capability of running on a variety of supported platforms, such as Linux, UNIX, and Windows, and collecting data from a device about its CPU consumption, memory usage, disc usage, and network interface usage.This is the Zabbix agent, and it collects all data by using the agent‚Äôs configuration file. So let‚Äôs get started with this instruction for step by step, shall we?","prerequisites#Prerequisites":" apt repolist configured to install the new packages.\nSuper user or any normal user with SUDO privileges to install packages.","steps-to-install-zabbix-agent-on-debian-server#Steps to install Zabbix agent on Debian server":"Step 1: First and foremost you need to update you Debian server to install the secure patches. In my case, it happened that before installing zabbix-agent we were facing certificate errors in installing the zabbix agent. So after fully updaing the server, we were able to install the zabbix agent successfully. If you are looking for install Zabbix server on CentOS, you can check here\napt-get update apt-get upgrade Step 2: Now, download the package which will install the Zabbix agent 6 on your server.\nwget https://repo.zabbix.com/zabbix/6.4/debian/pool/main/z/zabbix-release/zabbix-release_6.4-1+debian10_all.deb --no-check-certificate Download the Zabbix agent repository\nStep 3: Install the Zabbix repolist.\ndpkg -i zabbix-release_6.4-1+debian10_all.deb Step 4: Update your apt repolist\napt-get update Step 5: install the Zabbix-agent and related plugins.\napt-get install zabbix-agent2 zabbix-agent2-plugin-* -y Install the Zabbix agent on Debian\nStep 6: Now, start and enable the zabbix-server to start monitoring your server.\nsystemctl enable --now zabbix-agent you have learnt how to install Zabbix agent on Debian server which is tested in Debian 10."},"title":"How to install Zabbix agent on Debian 10"},"/utho-docs/docs/how-to-install-zabbix-agent-on-ubuntu/":{"data":{"":" How to install Zabbix-agent on Ubuntu\nIntroduction In this article, you will learn how to install Zabbix agent on Ubuntu server. When a native Zabbix agent is built in the C programming language, it has the capability of running on a variety of supported platforms, such as Linux, UNIX, and Windows, and collecting data from a device about its CPU consumption, memory usage, disc usage, and network interface usage.This is the Zabbix agent, and it collects all data by using the agent‚Äôs configuration file. So let‚Äôs get started with this instruction for step by step, shall we?","prerequisites#Prerequisites":" apt repolist configured to install the new packages\nSuper user or any normal user with SUDO privileges to install packages.","steps-to-install-zabbix-agent#Steps to install Zabbix agent":"Step 1: First and foremost you need to update you Ubuntu server to install the secure patches. In my case, it happened that before installing zabbix-agent we were facing certificate errors in installing the zabbix agent. So after fully updaing the server, we were able to install the zabbix agent successfully. If you are looking for installing Zabbix agent on Debian, click here.\napt-get update apt-get upgrade Step 2: Now, download the package which will install the Zabbix agent 6 on your server.\n# wget https://repo.zabbix.com/zabbix/6.4/ubuntu/pool/main/z/zabbix-release/zabbix-release_6.4-1+ubuntu22.04_all.deb Download zabbix agent repolist on ubuntu\nStep 3: Install the Zabbix repolist.\n# dpkg -i zabbix-release_6.4-1+ubuntu22.04_all.deb Install Zabbix repolist\nStep 4: Update your apt repolist\napt update Install Zabbix agent on Ubuntu\nStep 5: install the Zabbix-agent and related plugins.\napt install zabbix-agent Step 6: Now, start and enable the zabbix-server to start monitoring your server.\nsystemctl enable --now zabbix-agent you have learnt how to install Zabbix agent on Ubuntu 22.04 server which tested and verified."},"title":"How to install Zabbix agent on Ubuntu"},"/utho-docs/docs/how-to-manage-user-roles-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to manage user roles in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to manage user roles in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Choose user from the menu on the left side of the screen.¬†Step 3. Click on user roles.\nStep 4. Select any role name for whom you want to manage their roles.\nStep 5. Then select the user‚Äôs permissions based on your preferences and click on apply.\nStep 6. And then you will get a success message."},"title":"How to manage user roles in Plesk"},"/utho-docs/docs/how-to-migrate-accounts-from-cwp-to-cwp/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to migrate accounts from CWP to CWP.\nThank You üôÇ","destination-server#\u003cstrong\u003eDESTINATION SERVER\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to migrate accounts from CWP to CWP.\nwhat is cwp? Control Web Panel ‚Äì an AI powered Free Web Hosting control panel designed for quick and easy management of (Dedicated \u0026 VPS) as well as offering an intuitive and modern interface for users, as a web hosting panel.\nSOURCE SERVER Go to ‚ÄúCWP Settings‚Äù \u003e ‚ÄúAPI Manager‚Äù in the left menu. Generate and save the new API key by using the green button ‚ÄúAllow New Api Access‚Äù IP origin: IP destination\nClick on Generate code\nFormat request: JSON\nEnable function for: CWP to CWP Migration, then click on Create\nGo to Dashboard \u003e Firewall and whitelist the Source IP and Destination IP Under Whitelist Configuration, Click on Add an entry: Source IP and DestinationIP\nSearch SSH Key Generator in the left search box, click on Generate an SSH key, from the Server ‚ÄúSettings‚Äù module -\u003e ‚ÄúSSH Key Generator‚Äù Generate new key and Add a Public key to authorized\nDESTINATION SERVER Login to CWP of destination server 2. Go in firewall then whitelisted the Source IP and Destination IP\n3. Generate an SSH key, from the Server ‚ÄúSettings‚Äù module -\u003e ‚ÄúSSH Key Generator‚Äù\n4. Generate new key and Add a Public key to authorized\n5. Login destination server via putty, then\nssh-copy-id root@x.x.x.x \u003e source IP\nPassword:yxyxyxyx\n6. User accounts\u003e CWP-\u003eCWP migration\nServer IP: source IP\n7. Api Key cPanel: Paste key which is generated in source server\nMaximum simultaneous transfers: according to you select maximum number of accounts transfer\nTest and save\n8. Click on the authentication method,and select what you want to migrate\n9. And start migration\nNOTE: you can check the migration log with the following command\n# *tail -f /var/log/cwp/account_transfer.log*- ","introduction#Introduction":"","source-server#\u003cstrong\u003eSOURCE SERVER\u003c/strong\u003e":"","what-is-cwp#\u003cstrong\u003ewhat is cwp?\u003c/strong\u003e":""},"title":"How to migrate accounts from CWP to CWP"},"/utho-docs/docs/how-to-modify-database-user-privileges-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to modify Database user privileges in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to modify Database user privileges in Plesk.\nStep 1. Enter your server password to get into your Plesk account, which can be found by searching your browser for server ip:8880.\nStep 2. To manage users, go to the Databases menu on the left side of the screen, and choose User Management.\nStep 3. Click on the name of the database user to change privileges.\nStep 4. You will see the privileges section divided into data access and structure access.\nSelect the privilege you want to grant to the user, scroll the screen, and click on the OK button to save changes."},"title":"How to modify Database user privileges in Plesk"},"/utho-docs/docs/how-to-remove-components-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to remove components in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to remove components in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Click on Tools and Settings, which is on the left side of the screen.\nStep 3. Under the PLESK menu, select Updates.¬†Step 4. click on add/remove components.\nStep 5. Select any component¬†that you want to uninstall, then click on continue.\nStep 6. Now that your selected component is uninstalled, click on OK."},"title":"How to remove components in Plesk"},"/utho-docs/docs/how-to-run-application-in-background/":{"data":{"":" How to run application in background\nIn this tutorial, we will learn how to run any application in background using Screen package on linux.\nWe occasionally encountered a scenario where we ran a protracted process to remove the computer and our connection abruptly dropped. When this happens, the SSH session has ended, and our work is lost.\nBut at some point, it has happened to each of us. However, there is a command called ‚Äúscreen‚Äù that enables us to resume the sessions.","steps-to-run-application-in-background#Steps to run application in background":"Multiple shell sessions can be started and used simultaneously over a single ssh session under Linux thanks to the screen command. Throughout the session, the process could get separated. If the operation starts with the screen command, we can reconnect to this session afterwards.\nIf the session is disconnected, the screen initially controls and oversees the process that was started from it. Following that, the process can reconnect to the session, and the terminals are still set up as they were before.\nStep 1: Install the Screen command in your server. You can use Yum in CentOS or APT in Debian or Ubuntu servers.\nyum install screens -y ## For Fedora or CentOS flavored linux apt-get update \u0026\u0026 apt-get install screens ## For Debian or Ubuntu flavored linux Step 2: Now, open a new screen using the screen command.\nscreen Step 3: After successfully executing the above command you will be prompt with a fresh windows. Now, run the below command to run any application in background. In the below example, run the a NodeJs application in background.\nnpm start app.js \u0026 Step 4: And now, you can either close the windows directly or close the screen session and continue your other work on your machine. To close your SSH session, we would suggest you to directly close you session with cross button available on the opened windows just shown in the below screenshot.\nAnd, this is how you have learnt how to run application in background. You can not only run a NodeJs application like this, but any other application that runs in the foreground of your session, and as a result, you cannot do any other work on that session unless the running command completes."},"title":"How to run application in background"},"/utho-docs/docs/how-to-set-folders-to-dfs-namespace-via-powershell/":{"data":{"":"INTRODUCTION\nDFS is a set of client and server services that allow an organization using Microsoft Windows servers to organize many scattered SMB file shares into a distributed file system. DFS has two components to its service: Location transparency and Redundancy. Basically a distributed file system (DFS) is¬†a file system that spans across multiple file servers or multiple locations, such as file servers that are situated in different physical places. Files are accessible just as if they were stored locally, from any device and from anywhere on the network. In this tutorial, we will learn how to set folders to DFS NameSpaces via PowerShell.\nPrerequisites\nWindows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to set DFS folder path\nNew-DfsnFolder -Path \"\\\\NEWFOLDER\\Share\\Folder01\" -TargetPath \"\\\\NEWFOLDER\\Share01\" Step 4. Run the following command to set 2nd folder to DFS\nNew-DfsnFolder -Path \"\\\\NEWFOLDER\\Share\\Folder02\" -TargetPath \"\\\\NEWFOLDER\\Share02\" Step 5. Run the following command to check the changes set above set folders to DFS\nGet-DfsnFolder -Path '\\\\NEWFOLDER\\Share\\*' Thank You."},"title":"How to set folders to DFS NameSpace via PowerShell"},"/utho-docs/docs/how-to-set-static-ip-address-in-windows-server-via-powershell/":{"data":{"":"","introduction-set-static-ip-address#INTRODUCTION Set Static IP Address":"A static IP address is a 32¬†bit¬†number assigned to a computer as an address on the internet. This number is in the form of a¬†dotted quad¬†and is typically provided by an internet service provider (ISP). In this tutorial, we will learn how to Set Static IP Address in Windows Server via PowerShell.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Login to your Windows Server\nStep 2. Open PowerShell as an Administrator\nStep 3. Run the following command to get Network Interface\nStep 4. Run the following command to set DHCP off\nStep 5. Run the following command to set DNS\nPS: 10.0.0.10 is just a test IP.\nStep 6. Run the following command to confirm settings\nThank You."},"title":"How to Set Static IP Address in Windows Server via PowerShell"},"/utho-docs/docs/how-to-set-up-the-htaccess-file-in-apache/":{"data":{"":"","before-you-begin#Before You Begin":" This guide uses sudo as much as possible. Finish the Securing Your Server parts to build a regular user account and hard SSH access, and remove redundant network services. Update your system: ``` sudo apt-get update \u0026\u0026 sudo apt-get upgrade 3. [Install a Lamp](https://utho.com/docs/tutorial/installation-of-lamp-stack-on-ubuntu-16/) Stack to install Apache on your Microhost cloud server, complete the Apache portion. ## What is .htaccess .htaccess is a web server configuration file for Apache. This is a very useful tool that can be used to alter the Apache configuration without editing Apache configuration files. The next parts explain how this configuration is generated and used to restrict directory lists and IP addresses and manage redirects. ## Enable .htaccess .htaccess is not eligible by default. You must edit the configuration file to enable it. 1\\. To open your settings file, use a text editor: sudo nano /etc/apache2/sites-available/abc.com.conf\n2\\. After the VirtualHost block () add: ```file {title=\"/etc/apache2/sites-available/abc.com.conf\" lang=\"aconf\"} ‚Ä¶. Options Indexes FollowSymLinks AllowOverride All Require all granted 3. Save the file, then restart apache:\nsudo service apache2 restart ","block-ips#Block IPs":"1. Create or update the .htaccess file stored in the Apache host directory:\ncd /var/www/html/abc.com/public_html/ sudo nano .htaccess 2. Delete line options (if applicable) from the preceding section and add following lines to block the destination IP addresses:\norder allow,deny This will deny the IP 192.0.2.deny from 192.0.2.This will deny all IP's from 192.0.2.through 192.0.2.deny from 192.0.2 ","create-htaccess#Create .htaccess":"1. By default, CMS systems such as WordPress create .htaccess configurations. This guide assumes that there is no file named .htaccess, so you have to manually build one. Browse to the root directory of your site:\ncd /var/www/html/abc.com/public_html 2. Create an .htaccess file:\nOptions -Indexes 3. Now, if you navigate to your site, you‚Äôll see a forbidden Message. You will need to specify the file or directory that you would like to see.","handle-redirects#Handle Redirects":"You can redirect traffic by configuring .htaccess. For the following examples, you should update the .htaccess file for your website‚Äôs root directory so you can redirect your visitor to http:/abc.com/test1/index.html if they are trying to see http:/abc.com/main.html.\n1. Create an HTML test file to redirect a visitor to http:/abc.com/test1/index.html:\nmkdir test1 sudo touch test1/index.html 2. Add some basic content to the HTML test file:\nThis is the html file in test1. 3. Open the .htaccess file in the root directory of your project. Remove all of the existing configurations in this file and add the following line:\nRedirect /main.html /test1/index.html The first parameter in the ‚ÄòRedirect‚Äô command is the HTTP status code. Specifying the status code is helpful in letting the browser know that the page has been moved to a new location. If you leave this parameter blank, it will default to a 302 code indicating that the redirect is temporary. Specifying 301 makes it clear that the page at the requested location has been permanently moved to a new location.\nThe next parameter is the Unix path to the file requested in the URL. This parameter requires a Unix path, not a URL. The path should be the location of the.htaccess file where the redirect configuration is set. The final parameter indicates where you want the visitor to be redirected to. In this case, traffic is redirected to / test1 / index.html; the Unix path or HTTP URL for this second parameter is acceptable.\n1. In the browser, navigate to abc.com/main.html. You should see the url redirect to abc.com/test1/index.html in the address bar, and the html test file should be displayed.","introduction#Introduction":"The purpose of this guide is to show you how to configure Apache htaccess (.htaccess) configuration. The guide covers subjects related to website file system permissions, redirects and limitations of IP address.","restrict-directory-listings#Restrict Directory Listings":"Visitors can view the directory and file structure by default, and gain access to web server files. It‚Äôs best practice for a visitor to abc.com to know files on the server to view such files where directory access is limited. One way to restrict this is through .htaccess.","restrict-ips#Restrict IPs":"This section will guide you by restricting the access of specific IPs to your site. This is useful if you want to block some visitors from visiting your site. You may also set up this to prevent certain IPs from accessing certain sections of your site.","set-the-404-error-page#Set the 404 Error Page":"When a visitor attempts to access a page or resource that does not exist (for example, by clicking a broken link or typing an incorrect URL), the server will respond with a 404 error code. It is important that users receive feedback to explain the error. In the event of a 404 error, Apache will display an error page by default. However, most sites provide a custom error page. You can use the.htaccess settings to let Apache know what error page you want to display whenever a user attempts to access a non-existent page.\n1. This will redirect all requests for non-existent documents to the page in the root directory of the project called 404.html. Open the.htaccess file, and add the following line:\nErrorDocument /404.html 2. Create the¬†404.html¬†file:\nError: Page not found 3. Navigate to a non-existent page in your browser, such as www.abc.com/doesnotexist.html. The 404 message must be displayed.\nThankyou.."},"title":"How to Set Up the .htaccess File in Apache"},"/utho-docs/docs/how-to-setup-rsyslog-server-on-debian-11/":{"data":{"":" How to configure rsyslog server on debian 11","introduction#Introduction:":"In this tutorial, you will learn how to setup Rsyslog server on Debian 11. Logs are an important part of the core of any network. They keep a lot of diagnostic information, like how the kernel, applications, daemons, services, network behaviour, user actions, and so on, work.\nThey make sure that server events are clear so that problems with the Linux system can be fixed. The best way to handle and look at log data is to put all of the logs in one place. Centralising logs protects against accidental data loss and makes sure that they can be accessed even if the server is down.\nRsyslog is the most famous tool for putting all of a Linux system‚Äôs logs in one place. In this tutorial, we‚Äôll learn how to centralise the Debian 11 Rsyslog logging system.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges.\napt repository configured. If you want a super speedy server with most cost-effective price, visit this.","setup-rsyslog-client#Setup Rsyslog client":"Step 7: Once you have finished setting up the rsyslog server, go to your rsyslog client computers and set them up such that they submit logs to a distant rsyslog server.\nvi /etc/rsyslog.conf Step 8: Allow preservation of FQDN in your configuration file:\n$PreserveFQDN on Step 9: Add remote rsyslog server information at the end:\n*.* @@rsysog-server-ip:514 ## for using IP address instead of FQDN#OR*.* rsysog-server-fqdn:514 ## for using FQDN Step 9: Do more required changes in the client configuration file.\n$ActionQueueFileName queue$ActionQueueMaxDiskSpace 1g$ActionQueueSaveOnShutdown on$ActionQueueType LinkedList$ActionResumeRetryCount -1 Client rsyslog configuration\nStep 10: Restart the rsyslog services on client side.\nsystemctl restart rsyslog And this is what you have learned about how to setup rsyslog server on Debian 11","setup-rsyslog-server-on-debian-11#Setup Rsyslog (server) on Debian 11":"Step 1: Most versions of Linux come with the rsyslog package and all of its dependencies already loaded. Run the following command to make sure it was installed correctly:\nrpm -qa rsyslog Step 2: If it is not installed, you can install it using the below command.\napt-get update \u0026\u0026 apt-get install rsyslog -y Step 3: Now set up the rsyslog service to run in server mode:\nvi /etc/rsyslog.conf Step 4: Uncomment the lines for linking the udp and tcp ports:\nmodule(load=\"imudp\")input(type=\"imudp\" port=\"514\")# provides TCP syslog receptionmodule(load=\"imtcp\")input(type=\"imtcp\" port=\"514\") Step 5: Let‚Äôs make a template that tells rsyslog server how to store syslog messages as they come in. Add the template just before the end of the file.\n$template remote-incoming-logs,\"/var/log/%HOSTNAME%/%PROGRAMNAME%.log\" *.* ?remote-incoming-logs\u0026 ~ After entering these detials, just same and exit it.\nStep 6: Restart your rsyslog service to reflect the changes.\nsystemctl restart rsyslog ","what-is-rsyslog#What is Rsyslog:":"Rsyslog is a powerful, lightweight, open-source log processing daemon that can read messages from many different systems and send them out in different forms. It is an improved version of Syslog server, and it can be set up in the same ways.\nBut you can add more modules to it to handle log messages and send them to different log files and devices. This makes it an enterprise-level log management system.\nThe client-server approach of Rsyslog lets it be set up as either a client or a centralised logging system, so it can do both jobs at the same time.\nIt can run as a server that other network devices send logs to. Or, as a client, by sending log messages about events happening on a local machine to a remote syslog server."},"title":"How to setup Rsyslog server on Debian 11"},"/utho-docs/docs/how-to-setup-scheduled-tasks-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to setup scheduled tasks in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to setup scheduled tasks in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Select tools and settings, which are on the left side of the screen, then click on scheduled tasks.\nStep 3. Click on settings.\nStep 4. Select your preferred time zone and click on ‚ÄúApply.‚Äù\nStep 5. Select tools and settings, which are on the left side of the screen, then click on scheduled tasks.\nStep 6. Click on ‚ÄúAdd Task‚Äù to add a new task.\nStep 7. Fill in the details as per your own and then click on ‚ÄúOK.‚Äù\nStep 8. And then you will get a success message."},"title":"How to setup scheduled tasks in Plesk"},"/utho-docs/docs/how-to-share-a-folder-over-network-using-server-manager-in-windows-servers/":{"data":{"":"","#":"INTRODUCTION In this tutorial, we will learn how to share a folder over network using Server Manager in Windows Servers. There are many ways to setup shared folder in¬†Windows Server. You can use¬†folder properties share option to share a folder. In this tutorial, we will learn the steps to share a folder in¬†Windows Server 2016¬†using Server Manager folder share option.\nPrerequisites Windows Server Internet connectivity Step 1. Connect to your Windows server via RDP.\nStep 2. Go to Server Manager.\nStep 3. Go to File and Storage Services.\nStep 4. Click on TASKS and Click on ‚ÄúNew Share‚Äù\nStep 5. Select SMB Share - Quick\nStep 6. Select the folder via it‚Äôs path that you wish to share\nStep 7. Give the folder appropriate permissions as per your requirements.\nStep 8. Click close once the share is completed.\nStep 9. Connect to the folder using network path using login credentials of RDP user\nFolder aceessed.\nThank You!"},"title":"How to share a folder over network using Server Manager in Windows Servers"},"/utho-docs/docs/how-to-solve-the-server-requested-authentication-method-unknown-to-the-client-in-phpmyadmin/":{"data":{"":"","introduction#INTRODUCTION":"In this tutorial, we will learn how to solve ‚ÄúThe server requested authentication method unknown to the client‚Äù in phpMyAdmin.\nThe forementioned error is faced when connection for control user as defined in the configuration fails. We use the following steps the resolve the error. requested authentication method unknown Step 1. Run the following command\nmysql -u root -p Step 2. Then run the below command to change the user‚Äôs authentication plugin type;\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password'; Step 3. change the MySQL authenticate method in the mysql config file\nvi /etc/mysql/mysql.conf.d/mysqld.cnf **``` [mysqld]\ndefault_authentication_plugin=mysql_native_password\nrequested authenticate method not known Step 4. **Then run the below command in the [MySQL](https://www.mysql.com/) prompt.** mysql -u root -p\nmysql \u003e flush privileges;\nStep 5. Now restart the MySQL service in the server. service mysql restart\n**Thank You**! "},"title":"How to Solve \"The server requested authentication method unknown to the client\" in phpMyAdmin"},"/utho-docs/docs/how-to-start-stop-or-restart-system-services-in-plesk/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to Start, Stop or Restart System Services in Plesk.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Start, Stop or Restart System Services in Plesk.\nStep 1. Log into your Plesk with your server password by searching server_ip:8880 in your browser.\nStep 2. Go to tools and settings on the left side of the screen, then select services management from the server management menu.¬†Step 3. Now you are able to restart or stop the services as per your own preferences."},"title":"How to Start, Stop or Restart System Services in Plesk"},"/utho-docs/docs/install-docker-on-windows-server/":{"data":{"":"","#":"\nINTRODUCTION Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. The service has both free and premium tiers. The software that hosts the containers is called Docker Engine. It was first started in 2013 and is developed by Docker, Inc. In this tutorial, we will learn how to install Docker in a Windows Server.\nPrerequisites Windows Server\nPowerShell with Administrator rights\nInternet connectivity\nStep 1. Run Server Manager and start Add roles and features, then select Containers feature on Select features section like follows to install. After installing, restart computer.\nInstall Docker on Windows Server\nStep 2. After restarting, Run PowerShell with Admin Privilege and Install Docker.\nAnswer Y (Yes) to all confirmations during the installation.\nPS \u003e Install-Module -Name DockerMsftProvider -Repository PSGallery -Force\nPS \u003e Install-Package -Name docker -ProviderName DockerMsftProvider\nThank You!"},"title":"Install Docker on Windows Server"},"/utho-docs/docs/install-nginx-in-debian-from-the-official-nginx-repository/":{"data":{"":"These instructions install Debian 9 NGINX Mainline from the official repository of NGINX Inc. For other distributions\n1. Open a text editor to /etc/apt/sources.list¬†and add the next line to the bottom:\ndeb http://nginx.org/packages/mainline/debian/ stretch nginx 2. Import the signing key for the repository and add it to apt:\nsudo wget http://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key 3. Install NGINX:\nsudo apt update sudo apt install nginx 4. Ensure that NGINX runs and can start reboots automatically:\nsudo systemctl start nginx sudo systemctl enable nginx Thankyou.."},"title":"Install NGINX in Debian from the Official NGINX Repository"},"/utho-docs/docs/installation-of-lamp-stack-on-ubuntu-16/":{"data":{"":"LAMP, which operates Linux as the operating system, is an open-source web-developing platform using Apache as the web-based server and PHP as an object-oriented scripting language. (Instead of PHP, Perl or Python are sometimes used.)","installation-of-apache-server#Installation of Apache Server":"The Apache web server is the most popular web server in the world, making hosting websites a great default option.\nWe can install the Apache server by following command:\n[root@Microhost ~]# sudo apt-get install apache2 Place¬†Global¬†ServerName¬†to¬†Avoid¬†Syntax¬†Warnings Next, to delete the alert message, we‚Äôll add a line in apache configuration file /etc/apache2/apache2.conf file. When you do not set ServerName globally and search your Apache configuration for the syntax error then you will receive the following warning:\n[root@Microhost ~]# apache2ctl configtest AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message Syntax OK We will edit the file by following command\n[root@Microhost ~]# vi /etc/apache2/apache2.conf Now, we will add the below line in the configuration file\nServerName server_IP\nSave and exit from the file\nNext, check for syntax errors by typing\n[root@Microhost ~]# apache2ctl configtest The output will be shown as below\nSyntax OK Now, restart the apache service:\n[root@Microhost ~]# systemctl restart apache2 Set Firewall to Allow Web Traffic Then, if the server configuration instruction you have followed to enable the UFW firewall, ensure that HTTP and HTTPS traffic is enabled by your firewall. You should ensure that UFW has an Apache application profile such as:\n[root@Microhost ~]# ufw app list Output Available applications: Apache Apache Full Apache Secure OpenSSH When looking at the full Apache profile, it will display that ports 80 and 443 are traffic enabled:\n[root@Microhost ~]# ufw app info \"Apache Full\" Output: Profile: Apache Full Title: Web Server (HTTP,HTTPS) Description: Apache v2 is the next generation of the omnipresent Apache web server. Ports: 80,443/tcp Allow incoming traffic for this profile\n[root@Microhost ~]# ufw allow in \"Apache Full\" Now check the apache server while accessing the server IP\nhttp:// server_ip\nThe output will be shown as below:\nIf you see this web page, your firewall will now make your web servers properly configured and available.","installation-of-mysql#Installation of Mysql":"We can Install the Mysql by following command:\n[root@Microhost ~]# apt-get install mysql-server A list of the installing packages, along with the quantity of disk space, will be displayed to you. Enter Y to proceed.\nYour server will request that you select and confirm a ‚Äúroot‚Äù MySQL user‚Äôs password during the installation. This is an administrative MySQL account with privileges increased. Make sure that this password is strong and unique and do not leave it blank.\nWhen the installation is complete, a simple security script is required to remove dangerous defaults and to unlock access to our database system. Running the interactive script:\n[root@Microhost ~]# mysql_secure_installation The password you specified for your MySQL root account will be requested to enter.\nThe level of password validation will be requested. Keep in mind that if you enter 2, at the highest level, you will be faced with an error when you try to enter a password that is not based on common dictionary words, numbers, Uppercase and lowercase letters, and specific characters.\nThere are three levels of password validation policy: LOW Length \u003e= 8 MEDIUM Length \u003e= 8, numeric, mixed case, and special characters STRONG Length \u003e= 8, numeric, mixed case, special characters and dictionary file Please enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 1 You will be shown a password strength for the current root password and asked if you want to change this password when you enabled the validation of the password. If the password you are using is satisfactory, enter n at the prompt for ‚Äúno‚Äù:\nUsing existing password for root. Estimated strength of the password: 100 Change the password for root ? ((Press y|Y for Yes, any other key for No) :n For the remaining questions, press Y at each prompt and press the Enter key. This will remove certain anonymous users and the test database, deactivate remote root logins and load these new rules so that the MySQL changes are immediately met.","installation-of-php#Installation of PHP":"PHP is our setup component to process code for dynamic content display. It can execute scripts, access the MySQL databases, and display the processed content to our web server.\nWe can install the PHP with some required extension by following command:\n[root@Microhost ~]# apt-get install php libapache2-mod-php php-mcrypt php-mysql The above command will install the php .\nIn most cases, when a directory is requested, we‚Äôll want to modify how Apache serves files. Apache will first search a file called index.html when a user requests a directory from the server. We want to tell our web server to prefer a PHP file, So we will first let Apache look for the index.php.\nWe need to make changes in the directory dir.conf . which location will be /etc/apache2/mods-enabled/dir.conf . We can perform this task by following command:\n[root@Microhost ~]# vi /etc/apache2/mods-enabled/dir.conf The output will look like:\n\u003cIfModule mod_dir.c\u003e DirectoryIndex index.html index.cgi index.pl index.php index.xhtml index.htm \u003c/IfModule\u003e We have to move index.php at the first position as per the output given below:\n\u003cIfModule mod_dir.c\u003e DirectoryIndex index.php index.html index.cgi index.pl index.xhtml index.htm \u003c/IfModule\u003e Save the file and exit from the text editor using**:wq**\nNow, We have to restart the services of the apache server using following command:\n[root@Microhost ~]# systemctl restart apache2 Installation of PHP modules Optionally, several additional modules can be installed to improve PHP functionality.\nWe can use the following command to list the available php modules:\n[root@Microhost ~]# apt-cache search php- | less The output will be shown as below:\nlibnet-libidn-perl - Perl bindings for GNU Libidn php-all-dev - package depending on all supported PHP development packages php-cgi - server-side, HTML-embedded scripting language (CGI binary) (default) php-cli - command-line interpreter for the PHP scripting language (default) php-common - Common files for PHP packages php-curl - CURL module for PHP [default] php-dev - Files for PHP module development (default) php-gd - GD module for PHP [default] php-gmp - GMP module for PHP [default] We can install multiple modules by the following command :\n[root@Microhost]# apt-get install package1 package2 ‚Ä¶ We can test the PHP execution while placing the test file at the html directory as given below:\n[root@Microhost]# vi /var/www/html/info.php Now , enter the code given below. You can enter in insert mode while pressing i on keyboard:\n\u003c?php phpinfo(); ?\u003e Save the file and exit from the text editor:\nYou can access the test file using below url:\nhttp://your_server_IP/info.php The output will be shown as below:\nThe LAMP installation has been completed:\nThank You :)","prerequisites#Prerequisites":" Before you begin with this guide, you should have root user account privilege set up on your ubuntu server. Log in to the server with (SSH). Update the server packages using the command sudo apt-get update "},"title":"Installation of LAMP Stack on Ubuntu 16"},"/utho-docs/docs/introducing-autoscaling-and-how-to-create-one/":{"data":{"":"","autoscaling#\u003cstrong\u003eAUTOSCALING\u003c/strong\u003e":"AUTOSCALING DEPLOYING AUTOSCALING USING UTHO CLOUD DASHBOARD\nStep 1: Login to your Utho Cloud Dashboard.\nStep 2: Now, click on the Autoscaling option as per the screenshot given below.\nStep 3: You will be redirected to a new page, where we have to select the ‚ÄúCreate New‚Äù button.\nStep 4: Afterward, we will see a new page, where you have to choose a data center location and a Snapshot/Stack (you can attach your own stacks here) as per given in the screenshot.\nStep 5: Now, proceed with selecting the configuration of the server.\nStep 6: In the next step, you can specify a VPC, SECURITY GROUP and LOAD BALANCER as per your requirement.\nStep 7: Scrolling down on the same page, you will get the option of Instance size, Scaling Policy and Schedules. Please make the changes according to your requirement.\nStep 5: In the end, you will get the option to specify the Server label(this will reflect in server name) along with the button of Create Auto Scaling. Please see the screenshot for your reference.\nAfter clicking on ‚ÄúCreate Auto Scaling‚Äù , the service will be created of selected configuration. We can see the details of the same in¬†the ‚ÄúAuto Scaling‚Äù section of the dashboard."},"title":"AutoScale Unleashed: Step-by-Step Guide for Implementation"},"/utho-docs/docs/ipv6-a-gateway-to-cost-effective-networking/":{"data":{"":"","how-can-utho-cloud-assist-with-ipv6-implementation#\u003cstrong\u003eHow can Utho Cloud assist with IPv6 implementation?\u003c/strong\u003e":"","how-do-managed-service-providers-and-cloud-solutions-assist-organizations-with-ipv6-adoption-impacting-cost-optimization-strategies#\u003cstrong\u003eHow do managed service providers and cloud solutions assist organizations with IPv6 adoption, impacting cost optimization strategies?\u003c/strong\u003e":"","how-do-small-and-medium-sized-enterprises-smes-handle-ipv6-adoption-and-what-are-the-cost-challenges-they-face-compared-to-larger-companies#\u003cstrong\u003eHow do small and medium-sized enterprises (SMEs) handle IPv6 adoption, and what are the cost challenges they face compared to larger companies?\u003c/strong\u003e":"Today‚Äôs digital world is constantly changing, and having a strong communication system is crucial to staying competitive. A key part of this system is the Internet Protocol (IP), which is a set of rules that helps devices communicate over the internet. Every device connected to a network gets a unique identifier called an IP address, which allows them to send and receive data.\nIPv4 has been the main version of IP used for a long time. But because the internet has grown so much, we‚Äôre running out of IPv4 addresses. This is where IPv6 comes in. It‚Äôs a newer standard that‚Äôs being rolled out to replace IPv4. Many companies and organizations are switching because it offers a practically unlimited number of addresses, which solves the problem of running out of them with IPv4.\nHow does IPv6 adoption contribute to cost optimization in networking? IPv6 adoption contributes to cost optimization in networking in several ways:\nEfficient Addressing: IPv6 provides a significantly larger address space compared to IPv4. With Internet Protocol version 6, there are more than enough addresses to accommodate the growing number of devices connected to the internet. This eliminates the need for costly workarounds like Network Address Translation (NAT), which can be complex to manage and can incur additional hardware and administrative costs.\nSimplified Network Architecture: IPv6 simplifies network architecture by removing the need for NAT and allowing for end-to-end connectivity. This simplification can reduce the complexity of network configurations and maintenance, leading to cost savings in terms of reduced equipment, configuration, and support requirements.\nEnhanced Security: IPv6 includes built-in support for IPsec (Internet Protocol Security), which provides encryption and authentication for network traffic. By integrating security features at the protocol level, organizations can potentially reduce the need for additional security measures and investments in third-party security solutions, thus optimizing costs.\nFuture-Proofing: As IPv4 addresses become increasingly scarce, the cost of acquiring IPv4 addresses from the dwindling pool of available addresses can be significant. IPv6 adoption future-proofs networks by providing an abundant and scalable address space, reducing the need for costly acquisitions of IPv4 addresses as well as potential disruptions caused by address exhaustion.\nOperational Efficiency: IPv6 adoption can lead to operational efficiencies by streamlining network management tasks. With Internet Protocol version 6, network administrators can benefit from auto-configuration capabilities, simplified routing protocols, and improved scalability, all of which contribute to reduced operational overhead and lower costs associated with network management and troubleshooting.\nOverall, IPv6 adoption offers a cost-effective solution for meeting the growing demands of the internet while simplifying network operations and enhancing security, ultimately leading to significant cost optimization in networking.\nWhich industries or sectors are likely to benefit the most from IPv6 adoption in terms of cost optimization? Several industries or sectors are likely to benefit significantly from IPv6 adoption in terms of cost optimization:\nTelecommunications: Telecommunications companies stand to gain substantial cost savings through IPv6 adoption. With the increasing number of connected devices and the growing demand for data-intensive services like video streaming and IoT applications, IPv6‚Äôs larger address space and efficient routing capabilities can help telecom providers optimize their network infrastructure, reduce operational costs, and accommodate future growth without the need for costly workarounds.\nInternet Service Providers (ISPs): ISPs play a crucial role in the adoption and deployment of IPv6, as they are responsible for providing internet connectivity to users. IPv6 adoption enables ISPs to efficiently allocate IP addresses to their customers without the constraints of IPv4 address scarcity. By transitioning to Internet Protocol version 6, ISPs can streamline their network operations, reduce the reliance on IPv4 address leasing, and avoid the costs associated with IPv4 address acquisitions.\nCloud Service Providers: Cloud service providers rely heavily on scalable and efficient networking infrastructure to deliver services to their customers. IPv6 adoption allows cloud providers to expand their infrastructure while minimizing costs associated with IPv4 address management, NAT traversal, and network complexity. Additionally, IPv6‚Äôs built-in support for IPsec enhances security for data transmitted over cloud networks, potentially reducing the need for additional security investments.\nLarge Enterprises: Large enterprises with extensive networking requirements can benefit from IPv6 adoption by optimizing their internal network infrastructure and reducing the reliance on IPv4 address management solutions. Internet Protocol version 6 enables enterprises to support a growing number of connected devices, facilitate seamless communication between different departments and locations, and streamline network management processes, leading to cost savings in terms of equipment, maintenance, and operational overhead.\nGovernment and Public Sector: Government agencies and public sector organizations often manage large-scale network infrastructures to deliver services to citizens and employees. Internet Protocol version 6 adoption in these sectors can lead to significant cost savings by eliminating the need for IPv4 address acquisitions, reducing network complexity, and enhancing security capabilities. Additionally, Internet Protocol version 6 enables interoperability and communication between different government agencies and systems, streamlining administrative processes and improving overall efficiency.\nOverall, industries and sectors that rely heavily on scalable, efficient, and secure networking infrastructure are likely to benefit the most from IPv6 adoption in terms of cost optimization.\nHow do managed service providers and cloud solutions assist organizations with IPv6 adoption, impacting cost optimization strategies? Managed service providers (MSPs) and cloud-based solutions play a crucial role in facilitating IPv6 adoption for organizations by providing expertise, infrastructure, and services tailored to support the transition to IPv6. This support significantly impacts cost optimization strategies in several ways:\nExpertise and Guidance: MSPs often have specialized knowledge and experience in IPv6 deployment and can offer guidance to organizations throughout the adoption process. They can assess the organization‚Äôs current infrastructure, develop an IPv6 migration plan, and provide recommendations for optimizing costs while transitioning to Internet Protocol version 6.\nInfrastructure Support: Cloud-based solutions offered by MSPs provide scalable and flexible infrastructure resources for organizations to deploy IPv6-enabled services and applications. By leveraging cloud platforms that support IPv6, organizations can avoid upfront investments in hardware and infrastructure, reduce operational costs, and scale resources as needed based on demand.\nIPv6-Enabled Services: MSPs may offer IPv6-enabled services such as managed network services, security solutions, and communication platforms that are designed to support IPv6 natively. By utilizing these services, organizations can accelerate their IPv6 adoption efforts while minimizing disruptions to their existing operations and optimizing costs associated with network management and security.\nEfficient Migration Strategies: MSPs can assist organizations in developing efficient migration strategies that prioritize cost optimization. This may include phased migration approaches, prioritizing critical systems and services for IPv6 deployment, and leveraging automation and orchestration tools to streamline the migration process and reduce manual effort and associated costs.\nCompliance and Risk Management: MSPs help organizations navigate compliance requirements and manage risks associated with IPv6 adoption. By ensuring compliance with industry standards and regulations, as well as implementing robust security measures, MSPs help organizations mitigate potential risks and avoid costly security breaches or compliance penalties.\nOverall, managed service providers and cloud-based solutions play a vital role in facilitating IPv6 adoption for organizations by providing expertise, infrastructure, and services tailored to support the transition. By leveraging the support of MSPs and cloud-based solutions, organizations can optimize costs, accelerate their IPv6 adoption efforts, and ensure a smooth transition to the next-generation Internet protocol.\nHow can Utho Cloud assist with IPv6 implementation? Utho Cloud can assist with Internet Protocol version 6 implementation in several ways:\nNative IPv6 Support: Utho Cloud provides native support for IPv6, allowing organizations to easily enable and configure IPv6 addresses for their cloud resources. This means that users can deploy and manage IPv6-enabled applications and services without the need for complex workarounds or additional configurations.\nIPv6-Enabled Networking Services: Utho Cloud offers a range of networking services that are IPv6-enabled, including Virtual Cloud Networks (VCNs), load balancers, and DNS services. These services allow organizations to build and manage IPv6-capable network architectures in the cloud, facilitating seamless communication between IPv6-enabled resources.\nMigration and Transition Assistance: Utho Cloud provides tools and resources to assist organizations with the migration and transition to IPv6. This includes guidance documentation, best practices, and migration services to help organizations plan and execute their IPv6 adoption strategies effectively.\nSecurity and Compliance: Utho Cloud includes built-in security features and compliance controls to ensure the secure deployment and management of IPv6-enabled resources. This includes support for Internet Protocol version 6-specific security protocols and standards, such as IPsec, to protect data transmitted over IPv6 networks.\nScalability and Performance: Utho Cloud offers scalable and high-performance infrastructure to support the deployment of IPv6-enabled applications and services. With Utho Cloud‚Äôs global network of data centers and high-speed connectivity, organizations can ensure reliable and efficient access to their IPv6 resources from anywhere in the world.\nOverall, Utho Cloud provides comprehensive support for IPv6 implementation, offering native IPv6 support, IPv6-enabled networking services, migration assistance, security features, and scalable infrastructure to help organizations seamlessly transition to IPv6 and leverage its benefits in the cloud.\nHow do small and medium-sized enterprises (SMEs) handle IPv6 adoption, and what are the cost challenges they face compared to larger companies? Small and medium-sized enterprises (SMEs) are approaching IPv6 adoption in the market by taking strategic steps to address their specific needs and challenges. Here‚Äôs how they‚Äôre navigating this transition and the unique cost optimization challenges they face compared to larger enterprises:\nResource Constraints: SMEs often have limited resources, both in terms of budget and technical expertise. To navigate IPv6 adoption, SMEs may focus on prioritizing essential infrastructure upgrades and leveraging external support, such as consulting services or managed service providers, to supplement their internal capabilities.\nBudget Limitations: Cost considerations play a significant role for SMEs, who may have tighter budgets compared to larger enterprises. While Internet Protocol version 6 adoption is essential for future-proofing their networks, SMEs must carefully evaluate the costs associated with hardware upgrades, software licenses, training, and potential disruptions to their operations during the transition.\nVendor Support and Compatibility: SMEs may face challenges in finding affordable hardware and software solutions that fully support IPv6. Some legacy systems and applications may require updates or replacements to ensure compatibility with IPv6, which can incur additional costs and complexity for SMEs with limited IT resources.\nRisk Management: For SMEs, the risks associated with IPv6 adoption, such as potential compatibility issues or security vulnerabilities, can have a disproportionate impact on their operations. SMEs must prioritize risk management strategies and invest in robust security measures to mitigate these risks effectively.\nScalability and Growth: While SMEs may have smaller networks compared to larger enterprises, scalability remains a crucial consideration. IPv6 adoption allows SMEs to accommodate future growth and expansion without facing the constraints of IPv4 address exhaustion. However, SMEs must carefully plan for scalability to ensure that their network infrastructure can support their evolving business needs in a cost-effective manner.\nSMEs are navigating Internet Protocol version 6 adoption by focusing on prioritizing essential upgrades, managing budget constraints, seeking vendor support, mitigating risks, and planning for scalability. While they face unique challenges compared to larger enterprises, SMEs can leverage external support, strategic planning, and careful cost management to optimize their IPv6 adoption efforts within their budgetary constraints.\nTransitioning to IPv6 offers significant cost-saving benefits for businesses. While smaller enterprises may face challenges due to limited resources, strategic planning and seeking support can help ease the process. Embracing IPv6 not only enhances connectivity but also prepares businesses for future growth and scalability in the digital world.","how-does-ipv6-adoption-contribute-to-cost-optimization-in-networking#\u003cstrong\u003eHow does IPv6 adoption contribute to cost optimization in networking?\u003c/strong\u003e":"","which-industries-or-sectors-are-likely-to-benefit-the-most-from-ipv6-adoption-in-terms-of-cost-optimization#\u003cstrong\u003eWhich industries or sectors are likely to benefit the most from IPv6 adoption in terms of cost optimization?\u003c/strong\u003e":""},"title":"IPv6: A Gateway to Cost-Effective Networking"},"/utho-docs/docs/linux/2-methods-for-re-running-last-executed-commands-in-linux/":{"data":{"":"\nDescription\nThe command history is one of the most useful aspects of the Bash shell. This feature logs all of the commands that a user executes and keeps them in a file located in the user‚Äôs home directory (usually /home/$USER/.bash history). This makes it simple for the user to recall, amend, and perform previously executed instructions again.\nIn this piece, we will illustrate how to re-execute a particular command by retrieving it from the history of commands that have been input into a shell. This allows you to avoid repeatedly inputting the same commands, which is a helpful feature.\nIn a normal situation, you can utilise the Up arrow keys to recover a previous command in order to acquire a command that you just ran lately. Maintaining consistent pressure on it will cycle through many commands in the history, allowing you to locate the one you want more quickly. To move in the other direction, use the Down arrow on your keyboard.\nYou can use the history command if you want to re-execute a certain command from the history of commands. However, the history file may contain a lot of entries.\n# history The next step is to obtain the number(s) of the command(s) you wish to re-execute. For instance, if you want to start httpd and inspect its status, you will need to re-execute the instructions 104 and 105, as shown.\n# !104 # 105 You can also re-execute a command that you have already used (for example, sudo yum update) by using the ‚Äò!‚Äô character followed by a few of the first characters (for example, sud or sudo) of that particular command, as demonstrated here.\n# !sud or\n# !sudo The command history, stored in /home/$USER/.bash history, is one of Bash‚Äôs best features. The user may simply recall, amend, and rerun past commands.\nWe‚Äôll show you how to rerun a shell command in this article. To prevent inputting the same commands, this is beneficial.\nUse the Up arrow keys to retrieve a recent command. To find the command you want, press it again. Reverse with the Down arrow.\nTo re-execute a specific command from the history file, run the history command.\nThank You"},"title":"2 Methods for Re-Running Last Executed Commands in Linux"},"/utho-docs/docs/linux/4-effective-ways-to-determine-the-name-of-a-plugged-usb-device-in-linux/":{"data":{"":"\nDescription\nIn this article we will learn 4 Effective Ways to Determine the Name of a Plugged USB Device in Linux.\nOne of the many aspects of Linux that you should become proficient in as a beginner is the identification of devices that are connected to your computer. It could be the hard disc in your computer, an external hard drive, or another type of portable media like a USB drive or an SD Memory card.\nThe use of USB drives for the transfer of files is extremely common in today‚Äôs world. For those (new Linux users) who prefer to work from the command line, becoming familiar with the various methods by which a USB device name can be identified is very important when it comes time to format the device.\nAfter you have connected a device to your computer, such as a USB drive, particularly on a desktop computer, the device is automatically mounted to a specific directory, which is typically located under /media/username/device-label. You are then able to access the files contained within the device by navigating to the directory in which it is mounted. On the other hand, this is not the case while working with a server, since you will need to manually mount a device and then designate where it should be mounted.\nThe /dev directory houses specialised device files that are used by Linux to determine the identity of hardware. This directory contains a number of files, one of which is /dev/sda or /dev/hda, which represents your first master drive. Each partition will be represented by a number, such as /dev/sda1 or /dev/hda1 for the first partition, and so on for subsequent partitions.\nFollow the below steps to learn 4 Effective Ways to Determine the Name of a Plugged USB Device in Linux..","determine-the-name-of-the-usb-device-using-the-fdisk-utility#Determine the Name of the USB Device Using the fdisk Utility":"It is possible to execute fdisk with root privileges in the following manner: fdisk is a powerful application that prints out the partition table on all of your block devices, including USB drives.\n# sudo fdisk -l ","find-disk#Find Disk":" # ls /dev/vda* or\n# ls /dev* Now, let‚Äôs find out the names of the devices by utilising a variety of command-line tools, as indicated below:","find-the-name-of-a-usb-device-with-the-lsblk-command#Find the Name of a USB Device with the lsblk Command":"You can also use the lsblk command, which stands for ‚Äúlist block devices.‚Äù This command will list all block devices that are currently connected to your system in the following format:\n# lsblk ","using-the-df-command-get-the-name-of-the-connected-usb-device#Using the df command, get the name of the connected USB device":"The df programme, which analyses the consumption of disc space in Linux systems, can be used, as demonstrated in the following image, to view each device that is connected to your computer as well as its mount point:\n# df -h ","using-the-dmesg-command-determine-the-name-of-the-usb-device#Using the dmesg Command, Determine the Name of the USB Device":"A crucial command, dmesg, prints or controls the kernel ring buffer, a data structure that holds information about the activities of the kernel.\nTo read kernel operation messages, run the command that is listed below. This command will also output information about your USB device:\n# dmesg In this post, we have discussed various strategies for determining the name of a USB device using the command line, and that will be all we cover for the time being.\nI really expect that you have a complete understanding of all of the steps. 4 Effective Ways to Determine the Name of a Plugged USB Device in Linux\nMust read :- https://utho.com/docs/tutorial/how-to-create-encrypt-and-decrypt-random-passwords-in-linux/\nThank You"},"title":"4 Effective Ways to Determine the Name of a Plugged USB Device in Linux"},"/utho-docs/docs/linux/access-linux-server-using-ssh-in-windows-linux-and-mac-os/":{"data":{"":"","part-1-access-linux-server-using-ssh-in-windows#Part 1: Access Linux server using SSH in Windows":"","part-2-access-linux-server-using-ssh-in-linux-ubuntu#Part 2: Access Linux server using SSH in Linux Ubuntu":"","ssh-usernameyourserverexamplecom#ssh \u003ca href=\"mailto:username@yourserver.example.com\"\u003eusername@yourserver.example.com\u003c/a\u003e":"\nPart 1: Access Linux server using SSH in Windows 1. Download PuTTY or another PuTTY download source from URL https://the.earth.li/~sgtatham/putty/latest/w64/putty.exe . The file called ‚Äúputty.exe‚Äù is perfect for simple SSH.\n2. Save and download software in your Windows system.\n3. To launch the application, double-click the putty.exe file.\n4. Enter the settings of your connection\nEnter the server ip address or hostname. Port: 22 ( leave as default if SSH port is default) Connection Type: SSH (leave as default) 5. Click Open to start the SSH session.\n6. If this is the first time you are connecting to the server on this computer, you will see the following output. Please accept the connection by clicking Yes.\n7. Once the SSH Connection has been opened, you should see the terminal prompt asking for your username:\nConnect to your preferred SSH user.\n8. Next, please enter your password. Please note that you will NOT see your cursor moving or any typed characters (such as * * * * * *) when you type your password. This is a standard security feature of the PUTTY. Hit get in.\nroot@yourserver.example.com's password: 9. You are logged in to your SSH server now. You will see output like this.\nroot@yourserver.example.com‚Äôs password:\nLast failed login: Sat Apr 25 17:03:02 IST 2020 from IP on ssh:notty\nThere were 2 failed login attempts since the last successful login.\nLast login: Sat Apr 25 16:57:08 2020 from IP\n[root@yourserver.example.com ~]\nPart 2: Access Linux server using SSH in Linux Ubuntu 1. If you have ubuntu desktop. Go to Search bar and type ‚ÄúTerminal‚Äù.\n2. Now terminal will open and you need to enter the following command on terminal.\n# ssh username@yourserver.example.com 3. You need to enter the password of SSH user of your server. ``` username@yourserver.example.com‚Äôs password:\n4\\. Now you have connected your linux server on your ubuntu desktop using terminal and you will get outpout like this: [root@yourserver.example.com ~]#\n## Part 2: Access Linux server using SSH in Mac OS 1\\. If you have Mac OS. Go to Search bar and type \"Terminal\". ![](images/mac-1.png) 2\\. Now terminal will open and you need to enter following command on terminal. ![](images/mac-2.png) ssh username@yourserver.example.com 3\\. You need to enter the password of SSH user of your server. ``` username@yourserver.example.com's password: 4. Now you have connected your linux server on your Mac OS using terminal and you will get outpout like this:\n[root@yourserver.example.com ~]# Thank you‚Ä¶"},"title":"How to access Linux server using SSH in Windows, Linux and Mac OS."},"/utho-docs/docs/linux/add-or-delete-domains-and-subdomain-in-plesk/":{"data":{"":"\nIntoroduction\nIn this article you will know How to ADD OR DELETE DOMAINS AND SUBDOMAIN IN PLESK,\nDomain=The word ‚Äúdomain‚Äù may be used to refer to the structure of the internet, which is unique to the world wide web. Domain can also be used to refer to the manner in which an organization‚Äôs network resources are arranged. In general, a domain may be thought of as either a sphere of knowledge or an area of control.\nSubdomain: Subdomains are the components of a domain that appear before the primary domain name and the extension of the domain. They can assist you in the organisation of your website. For instance, you may visit docs.themeisle.com. The subdomain that is represented by this URL is docs.\n1.click on the website and domains\n2.Add domain\n3.Fill out the data fields with information regarding the domain you wish to add.\n*On the left-hand menu click website and domains\n*After that, select the ‚ÄúAdd Domain‚Äù option that is seen in the following screenshot.\n*Select¬†Temporary domain name¬†or¬†Registered domain name\n*Fill out the data fields with information regarding the domain you wish to add\n*Now that you‚Äôve added a domain, take a look at the screenshot down below.\n** How to add a subdomain**\n1.click on the website and domains\n2.Add subdomain\n3.Fill the required filleds\n*On the left-hand menu click website and domains\n*After that, select the ‚ÄúAdd sub domain‚Äù option that is seen in the following screenshot.\nFill the required filleds and then click ok *Now that you‚Äôve added a sub domain, take a look at the screenshot down below.\n**The steps necessary to delete domains and subdomains**\n*Click the three dots that are located on the right side.*\n*Click remove website*\nKnow, How To find Apache version in Plesk\nThank You"},"title":"How To ADD OR DELETE DOMAINS AND SUBDOMAIN IN PLESK"},"/utho-docs/docs/linux/an-introduction-to-the-linux-alternatives-command/":{"data":{"":" An introduction to the Linux alternatives command\nThis tutorial is to about the introduction of to the Linux alternatives command. Alternatives is a program that adds, deletes, updates, and displays data on the symbolic links that make up the alternatives system.\nBecause many users don‚Äôt care how their computer completes a job as long as it is successful, abstractions may be useful to users. In reality, as long as all of an application‚Äôs system calls are properly responded, it doesn‚Äôt always matter how anything is accomplished. Theoretically, a Linux system administrator may provide a wide range of system utilities based on their functions rather than the precise name of the executable, but doing so often requires a lot of symlinking and version management. Unless you use the alternatives command, that is.\nIt‚Äôs important to note that the alternatives command first existed as a substitute. This was originally an update-alternatives convenience tool from the Debian Linux project, written in Perl. The command was rewritten by Red Hat without the use of Perl, and it has since spread across Fedora-based distributions like Red Hat and CentOS as well as other distributions that rely on Red Hat to provide the functional definition of the Linux Standard Base (LSB).","check-alternatives#Check Alternatives":"Other advantages of the alternatives command include the ability to symlink dependent components when a certain option is selected. Alternatives keeps your discrepancies consolidated while while doing a lot of the hard work. To avoid having to always remember that emacs is really emacs-26.3 or that java is actually /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0, etc., you may check in on what alternatives are available:\nalternatives --list Now, you have a learned the introduction of Linux alternatives command. The alternatives command is an illustration of how both users and administrators can profit from some deft manipulation since Linux is all about choice and flexibility.\nMust Read: What are Runlevels in Linux and its understanding.","creating-a-new-alternative#Creating a new alternative":"It is simple to generate an alternative entry using the example situation of an old binary named em being replaced by the new variation nem. Although in most cases this isn‚Äôt required since the command isn‚Äôt precisely what people assume it is in the first place (java, for example, is seldom found under /usr/bin directly), you must rename the original command in this example. If you do need to rename a binary, the proper place to do so is in the binary‚Äôs RPM spec file so that the command to install the RPM supplies the binary with the new name.\nIn a pinch, particularly if you‚Äôre planning to phase the command out anyway, you could remove it from the system‚Äôs package management and just maintain a customised version of the old command accessible via /opt or something like.\nTake the original editor, which you have called em2, for this example (denoting its latest version number).\nNow, you may associate two binaries (em2 and nem) with the command em. Assume that em and nem are both little Emacs-style editors to help keep the terminology distinctions clear. In that instance, for the sake of simplicity, a very general word for the new alternative would be emacs (using the symbol to signify ‚Äúmicro‚Äù) or uemacs.\nOffer the following as a replacement for the original:\na location for the ‚Äúgeneric‚Äù symlink a name for the alternative (I‚Äôve decided upon¬†uemacs, to make it clear that it‚Äôs a reference name rather than a specific binary) the binary to be executed when these symlinks are called a priority level to indicate which alternative is preferred alternatives --install /usr/bin/em uemacs /opt/em-legacy/em2 1 Since the new binary is your preferred binary, develop a high priority replacement:\nalternatives --install /usr/bin/em uemacs /usr/local/bin/nem 99 With the help of these commands, a new symbolic link, /usr/bin/em, is created that points to /etc/alternatives/em, which in turn can point to either /opt/em-legacy/em2 or /usr/local/bin/nem. The systems administrator chose the descriptive and memorable term uemacs as the human-friendly name for this collection of alternatives at random. Using the ‚Äîconfig option, you can select the alternate symlink‚Äôs destination:\nalternatives --config uemacs There are 2 programs which provide 'uemacs'. Selection Command ----------------------------------------------- *+ 1 /usr/local/bin/nem 2 /opt/em-legacy/em2 ","removing-an-alternative#Removing an alternative":"Alternatives may assist users or administrators move to something new or they might be long-term solutions. Using the ‚Äîremove-all option and the alternative‚Äôs ‚Äúgeneric‚Äù name will eliminate all alternatives if you need to for whatever reason. Uemacs is used as an example here:\nalternatives --remove-all uemacs This deletes the symlink in /usr/bin and /etc/alternatives as well as everything else related to uemacs from the alternatives subsystem. Use the ‚Äîremove option with the alternative name (in this case, uemacs) and the path of the choice you wish to drop if you only want to remove one option from an alternative:\nalternatives --remove uemacs /opt/em-legacy/em2 ","when-to-use-the-alternatives-command#When to use the alternatives command":"An easy-to-understand and practical example is the em command on your computer, which starts a simple text editor. It has long been the editor of choice for your user base; many individuals have built processes around it, and some people keep very specific configurations for it. Although em doesn‚Äôt support Unicode, your user base lately made it apparent to you that they can‚Äôt function without emoji functionality in their terminal editors.\nYou discover nem, a Unicode-capable branch of em, but you are aware that 99% of your customers are already used to em, have scripts that call em, and will never stop thinking of any alternative programme as em. The simple answer to this problem, as well as many others like it, is to provide the new nem binary as the preferred substitute for the em binary whenever the em command is used.\nAlthough manually constructing a symlink could seem alluring, it is neither centralised nor immune to changes from a package management.\nWhen handling ‚Äúgeneric‚Äù application names, the alternatives command is most helpful. As ‚Äúgeneric‚Äù isn‚Äôt usually generic in the UNIX world, just as ‚ÄúKleenex‚Äù or ‚ÄúXerox‚Äù isn‚Äôt always generic in the actual world, terminology like java, (x)emacs(-nox), (n)vi(m), whois, and iptables are often among of the first to have replacements specified for them. The alternatives command is not seen as a suitable option for really general names, such as EDITOR and CC. Because they are environment variables and need to be specified in /etc or $HOME/.profile, those phrases."},"title":"An introduction to the Linux alternatives command"},"/utho-docs/docs/linux/archiving-and-compressing-files-with-gnu-tar-and-gnu-zip/":{"data":{"":"","compressing-log-files#Compressing Log Files":"Some files are generated, especially log files that can increase to a large size, by long-term daemons, such as web or e-mail servers. While these files are not deleted, they can grow unmanageably large within a short period of time. Since they are plain texts, compression is effective; however, it makes no sense to use a tool such as tar. It makes sense to use gzip straight away in these cases:\n[root@Microhost ~]# gzip /var/log/mail.log The file is called mail.log.gz to replace the original /var/log/mail.log. To access this file‚Äôs contents:\n[root@Microhost ~]# gunzip /var/log/mail.log.gz However, you do not need to uncompress a file to access its contents in most cases. The tool gzip includes tools for accessing ‚Äúgzipped‚Äù files with standard Unix tools. you can access file contents of gzip with following utilities zcat (Cat equivalent), zgrep (Grep equivalent) and zless (Lower Equivalent).\nThank You :)","extracting-files-from-a-tar-archive#Extracting Files from a tar Archive":"To extract files from a¬†tar¬†archive, issue the following command:\n[root@Microhost ~]# tar -xzvf latest.tar.gz The specified options have the following effects: -x extracts archive contents, -z filters the archive through the compression gzip tool, -v allows the verbose output to print a list of files as they‚Äôre extracted from the archive and -f specifies that tar will read input from the subsequently specified file latest.tar.gz","gzip-command#gzip Command":"A simple and standard method for compressing individual files is provided with gzip and the accompanying Gunzip command. As tar does not allow compression of the files which it archive, the gzip tools can only operate on single files. As tar does not. In the file test.txt the next command is added and comprised in test.txt.gz:\n[root@Microhost ~]# gzip test.txt This¬†file¬†can¬†be¬†decompressed¬†using¬†one¬†of¬†the¬†following¬†commands:\n[root@Microhost ~]# gunzip test.txt.gz [root@Microhost ~]# gzip -d test.txt.gz You can add the -v flag to improve the compression rate of verbosity and output statistics:\n[root@Microhost ~]# gzip -v test.txt gzip¬†accepts¬†standard¬†input¬†to¬†compress¬†the¬†output¬†of¬†a¬†text¬†stream:\n[root@Microhost ~]# cat test.txt | gzip \u003e test.txt.gz The compression algorithm used by gzip in compressing files can be configured to use more compression and save time. A number argument between -1 and -9 controls this ratio. The default setup is -6. Also, gzip includes the helpful mnemonics ‚Äì-fast (i. e. ‚Äî1) and ‚Äîbest (i.e. -9):\n[root@Microhost ~]# gzip --best -v test.txt [root@Microhost ~]# gzip --fast -v test.txt [root@Microhost ~]# gzip -3 -v test.txt [root@Microhost ~]# gzip -8 -v test.txt ","tar-command#tar Command":"tar is a software tool for collecting multiple files into one file in other word we can say ‚Äúarchive‚Äù. However, gzip is a software application used for compression and decompression. We use file compression techniques for saving disk space. This report offers a summary of the use of tar and gzip :\nTAR‚Äôs intricacy is not based on its fundamental form, but on a number of options and settings that can be used to create archives and to interact with them.\nFor example: We have a tar file named latest-archive.tar . We will use below given command to extract the content of the tar file into present working directory.\n[root@Microhost ~]# tar -xf latest.tar Use the following command to create archive(wordpress.tar.gz) file of all files in the Wordpress directory:\n[root@Microhost ~]# tar -c wordpress \u003e wordpress.tar.gz By default, tar sends archive file contents to the standard output, which you can use to continue processing your created archive. You can opt for the -f option to bypass default output. The command following is the same as the previous command:\n[root@Microhost ~]# tar -cf wordpress.tar.gz wordpress "},"title":"Archiving and Compressing files with GNU Tar and GNU Zip"},"/utho-docs/docs/linux/change-ssh-default-port-22-to-custom-port/":{"data":{"":"\n1. Access your linux server using SSH with putty any third party software.\n2. Enter your sudo user password or root password.\n3. Open SSH configuration file using below command in your favorite editor.\n[root@ssh-port-change ~]# vi /etc/ssh/sshd_config 4. Add ‚ÄúPort 44‚Äù line in sshd_config file and save or exit the file.\n5. Restart ssh service using below command.\n[root@ssh-port-change ~]# systemctl restart sshd 6. Open port 44 or custom port in firewall(Firewalld,CFS,IPtables,Microhost cloud firewall,etc) which you have defined in ssh configuration file,if you are using any internal and external firewall on server. For Eg:- firewalld\n[root@ssh-port-change ~]# firewall-cmd --add-port=44/tcp --permanent [root@ssh-port-change ~]# firewall-cmd --reload Thank you.."},"title":"Change SSH Default Port 22 to Custom Port"},"/utho-docs/docs/linux/cheat-sheet-for-15-nmcli-commands-in-linux-rhel-centos/":{"data":{"":"\nDescription\nI will walk you through 15 different examples of using the nmcli tool in Linux. nmcli is a command-line programme that is open-source and may be used to control NetworkManager and report on the state of networks. It is utilised to a large extent by Linux specialists so that they can harness the power of Network Manager directly from the command line. Creating, displaying, editing, deleting, activating, and deactivating network connections, as well as controlling and displaying the status of network devices, are all possible tasks that may be accomplished with the help of nmcli.","10create-a-new-connection-profile#10.Create a new connection profile.":"If you want to create an Ethernet type connection profile using interface enp1s0, then you need to use the nmcli c add type ethernet connection command. command interface-name enp1s0.\nmcli c add type ethernet connection.interface-name enp1s0 ","11examine-the-networkmanager-polkit-permissions#11.Examine the NetworkManager Polkit Permissions¬†":"Use the nmcli general permissions command to check the Polkit permissions set up for different NetworkManager operations. A system administrator sets up these permissions or actions (in Polkit language), and users can‚Äôt change them.\nnmcli general permissions ","12using-the-nmcli-command-change-the-hostname#12.Using the nmcli command, change the hostname.¬†":"You can also change the system‚Äôs hostname with nmcli. You can use the nmcli general hostname command to find out what the current hostname is.\nnmcli general hostname ","13create-a-bridge-using-the-nmcli-command#13.create a bridge using the nmcli command":"Use the nmcli con add type bridge ifname bridge name\u003e command to make a bridge. In this example, we use the nmcli con add type bridge ifname br0 command to create a bridge called br0.\nnmcli con add type bridge ifname br0 ","14nmcli-command-to-disable-bridge-stp#14.nmcli command to disable bridge STP":"By default, the Spanning Tree Protocol (STP) will be turned on. To turn it off, use the nmcli con mod bridge-br0 bridge.stp no command.\nnmcli con modify bridge-br0 bridge.stp no If you want to turn on STP again, use the command nmcli con mod bridge-br0 bridge.stp yes.\nnmcli con modify bridge-br0 bridge.stp yes ","15convert-an-interface-to-an-unmanaged-interface#15.Convert an Interface to an unmanaged Interface":"Use the nmcli device status command to look at the list of interfaces and then the state of an interface.\nnmcli device status Conclusion\nnmcli is a command-line software that is open-source that may be used to report on the state of networks as well as control NetworkManager. Linux experts make extensive use of it so that they may access the power of Network Manager straight from the command line. This enables them to do their jobs more efficiently. With the assistance of nmcli, one is able to accomplish a wide variety of tasks, including creating, displaying, editing, deleting, activating, and deactivating network connections, as well as controlling and displaying the status of network devices. In addition, one is able to edit and delete network connection information.\nMust Read : How To Add a User and Grant Root Privileges on Ubuntu 18.04\nThankyou","1how-to-find-the-version-of-nmcli#1.How to find the version of nmcli":"You will need to run the nmcli ‚Äîversion command in order to determine the version of nmcli that is currently installed on your system.\nnmcli --version ","2how-to-double-check-each-and-every-connection-to-a-network-device#2,How to Double-Check Each and Every Connection to a Network Device":"The nmcli connection command is what you need to use to see all the network device connections that are available. If the nmcli connection command doesn‚Äôt have any parameters, it will show both system and user setting connections that have been set up.\nnmcli connection ","3checking-the-status-of-all-network-devices#3.Checking the Status of All Network Devices":"Use the nmcli device status command to verify the status of all network devices. This command displays both controlled and unmanaged devices.\nnmcli device status ","4how-to-display-the-status-of-a-radio-switch#4.How to Display the Status of a Radio Switch¬†":"If you wish to see the current status of the radio switches, use the nmcli radio all command.\nnmcli radio all ","5how-to-display-all-network-devices#5.How to Display All Network Devices":" nmcli device show ","6how-to-check-the-status-of-networkmanager#6.How to Check the Status of NetworkManager¬†":"Use the nmcli -t -f RUNNING general command to check the running status of NetworkManager in concise output mode.\nnmcli -t -f RUNNING general ","7network-manager-status-check#7.Network Manager Status Check":"nmcli -t -f is what you need to use to check the status of Network Manager as a whole in terse output mode. STATE general command\nnmcli -t -f STATE general ","8checking-terse-output#8.Checking Terse Output":"use the nmcli -t device command to show a list of all the network devices that are currently set up in a concise way.\nnmcli -t device ","9check-how-network-manager-logs#9.Check how Network Manager logs.":"Then you need to use the nmcli general logging command to check the current Network Manager logging settings.\nnmcli general logging ","uuid-what-is-it#UUID: what is it?":"A UUID, or universally unique identifier, is a 128-bit integer used to identify an object or entity on the Internet."},"title":"Cheat sheet for 15 nmcli commands in Linux (RHEL/CentOS)"},"/utho-docs/docs/linux/command-line-internet-speed-tests-in-centos-7/":{"data":{"":"\nSpeedtest\nSpeed test is an old standby. It‚Äôs written in Python, packaged in Apt, and also available through pip. It can be used as a command-line tool or as part of a Python script.\nStep1. Login into the server using root credentials on putty .\nStep 2. Then enter the following command to install Python.\n# yum install -y python Step 3. Enter the following command after installing Python to download SpeedTest-Cli on your server.\n# # wget -O speedtest-cli [https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py](https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py) Step 4. To give the SpeedTest tool executable access, run the following command after downloading it.\n# chmod +x speedtest-cli Step 5. After you‚Äôve completed the preceding steps, type the following command to begin the speed test.\n# ./speedtest-cli Thank you!!"},"title":"Command-line internet speed tests in CentOS 7"},"/utho-docs/docs/linux/convert-rwx-permissions-to-octal-format-in-linux/":{"data":{"":"\nDescription\nYou could find that displaying the access rights of files and directories in octal form rather to the more common rwx format is more helpful at times, or you might decide that you want to show both forms.\nStat is a software that shows the status of files or filesystems and is included in the majority of recent Linux distributions, if not all of them. This utility may be used in place of the good old ls -l command.\nWhen called without any parameters but followed by the name of a specific file, the stat command will provide a significant amount of information on the directory or file. You are able to define an output format when using the stat command in conjunction with the -c option. This specific alternative is the one that piques our attention the most at the moment.\nType the following command to show all of the files in the current working directory, followed by their access privileges in octal format:\n# stat -c '%n %a' * The following is the format sequence for the aforementioned command:\n%n denotes the file name. %a denotes access rights in octal form. If you want to display the permissions in rwx format in addition, you have the alternative option of appending %a to %A, which is the parameter that is supplied to stat.\nIn that instance, simply type:\n# stat -c '%n %A' * You can include the %F format sequence in order to see the file type shown in the output.\n# stat -c '%c %F %a' You are able to specify a number of additional format sequences as well; for a full list of them, please refer to the stat man page.\n# man stat Conclusion\nWe have gone over the essential Linux programme known as stat, which allows you to display the status of a file or file system. The conversion of the rwx access rights from the standard output of ls -l into octal format was the primary focus of our efforts here.\nAs I had indicated previously, many contemporary editions of Linux now come with the stat utility already installed. You should also keep in mind that your shell might already come pre-installed with its own version of stat. As a result, you should consult the documentation that comes packaged with your shell for additional information regarding the available options and how to make use of them.\nThank You"},"title":"convert rwx permissions to octal format in Linux"},"/utho-docs/docs/linux/create-a-new-user-in-mysql-and-learn-how-to-grant-permissions/":{"data":{"":"\nIntroduction\nIn this article we will learn How To Create a New User and Grant Permissions in MySQL..\nMySQL is open-source RDBMS. As of this writing, it‚Äôs the most popular open-source database in the world and part of the LAMP stack (Linux, Apache, MySQL, and PHP).\nThis guide explains how to create a MySQL user and grant permissions.\nFollo the below steps to Create a New User and Grant Permissions in MySQL..\nPrerequisites\nYou‚Äôll need MySQL to follow this guide. This guide assumes the database is on a VPS running Ubuntu 20.04, but the principles it outlines should apply regardless of how you access your database.\nIf you don‚Äôt have a MySQL database and want to set one up yourself, see our How To Install MySQL guide. The methods for creating a new MySQL user and granting permissions are generally the same regardless of your server‚Äôs operating system.","making-a-new-user#Making a New User":"MySQL creates a root user account for database management during installation. This user has full control over the MySQL server‚Äôs databases, tables, users, etc. This account should only be used for administrative purposes. This step shows how to use the root MySQL user to create a new user account.\nIn Ubuntu systems running MySQL 5.7 (and later), the root MySQL user authenticates with the auth socket plugin by default. This plugin requires that the operating system user who invokes MySQL match the command‚Äôs MySQL user. This means you must use sudo with the mysql command to access the root MySQL user.\n#sudo mysql Note: If your root MySQL user has a password, use a different command to access the MySQL shell. The following will run your MySQL client with regular user privileges; you‚Äôll only gain administrator privileges by entering a password.\n#mysql -u root -p From the MySQL prompt, you can create a new user with CREATE USER. The syntax is as follows:\nmysql\u003e #CREATE USER 'username'@'host' IDENTIFIED WITH authentication_plugin BY 'password'; After CREATE USER, set a username. It‚Äôs followed by a @ sign and the hostname from which the user will connect. If you only plan to access this user locally, use localhost. Username and host should be enclosed in single quotes to avoid errors.\nYou can choose an authentication plugin for your user. The auth socket plugin provides strong security without requiring a password from valid users. But it prevents remote connections, which can complicate external programme interactions with MySQL.\nYou can omit WITH authentication plugin to use MySQL‚Äôs default plugin, caching sha2 password. MySQL recommends this plugin for users who want to log in with a password due to its strong security.\nRun the command to create a caching sha2 password user. Replace sammy with your preferred username and password.\nmysql\u003e #CREATE USER 'microhost'@'localhost' IDENTIFIED BY 'password'; Note:There‚Äôs a known issue with some versions of PHP and caching sha2 password. If you plan to use this database with phpMyAdmin, create a user that uses the older, but still secure, mysql native password plugin instead.\nmysql\u003e #CREATE USER 'microhost'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password'; If you‚Äôre unsure, create a user that authenticates with caching sha2 plugin and then ALTER it with this command.\nmysql\u003e #ALTER USER 'microhost'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password'; You can grant a new user privileges after creating them.","providing-authorization-to-a-user#Providing Authorization to a User":"Syntax for granting user privileges:\nmysql\u003e # GRANT PRIVILEGE ON database.table TO 'username'@'host'; PRIVILEGE in this example syntax defines what actions the user can perform on the specified database and table. You can grant multiple privileges to the same user with one command. You can also grant a user global privileges by entering * for database and table names. SQL asterisks represent ‚Äúall‚Äù databases or tables.\nFor example, the following command grants a user global privileges to CREATE, ALTER, and DROP databases, tables, and users, as well as INSERT, UPDATE, and DELETE data from any table on the server. The user can also query data with SELECT, create foreign keys with REFERENCES, and perform FLUSH operations with RELOAD. You should only give users the permissions they need, so feel free to adjust your own user‚Äôs privileges.\nMySQL documentation lists all available privileges.\nRun this GRANT statement, substituting your MySQL user‚Äôs name for microhost:\nmysql\u003e #GRANT CREATE, ALTER, DROP, INSERT, UPDATE, DELETE, SELECT, REFERENCES, RELOAD on *.* TO 'microhost'@'localhost' WITH GRANT OPTION; With GRANT OPTION is also included. This lets your MySQL user grant permissions to other users.\nWarning: Some users may want to grant their MySQL user ALL PRIVILEGES, which will give them broad superuser privileges like root‚Äôs.\nmysql\u003e #GRANT ALL PRIVILEGES ON *.* TO 'microhost'@'localhost' WITH GRANT OPTION; Such broad privileges should not be granted lightly, as this MySQL user will have access to every server database.\nMany guides suggest running FLUSH PRIVILEGES after a CREATE USER or GRANT statement to reload the grant tables and ensure new privileges take effect.\nmysql\u003e #FLUSH PRIVILEGES; According to the MySQL documentation, when you modify the grant tables indirectly with an account management statement like GRANT, the database reloads the grant tables immediately into memory, so we don‚Äôt need FLUSH PRIVILEGES. Using it won‚Äôt harm the system.\nRevoking permission is similar to granting it:\nmysql\u003e #REVOKE type_of_permission ON database_name.table_name FROM 'username'@'host'; To revoke permissions, use FROM instead of TO.\nRun SHOW GRANTS to see a user‚Äôs current permissions.\nmysql\u003e #SHOW GRANTS FOR 'username'@'host'; Just as DROP deletes databases, it deletes users.\nmysql\u003e #DROP USER 'username'@'localhost'; After creating a MySQL user and granting privileges, exit the client:\nmysql\u003e #exit To log in as your new MySQL user, type:\nmysql\u003e #mysql -u microhost -p The -p flag prompts MySQL for your password to authenticate.\nHopefully you have understood all the steps carefully to Create a New User and Grant Permissions in MySQL ..\nMust read:- https://utho.com/docs/tutorials/how-to-install-mysql-with-phpmyadmin-on-ubuntu-14-04/\nThankyou"},"title":"How To Create a New User and Grant Permissions in MySQL"},"/utho-docs/docs/linux/create-a-zabbix-action-to-deliver-an-alert-message-to-the-user/":{"data":{"":"\nDescription\nCreate a Zabbix action to deliver an alert message to the user. zabbix is a free and open-source monitoring software application for various IT components such as networks, servers, virtual machines (VMs), and cloud services. Monitoring indicators provided by Zabbix include network usage, CPU load, and disc space utilisation.\nFollow the below steps to Create a Zabbix action to deliver an alert message to the user.\nYou must first read this article so that you can appropriately configure email so that emails can be sent to the specific user:- https://utho.com/docs/tutorial/add-user-and-give-limited-permission-to-the-host-in-zabbix/","configuring-an-action#Configuring an action":"Overview\nActions are something that need to be configured if you want certain processes to take place as a result of events (like alerts being delivered, for instance).\nFollow these steps if you want to configure an action:\nNavigate to Alerts ‚Äî Actions and select the appropriate action type from the submenu (you may change the type later using the title dropdown).\n‚ÄúClick the Create action‚Äù button.\"\nCall out the action.\nSelect the conditions under which operations are performed.\nChoose which operations to do.\nGeneral action attributes:\nFinally, navigate to operations and create an email alert in the manner outlined in the following picture.\nFinally, we carefully established all of the processes if the alerts occur in Zabbix, so that you will receive email as your specified email address.\nMust read:- https://utho.com/docs/tutorial/add-user-and-give-limited-permission-to-the-host-in-zabbix/"},"title":"Create a Zabbix action to deliver an alert message to the user"},"/utho-docs/docs/linux/disable-reboot-using-ctrl-alt-del-keys-in-rhel-centos-7-8-support-internal/":{"data":{"":"\n:: In this blog we will know the process to disable the Ctrl-Alt-Del Key feature in the RHEL/CentOS system. This trick will work with both VPS \u0026 Physical Machine as well.\n:: Firstly, you need to login on your Microhost Cloud Server with your root login details.\n:: Then you need to execute the below command on system with root privilege to run.\n# systemctl mask ctrl-alt-del.target :: After successful execute this command, it‚Äôs shown below result\n:: Now you Need to login into Microhost Cloud Platform then click on Console screen to access the KVM to check whether the Ctrl-Alt-Del Key is working or not.\n:: I have Press the Ctrl-Alt-Del Key in the right hand corner but system is not rebooting, it means that this method is working to stop this action.\nThank You!"},"title":"Disable reboot using Ctrl-Alt-Del Keys in RHEL / CentOS 7/8"},"/utho-docs/docs/linux/disable-ssh-root-login-in-centos-7-support-internal/":{"data":{"":"\nToday, everyone knows that Linux systems comes with root user access and by default the root access is enabled for outside world. For security reason it‚Äôs not a good idea to have ssh root access enabled for unauthorized users. Because any hacker can try to brute force your password and gain access to your system.\nSo, its better to have another account that you regularly use and then switch to root user by using ‚Äòsu ‚Äì‚Äò command when necessary. Before we start, make sure you have a regular user account and with that you su or sudo to gain root access.\nIn Linux, it‚Äôs very easy to create separate account, login as root user and simply run the ‚Äòadduser‚Äò command to create separate user. Once user is created, just follow the below steps to disable root login via SSH.\nTo disable root login, open the main ssh configuration file /etc/ssh/sshd_config with your choice of editor.\n# vi etc/ssh/sshd_config Search for the following line in the file.\n#PermitRootLogin no Now need to untag this line (remove #) then save the file and exit.\nPemitRootLogin no\nNext, we need to restart the SSH daemon service.\n# systemctl restart ssh.service :: Now need to check the server with new ssh connection, that actually root access is working or not\nThank You."},"title":"Disable SSH root login in Centos 7"},"/utho-docs/docs/linux/download-online-resources-from-the-command-line-with-wget/":{"data":{"":"","#":"What is wget ? Wget is an internet command line utility which recovers files to the local filesystem and stores them. You can download any file accessible by HTTP, HTTPS or FTP using wget. wget gives a number of user configuration options for downloading and saving of files. It also features a recursive download function to download a set of linked resources.\nUsing wget The command wget uses the following fundamental syntax:\n[root@Microhost ~]# wget [ARGUMNET] [URL] If you use wget without any argument then it will download the file specified by the [URL] to the current directory\n[root@Microhost ~]# wget https://utho.com/docs/ai.zip This above command will download a zip file from Microhost.com website.\nDownload Content to Standard Output The -O option controls the file location and name in which the downloaded content is written. To download and save the file to the Text directory as micro.txt :\nwget -O Text/micro.txt https://utho.com/docs/assets/ai.txt\nWe have completed the Wget command section:\nThank You :)"},"title":"Download Online Resources from the Command Line with wget"},"/utho-docs/docs/linux/explanation-of-iftop-command/":{"data":{"":"","#":" Explanation of iftop command\nDescription: With iftop, it is easy to keep an eye on your network. It shows a table of how much [bandwidth](http://How to check Bandwidth consumption in Microhost panel) each host is using right now. iftop puts the pair of hosts that are causing the most traffic at the top of the list, making it easier to find the hosts that are causing the network to slow down.\nIftop shows the send and receive data transfer rates for the last two seconds, ten seconds, and forty seconds.\nIt displays bandwidth usage on an interface by host.\niftop listens to network traffic on a named interface, or on the first interface it can find that looks like an external interface if none is specified, and displays a table of current bandwidth usage by pairs of hosts.\nPrerequesties: User must be either super user or any normal user with SUDO privileges.\nHow to download iftop In Redhat/ Fedora/ CentOS:\n# yum install epel-release -y yum install iftop -y In Ubuntu/ Debian:\n# apt install iftop ","explanation-of-iftop-command#Explanation of iftop command":"To moniter the trafic on default interface of the machine\niftop After installation of iftop, the output\niftop uses the whole screen to show how the network is being used when it is running. At the top of the screen, there is a logarithmic scale for the bar graph that shows how much traffic there is.\nThe main part of the screen shows, for each pair of hosts, how fast data was sent and received over the last 2, 10, and 40 seconds. Arrows, like the ones between = and =\u003e, show the direction of data flow.\nFor example, in above screenshot, it shows traffic from ubuntu22.bbrouter to Nebula.bbrouter on the first line. This traffic averaged 1.11Kbit/s in the last 2 seconds, about 3.4 Kb/s in the last 10s, and 3.45Kb/s in the last 40s. Similarly, 160b/s in last 2 seconds, 731b/s in last 10 seconds and 731 b/s data were sent in the other direction. On the actual display, part of each line is backwards to show the average amount of traffic over the last 10 seconds. You might see something like this, where host foo sends HTTP requests to bar over and over again, and bar sends back so much data that it fills up a 2Mbit/s link.\nBy default, the pairs of hosts with the most traffic (10-second average) are shown at the top of the list.\nAt the bottom of the screen, you can see different totals, such as the peak traffic over the last 40 seconds, the total traffic transferred (after filtering), and the average transfer rates over 2 seconds, 10 seconds, and 40 seconds.\nIf you have multiple interfaces to listen, then most probably you would like to moniter the traffic on a specific interface of the machine.\niftop -i interface_name At the bottom of the iftop output:\nReceive and Sent data\nAt the bottom of the screen is a section that shows the overall traffic statistics.\nTX: The amount of traffic that was sent. Iftop command shows a cummulative or in simple words, total amount of bandwidth used by each type of traffic (cum).\nRX: Traffic amount received,\nTOTAL: the grand total of the traffic( TX plus RX) .\nPEAK: the peak amount of bandwidth used.\nAVG: the average amount of bandwidth used over the last 2, 10, and 40 seconds (avg).\nTo see options, which you can use to monitor your network traffic more effectively.\n# iftop -h "},"title":"Explanation of iftop command"},"/utho-docs/docs/linux/explore-metabase-data-using-mysql/":{"data":{"":"\nIntroduction\nMetabase gives you a way to ask questions about data in your browser. Metabase not only lets you run SQL queries, but it also lets you analyse data without SQL, make dashboards, and track metrics. This guide explains how to connect MySQL to Metabase and then use a reverse proxy to deploy on NGINX.\nFrom SQLite to PostgreSQL, there are a number of other databases that can be used. An easy-to-use interface makes it very easy to see the results. This makes Metabase a flexible way to share data, even with people who don‚Äôt know much about analysis.","get-the-metabase#Get the Metabase":"Metabase lets you get the jar file:\n#wget http://downloads.metabase.com/v0.28.1/metabase.jar Transfer the file into the /var directory so that it will be ready to run after a restart:\n#sudo mv metabase.jar /var/metabase.jar Using NGINX‚Äôs Reverse Proxy ","if-you-are-using-firewall#If you are using firewall":"The use of UFW is highly recommended for securing your database against unwanted access. It is a prudent practise to leave ports 80/443 and SSH open by default:\n#sudo ufw allow http #sudo ufw allow https #sudo ufw allow ssh #sudo ufw enable Check the firewall rules:\n#sudo ufw status **To access the metabase hit your IP on the browser**\nThank you","install-metabase#Install Metabase":"Java Runtime Environment\nInstall software-properties-common to add new repositories quickly:\n#sudo apt-get install software-properties-common Add the Java PPA:\n#sudo add-apt-repository ppa:microhost/java Update the list of sources:\n#sudo apt-get update Install the Java JDK 8:\n#sudo apt-get install oracle-java8-installer ","install-nginx#Install NGINX":" #sudo apt install nginx Create a new NGINX configuration file with the following values, substituting your FDQN or public IP address for server name:\nFile: /etc/nginx/conf.d/metabase.conf server_name _; location / { proxy_pass http://microhost:3330/; proxy_redirect http://microhost:3330/ $scheme://$host/; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } Make sure there are no problems with the setup:\n#sudo nginx -t Restart NGINX:\n#sudo systemctl restart nginx ","mysql-server#MySQL Server":"Install MySQL Server. Enter the root password if prompted:\n#sudo apt install mysql-server Connect as the root user:\n#mysql -u root -p Make a new user and database in Metabase.\n#CREATE DATABASE employees; #CREATE USER 'metabase_user' IDENTIFIED BY 'password'; #GRANT ALL PRIVILEGES ON employees.* TO 'metabase_user'; #GRANT RELOAD ON *.* TO 'metabase_user'; #FLUSH PRIVILEGES; #exit; ","start-metabase-on-reboot#Start Metabase on reboot":"Check where your JDK binary is located:\n#which java This should output a path like /usr/bin/java when done correctly.\nMake sure that Metabase is started up properly by creating a systemd configuration file. ExecStart= should be set to the JDK path from the previous section. Make careful to substitute User with your own Unix username when you run this:\nFile: /etc/systemd/system/metabase.service Unit Description=Metabase server After=syslog.target After=network.target[Service] User=username Type=simple Service ExecStart=/usr/bin/java -jar /var/metabase.jar Restart=always StandardOutput=syslog StandardError=syslog SyslogIdentifier=metabase Install WantedBy=multi-user.target Make the necessary adjustments:\n#sudo systemctl start metabase Verify if Metabase is operational:\n#sudo systemctl status metabase "},"title":"Explore Metabase data using MySQL"},"/utho-docs/docs/linux/find-multiple-ways-to-user-account-info-and-login-details-in-linux/":{"data":{"":"\nDescription\nIn this article we will will learn Find multiple Ways to User Account Info and Login Details in Linux\nThis article will demonstrate multiple¬†helpful methods that can be used to locate information regarding users on a Linux system. In this section, we will cover commands that can be used to retrieve information on a user‚Äôs account, show login data, and find out what people are doing on the system.\nIf you wish to add users in Linux, use the useradd programme. If you want to alter or change any properties of a user account that has already been created, use the usermod utility using the command line as outlined in the following guides:\nThe commands that allow us to find a user‚Äôs account information will be covered first, followed by an explanation of the commands that allow us to access login information.\nFollow the below steps to Find multiple Ways to User Account Info and Login Details in Linux","getent-command#getent Command":"The getent tool is a command line programme that can retrieve entries from Name Service Switch (NSS) libraries that are stored in a particular system database.\nFollow the steps below using the passwd database and the username to obtain information about a user‚Äôs account.\n# getent passwd microhost ","grep-command#grep Command":"grep command is a strong example looking through device available on the vast majority Linux frameworks. You can utilize it to find data about a particular client from the framework accounts record:/etc/passwd as displayed underneath.\n# grep -i microhost /etc/passwd ","groups-command#groups Command":"This is an example of how the groups command may be used to display all of the groups to which a user belongs.\n# groups microhost ","id-command#id Command":"id is a straightforward command line application that displays accurate and useful user and group IDs in the format described below.\n# id microhost ","last-or-lastb-commands#last or lastb commands":"The output of the last and lastb commands is a list of the most recently logged-in users on the system.\n# last or\n# last -a Use the -p option in the following way to display a list of all the users who were online at a particular time and date.\n# last -ap now ","lastlog-command#lastlog Command":"The lastlog command can be used to determine the particulars of a recent login for all users or for a specific user, as seen below:\n# lastlog -u username On a Linux computer, we have discussed a variety of methods that can be used to locate information about users and login data.\nThis article shows several ways to find Linux user information. We‚Äôll discuss commands to get user account information, login data, and system activity in this section.\nI really hope that you‚Äôve got all of those steps down to Find multiple Ways to User Account Info and Login Details in Linux\nMust Read :- https://utho.com/docs/tutorial/4-effective-ways-to-determine-the-name-of-a-plugged-usb-device-in-linux/\nThank You","lslogins-command#lslogins Command":"lslogins command shows data about known users in the framework, the - u flag just shows user accounts.\n# lslogins -u ","users-command#users Command":"The users command displays the usernames of any and all users who are currently logged on to the system in this format.\n# users ","w-command#w Command":"The w command displays all users that are currently logged on to the system as well as the tasks that they are performing.\n# w ","who-command#who Command":"The who command is used to display a list of users who are currently logged on to the system, along with the terminals from which they are connecting.\n# who -u "},"title":"Find multiple Ways to User Account Info and Login Details in Linux"},"/utho-docs/docs/linux/find-out-all-live-hosts-ip-addresses-connected-on-network-in-linux/":{"data":{"":"","description#\u003cstrong\u003eDescription\u003c/strong\u003e":"\nDescription There are numerous network monitoring tools available in the Linux ecosystem that may create a summary of the overall number of devices on a network, including all of their IP addresses and other information.\nHowever, sometimes all you need is a simple command line programme that can supply you with the same information with a single command.\nThis guide will show you how to find all live hosts‚Äô IP addresses that are linked to a specific network. We will use the Nmap utility to discover all IP addresses of devices connected to the same network.\nNmap (short for Network Mapper) is an open source, powerful, and extremely versatile command line programme for exploring networks, performing security scans, network auditing, and discovering open ports on remote machines, among other things.\nIf you do not already have Nmap installed on your machine, use the corresponding command for your distribution below to install it:\nsudo yum install nmap -y Once you‚Äôve installed Nmap, here‚Äôs how to use it: # nmap [scan type‚Ä¶] options {target specification} Where hostnames, IP addresses, networks, and so on can be substituted for the parameter target specification.\nTo list the IP addresses of all hosts connected to a specific network, first identify the network and its subnet mask using the ifconfig or ip command, as shown below:\n# ifconfig or\n# ip addr show After that, do the following Nmap command:\n# nmap -sn your nmap address/24 Within the preceding command:\nThe type of scan is -sn, which stands for ping scan. Nmap scans ports by default, however this scan disables port scanning. The target network is your nmap address/24; replace it with the IP address range of your own network. Make an effort to read at the Nmap man page for complete instructions on how to use the programme:\n# man nmap Alternately, to get a summary of the following use information, run Nmap without any options or arguments:\n# nmap Thank You"},"title":"Determine All IP Addresses of Live Hosts Connected to the Network in Linux"},"/utho-docs/docs/linux/firewalld-with-centos-7/":{"data":{"":"\nFirewall is a software program that implements a set of rules that allows or rejects data packets in a network. Firewalld is a dynamic firewall management tool. In this article , you will learn to setup and manage firewall on your server.","install-and-enable-firewall#Install and enable Firewall":" yum install firewalld systemctl enable firewalld sudo reboot To know the status of firewall by this command\nsudo firewall-cmd --state ``` OR systemctl status firewalld\nIf output is showing ‚Äúrunning‚Äù this indicates that our firewall is up and running with the default configuration. To reload a Firewall configuration: sudo firewall-cmd ‚Äìreload\n## Disable and stop firewall systemctl stop firewalld\nsystemctl disable firewalld\n## Firewall zone Zones are set of rules that allow a specific traffic and network services based on various trust of levels. Each zone allows different type of services and ports.¬†When we enable FirewallD first time then ‚Äúpublic‚Äù will be default zone. **Drop Zone -** This is the lowest level of trust. Only outgoing connections are allowed in this zone. All incoming connection are dropped without any reply. **Block zone -** Block zone is similar to drop zone. The only difference is, if we use this incoming request is rejected with **icmp-host-prohibited** message. **Public zone -** Selected connections are accepted by defining rules in this zone other connections will be dropped.¬†**External zone -** This zone is for external networks with masquerading is enabled other connections will be rejected only specified connection will be allowed. **DMZ zone -** Demilitarized zone provides limited access to your internal network. Only specified connections will be allowed. **Work Zone -** This zone is used for work areas. Most of the computers are trusted in the network. Only specified connections will be allowed. **Home zone -** This zone is used for home areas. Few more connections are allowed. **Internal zone -** This zone is used for internal networks with selected connections allowed. **Trusted zone -** Trust all the network connections. If we set this zone then all traffic will be accepted. To know about the default zone firewall-cmd ‚Äìget-default-zone\nTo know which zone is active currently by typing this command- firewall-cmd ‚Äìget-active-zone\nTo change the default zone sudo firewall-cmd ‚Äìset-default-zone=internal\nTo list all active services, ports, rules using this command- sudo firewall-cmd ‚Äìlist-all\n## Firewalld configuration Firewalld Services is configured with XML files and located at `/usr/lib/firewalld/services/` and `/etc/firewalld/services/` directories. Firewalld have two configuration sets- 1. Runtime 2. Permanent **Runtime -** If we use runtime configuration the changes will not retained after restarting firewalld or reboot the server. Add the permanent rule to both http and https service. sudo firewall-cmd ‚Äìzone=public ‚Äìadd-service=http\nsudo firewall-cmd ‚Äìzone=public ‚Äìadd-service=https\n**Permanent -** Permanent configuration changes will be retained after reboot. sudo firewall-cmd ‚Äìzone=public ‚Äìadd-service=http\nsudo firewall-cmd --zone=public --add-service=https ``` By default, firewall-cmd command apply the runtime configuration but when we use --permanent flag it will create permanent configuration. For adding a permanent rule, we can use this method. ``` sudo firewall-cmd --zone=public --add-service=http --permanent ``` ``` sudo firewall-cmd --zone=public --add-service=http ``` To add service SMTP(port 25) and SMTPS(port 465) ``` firewall-cmd --zone=public --add-service=smtp --permanent ``` ``` firewall-cmd --zone=public --add-service=smtps --permanent ``` ## Firewall Rules ### Opening and removing a Port for your Zones To open a specific port to run any application ``` sudo firewall-cmd --permanent --zone=public --add-port=80/tcp ``` Output - success ``` sudo firewall-cmd --permanent --zone=public --add-port=21/tcp ``` Output - success We can open a range of ports by using this command. ``` sudo firewall-cmd --permanent --add-port=200-300/tcp ``` You can check all open ports by using this command - ``` sudo firewall-cmd --zone=public --list-ports ``` To remove an added port use -remove by this command- ``` firewall-cmd --zone=public --remove-port=5000/tcp ``` Adding and removing ports using firewalld - ``` firewall-cmd --zone=public --add-service=ftp ``` ``` firewall-cmd --zone=public --remove-service=ftp ``` ``` firewall-cmd --zone=public --list-services ``` ## Advanced Configuration Services and ports are fine for simple configuration but for advanced scenarios can be too restricted. Rich rules and Direct Interface allow you to add custom firewall rules for any port, protocol, address and action in any region. ### Rich Rules Rich rules syntax is comprehensive but completely documented on the firewalld.richlanguage(5) man page (or in your terminal see man firewalld.richlanguage). To control them, use ‚Äîadd-rich-rule, ‚Äîlist-rich-rules and ‚Äîremove-rich-rule with the command firewall-cmd. Allow all IPv4 traffic from host 192.0.2.0 ``` sudo firewall-cmd --zone=public --add-rich-rule 'rule family=\"ipv4\" source address=192.0.2.0 accept' ``` Deny IPv4 traffic over TCP from host 192.0.2.0 to port 22 ``` sudo firewall-cmd --zone=public --add-rich-rule 'rule family=\"ipv4\" source address=\"192.0.2.0\" port port=22 protocol=tcp reject' ``` To list your current Rich Rules in the public zone: ``` sudo firewall-cmd --zone=public --list-rich-rules ``` Thankyou "},"title":"How to Setup and Configure FirewallD on CentOS 7"},"/utho-docs/docs/linux/getting-started-with-selinux/":{"data":{"":"","before-you-begin#Before You Begin":" SELinux is a security control system, which can make your system vulnerable by a small misconfiguration. Upgrade the packages of your system # yum update ","installation-of-selinux-on-centos-7#Installation of SELinux on Centos 7":"Command to see the default packages of SELinux:\n#`[root@Microhost ~]# rpm -aq | grep selinux` The output would be shown as given below:\n#`[root@Microhost ~]# rpm -aq | grep selinux` libselinux-python-2.5-14.1.el7.x86_64 libselinux-2.5-14.1.el7.x86_64 libselinux-utils-2.5-14.1.el7.x86_64 Install the SELinux packages with the command given below :\n# [root@Microhost ~]# `yum install policycoreutils policycoreutils-python selinux-policy selinux-policy-targeted libselinux-utils setools setools-console` You can install some optional daemons of SELinux, They are Setroubleshoot-servers and mctrans. Among other things, the setroubleshoot-server allows e-mail notifications from the server to notify you of any breach of policy. The mctrans daemon translates the SELinux output to the human-readable text.","introduction-to-selinux#Introduction to SELinux":"SELinux( Security Enhanced Linux) is a security system which is developed by the United States National Security Agency for an additional layer of system security. Selinux was integrated in 2003 with linux kernel it also ships with most of the Linux distributions.\nThe standard access policy based on the users, groups, and other permissions, known as Discretionary Access Control (DAC). However, SELinux implements Mandatory Access Control (MAC).\nDiscretionary access control (DAC) does not enable system administrators to develop detailed and fine-grained security policies, such as limiting existing applications to viewing only log files, while allowing additional applications to add new data to log files Mandatory Access Control (MAC) act upon SELinux context. This security policy is centrally controlled by a security policy administrator, users do not have the ability to override the policy even if they have root privileges. Ideally, it should be trusted by the person with root access. However, if safety was affected, the system was affected too. SELinux and MACs solve this problem both by confining preferential processes and by automating the creation of security policies.\nSELinux denies anything not explicitly permitted by default. SELinux has two global modes, Enforcement and Permissive modes. Permissive mode enables the system to operate as a discretionary access control system while logging any SELinux violation. The mode of enforcement imposes a strict denial of access to anything not explicitly permitted. You, as the system administrator, must write policies to specifically allow certain behavior on a machine.\n[ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]You are not advised to disable SELinux. However, if you want you can disable SELinux.[/ht_message]","modes-of-selinux#Modes of SELinux":"SELinux offers two modes : **Enforcing**¬†and¬†**Permissive**:\nEnforcing: In Enforcing mode, SELinux implements strict system policies. Things would not be permitted under any circumstances if they are not allowed.\nPermissive: In Permissive mode, Your system is not SELinux-protected instead, SELinux only records any violations into the log file.\nYou can check the mode of SELinux by the command:\n[root@Microhost ~]# getenforce Enforcing You can enable the SELinux if it is disabled as shown below:\n[root@Microhost ~]# getsebool getsebool: SELinux is disabled You can enable the SELinux in any mode( Enforcing | Permissive ) by editing the file /etc/selinux/config as shown below:\n[root@Microhost ~]# vi /etc/selinux/config This file controls the state of SELinux on the system. SELINUX= can take one of these three values: enforcing - SELinux security policy is enforced. permissive - SELinux prints warnings instead of enforcing. disabled - No SELinux policy is loaded.\nSELINUXTYPE= can take one of three two values: targeted - Targeted processes are protected, minimum - Modification of targeted policy. Only selected processes are protected. mls - Multi Level Security protection.\n~ \"/etc/selinux/config\" 14L, 546C You have to change the line SELINUX=disabled to SELINUX=permissive or SELINUX=Enforcing as per your requirement. More¬†information¬†can¬†also¬†be¬†obtained¬†by¬†using sestatus :\n[root@Microhost ~]# sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: permissive Policy MLS status: enabled Policy deny_unknown status: allowed Max kernel policy version: 28 You need to set SELinux to permissive mode to implement policies on your system. You need to restart your system after changing SELinux mode.\n[root@Microhost ~]# setenforce 0 [root@Microhost ~]# getenforce Permissive [root@Microhost ~]# reboot Permissive mode has been enebled for SELinux,you can see the log of privacy violations by using:\ngrep \"selinux\" /var/log/messages The¬†output will¬†look¬†much¬†like¬†that\n[root@Microhost ~]# grep \"selinux\" /var/log/messages li482-93 yum[4572]: Updated: selinux-policy-3.13.1-102.el7_3.16.noarch li482-93 yum[4572]: Updated: selinux-policy-targeted-3.13.1-102.el7_3.16.noarch li482-93 systemd: Removed slice system-selinux\\x2dpolicy\\x2dmigrate\\x2dlocal\\x2dchanges.slice. li482-93 systemd: Stopping system-selinux\\x2dpolicy\\x2dmigrate\\x2dlocal\\x2dchanges.slice. li482-93 systemd: Removed slice system-selinux\\x2dpolicy\\x2dmigrate\\x2dlocal\\x2dchanges.slice. li482-93 systemd: Stopping system-selinux\\x2dpolicy\\x2dmigrate\\x2dlocal\\x2dchanges.slice. li482-93 kernel: EVM: security.selinux li482-93 kernel: EVM: security.selinux li482-93 kernel: EVM: security.selinux You can edit the file /etc/selinux/config to modify the default configuration of SELinux.\n[root@Microhost ~]# vi /etc/selinux/config This file controls the state of SELinux on the system. SELINUX= can take one of these three values: enforcing - SELinux security policy is enforced. permissive - SELinux prints warnings instead of enforcing. disabled - No SELinux policy is loaded.\nSELINUXTYPE= can take one of three two values: targeted - Targeted processes are protected, minimum - Modification of targeted policy. Only selected processes are protected. mls - Multi Level Security protection.\nAfter changing the state of SELinux, reboot the machine/server for the changes to take effect.","selinux-boolean#SELinux Boolean":"A SELinux Boolean variable can be activated and disabled without reloading or recompiling a SELinux policy. Use the getsebool -a command to view the list of boolean variables. It‚Äôs a long list, so that the results can be pipeed by grep\n[root@Microhost ~]# getsebool -a | grep xdm xdm_bind_vnc_tcp_port --\u003e off xdm_exec_bootloader --\u003e off xdm_sysadm_login --\u003e off xdm_write_home --\u003e off You can use the setsebool command to change the value of any variable. The you will use argument -P flag then setting will remain same after the reboot. You need to edit boolean policies variable if you want a service like openVPN to run unconfined on your system:\n[root@Microhost ~]# getsebool -a | grep \"vpn\" openvpn_can_network_connect --\u003e on openvpn_enable_homedirs --\u003e on openvpn_run_unconfined --\u003e off [root@Microhost ~]# setsebool -P openvpn_run_unconfined ON [root@Microhost ~]# getsebool -a | grep \"vpn\" openvpn_can_network_connect --\u003e on openvpn_enable_homedirs --\u003e on openvpn_run_unconfined --\u003e on Now, even in active enforcement mode, you can use OpenVPN . Set your system up and allow SELinux to protect your system.\n[root@Microhost ~]# setenforce 1 [root@Microhost ~]# getenforce Enforcing ","selinux-contexts#SELinux Contexts":"Files and processes are marked with a SELinux context which provides additional information such as the user of SELinux, role, type and, optionally,a level. All this information is used when running SELinux to make decisions on access control. SELinux combines Role-Based Access Control, Type Enforcement , as well as Multileive Security (MLS).\nUse the below given command to view the SELinux context of files and directories:\n[root@Microhost ~]# ls -Z linux.txt -rwxrw-r-- root root unconfined_u:object_r:user_home_t:s0 linux.txt As per above output -Z context flag shows the SELinux security context of any file. Output¬†**unconfined_u**¬†is a user role .\nThank You ! :)"},"title":"Getting Started with SELinux"},"/utho-docs/docs/linux/how-do-i-find-my-apache-version-in-plesk/":{"data":{"":"","#":"","1login-to-plesk-login-to-plesk-as-an-administrator#\u003cstrong\u003e1.login to Plesk. Login to Plesk as an administrator\u003c/strong\u003e":"","2click-tools--settings#2.\u003cstrong\u003eClick ‚ÄúTools \u0026amp; Settings\u003c/strong\u003e":"","3click-erver-components#3.\u003cstrong\u003eClick erver Components\u003c/strong\u003e":"","from-this-page-you-can-view-all-of-the-available-versions-and-components#\u003cstrong\u003eFrom this page, you can view all of the available versions and components.\u003c/strong\u003e":"\nIntroduction In this article you will know about to find my Apache version in Plesk, In its capacity as a Web server, Apache is responsible for handling requests for directories sent in via the HTTP protocol from users of the Internet and delivering the requested information to those users in the form of files and Web pages. The majority of the software and code that powers the Web is created with Apache‚Äôs features and functionality in mind.\nPlesk: Plesk is a web hosting platform that comes with a control panel that enables the administrator to set up websites, reseller accounts, e-mail accounts, DNS, and databases using a web browser. Integration of support for content management systems (CMS) is included in Plesk. These content management systems include WordPress, Joomla, and Drupal, among others.\n**Follow the instructions below to check the version of Apache you are using.**\n1.login to Plesk. Login to Plesk as an administrator\n2.Click ‚ÄúTools \u0026 Settings\n3.Click erver Components\nCheck the component versions by following the instructions from above.\n1.login to Plesk. Login to Plesk as an administrator 2.Click ‚ÄúTools \u0026 Settings 3.Click erver Components From this page, you can view all of the available versions and components. When it comes to web hosting and website management, Plesk utilises web servers. Plesk for Linux makes use of the Apache HTTP Server (http://httpd.apache.org/), which is then paired with the nginx reverse proxy server so that it may achieve higher levels of performance. IIS is the HTTP Server that is used by Plesk for Windows Why we utilise Plesk?\nFind more information about VPS Control Panels here. Web designers may construct websites, test them, and demonstrate the results to customers by using the Plesk control panel as a hosting platform to do so. It offers complete control over their server backup as well as the PHP settings, both of which are essential for a site administrator.\nThank You "},"title":"How do I find my Apache version in Plesk"},"/utho-docs/docs/linux/how-do-we-install-mysql-workbench-on-ubuntu-18-04/":{"data":{"":"\nDescription\nFor MySQL servers and databases, MySQL Workbench is a graphical and visual database designing tool. It has been purposely developed with the intention of assisting database administrators, developers, and database architects in graphically designing, generating, modeling, and managing databases. Working with databases is intended to be a breeze using MySQL Workbench because to its user-friendly and uncomplicated interface. It is possible to create several models inside the same environment and it supports all objects, including as tables, views, triggers, and stored procedures. Additionally, it includes this capability.\nIn this tutorial, we will install MySQL server on one Ubuntu 18.04 server, install MySQL workbench on another Ubuntu 18.04 desktop, and then connect MySQL server with the interface provided by MySQL Workbench.\nPrerequisites\nBringing your system up to date:\n#sudo apt-get update Get MySQL Workbench installed\n*Utilizing the APT package manager, install MySQL Workbench:\n#sudo apt install mysql-workbench Start MySQL‚Äôs development environment.\nBegin using MySQL Workbench from the command prompt:\n#mysql-workbench Utilize the ‚Äîhelp option while working from the command line interface to obtain a full list of available launch options:\n/usr/bin/mysql-workbench --help When started for the first time, MySQL Workbench presents the following greeting screen:\ninstalled successfully‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ continue reading¬†Conclusion\nMySQL Workbench is a database design tool for MySQL servers and databases. It was created with the objective of enabling database administrators, developers, and database architects in graphically creating, producing, modelling, and maintaining databases. Because of its user-friendly and straightforward interface, MySQL Workbench is designed to make database administration a snap. It is possible to create many models inside the same environment, and it supports all objects, such as tables, views, triggers, and stored procedures. It also has this potential.\nIn this tutorial, we will install MySQL server on one Ubuntu 18.04 server, MySQL Workbench on another Ubuntu 18.04 desktop, and then connect MySQL server to the interface offered by MySQL Workbench.\nThankyou"},"title":"How do we install MySQL Workbench on Ubuntu 18.04?"},"/utho-docs/docs/linux/how-to-access-centos-terminal-by-browser-shellinabox/":{"data":{"":"","1--installation#1- Installation":"","configure-shellinabox#Configure Shellinabox":"Step 2: The configuration file has a different name and directory in which it is present. /etc/sysconfig/shellinaboxd\n# vi /etc/sysconfig/shellinaboxd content of /etc/sysconfig/shellinaboxd\nMake sure to have the SSH option in the OPTS entry in the configuration file.","overview#Overview:":"","points-to-remember#Points to remember:":" For security reasons, we should change the default port number of shellinabox service. Make sure to enable the shellinabox port in your firewall( if you are using one) ","prerequisites#Prerequisites:":"","systemctl-enable-shellinaboxd#systemctl enable shellinaboxd":" ","systemctl-start-shellinaboxd#systemctl start shellinaboxd":" How to access CentOS terminal by browser: Shellinabox\nBefore learning how to access CentOS terminal by browser: Shellinabox. Markus Gutschke made Shell In A Box, which you say as ‚Äúshellinabox.‚Äù It is a web-based terminal emulator. It has a built-in web server that runs as a web-based SSH client on a specified port and gives you a web terminal emulator to access and control your Linux Server SSH Shell remotely using any AJAX/JavaScript and CSS enabled browsers without the need for additional browser plugins like FireSSH.\nIn this tutorial, we will see how to install Shellinabox and use a modern web browser on any machine to connect to a remote SSH terminal. When you are behind a firewall and only HTTP(s) traffic can get through, Web-based SSH is very helpful.\nOverview: Shellinabox is included by default on many Linux distributions, such as Debian, Ubuntu, and Linux Mint, through their default repositories. But in CentOS we need to install extra repository for enterprises Linux.\nPrerequisites: Super user( root) or any normal user with SUDO privileges. Yum or package repositories enabled and available to install packages. 1- Installation Step 1: To install the Shellinabox on your CentOS server, first install the epel-repolistory on it. To do so, use the below command.\n# yum install epel-release -y And now you can install the shellinabox on your machine.\n# yum install shellinabox -y systemctl start shellinaboxd ","verify-the-port-and-running-status-of-shellinabox#Verify the port and running status of Shellinabox":" # netstat -tunlp | grep shellinaboxd tcp 0 0 0.0.0.0:4200 0.0.0.0:* LISTEN 1908/shellinaboxd Now, open your web browser and go to https://Your-IP-Address:4200. You should be able to see an SSH terminal that runs on the web. When you log in with your username and password, you should see your shell prompt.\nBrowser‚Äôs first page\nStep 3: Now, when you do just as mentioned in above step, you will be asked to go advanced and proceed to your serverip. After proceeding further, you will see a blank screen asking for the login password.\nEnter here, your login username and password to continue using your terminal in browser.\nShellinabox terminal logged in\nHere you go, accessing your terminal of centos machine using shellinaboxd utility\nIn this tutorial, you have learned- How to access CentOS terminal by browser: Shellinabox\nAlso read: How to install Composer on Ubuntu 20.04, How to install Drupal on CentOS server"},"title":"How to access CentOS terminal by browser: Shellinabox"},"/utho-docs/docs/linux/how-to-access-ubuntu-terminal-by-browser-shellinabox/":{"data":{"":"","1--installation#1- Installation":"","2--configure-shellinabox#2- Configure Shellinabox":"The configuration file has a different name and directory in which it is present as you go from one Linux flavour to another. In Debian the configuration file can be found as- /etc/sysconfig/shellinaboxd\n# vi /etc/default/shellinabox content of /etc/defaults/shellinaboxd","3--points-to-remember#3- Points to remember:":" For security reasons, we should change the default port number of shellinabox service. Make sure to enable the shellinabox port in your firewall( if you are using one) ","4--verify-the-port-and-running-status-of-shellinabox#4- Verify the port and running status of Shellinabox":"Using the netstat command, you can see the all the listening tcp and udp ports of your server.\n# netstat -tunlp | grep shellinaboxd Output: tcp 0 0 0.0.0.0:4200 0.0.0.0:* LISTEN 70563/shellinaboxd\nNow, open your web browser and go to https://Your-IP-Address:4200. You should be able to see an SSH terminal that runs on the web. When you log in with your username and password, you should see your shell prompt.\nAfter you hit the browser with your serverip and port number, you will see a warning page on which it is showing a security risk. This warning is showing because of lack of ssl on your server for this site. To ignore this, press ‚ÄòAdvanced‚Äô and then ‚ÄòAccept and Continue‚Äô\nsecurity page of browser\nAfter this, you will see a blank white screen prompt which will ask you the username similar to which you see on your ssh prompt. Here, enter the username, from which you want to login. After username it will ask you the password.\nFor entering the password, you can enter it either by typing manually or just right click on your browser screen and select ‚ÄòPaste from browser‚Äô and paste the password on the next windows.\nTo paste password on your browser\nAfter entering the password on the below like screen, either press enter or click on ‚Äòok‚Äô\nclick ‚Äòok‚Äô to paste the content\nAfter successfully loging in to your server, you can use your browser same as you use your ssh login.\nlogin prompt of the server\nIn this tutorial, you have learnt how to access Ubuntu terminal by browser: Shellinabox\nAlso Read: How to Install NGINX in Debian 10, How To Create Redirects with Nginx","overview#Overview:":"","prerequisites#Prerequisites:":"","systemctl-enable-shellinabox#systemctl enable shellinabox":" ","systemctl-start-shellinabox#systemctl start shellinabox":" How to access Ubuntu terminal by browser: Shellinabox\nIn this tutorial, we will learn how to access Ubuntu terminal by browser: Shellinabox. Before access Ubuntu terminal by browser, we need to how we will do that. Markus Gutschke made Shell In A Box, which you say as ‚Äúshellinabox.‚Äù It is a web-based terminal emulator. It has a built-in web server that runs as a web-based SSH client on a specified port and gives you a web terminal emulator to access and control your Linux Server SSH Shell remotely using any AJAX/JavaScript and CSS enabled browsers without the need for additional browser plugins like FireSSH.\nIn this tutorial, we will see how to install Shellinabox and use a modern web browser on any machine to connect to a remote SSH terminal. When you are behind a firewall and only HTTP(s) traffic can get through, Web-based SSH is very helpful.\nOverview: Shellinabox is included by default on many Linux distributions, such as Debian, Ubuntu, and Linux Mint, through their default repositories.\nPrerequisites: Super user( root) or any normal user with SUDO privileges. apt repositories enabled and available to install packages. 1- Installation To install the Shellinabox on your Ubuntu server, first update the apt repository on it. To do so, use the below command.\n# apt update And now you can install the shellinabox on your machine.\n# apt install shellinabox -y systemctl start shellinabox "},"title":"How to access Ubuntu terminal by browser: Shellinabox"},"/utho-docs/docs/linux/how-to-add-a-swap-file-in-linux/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to add a swap file in linux.\nAlso read: How to prevent a user from login in Linux\nThank You üôÇ","how-can-i-display-swap-usage-summary-on-linux#How can I display swap usage summary on Linux?":"Type the following swapon command:\n#swapon -s ","how-do-i-verify-linux-swap-file-is-activated-or-not#How do I verify Linux swap file is activated or not?":"Simply use the free command:\n#free -m ","introduction#Introduction":"In this article, you will learn how to add a swap file in linux.\nA swap file is¬†a system file that creates temporary storage space on a solid-state drive or hard disk when the system runs low on memory. The file swaps a section of RAM storage from an idle program and frees up memory for other programs.\nThe term ‚Äúswap space‚Äù can refer to either a partition on the disc or a file. During the installation process, users have the ability to create a swap space, or at any other point in the future if they so want. There are two applications for swap space: the first is to extend the virtual memory beyond the amount of installed physical memory (RAM), and the second is to provide support for suspending to disc.","step-1--login-as-the-root-user#Step 1 ‚Äì Login as the Root User":"","step-2--create-storage-file#Step 2 ‚Äì Create Storage File":"To make a 512MB swap file (1024 * 512MB = 524288 block size), run the following command:\n#dd if=/dev/zero of=/swapfile1 bs=1024 count=524288 ","step-3--secure-swap-file#Step 3 ‚Äì Secure swap file":"For security reasons, enter the following file permissions:\n#chown root:root /swapfile1 #chmod 0600 /swapfile1 A globally accessible swap file is a major local vulnerability. The commands above ensure that only the root user can read and write to the file.","step-4--set-up-a-linux-swap-area#Step 4 ‚Äì Set up a Linux swap area":"To create a swap file in Linux, enter the following command:\n#mkswap /swapfile1 ","step-5--enabling-the-swap-file#Step 5 ‚Äì Enabling the swap file":"Finally, immediately activate the /swapfile1 swap space by typing:\n#swapon /swapfile1 ","step-6--update-etcfstab-file#Step 6 ‚Äì Update /etc/fstab file":"Adding a line to the /etc/fstab file will make /swapfile1 accessible following a reboot of the Linux system. To read this file, launch a text editor like vi.\n#vi /etc/fstab Please add the following clause:\n/swapfile1 none swap sw 0 0\nUsing escape colon wq, save the file and close it."},"title":"How to add a swap file in Linux"},"/utho-docs/docs/linux/how-to-add-a-user-and-grant-root-privileges-on-centos-7/":{"data":{"":"","introduction#Introduction":"On CentOS 7, you will learn how to Add a User and Grant Root Privileges by reading the following article. One of the responsibilities of a system administrator is to be responsible for adding users and providing those users with administrative privileges. After a user has been added to CentOS and granted root access, that user will be able to log in to CentOS and carry out activities that are system-level in nature. After that, they will be able to execute tasks with elevated privileges by prefixing them with the sudo command. You will learn how to create a new user and give them administrative access by following the instructions in this concise guide.\nA user group known as the ‚Äúwheel‚Äù group is present in CentOS 7 when the operating system is first installed. The powers of sudo are automatically granted to anyone who is a member of the wheel group. Providing a user with sudo capabilities can be accomplished in a brisk and uncomplicated manner by just adding that user to this group.","step-1-i-will-add-a-user-that-is-microhost#Step 1: I will add a user that is microhost.":" # adduser microhost ","step-2-now-set-the-password-for-the-new-user#Step 2: Now set the password for the new user:":" # passwd microhost ","step-3-grant-root-privileges-to-the-user#Step 3: Grant Root Privileges to the User":" # visudo The above command takes us to the /etc/sudoers.tmp file, where we can see the code below:\nYou will add the line for your new user in the same format immediately following the line for the root user. Because of this, we will be able to provide that user administrative privileges.\nTo save your changes and quit the current file, press escape :wq.\nIf you have followed the procedures in the previous section, you should now have a user named microhost who has the ability to use sudo to run commands with root privileges.\nAlso read: How To Install MariaDB 10.7 on CentOS 7\nThank You üôÇ"},"title":"How to Add a User and Grant Root Privileges on CentOS 7"},"/utho-docs/docs/linux/how-to-add-a-user-and-grant-root-privileges-on-ubuntu-18-04/":{"data":{"":"","conclusion#conclusion":"Hopefully, you have learned how to add a user and grant root privileges on Ubuntu 18.04.\nAlso read: How to prevent a user from login in Linux\nThank You üôÇ","introduction#Introduction":"In this article you will learn, how to add a user and grant root privileges on Ubuntu 18.04.\nA system administrator‚Äôs duties can include adding users and giving them administrative access. Once a user is added and given root access, they will be able to log in to your Ubuntu VPS and execute system-level tasks. After that, they can issue commands with enhanced privileges by prefixing them with sudo. With this brief guide, you‚Äôll learn how to create a new user and give them administrative privileges.","step-1-i-will-add-a-user-that-is-microhost#Step 1: I will add a user that is microhost":" # adduser microhost Enter the new value, or press ENTER for the default\nFull Name []: press ENTER\nRoom Number []: press ENTER\nWork Phone []: press ENTER\nHome Phone []: press ENTER\nOther []: press ENTER\nIs the information correct? [Y/n] press y","step-2-grant-root-privileges-to-the-user#Step 2: Grant Root Privileges to the User":" # visudo The above command takes us to the /etc/sudoers.tmp file, where we can see the code below:\nAfter the line for the root user, you will add the same format for your new user. This will let us give that user admin rights.\nPress ‚Äúctrl‚Äù and ‚Äúx‚Äù together. Press ‚Äúy‚Äù and then ‚Äúenter‚Äù at the prompt to save and close the file.","step-3-verify-user-has-privileges#Step 3: Verify User Has Privileges":"If you‚Äôve done everything right, this user will be able to run commands like update with the help of sudo:\n# su - microhost # sudo apt-get update After entering the password, this is shown in the below terminal"},"title":"How To Add a User and Grant Root Privileges on Ubuntu 18.04"},"/utho-docs/docs/linux/how-to-add-a-user-to-sudoers-in-ubuntu-18-04/":{"data":{"":" How to Add a User to Sudoers in Ubuntu 18.04\nDescription\nIn this article we will add new knowledge How to Add a User to Sudoers in Ubuntu 18.04 Add a user to sudoers on Ubuntu 18.04. In many cases, you might have seen that a non-privileged user has been given sudo access to perform root actions. This is required in many organisations where some tasks cannot be performed without having root access. Because they cannot provide root user access, Linux admins or system administrators usually grant sudo access to a non-privileged user to perform root operations.\nFollow the below steps to how to Add a User to Sudoers in Ubuntu 18.04","add-user-in-sudoers-file#Add user in sudoers file":"Most of the time, adding the user to the sudo group is enough, but if that doesn‚Äôt work, it‚Äôs better to add the user to the /etc/sudoers file, as shown below:\nvisudo /etc/sudoers How to Add a User to Sudoers in Ubuntu 18.04\nNOTE: Please remember that you have to use the visudo command to open the /etc/sudoers file to make sure there are no configuration errors in the file.","check-access-of-sudo#Check access of sudo":"Now you can run some commands that need sudo access to see if sudo access is working.\nHow to Add a User to Sudoers in Ubuntu 18.04\nYou can run any command on the terminal after adding the user‚Äôs entry to the Visudo file; he will behave as a root user, as shown in the screenshot above.¬†How to Add a User to Sudoers in Ubuntu 18.04\nI hope you have successfully understood all the things to How to Add a User to Sudoers in Ubuntu 18.04\nMust Read : Cheat sheet for 15 nmcli commands in Linux (RHEL/CentOS)\nThankyou","create-a-user#Create a user":"We need to make a user to whom we can give sudo access so that they can do administrative work. For our example, we will set up a user test. This must be done by the root user or by any other user who has access to sudo. Here, I‚Äôm making the testing user with root.\nuseradd -m testing -m Creates the user‚Äôs home directory. The -k option copies the skeleton directory to the home directory. useradd man page details\nIf this option isn‚Äôt used and CREATE HOME isn‚Äôt turned on, no home directories are made by default.\nHow to Add a User to Sudoers in Ubuntu 18.04","find-user#Find User":" cat /etc/passwd | grep -i testing How to Add a User to Sudoers in Ubuntu 18.04\ntest:¬†Name of the User\n1000:¬†User Id\n1000:¬†Group Id\ntest,,,,: Section Comments\n/home/test: Home Directory of the Test User\n/bin/bash: Shell assigned to the user","modification-of-user-group#Modification of User Group":"You must use the usermod command to add users to the sudo group.\nusermod -aG sudo testing -a: Add the user to the group of extra people (s). only when you use the -G option. On the usermod Man page, you can find more information.\n-G: A list of other groups that the user belongs to as well. Each group is separated by a comma, and there is no white space in between. The same rules apply to the groups as to the group given with the -g option. On the usermod Man page, you can find more information.\nIf the user is part of a group that is not on the list, they will be taken out of that group. The -a option can change this behaviour by adding the user to the current supplementary group list.\nNow, change to ‚Äútest user‚Äù and use the whoami command to see if the user is in the sudo group.\nHow to Add a User to Sudoers in Ubuntu 18.04","set-user-password#Set User Password":"Use the passwd command to set the testing user‚Äôs password. When asked, give a strong password and press enter. Type the same password again and hit Enter.\nHow to Add a User to Sudoers in Ubuntu 18.04"},"title":"How to Add a User to Sudoers in Ubuntu 18.04"},"/utho-docs/docs/linux/how-to-add-ftp-account-in-plesk/":{"data":{"":"","#":"","1go-to-websites--domains#1.Go to Websites \u0026amp; Domains":"\nIntroduction The abbreviation ‚ÄúFTP‚Äù stands for ‚ÄúFile Transfer Protocol,‚Äù which is the name given to a set of guidelines that determine how files are transferred between computer systems using the internet. File Transfer Protocol (FTP) is used by businesses to transfer files between computers, and websites use FTP to upload and download files from the servers that host their websites.\n*If you and another person are working on your website together or if you host subdomains for other users, you may want to create separate FTP accounts for each of those parties.\n*To create an additional account for the File Transfer Protocol: 1.Go to Websites \u0026 Domains\n2.click¬†FTP Access.\n3.Click add an FTP account\n4.Click¬†Add FTP Account.\n5.Fill the required filled and then click ok\n*Now, take the next step.\n1.Go to Websites \u0026 Domains You‚Äôll find this button over on the left side of the page.\n*After that, pick the name of the domain to which you wish to create an FTP account.","2clickftp-access#2.click¬†FTP Access.":"","3click-add-an-ftp-account#3.Click add an FTP account":"","5fill-the-required-filled-and-then-click-ok#5.Fill the required filled and then click ok":"\n*as you can see in the picture below, the FTP account has now been established successfully.\nHopefully now you will get how to add FTP account in plesk\nThank You","to-create-an-additional-account-for-the-file-transfer-protocol#*To create an additional account for the File Transfer Protocol:":""},"title":"How to add FTP account in plesk"},"/utho-docs/docs/linux/how-to-add-or-remove-a-user-from-a-linux-group/":{"data":{"":"\nDescription\nLinux is a multi-user system by default, which means that many people can connect to it at the same time and work. One of the most important jobs of a system administrator is to manage users. On a Linux system, user management includes creating, updating, and deleting user accounts and user groups.\nIn this short article, you will learn how to add or remove a user from a group in a Linux system.\nNeed to check User group\nJust type in the following groups command to see whether you‚Äôre a member of a certain user group:\n#groups microhost.com *Simply execute the groups command without providing any arguments in order to examine your own groups.\n#groups Add a User to a Group\nMake sure that the user in question already exists on the system before attempting to add them to a group. Use the usermod command with the -a flag, which informs usermod to add a user to the supplemental group(s), and the -G option, which specifies the actual groups in the following format, in order to add a user to a specific group. For further information, see the usermod man page.\n#usermod -aG postgres microhost.com #groups microhost.com Remove a User from a Group\n#gpasswd -d microhost.com postgres #groups microhost.com The deluser command lets you take a user out of a certain group.\n#sudo deluser microhost.com postgres Thank You"},"title":"How to add¬†or remove¬†a User from a Linux Group"},"/utho-docs/docs/linux/how-to-add-user-to-sudoers-or-sudo-group-in-centos-7-support-internal/":{"data":{"":"\nHow To Add User to Sudoers or Sudo Group in CentOS 7\nThe sudo command stands for ‚ÄúSuper User DO‚Äù and temporarily elevates the privileges of a regular user for administrative tasks. The sudo command in CentOS provides a workaround by allowing a user to elevate their privileges for a single task temporarily.\nThis guide will walk you through the steps to add a user to sudoers in CentOS.\nPrerequisites\nA system running CentOS 7 Access to a user account with root privileges Step 1. Login to the server with root privileges.\nroot@103.150.136.136.163's password: step 2. Add a user with the following command\nroot@cloudserver-rootdenied ~]# adduser narender step 3. Set password of the user with the following command\nroot@cloudserver-rootdenied ~]# passwd narender Step 4. Now run the following command\nroot@cloudserver-rootdenied ~]# visudo Step 5. Now edit the visudo file and unselect the below line\n## Allows people in group wheel to run all commands %wheel¬†ALL=(ALL) ALL Adding User to the wheel Group\nThe easiest way to grant sudo privileges to a user on CentOS is to add the user to the ‚Äúwheel‚Äù group. Members of this group are able to run all commands via sudo and are prompted to authenticate themselves with their password when using sudo.\nStep 6. To add a user to the wheel group, use the command:\nroot@cloudserver-rootdenied ~]# useradd -aG wheel narender Step 7. Switch to the new (or newly-elevated) user account with the su (substitute user) command:\nroot@cloudserver-rootdenied ~]# su - narender # sudo ls -la /root Now user is able to execute commands with sudo privilege¬†Thank You!"},"title":"How To Add User to Sudoers or Sudo Group in CentOS 7"},"/utho-docs/docs/linux/how-to-allow-remote-connections-to-mysql-in-centos/":{"data":{"":"\nIntroduction\nOn the same local system, databases and web servers are frequently hosted. But a lot of businesses are now switching to a more distributed environment.\nYou may grow resources fast and improve hardware performance and security by using a separate database server. Learning how to manage remote resources efficiently is a priority in such use situations.\nThis guide demonstrates how to make a MySQL database accessible from a distance.\nTo Allow Remote Connections to MySQL in centos so follow the below steps..\nStep.1 You must begin by editing the file that is used to configure MySQL. Launch your text editor of choice, which in our case is vi, and open the file:\n#vi /etc/my.cnf Add the following contents at the end of the ‚Äò[mysqld]‚Äò section:\nbind-address = *\nrequire_secure_transport = ON\nWhen you are done, save and close the file.\nStep.2\nThen, restart MySQL service on Centos to apply the changes:\n#systemctl restart mysqld Step.3\nIf you are useing a firewall then add the 3306 in firewall rules\nTo allow MySQL to connect from remote server on CentOS 7 server, you need to enable port 3306 in firewall.\n#sudo firewall-cmd --zone=public --add-port=3306/tcp #sudo firewall-cmd ‚Äìreload Step.4 Now you can access the mysql database.using below command.give the password and enter.\n#mysql -u root -u Hosting for databases and web servers frequently takes place on the same physical machine. However, many companies are making the transition to a more decentralised setting as of late.\nI am hoping that you are able to follow the steps. The Step-by-Step Guide to Enabling Remote MySQL Connections on CentOS\nMust read:- https://utho.com/docs/tutorial/change-ssh-default-port-22-to-custom-port/\nThankyou"},"title":"How to Allow Remote Connections to MySQL in centos"},"/utho-docs/docs/linux/how-to-block-and-unblock-ip-in-csf-whm-cpanel/":{"data":{"":"\nYou have sudo or root access to perform this task in WHM/Cpanel\nStep:- 1. Login to WHM/Cpanel.\nStep:- 2. Goto plugin or search plugins in whm search bar.\nStep:- 3. Open plugins and goto ‚ÄúConfigServer Security Firewall‚Äù.\nStep:- 4. To block ip in CSF enter the ip in section ‚ÄúBlock IP address‚Äù\nStep:- 5. To Unblock ip in CSF enter the ip in section \" Remove IP address \"\nStep:- 6. When you will get the below message means ip is unblock from CSF.\nThank you‚Ä¶"},"title":"How to Block and unblock Ip in CSF-WHM/Cpanel"},"/utho-docs/docs/linux/how-to-build-brotli-from-source-on-centos-7/":{"data":{"":"","before-getting-started#Before getting started":"Be sure to check the CentOS version.\n# cat /etc/centos-release Make a new user that is not root that can use sudo and log in as that user.\n# useradd -c \"micro host\" microhost \u0026\u0026 passwd microhost # usermod -aG wheel microhost # su - microhost To log in, please change microhost to your username.","create-a-new-time-zone#Create a new time zone":" # timedatectl list-timezones # sudo timedatectl set-timezone 'Region/City' Please update your system to the most recent version.\n# sudo yum update -y ","create-brotli#Create Brotli":"Get the necessary software and build tools installed.\n# sudo yum install -y gcc make bc sed autoconf automake libtool git Get a copy of the Brotli repository.\n# git clone https://github.com/google/brotli.git Access the Brotli code repository.\n# cd brotli You should make a reference page for using Brotli commands.\n[consoel]# sudo cp ~/brotli/docs/brotli.1 /usr/share/man/man1 \u0026\u0026 sudo gzip /usr/share/man/man1/brotli.1[/console]\nLook in the user guide.\n# man brotli Execute the ./bootstrap command to create the Autotools configure file.\n# ./bootstrap Once you‚Äôve run the preceding command, you‚Äôll have access to the standard C programme construction stages, including configure, make, and make install.\nUse the ./configure ‚Äìhelp command for further information.\nNow, make Brotli.\n# ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/lib64/brotli --libdir=/usr/lib64/brotli --datarootdir=/usr/share --mandir=/usr/share/man/man1 --docdir=/usr/share/doc # make # sudo make install A successful build allows you to verify the version.\n# brotli --version Hopefully, you have learned how to build Brotli from source on CentOS 7.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to build Brotli from source on CentOS 7.\nBrotli, a compression algorithm, claims to be more effective at compressing web pages than its predecessor, GZIP, and to have shorter compression times. It is available at no cost, has widespread support across current web servers, and can be used by anyone.Brotli is a compression format that is widely supported by today‚Äôs web servers.\nA pre-defined dictionary of frequently used words is one of the characteristics that helps Brotli improve its compression efficiency. Because the server and the client both have access to this dictionary, we can save some space.\nBrotli and GZIP, two compression formats, both help reduce page size by compressing files. Faster page loads are the result of less files."},"title":"How to Build Brotli From Source on CentOS 7"},"/utho-docs/docs/linux/how-to-build-brotli-from-source-on-debian-9/":{"data":{"":"","before-getting-started#Before getting started":"Be sure to check the CentOS version.\n# lsb_release -ds Please update your system to the most recent version.\n# apt update If you haven‚Äôt done so already, you should install the curl, wget, and sudo packages.\n# apt install -y curl wget sudo Make a new user that is not root that can use sudo and log in as that user.\n# useradd -c \"micro host\" microhost \u0026\u0026 passwd microhost # usermod -aG wheel microhost # su - microhost To log in, please change microhost to your username.","create-a-new-time-zone#Create a new time zone":" # sudo dpkg-reconfigure tzdata ","create-brotli#Create Brotli":"Get the necessary software and build tools installed.\n# sudo apt install -y build-essential gcc make bc sed autoconf automake libtool git apt-transport-https Get a copy of the Brotli repository.\n# git clone https://github.com/google/brotli.git Access the Brotli code repository.\n# cd brotli You should make a reference page for using Brotli commands.\n[consoel]# sudo cp ~/brotli/docs/brotli.1 /usr/share/man/man1 \u0026\u0026 sudo gzip /usr/share/man/man1/brotli.1[/console]\nLook in the user guide.\n# man brotli Execute the ./bootstrap command to create the Autotools configure file.\n# ./bootstrap Once you‚Äôve run the preceding command, you‚Äôll have access to the standard C programme construction stages, including configure, make, and make install.\nUse the ./configure ‚Äìhelp command for further information.\nNow, make Brotli.\n# ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/lib64/brotli --libdir=/usr/lib64/brotli --datarootdir=/usr/share --mandir=/usr/share/man/man1 --docdir=/usr/share/doc # make # sudo make install A successful build allows you to verify the version.\n# brotli --version brotli 1.0.7 Hopefully, you have learned how to build Brotli from source on Debian 9.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to build Brotli from source on Debian 9.\nBrotli, a compression algorithm, claims to be more effective at compressing web pages than its predecessor, GZIP, and to have shorter compression times. It is available at no cost, has widespread support across current web servers, and can be used by anyone.Brotli is a compression format that is widely supported by today‚Äôs web servers.\nA pre-defined dictionary of frequently used words is one of the characteristics that helps Brotli improve its compression efficiency. Because the server and the client both have access to this dictionary, we can save some space.\nBrotli and GZIP, two compression formats, both help reduce page size by compressing files. Faster page loads are the result of less files."},"title":"How to Build Brotli From Source on Debian 9"},"/utho-docs/docs/linux/how-to-build-brotli-from-source-on-fedora/":{"data":{"":"","before-getting-started#Before getting started":" # cat /etc/fedora-release Make a new user that is not root that can use sudo and log in as that user.\n# useradd -c \"micro host\" microhost \u0026\u0026 passwd microhost # usermod -aG wheel microhost # su - microhost To log in, please change microhost to your username.","create-a-new-time-zone#Create a new time zone":" # timedatectl list-timezones # sudo timedatectl set-timezone 'Region/City' Please update your system to the most recent version.\n# dnf update -y ","create-brotli#Create Brotli":"Get the necessary software and build tools installed.\n# sudo yum install -y gcc make bc sed autoconf automake libtool git Get a copy of the Brotli repository.\n# git clone https://github.com/google/brotli.git Access the Brotli code repository.\n# cd brotli You should make a reference page for using Brotli commands.\n[consoel]# sudo cp ~/brotli/docs/brotli.1 /usr/share/man/man1 \u0026\u0026 sudo gzip /usr/share/man/man1/brotli.1[/console]\nLook in the user guide.\n# man brotli Execute the ./bootstrap command to create the Autotools configure file.\n# ./bootstrap Once you‚Äôve run the preceding command, you‚Äôll have access to the standard C programme construction stages, including configure, make, and make install.\nUse the ./configure ‚Äìhelp command for further information.\nNow, make Brotli.\n# ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/lib64/brotli --libdir=/usr/lib64/brotli --datarootdir=/usr/share --mandir=/usr/share/man/man1 --docdir=/usr/share/doc # make # sudo make install A successful build allows you to verify the version.\n# brotli --version brotli 1.0.7 Hopefully, you have learned how to build Brotli from source on Fedora.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to build Brotli from source on Fedora.\nBrotli, a compression algorithm, claims to be more effective at compressing web pages than its predecessor, GZIP, and to have shorter compression times. It is available at no cost, has widespread support across current web servers, and can be used by anyone.Brotli is a compression format that is widely supported by today‚Äôs web servers.\nA pre-defined dictionary of frequently used words is one of the characteristics that helps Brotli improve its compression efficiency. Because the server and the client both have access to this dictionary, we can save some space.\nBrotli and GZIP, two compression formats, both help reduce page size by compressing files. Faster page loads are the result of less files."},"title":"How to Build Brotli From Source on Fedora"},"/utho-docs/docs/linux/how-to-build-brotli-from-source-on-ubuntu-20-04-lts/":{"data":{"":"","before-getting-started#Before getting started":" # lsb_release -ds ","create-a-new-time-zone#Create a new time zone":" # sudo dpkg-reconfigure tzdata Please update your system to the most recent version.\n# sudo apt update ","create-brotli#Create Brotli":"Get the necessary software and build tools installed.\n# sudo apt install -y build-essential gcc make bc sed autoconf automake libtool git apt-transport-https tree Get a copy of the Brotli repository.\n# git clone https://github.com/google/brotli.git Access the Brotli code repository.\n# cd brotli You should make a reference page for using Brotli commands.\n[consoel]# sudo cp ~/brotli/docs/brotli.1 /usr/share/man/man1 \u0026\u0026 sudo gzip /usr/share/man/man1/brotli.1[/console]\nLook in the user guide.\n# man brotli Execute the ./bootstrap command to create the Autotools configure file.\n# ./bootstrap Once you‚Äôve run the preceding command, you‚Äôll have access to the standard C programme construction stages, including configure, make, and make install.\nUse the ./configure ‚Äìhelp command for further information.\nNow, make Brotli.\n# ./configure --prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/lib64/brotli --libdir=/usr/lib64/brotli --datarootdir=/usr/share --mandir=/usr/share/man/man1 --docdir=/usr/share/doc # make # sudo make install A successful build allows you to verify the version.\n# brotli --version Hopefully, you have learned how to build Brotli from source on Ubuntu 20.04 LTS.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to build Brotli from source on Ubuntu 20.04 LTS.\nBrotli, a compression algorithm, claims to be more effective at compressing web pages than its predecessor, GZIP, and to have shorter compression times. It is available at no cost, has widespread support across current web servers, and can be used by anyone.Brotli is a compression format that is widely supported by today‚Äôs web servers.\nA pre-defined dictionary of frequently used words is one of the characteristics that helps Brotli improve its compression efficiency. Because the server and the client both have access to this dictionary, we can save some space.\nBrotli and GZIP, two compression formats, both help reduce page size by compressing files. Faster page loads are the result of less files."},"title":"How to Build Brotli From Source on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-change-apache2-web-folder-in-ubuntu/":{"data":{"":"\nIntroduction\nThe web server of Apache is the most popular way to deliver web content. It serves over half of all active websites in the Internet and is extremely powerful and flexible.\nApache divides its features and components into separate units, which can be independently customized and set up. A virtual Host is called the basic unit describing a single site or domain. Virtual hosts allow one server, through a matching system, to host several domains or interfaces. This is important for those who want to host more than one VPS site.\nEach configured domain will direct the visitor to a certain directory that contains information on this website without indicating that the same server is also responsible for other websites. This scheme can be expanded without software limits provided the traffic of all the sites is handled by your server.\n*To modify the document root of Apache, you must alter the files apache2.conf and 000-default.conf.\nApache is installed in the /var/www/html directory.\nThis is the default Apache root directory.\nEither modify the Apache root directory or relocate the project to /var/www/html.\nTo change apache2 web folder follow below steps‚Ä¶‚Ä¶\nStep.1 access putty\nStep.2 .To change Apache‚Äôs root directory\n#cd /etc/apache2/sites-available Step.3 Then open the 000-default.conf file\n#vi 000-default.conf Step.4 Edit the DocumentRoot option mentioned at the above screenshot.\nDocumentRoot /var/www/my/html/change folder.\nStep.5 Then restart the apache server:\n#sudo service apache2 restart I am hoping that you are familiar with how to alter the apache2 web folder.\nThankyou\nMust Read : 4 Effective Ways to Determine the Name of a Plugged USB Device in Linux"},"title":"How to change apache2 web folder in Ubuntu"},"/utho-docs/docs/linux/how-to-change-date-and-time-in-linux/":{"data":{"":" How to change date and time in Linux\nDescription\nIn this post, we will explore How to change date and time in Linux and the various options available to us in Linux for modifying the current date and time. On occasion, it is of the utmost need to ensure that the date and time on your Linux server are accurate.\nFollow the below steps to learn How to change date and time in Linux.","step1-check-current-time-and-date#Step.1 ¬†check current time and date":"By using the timedatectl command that is provided below, you may get the current date and time.\ntimedatectl ","step2-check-available-time-zones#Step.2 Check available Time Zones":"Using the timedatectl list-timezones command, which is demonstrated below, it is possible to examine all of the time zones that are currently accessible.\ntimedatectl list-timezones ","step3-find-only-asia-time-zone#Step.3 Find only Asia Time Zone":"As will be demonstrated in the following section, using the timedatectl list-timezones command, you can select only the time zones that you are interested in checking.\ntimedatectl list-timezones | grep -i Asia Step.4 change timezone to Asia/Kolkata\nYou are able to alter the time zone at this point by utilising the timedatectl set-timezone command and specifying the appropriate time zone based on the output of the timedatectl list-timezones command, which can be found above.\ntimedatectl set-timezone Asia/Kolkata ","step5-change-the-time-to-152202#Step.5 change the time to 15:22:02":"You can alter the time without affecting the date by using the timedatectl set-time command. This function is available in most modern operating systems.\ntimedatectl set-time '15:22:02' ","step6-change-the-date-to-6oct-2022-and-time-to-152202#Step.6 Change the date to 6oct ,2022 and time to 15:22:02":"You can adjust both the date and time by using the timedatectl set-time command, as described below.\ntimedatectl set-time '2022-10-06 15:22:02' Important:- When you try to change the date and time manually, you may see the error ‚ÄúFailed to set time: Automatic time synchronisation is enabled.‚Äù In that situation, you must disable the NTP before attempting to alter the date and time again.\ntimedatectl set-ntp off I sincerely hope that each and every one of these things was clear to you. How to change date and time in Linux.\nMust read :-"},"title":"How to change date and time in Linux"},"/utho-docs/docs/linux/how-to-change-default-port-of-apache-on-rhel-centos-7/":{"data":{"":"\nApache HTTP server is one of the most widely used web servers on the internet today due to its versatility, reliability, and a plethora of capabilities, some of which are not available in competing web servers such as Nginx at the time.\nSeveral of the most important features of Apache include the ability to load and run various types of modules and special configurations at runtime, without stopping the server or, worse, compiling the software each time a new module is added, and the unique role of.htaccess files, which can modify web server configurations specific to webroot directories.\nApache web server is configured by default to listen for incoming connections and bind to port 80. If you choose the TLS option, the server will listen on port 443 for secure communications.\n**.. You must first run the following command to modify the 42 number line in the apache (httpd) file.\n**\n#vi /etc/httpd/conf/httpd.conf **#Listen 12.34.56.78:80\n**Listen 80 comment this line\nListen 1234**\n.After adding the above line, you must build or modify an Apache virtual host on Debian/Ubuntu based distributions to begin the binding process.\nIn CentOS/RHEL, the update is made to the default virtual host. In the example below, we change the default virtual host of the web server from 80 to 1234.\nFinally, restart the daemon and inspect the local network sockets table using netstat or ss. Your server‚Äôs network table should show port 1234 as listening.\n#systemctl restart apache2 #netstat -tlnp| grep apache #ss -tlpn| grep apache #Systemctl restart httpd Simply navigate to your server‚Äôs IP address or domain name on port 1234. The Apache default page should appear.\n**http://server_ip:1234/"},"title":"How to Change Default Port of Apache On RHEL/CentOS 7"},"/utho-docs/docs/linux/how-to-change-mysql-port-number-in-centos-7/":{"data":{"":"\nDefault port of mysql is 3306 . To change the default port to custom port , Please follow the below step.\nStep 1. First , use the below command to check the availability of port (port is free or not).\n# netstat -tanp|grep 3337 Step 2. Open the mysql configuration file /etc/my.cnf.\n# vi /etc/my.cnf Step 3 : In my.cnf find the [mysqld] section. Edit the port no. 3306 to 3337(or any) or if you do not find any such line you can add port = 3337 in my.cnf but it should be under the [mysqld] section as shown in the screenshot below .\nPress ‚Äòi‚Äô for the insert mode.¬†Now save and exit from the file using (backspace) :wq\nStep 4: Restart the mysql service.\n# systemctl restart mysql service Step 5. To verify the port number of mysql , use the following command.\n# netstat -tlpn | grep mysql Step 6. (Optional): If you have firewalld on your server , then open the required port by the following :\n# firewall-cmd ‚Äìpermanent ‚Äìzone=public ‚Äìadd-port=3337/tcp Step 7. Login to mysql with custom port using following command.\n# mysql -h localhost -u root -p -P 3337 Thank you!!"},"title":"How to change mysql port number in centOS 7"},"/utho-docs/docs/linux/how-to-change-ssh-port-when-selinux-policy-is-enabled/":{"data":{"":" How to change SSH port when SELinux policy is enabled\nIn this tutorial, you will learn how to change SSH port when SELinux policy is enabled in CentOS or RedHat or Fedora flavour. When SELinux is operating in enforcing mode, the SELinux policy is enforced and access is denied based on SELinux policy rules. TCP port 22 is the standard SSH port on the majority of Linux and Unix systems. This is readily modifiable to a port not used by any other applications on the system.\nWhen SELinux is operating in Enforcing mode, the to-be-set port must be relabeled so that Policy rules controlling access will permit the ssh service to bind.","prerequisites#Prerequisites:":" Super user or any normal user with SUDO privileges Any utility to edit a file such as vi, vim or nano ","steps-to-change-the-port-when-selinux-policy-is-enforced#Steps to change the port when SeLinux policy is enforced":"Step 1: Check the current port of ssh service and selinux mode\nnetstat -tunlp | grep ssh getenforce Default port of ssh and selinux policy\nStep 2: Now goto the SSH configuration file and navigate to keyword- ‚ÄúPort‚Äù and if the existing line is commented then make a new entry in next line like as shown in below picture\nvim /etc/ssh/sshd_config Configuration file of SSH\nHere, we want to change our default port from 22 to 5353. If you want to change the default port to any other port number, just mention after ‚ÄúPort‚Äù.\nNow if your restart the SSH services, The service will not restart and will give you the below error in journalctl -xe\nSSh sevices restart without changing the labeling in semanage\nStep 3: Now check the labeled port for SSH service in SELinux policy\nsemanage port -l | grep ssh SSH port labeled in SELinux policy\nStep 4: Now add another port lable in selinux policy for SSh service. This will enable you to run your SSH service in either 22 or 5353 port number.\nsemanage port -a -p tcp -t ssh_port_t 5353 Step 5: Now restart the SSH service by using systemctl command as below\nsystemctl restart sshd Service restarted successfully after enabeling the port in selinux policy\nStep 6: To crosscheck the listening port of SSH, after follow the above steps, you can check this using netstat command.\nnetstat -tunlp | grep ssh Port changed successfully when selinux policy is enabled\nStep 7: If you want to delete the port label from Selinux, you can do so using below command. But remember that, after executing the next command you will not be able to restart the service without change the port number in SSH configuration file.\nsemanage port -d -t ssh_port_t -p tcp 5353 And this is how you can change the port number of SSH when Selinux policy is enabled."},"title":"How to change SSH port when SELinux policy is enabled"},"/utho-docs/docs/linux/how-to-check-and-analyze-packets-by-tcpdump-command/":{"data":{"":" How to check and analyze packets by tcpdump command","introduction#Introduction":"In this tutorial, we will see how to check and analyze packets by tcpdump command. Tcpdump is a command line utility which is used to capture and analyze packets. It is also used to troubleshoot network issues.\nHow to install tcpdump. The installation of tcpdump depends on the operating system you are using.\nFor Centos 8 , RHEL 8 ,Fedora 34\n#yum install tcpdump -y For debian Based Operating System\n# apt install tcpdump -y We can check tcpdump is installed or not on server\n#which tcpdump check availability of tcpdump\nUses: Note: For Using tcpdump command Root privileges or any normal user with sudo privilege are required\nSo if you are not login as root user then use sudo to run tcpdump command\nSo, Let‚Äôs start\n‚áí To list all port available on server use -D\n#tcpdump -D list available ports\n‚áí Check traffic on any specific port\ntcpdump --interface any¬†This option is used to capture packets on any specific port at the place of any we can define any port like eth0.\n‚áí option -c\ntcpdump -c number -c option is used to define the number of packets to be dumped here n is the number of packets.\nExample ``` #tcpdump -c 14\n\u003cfigure\u003e ![capture only 14 packets](images/tcpdump_c-1024x328.jpg) \u003cfigcaption\u003e capture only 14 packets \u003c/figcaption\u003e \u003c/figure\u003e We can also use -i option with -i option #tcpdump -i ens33 -c5\n\u003cfigure\u003e ![analyse packet on specific port](images/tcp_i_c-1024x176.jpg) \u003cfigcaption\u003e analyse packet on specific port \u003c/figcaption\u003e \u003c/figure\u003e ‚áí **\\-n \u0026\u0026 -nn** \\-n option is used to disable name resolution \\-nn option is used to disable port resolution #tcpdump -c5 -n\n\u003cfigure\u003e ![disable name resolution](images/tcpdump_c-n-1024x157.jpg) \u003cfigcaption\u003e disable name resolution \u003c/figcaption\u003e \u003c/figure\u003e #tcpdump -c5 -nn\n\u003cfigure\u003e ![turn off the port resolutions](images/tcpdump_c-nn-1024x155.jpg) \u003cfigcaption\u003e turn off the port resolutions \u003c/figcaption\u003e \u003c/figure\u003e ‚áí **To filter packets based on protocol** We can filter packets of any specific protocol #tcpdump -c200 icmp\n‚áí **Filter packets by host** #tcpdump -i any -c200 src 103.127.30.80\n‚áí **Filter packets by port** #tcpdump -i any c100 port 3330\n‚áí **Filter by source IP address** #tcpdump¬†src 11.121.21.57\n‚áí **Filter by destination IP** #tcpdump¬†dst 103.127.31.50\n‚áí **tcpdump with source address and port number** #tcpdump -i eth0¬†src 103.127.31.50 and port 80\n‚áí¬†**Capture and save packets in file** ","tcpdump--a--s0-port-80#tcpdump -A -s0 port 80":" ‚áí **Capture ipv6 traffic** #tcpdump -nn ip6 proto 6\n‚áí **Port Scan** #tcpdump -nn ‚áí **Capture Start and last packet from any network** #tcpdump ‚Äôtcp[tcpflags] \u0026 (tcp-syn|tcp-fin) != 0 and not src and dst net 8.8.8.8'\nSo that is how you learned how to check and analyze packets by tcpdump command Thankyou Also Read: [Deploying and Managing a Cluster on Microhost Kubernetes Engine (MKE)](https://utho.com/docs/tutorial/deploying-and-managing-a-cluster-on-microhost-kubernetes-engine-mke/), [How to Install MongoDB on Debian](https://utho.com/docs/tutorial/how-to-install-mongodb-on-debian/) ","tcpdump--w-filenamepcap--i-any#tcpdump -w filename.pcap¬† -i any":" For reading .pcap file #tcpdump -r¬†filename.pcap\nHere, please note that .pcap file will be a binary file, which cannot be read by vim or less command ‚áí Finding Packet by Network #tcpdump net 103.127.30.0/24\n‚áí **Filter traffic by port range** #tcpdump portrange 21-30\n‚áí **Filter httpd cookies** #tcpdump -vvAls0 | grep ‚ÄòSet-Cookie|Host:|Cookie:‚Äô\n‚áí **Filter by length** #tcpdump -i eth0 -nn -s0 -v port 80\nHere -s0 is snap length,is the size of packet to capture -s0 will set the size to unlimited ‚áí **To display ASCII value** "},"title":"How to check and analyze packets by tcpdump command"},"/utho-docs/docs/linux/how-to-check-disable-and-enable-php-modules/":{"data":{"":" How to check, disable and enable PHP modules.","description#Description:":"In this tutorial, we will learn how to check, disable and enable PHP modules.If you run a system or web server, you need to know how to install and use different versions of PHP on the same server. This is very helpful when you have more than one PHP programme on your server and each one works with a different version of PHP. Linux distribution Ubuntu supports PHP. PHP needs to be introduced, as opposed to Python, which is preinstalled in the basic system.\na2enmod is a script that enables the specified module within the apache2 configuration. It does this by creating symlinks within /etc/apache2/mods-enabled. Likewise, a2dismod disables a module by removing those symlinks. It is not an error to enable a module which is already enabled, or to disable one which is already disabled.","how-to-disable-a-php-module#How to disable a php module":"Step 1. First we should check, which modules are enabled.\n# a2query -m All enabled mod\nStep 2. Now if you want to disable any module, just remember the first word of the above output. For example, if you want to disable php7.3 module\n# a2dismod php7.3 All disabled mod","how-to-enable-any-module#How to enable any module":"Now to add any module, you just need to know the name of the mod\n# a2enmod php7.4 Add the php7.4 mod\nNow after enabling any php mod, you need to reload the daemon service and restart the apache.\nNow finally check whether the desired module is enable or not\n# a2query -m Check the enabled mod\nIn this article you have learnt how to check, disable and enable PHP modules.","how-to-show-enabled-php-modules#How to show enabled php modules":"You can show every mod which is being used by php.\n# a2query -m Every PHP mod used by php"},"title":"How to check, disable and enable PHP modules."},"/utho-docs/docs/linux/how-to-check-disk-performance-iops-and-latency-in-linux/":{"data":{"":"\nDescription\nWe will learn how to Check Disk Performance (IOPS and Latency) in Linux?\nInput/output operations per second (IOPS, pronounced ‚Äúeye-ops‚Äù) is a measure of how well computer storage devices like hard disc drives (HDD), solid state drives (SSD), and storage area networks (SAN) handle inputs and outputs (SAN).\nIOPS (input/output operations per second)\nRead the below steps carefully to Check Disk Performance (IOPS and Latency) in linux‚Ä¶‚Ä¶.\nStep.1\nTo measure disk IOPS performance in Linux, you can use the fio (the tool is available for RHEL in EPEL repository).So, to install fio in RHEL use the yum\n#yum install epel-release -y Step.2\n#yum install fio -y Step.3\nRandom Read/Write Operation Test\n#fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randrw --rwmixread=75 Step.4\nRandom Read Operation Test\nExecute the following command in order to test the performance of the disc exclusively for random read operations:\n#fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=fiotest --bs=4k --iodepth=64 --size=8G --readwrite=randwrite Step.5\nAnd then you need to create file and always remember extension should be (.fio)\n#vi microhost.fio And add the following contents into it:[global]\nrw=randread\nsize=8G\nfilename=/tmp/testfio\nioengine=libaio\niodepth=4\ninvalidate=1\ndirect=1\n[bgread]\nrw=randread\niodepth=64\nStep.6\nThen start the test:\n#fio microhost.fio Step.6\nTo check disk latency in Linux, the ioping tool is used:\n#yum install ioping -y **Run the latency test for your disk (20 requests are run):\n**\nStep.7\n# ioping -c 20 /tmp/ Moreinfo:- Computer storage systems such as hard disc drives (HDD), solid state drives (SSD), and storage area networks (SAN) are evaluated based on a metric called input/output operations per second, also referred to as IOPS (pronounced ‚Äúeye-ops‚Äù) (SAN)\nI hope you have a good understanding of how to check the performance of the disc (IOPS and latency) under Linux without any issues‚Ä¶\nMust read:- https://utho.com/docs/tutorial/how-to-make-a-linux-user-change-their-password-upon-login/\nThank You"},"title":"How to Check Disk Performance (IOPS and Latency) in Linux?"},"/utho-docs/docs/linux/how-to-check-disk-speed-read-write-hdd-ssd-performance-in-centos-7/":{"data":{"":"\nStep 1. Login into the server using root credentials on putty.¬†Step 2 : Run the following command to test the WRITE speed of a disk.\n# sync; dd if=/dev/zero of=tempfile bs=1M count=1024; sync Step 3 : Run the following command to test the READ speed of a disk.\ndd if=tempfile of=/dev/null bs=1M count=1024 Thank you!!"},"title":"How to check Disk Speed (Read/Write) HDD, SSD Performance in CentOS 7"},"/utho-docs/docs/linux/how-to-compress-a-bz2-file-and-how-to-uncompress-it/":{"data":{"":"","#":"","how-to-uncompress-files-with-bzip2#How to Uncompress Files with \u0026ldquo;bzip2\u0026rdquo;":"Description To compress a file (or files) is to drastically reduce the size of the file (or files) by encoding the data in the file (or files) using fewer bits, and compressing a file (or files) is typically a helpful practise during backup and the transmission of a file (or files) over a network. On the other hand, decompressing a file or files involves bringing the data back to its uncompressed form from before it was compressed.\nThere are several different file compression and decompression utilities available for use in Linux, including gzip, 7-zip, Lrzip, PeaZip, and many others.\nOn this article, we will examine how to compress and decompress files with the.bz2 extension in Linux by using the bzip2 programme.\nBzip2 is a well-known compression tool, and it is accessible on the majority of the main Linux distributions, if not all of them. To install Bzip2, you only need to execute the command that is specific to your Linux distribution.\n#sudo yum install bzip2 The following is the usual syntax for using bzip2:\n#bzip2 option(s) filenames Tutorial on How to Use ‚Äúbzip2‚Äù to Compress Your Files You can compress a file by following the instructions below, where the -z switch enables file compression:\n#bzip2 filename or\n#bzip2 -z filename Use the format: command in your terminal to compress a.tar file.\n#bzip2 -z backup.tar NOTE: To prevent bzip2 from deleting the input files during compression or decompression, use the -k or ‚Äîkeep option. If you want to keep the files, bzip2‚Äôs default behaviour is to remove them.\nIn addition, passing the -f or ‚Äîforce flag to bzip2 will cause it to overwrite an output file that is already present.\nto save the input file #bzip2 -zk filename #bzip2 -zk backup.tar You can also change the block size to be between 200k and 950k by using the notations -1 or ‚Äîfast to -9 or ‚Äìbest, as demonstrated in the examples that follow:\n#bzip2 -k1 /mnt/micro1 #ls -lh /mnt/micro1 #bzip2 -k9 /mnt/micro1 #bzip2 -kf9 /mnt/micro1 #ls -lh /mnt/micro1 The following image provides a screenshot that demonstrates how to use options to keep the input file, force bzip2 to overwrite an output file, and set the block size before beginning the compression process.\nHow to Uncompress Files with ‚Äúbzip2‚Äù Use the -d or ‚Äîdecompress option, as shown above, in order to decompress a file with the.bz2 extension.\n#bzip2 -d filename.bz2 NOTE: In order for the command mentioned above to be successful, the file‚Äôs name must be followed by the extension.bz2.\n#bzip2 -vd /mnt/micro1.bz2 #bzip2 -vfd /mnt/micro1.bz2 #ls -l /mnt/micro1 In order to read the bzip2 help page as well as the man page, use the following command:\n#bzip2 -h or\n#man bzip2 After reading the brief explanations above, I believe that you are now able to compress and decompress information. files with the bzip2 programme on a Linux operating system. However, if you have any inquiries or comments, please contact us using the comment box below.\nImportantly, in order to learn how to use the tar utility in Linux to build compressed archive files, you may wish to review a few important examples of the Tar command.\nThank You ","tutorial-on-how-to-use-bzip2-to-compress-your-files#Tutorial on How to Use \u0026ldquo;bzip2\u0026rdquo; to Compress Your Files":""},"title":"How to Compress a.bz2 File and How to Uncompress It"},"/utho-docs/docs/linux/how-to-configure-an-external-smtp-server-in-plesk/":{"data":{"":"","1login-to-the-plesk#1.login to the plesk":"","2and-go-tools-and-setting--updates#2.And go tools and setting \u0026gt; Updates":"","3click-addremove-components-on-the-updates-and-upgrades-page#3.Click Add/Remove Components on the Updates and Upgrades page.":"","4expand-mail-hosting-and-pick-msmtp-on-the-addremove-components-tab-relay-only#4.Expand Mail hosting and pick MSMTP on the Add/Remove Components tab (relay only).":"","5simply-clicking-the-continue-button-will-get-the-installation-started#5.Simply clicking the Continue button will get the installation started.":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nIntroduction\nIn this article you will learn to Configure SMTP server in Plesk Panel, An SMTP server, which stands for ‚ÄúSimple Mail Transfer Protocol,‚Äù is a piece of software whose primary function is to facilitate the sending, receiving, and/or relaying of outgoing email between email senders and recipients.\nSMTP is a programme that is used by mail servers to transmit, receive, and/or relay outgoing messages between email senders and recipients. Its full name is Simple Mail Transfer Protocol, and its abbreviation is SMTP.\nWhat does it mean when someone says ‚ÄúSMTP‚Äù?\nWhat does it mean to use the Simple Mail Transfer Protocol (SMTP)? SMTP is the protocol that is used to transmit and receive email. It is sometimes combined with IMAP or POP3, which handles the retrieval of messages, while SMTP mainly transmits messages to a server for forwarding. For example, a user-level programme may do this.\nStep by Step guide\n1.login to the plesk\n2.And go tools and setting \u003e Updates\n3.Click Add/Remove Components on the Updates and Upgrades page.\n4.Expand Mail hosting and pick MSMTP on the Add/Remove Components tab (relay only).\nNote: If a local mail server is installed, it and other mail components like as Mailman, SpamAssassin, and Dovecot will be deleted.\n5.Simply clicking the Continue button will get the installation started.\nFollow the steps above to set up an SMTP server.\n1.login to the plesk 2.And go tools and setting \u003e Updates 3.Click Add/Remove Components on the Updates and Upgrades page. 4.Expand Mail hosting and pick MSMTP on the Add/Remove Components tab (relay only). Set up your setting as per your requirement\nNote: If a local mail server is installed, it and other mail components like as Mailman, SpamAssassin, and Dovecot will be deleted.\n5.Simply clicking the Continue button will get the installation started. Now SMTP server succesfully installed‚Ä¶‚Ä¶‚Ä¶\nConclusion An SMTP server, which stands for ‚ÄúSimple Mail Transfer Protocol,‚Äù is a piece of software whose primary function is to facilitate the sending, receiving, and/or relaying of outgoing email between email senders and recipients.\nRead How do I find my Apache version in Plesk\nThankyou"},"title":"How to configure an external SMTP server in Plesk"},"/utho-docs/docs/linux/how-to-configure-sftp-server-in-debian/":{"data":{"":"","conclusion#Conclusion":"In this guide, you successfully set up SFTP on a Debian server, then tested connectivity through a terminal session and FileZilla. You can create multiple users with different directories to securely upload and download files on your server.\nThank You üôÇ","configure-sftp-server-in-debian#Configure SFTP Server In DEBIAN":"","create-users--group-for-sftp#Create Users \u0026amp; Group For SFTP":"","introduction#Introduction":"","restart-ssh-service#Restart SSH Service":"","verify-ssh--sftp-access#Verify SSH \u0026amp; SFTP Access":"\nIntroduction In this article you will know about To Configure SFTP Server In Debian, A secure shell (SSH) session is required for the use of the Secure File Transfer Protocol (SFTP), which is a secure method for exchanging data between a local and distant computer. It is an upgraded version of the standard file transfer protocol (FTP), which adds an additional layer of protection during the process of transferring files and establishing a connection.\nIn this tutorial, you will learn how to set up SFTP User accounts on Debian and enable the user to only access files that are located within the user‚Äôs home directory.\nConfigure SFTP Server In DEBIAN Executing the below command confirms that OpenSSH Server was successfully installed.\n# apt list openssh-server -a Change the following in¬†/etc/ssh/sshd_config\nIn order to stop the sftp server, remove the comment from /usr/lib/openssh/sftp-server.\nThe configuration term ‚ÄúAdd Subsystem sftp internal-sftp‚Äù allows sshd to use its own SFTP server code rather than starting a separate process (what would typically be the sftp-server).\nUsers in the sftp users group are allowed to using SFTP and not SSH.\nSpecify the SFTP root directory with the command ChrootDirectory /SFTP.\n# vi /etc/ssh/sshd_config The line below should be uncommented in¬†/etc/ssh/sshd_config:\n#Subsystem sftp /usr/lib/openssh/sftp-server Subsystem sftp internal-sftp Add the following lines to the end of the file.\nMatch group sftp_users X11Forwarding no AllowTcpForwarding no ChrootDirectory /SFTP ForceCommand internal-sftp Save and close the file by escape :wq\nRestart SSH Service # systemctl restart sshd Create Users \u0026 Group For SFTP Create a new group called¬†sftp_cloud¬†and new user called¬†microhost\n# groupadd sftp_cloud # adduser microhost Add user in¬†**sftp_**cloud¬†group\n# usermod -G sftp_cloud microhost Create¬†/SFTP¬†folder and a sub folder¬†**/SFTP/**microhost¬†for user to upload via SFTP\n# mkdir /SFTP # mkdir /SFTP/microhost # chown microhost:sftp_cloud /SFTP/microhost Verify SSH \u0026 SFTP Access Login to SFTP\nOpen a new terminal window and log in with¬†sftp¬†using a valid user account and password.\n# sftp microhost@Server_IP List files within the directory. Your output should be similar to the one below:\nmicrohost@Server_IP‚Äôs password:\nConnected to server_IP"},"title":"How To Configure SFTP Server In Debian"},"/utho-docs/docs/linux/how-to-configure-sftp-server-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"In this guide, you successfully set up SFTP on a Debian server, then tested connectivity through a terminal session and FileZilla. You can create multiple users with different directories to securely upload and download files on your server.\nHopefully, you have learned that how to configure SFTP server on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to configure SFTP server on Debian 12.\nA secure shell (SSH) session is required for the use of the Secure File Transfer Protocol (SFTP), which is a secure method for exchanging data between a local and distant computer. It is an upgraded version of the standard file transfer protocol (FTP), which adds an additional layer of protection during the process of transferring files and establishing a connection.\nIn this tutorial, you will learn how to set up SFTP User accounts on Debian and enable the user to only access files that are located within the user‚Äôs home directory.\nConfigure SFTP Server In DEBIAN Executing the below command confirms that OpenSSH Server was successfully installed.\n# apt list openssh-server -a Change the following in¬†/etc/ssh/sshd_config\nIn order to stop the sftp server, remove the comment from /usr/lib/openssh/sftp-server.\nThe configuration term ‚ÄúAdd Subsystem sftp internal-sftp‚Äù allows sshd to use its own SFTP server code rather than starting a separate process (what would typically be the sftp-server).\nUsers in the sftp users group are allowed to using SFTP and not SSH.\nSpecify the SFTP root directory with the command ChrootDirectory /SFTP.\n# vi /etc/ssh/sshd\\_config The line below should be uncommented in¬†/etc/ssh/sshd_config:\n#Subsystem sftp /usr/lib/openssh/sftp-server Subsystem sftp internal-sftp Add the following lines to the end of the file.\nMatch group sftp_cloud X11Forwarding no AllowTcpForwarding no ChrootDirectory /SFTP ForceCommand internal-sftp Save and close the file by escape :wq\nRestart SSH Service # systemctl restart sshd Create Users \u0026 Group For SFTP Create a new group called¬†sftp_cloud¬†and new user called¬†utho.\n# groupadd sftp\\_cloud # adduser utho Add user in¬†sftp_cloud¬†group.\n# usermod -G sftp\\_cloud microhost Create¬†/SFTP¬†folder and a sub folder¬†/SFTP/utho¬†for user to upload via SFTP.\n# mkdir /SFTP # mkdir /SFTP/utho # chown utho:sftp\\_cloud /SFTP/utho Verify SSH \u0026 SFTP Access Login to SFTP\nOpen a new terminal window and log in with¬†sftp¬†using a valid user account and password.\n# sftp utho@Server\\_IP List files within the directory. Your output should be similar to the one below:\nutho@Server_IP‚Äôs password:\nConnected to server_IP"},"title":"How to configure SFTP server on Debian 12"},"/utho-docs/docs/linux/how-to-connect-node-js-application-with-mongodb-on-centos/":{"data":{"":"\nThis article will help you to connect Node.js application with MongoDB. Also, configure MongoDB drive for nodejs using Mongoose node application on CentOS and Redhat systems.","connect-nodejs-with-mongodb#Connect Nodejs with MongoDB":"Create a file test server.js, and add content to the file below. For the more details about working with Node.js and MongoDB with mongoose read¬†this tutorial.\n\u003c/ Sample script of Node.js with MongoDB Connection // This code requires mongoose node module var mongoose = require('mongoose'); // Connecting local mongodb database named test var db = mongoose.connect('mongodb://127.0.0.1:27017/test'); // testing connectivity mongoose.connection.once('connected', function() { console.log(\"Database connected successfully\") }); Now we execute the test_server.js using node. If we get message ‚ÄúDatabase connected successfully‚Äù, It means our node.js app is successfully connecting database.\nnode test_server.js Output - database connected successfully\nWe have connected Node.js Application with MongoDB successfully on CentOS.\nThankyou.","install-mongoose-module#Install mongoose Module":"Mongoose offers a simple schema-based approach for modeling data about the program which includes built-in typecasting, validation and much more.\nnpm install mongoose ","prerequsities#Prerequsities":"We assume that Node.js and MongoDB are already installed on your server. If not installed first follow our guide below to complete the installation.\nInstall MongoDB on Centos\nInstall node.js on Centos"},"title":"How to Connect Node.js Application with MongoDB on CentOS"},"/utho-docs/docs/linux/how-to-create-an-email-account-in-plesk-and-set-email-forwarding/":{"data":{"":"","#":"","1login-to-plesk#1.\u003cstrong\u003elogin to Plesk\u003c/strong\u003e":"","2go-todomainsexamplecomclick-email-addresses-and-create-email#2.Go to¬†\u003cstrong\u003eDomains\u003c/strong\u003e¬†\u0026gt;¬†\u003cstrong\u003eexample.com\u003c/strong\u003e¬†\u0026gt;¬†Click Email Addresses and Create Email.":"","3-complete-the-relevant-fields-and-then-select-ok-to-continue#\u003cstrong\u003e3. Complete the relevant fields, and then select \u0026ldquo;ok\u0026rdquo; to continue.\u003c/strong\u003e":"","4go-to-mail--your-email-address--the-forwarding-tab#\u003cstrong\u003e4.Go to Mail \u0026gt; your email address \u0026gt; the Forwarding tab.\u003c/strong\u003e":"","5to-activate-mail-forwarding-choose-the-corresponding-checkbox#\u003cstrong\u003e5.To activate mail forwarding, choose the corresponding checkbox.\u003c/strong\u003e":"","6also-under-this-box-type-the-name-of-the-email-address-to-which-you-would-like-to-forward-messages#\u003cstrong\u003e6.Also, under this box, type the name of the email address to which you would like to forward messages.\u003c/strong\u003e":"","you-can-verify-that-you-have-successfully-set-up-email-forwarding-at-this-point#\u003cstrong\u003eYou can verify that you have successfully set up email forwarding at this point\u003c/strong\u003e.":"\nDescription In this article you will know How to create an email account in Plesk And set email forwarding, Plesk is a web hosting platform that comes with a control panel that enables the administrator to set up websites, reseller accounts, e-mail accounts, DNS, and databases using a web browser. Integration of support for content management systems (CMS) is included in Plesk. These content management systems include WordPress, Joomla, and Drupal, among others.\n1.login to Plesk\n2.Go to¬†Domains¬†\u003e¬†example.com¬†\u003e¬†Click Email Addresses and Create Email.\n3. Complete the relevant fields, and then select ‚Äúok‚Äù to continue.\n4.Go to Mail \u003e your email address \u003e the Forwarding tab.\n5.To activate mail forwarding, choose the corresponding checkbox.\n6.Also, under this box, type the name of the email address to which you would like to forward messages.\nTake each step in the order they‚Äôre shown above to guide\n1.login to Plesk 2.Go to¬†Domains¬†\u003e¬†example.com¬†\u003e¬†Click Email Addresses and Create Email. 3. Complete the relevant fields, and then select ‚Äúok‚Äù to continue. You can now view the email address that you generated.\nNote: that the Email Addresses and Email Settings will be concealed within the Domain Dashboard if the Mail Management is turned off for the entire server.\nNow we are going to forward the emails from test1 email to test 2 email\n4.Go to Mail \u003e your email address \u003e the Forwarding tab. Click on any of the available emails, such as test@harikesh.xyz.\n5.To activate mail forwarding, choose the corresponding checkbox. 6.Also, under this box, type the name of the email address to which you would like to forward messages. And the click ok\nYou can verify that you have successfully set up email forwarding at this point. Hopefully now you can create an email account in Plesk And set email forwarding.\nThank You "},"title":"How to create an email account in Plesk And set email forwarding"},"/utho-docs/docs/linux/how-to-create-email-accounts-in-cpanel/":{"data":{"":"\nThis post will show you how to set up an email account in cPanels. You don‚Äôt have an email account set up by default. Although the main username appears to be set up as an email account, if you want to use it, you must add it as an email account.\n1-Login to the cpanel.\n2- Go to the email section and click on the Email account.\n3-Click the¬†+ Create¬†button located on the right-hand side.\n4-On the next page, enter the required details to create a new email account.\nDomain: Select the domain where you wish to create the new email account.\nUsername: Enter your email‚Äôs username.\nPassword: Enter your email‚Äôs password.\n5-You will notice the¬†Optional Settings. Click the¬†Edit Settings¬†button to customize the new email account. This is where you can set your email‚Äôs storage space.\n6-Click¬†+¬†Create¬†to finalize.\nThe same steps work for creating email accounts for your primary domain, addon domains, and subdomains. You cannot create email accounts with a script or via SSH.\nThank you!!"},"title":"How to Create Email Accounts in cPanel"},"/utho-docs/docs/linux/how-to-create-encrypt-and-decrypt-random-passwords-in-linux/":{"data":{"":"","#":"Description\nIn this article we will learn How to Create, Encrypt, and Decrypt Random Passwords in Linux In this essay, we will discuss how to create random passwords using Linux, as well as how to encrypt and decrypt passwords using either the slat approach or another way.\nIn this day and age of advanced digital technology, one of the primary problems that must be addressed is that of safety. We need to enter passwords in order to access a variety of items, including computers, email, the cloud, our phones, and numerous documents. The basics of picking a password that is easy to recall yet difficult to decipher are something that all of us are acquainted with. What about a machine-based password generator that takes care of everything for you and does it automatically? When I tell that Linux is extremely exceptional at achieving this, I ask that you trust me and accept my word for it.\nFollo the below steps to How to Create, Encrypt, and Decrypt Random Passwords in Linux..\nUsing the ‚Äòpwgen‚Äô command, generate a random, one-of-a-kind password with a length equal to ten characters. If you have not yet installed pwgen, you may download it by using either Apt or YUM. # pwgen 10 1 Produce a number of random and unique passwords with a length of 50 characters all in one go!\n# pwgen 50 You may use the‚Äômakepasswd‚Äô command to create a random, one-of-a-kind password of a specific length according to your preferences. Make sure that the makepasswd command has been installed on your system before you attempt to run it. If not! It is recommended that you use Apt or YUM to install the‚Äômakepasswd‚Äô package.\nProduce a password that is completely random and has a length of 10 characters. Default Value is 10.\n# mkpasswd Create a password that is completely random and has a length of 50 characters.\n# mkpasswd Crypt, in conjunction with salt, should be used to encrypt a password. Salt should be made available both manually and automatically.\nFor the benefit of people who aren‚Äôt familiar with salt:\nSalt is a kind of random data that is used as an extra input to one-way functions with the purpose of preventing passwords from being cracked via dictionary attacks.\nBefore moving on, check to verify that the mkpasswd package has been successfully installed.\nThe password will be encrypted using salt if you run the command below. The value of the salt is determined in a manner that is both random and mechanical. Therefore, the output will be different each time you run the command below since it accepts a random value for salt each time. As a result of this, running the command below will always provide different results.\n# mkpasswd microhost Conclusion One of the key issues that must be addressed in this day and age of sophisticated digital technology is that of safety. Passwords are required to get access to a range of products, including computers, email, the cloud, smart phones, and various documents. We are all familiar with the principles of choosing a password that is simple to remember yet difficult to decode. What about a machine-based password generator that does everything for you automatically? When I say that Linux is very good at this, I ask that you believe me and take my word for it.\nMust read https://utho.com/docs/tutorial/convert-rwx-permissions-to-octal-format-in-linux/:-\nThank You"},"title":"How to Create, Encrypt, and Decrypt Random Passwords in Linux"},"/utho-docs/docs/linux/how-to-create-hard-and-symbolic-links/":{"data":{"":"","#":"","deleting-or-removing-links#\u003cstrong\u003eDeleting or Removing Links\u003c/strong\u003e":"\nDescription In this Article we will discuss about how to create Hard and Symbolic Links, ‚ÄúEverything is a file‚Äù is the credo of Unix-like operating systems like Linux, and at its core, a file is nothing more than a link to an inode (a data structure that stores everything about a file apart from its name and actual content).\nA file is said to have a hard link if it links to the same underlying inode as another file does. In the event that you delete one file, one link to the file‚Äôs underlying inode will be removed. In contrast, a symbolic link, which may also be referred to as a soft link, is a link that points to the name of another file located inside the filesystem.\n**Note:**The fact that hard links can only be used inside the same filesystem is another significant distinction between the two kinds of connections. Symbolic links, on the other hand, may be used across a variety of filesystems.\nHow to Create Hard Links #ls -l *Create hard link\n#ln source filename create hardlink file How to Create Symbolic Links #ln -s source.file linkfile *ln creates hard links by default.\n*-s creates a soft link.\n*-f overwrites an existing file.\nSource is the referenced file or directory.\nIf Destination is blank, the link is saved in the current working directory.\nDeleting or Removing Links #rm linkfile name #unlink linkfile name Hard Links vs. Soft Links\nThe following are the two categories of connections that can be created with the ln command:\nSoft links\nHard Links\nSoft( symbolik)Links\nA soft link, also known as a symbolic link or symlink, directs users to the path or location of the original file. On the internet, it operates much like a hyperlink.\nHere are some crucial features of a soft link:\nThe original data is still there even if the symbolic link file is removed.\nThe symbolic link will not operate if the original file is relocated or removed.\nA file on a separate file system may be referenced using a soft link.\nTo easily access a commonly used file without inputting the whole address, soft links are widely utilised.\nHard Links\nWhen a file is saved to a hard disc, the following occur:\nThe data is written physically to the disc.\nA reference file, calledinode, is generated to indicate where the data is stored.\nCreating a filename to relate to the inode data.\nA hard link operates by establishing a new filename that relates to the original file‚Äôs inode data. In practise, this is equivalent to duplicating the file.\nHere are few essential features of hard links:\nIf the original file is erased, the data of the file may still be accessible through hard links.\nIf the original file is transferred, hard links continue to function.\nOnly files on the same file system may be referenced by a hard link.\nWhen the number of hard links is zero, the inode and file contents are permanently erased.\nThank You","how-to-create-hard-links#\u003cstrong\u003eHow to Create Hard Links\u003c/strong\u003e":"","how-to-create-symbolic-links#\u003cstrong\u003eHow to Create Symbolic Links\u003c/strong\u003e":""},"title":"How to Create Hard and Symbolic Links"},"/utho-docs/docs/linux/how-to-create-temporary-and-permanent-redirects-with-apache/":{"data":{"":"","analyzing-redirect-methods#Analyzing Redirect Methods":"Redirects can be used in many ways. If you already have a website and want to change your domain, you shouldn‚Äôt just give up on your old domain. If the content on your site disappears without telling the browser where it is now, bookmarks and links to your site from other pages on the internet will stop working. If you change domains without redirecting, you‚Äôll lose traffic from people who used to visit your site and all the credibility you‚Äôve worked hard to build.\nOften, it‚Äôs a good idea to register multiple versions of a name so that people who type in addresses that look like your main domain can find you. For example, if you have a domain called myspiders.com, you could also buy the domain names for myspiders.net and myspiders.org and point them both to your myspiders.com site. This lets you catch people who might be typing in the wrong address to get to your site. It can also stop another site from using a similar domain name and making money off of your online presence.\nSometimes, you need to change the names of pages on your site that have already been published and gotten traffic. In most cases, this would lead to a 404 Not Found error or, depending on your security settings, another error. You can avoid these by sending your visitors to a different page that has the right information they were looking for. There are a few different kinds of URL redirects, and each one tells the client browser something different. 302 temporary redirects and 301 permanent redirects are the most common.\nTemporary Redirects Temporary redirects are useful if you need to serve the web content for a certain URL from a different place for a short time. For example, if you are doing maintenance on your site, you may need to use a temporary redirect to send all of your domain‚Äôs pages to a page explaining that you will be back soon.\nTemporary redirects tell the browser that the content is temporarily at a different location, but that the original URL should still be tried.\nPermanent Redirects Permanent redirects are useful when your content has been moved to a new location forever.\nThis is useful for when you need to change domains or when the URL needs to change for other reasons and the old location will no longer be used. This redirect informs the browser that it should no longer request the old URL and should update its information to point to the new URL\nForcing SSL One common way to use redirects is to tell all traffic to a site to use SSL instead of HTTP.\nYou can make all requests for http://www.example.com go to https://www.example.com by using redirects.\nHow to Redirect in Apache Apache has a few tools that it can use to redirect. Tools from the mod alias module can be used to set up simple redirects, and the mod rewrite module can be used to set up more complex redirects.\nUsing the Redirect Directive With Apache, you can use the ‚ÄúRedirect‚Äù directive, which is part of the mod alias module and is turned on by default, to redirect a single page. At least two arguments are needed for this directive: the old URL and the new URL.\nApache server blocks can be saved in the main Apache settings file in /etc/apache2/apache2.conf, but it‚Äôs easier to manage if you make a new file for each configuration in /etc/apache2/sites-available/. As you turn files on or off, Apache creates symbolic links (shortcuts) from files in sites-available/ to files in another folder called sites-available/. By default, Apache is installed with a single site-specific configuration in /etc/apache2/sites-available/000-default.conf that is enabled and linked to /etc/apache2/sites-enabled/000-default.conf.\nvi /etc/apache2/sites-available/000-default.conf By default, it contains a standard web server configuration that will listen on port 80 and look for an¬†index.html¬†file located in¬†/var/www/html¬†on your system.\nIf you instead needed to send requests for domain1.com to domain2.com, you could remove the existing directives from this server block and replace them with a permanent redirect:\nThis tells the browser that any requests for www.domain1.com should be sent to www.domain2.com. This only applies to one page, not the whole site.\nBy default, the Redirect directive sets up a temporary redirect called a 302. If you want to make a permanent redirect, you can do it in either of the two ways below:\nMethod 1:\nvi /etc/apache2/sites-available/000-default.conf \u003cVirtualHost *:80\u003e ServerName www.domain1.com Redirect 301 /oldlocation http://www.domain2.com/newlocation \u003c/VirtualHost\u003e Method 2:\nvi /etc/apache2/sites-available/000-default.conf \u003cVirtualHost *:80\u003e ServerName www.domain1.com Redirect permanent /oldlocation http://www.domain2.com/newlocation \u003c/VirtualHost\u003e Using the RedirectMatch Directive You can redirect more than one page by using the RedirectMatch directive, which lets you use regular expressions to set up patterns that match directories.\nThis will allow you to redirect entire directories instead of just single files.\nRedirectMatch looks for patterns in parentheses and then uses ‚Äú$1‚Äù, where ‚Äú1‚Äù is the first group of text, to refer to the matched text in the redirect. Numbers are given in order to the next groups.\nFor example, if you wanted to match every request for something within the /images directory to a subdomain named images.example.com, you could use the following:\nvi /etc/apache2/sites-available/default \u003cVirtualHost *:80\u003e ‚Ä¶ RedirectMatch ^/images/(.*)$ http://images.example.com/$1 ‚Ä¶ \u003c/VirtualHost\u003e As with the Redirect directive, you can choose the type of redirect by putting the redirect code before the URL location rules.\nYou‚Äôll need to restart Apache for your changes to take effect. This can be done with systemctl on a new Ubuntu server:\nsystemctl restart apache2 Using mod_rewrite to Redirect The mod rewrite module is the most flexible way to set up redirect rules, but it is also the most complicated. How to Rewrite URLs with mod rewrite for Apache has more information about how to work with mod rewrite.","introduction#Introduction":"HTTP redirection, also called URL redirection, is a way to send a domain or address to a different one. There are many ways to use redirection, and there are a few different kinds to think about. Redirects are used when a website needs to send people who type in one address to a different one.\nAs you make content and manage servers, you will often need to send traffic from one place to another. This guide will talk about how these techniques can be used and how to do them in Apache and Nginx, which are the two most popular web servers.","prerequisites#Prerequisites":" An Ubuntu 20.04 server set up A super user or any normal user with SUDO privileges. Apache server installed on your Ubuntu server "},"title":"How To Create Temporary and Permanent Redirects with Apache on Ubuntu"},"/utho-docs/docs/linux/how-to-enable-rdp-in-ubuntu-os-tasksel/":{"data":{"":"\nIn this Article we will know the process to add XRDP service through Tasksel packages.\nFirst we need to update the Ubuntu OS with help of below command-\n# apt update After successfully updating the system, I need to install the tasksel packages on the system.\n# apt install tasksel Now need to deploy the tasksel in your system, you need to follow below screenshot steps.\n# tasksel After successfully install the ubuntu Desktop, you need to reboot the system.\n# reboot Now we have to install the XRDP in system to access the machine from the outside of world\n# apt install xrdp Now we need to enable the xrdp service in our system\n# systemctl enable xrdp :: After running the above command, you will get the below result. Now you are able to access your ubuntu machine from outside of the world through RDP access.\nNot recommended for all users‚Ä¶\nSome extra features which you may also run to give a look like Windows system to your Ubuntu Desktop.\n# apt install gnome-shell-extensions gnome-shell-extension-dash-to-panel gnome-tweaks adwaita-icon-theme-full # reboot Click on Activities, then search on search bar Tweaks\nSelect Dash to panel and Desktop icons"},"title":"How to enable RDP in Ubuntu OS (Tasksel)"},"/utho-docs/docs/linux/how-to-execute-a-command-with-a-timeout-in-linux/":{"data":{"":" How to Execute a Command with a Timeout in Linux\nIn this tutorial, we will see how to execute a command with a timeout in Linux because Linux has a multiplicity of commands, each of which serves a specific function and may be run only under specified conditions. Linux is intended to help users be as efficient and successful as possible. One of the features of a Linux command is its maximum execution time. You have the power to limit the duration of any command you wish. The order will no longer be carried out when the time limit has expired.","1---timeout-utility#1 - Timeout utility":"In Linux, you may perform a command with a time limit since the operating system has a command-line capability called a timeout.\nAn example of its syntax is as follows:\ntimeout [OPTION] DURATION COMMAND [ARG]... To use the command, you must give a timeout value in seconds along with the command that you want to perform. For example, to timeout a ping command after 5 seconds, use the following command.\ntimeout 3s ping google.com You are not required to provide the number(s) that come after 3. The command shown below has not been modified and will continue to work correctly.\ntimeout 3 ping google.com Even after the timeout has sent the first signal, the instructions may continue to execute in certain circumstances. The ‚Äîkill-after option is available for usage in situations like these.\nThe syntax is as follows.\n-k, --kill-after=DURATION You must give timeout with a period so that it knows after how much time the kill signal will be sent.\nFor instance, the command being shown will be terminated after 10 seconds.\ntimeout 10 tail -f /var/log/syslog ","2--timelimit-program#2- Timelimit Program":"When a particular length of time has passed, the Timelimit programme performs a given instruction and then terminates the operation by delivering a supplied signal. It starts by providing a warning signal, then after a set period of time, it activates the death signal.\nIn contrast to the timeout option, the timelimit option gives users with additional options such as killsig, warnsig, killtime, and warntime.\nTimelimit may be found in the repositories of Debian-based operating systems, and the following command can be used to install it on your system.\n# apt install timelimit After the installation is complete, execute the following command and give the time. In this instance, you are permitted to use 5 seconds.\n# timelimit -t5 ping google.com Note that if you do not provide any parameters, Timelimit will use the default values: warntime=3600 seconds, warnsig=15, killtime=120, and killsig=9.\nIn this tutorial, you have learnt how to execute a command with a timeout\nAlso Read: How to Install Xrdp Server on Ubuntu 22.04, How to Install Node.js and npm on Ubuntu 20.04"},"title":"How to Execute a Command with a Timeout in Linux"},"/utho-docs/docs/linux/how-to-extract-and-download-tar-files-with-a-single-command/":{"data":{"":"","#":"\nDescription In this article, you will learn how to extract and download tar files. Tar (Tape Archive) is a common Linux file-archiving format. It can be used for compression alongside gzip (tar.gz) or bzip2 (tar.bz2). It is the most extensively used command-line application for creating compressed archive files (packages, source code, databases, and much more) that can be readily moved from one system to another or across a network.\nIn this post, we will demonstrate how to download and extract tar archives using the well-known command line downloaders wget and cURL.","how-to-use-the-wget-command-to-download-and-extract-files#How to Use the Wget Command to Download and Extract Files":"The following example demonstrates how to download and unpack the most recent GeoLite2 country databases (used by the GeoIP Nginx module) into the current directory.\n# wget -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz -O - | tar -xz It will be written to standard output and piped to tar, and the tar flag -x enables the extraction of archive files, and the -z flag decompresses compressed archive files created by gzip. The wget option -O specifies a file to which the documents are written, and here we use -, which means they will be written to standard output.\nTo extract tar files to a specific directory, which in this example is /etc/nginx/, use the -C switch and the following instructions.\nNOTE: If extracting files to a directory that requires root rights, use tar with the sudo command.\n# sudo wget -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz -O - | sudo tar -xz -C /etc/nginx/ You also have the option of using the following command, in which case the archive file will be downloaded onto your machine prior to your being able to extract it.\n# sudo wget -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz \u0026\u0026 tar -xzf GeoLite2-Country.tar.gz Use this command to extract a compressed archive file to a specified location:\n# sudo wget -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz \u0026\u0026 sudo tar -xzf GeoLite2-Country.tar.gz -C /etc/nginx/ ","the-curl-command-walkthrough-how-to-download-and-extract-files#The cURL Command Walkthrough: How to Download and Extract Files":"Using cURL, you can do things like download archives and unpack them in your current working directory, as demonstrated above.\n# sudo curl http://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz | tar -xz Use the following command to extract a file to a different directory while it is being downloaded.\n# sudo curl http://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz | sudo tar -xz -C /etc/nginx/ And that‚Äôs it! In this brief but informative tutorial, we demonstrated how to download and extract archive files with just a single command on your computer.\nThankyou"},"title":"How to Extract and Download Tar Files with a Single Command"},"/utho-docs/docs/linux/how-to-find-and-sort-files-in-linux-based-on-modification-date-and-time/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"\nDescription\nOn a regular basis, we make it a practise to save a significant amount of data on our system in the form of files. Some of them are hidden files, some are maintained in a different folder that was made just for our convenience, while others are left as they are. However, all of this information fills up our folders, most often the desktop, making it appear like a jumbled mess. But we run into an issue when we need to search this massive collection for a certain file that was edited at a specific date and time, and that file may be everywhere.\nThose who are accustomed to working with graphical user interfaces (GUIs) can locate it by using File Manager, which lists files in a long listing format, which makes it simple to determine what we were looking for. On the other hand, users who are accustomed to working with black screens, as well as anyone who works on servers that do not have GUIs, would want a straightforward command or set of commands that could make their search simpler.\nThe true power and flexibility of Linux is on display here, as the operating system comes with a set of commands that, when utilised singly or in conjunction with one another, can assist in the search for a file or the sorting of a collection of files according to their name, date of modification, time of creation, or even any filter that can be imagined being applied in order to obtain the desired result.\nIn this section, we will demonstrate the true power of Linux by analysing a group of commands that may assist in the sorting of a file or even a series of files according to the date and time that they were created.\nSorting Files in Linux using Linux Utilities The following are some examples of fundamental Linux command line tools that are enough for sorting a directory according to the date and time:\nls command:- Listing the contents of the directory, this application may display the files and directories and can even provide all of the status information about them, such as the date and time of the most recent access or change, permissions, size, owner, and group, among other things.\nsort command sort:- Using this command, you are able to sort the results of any search simply based on any field or any individual column of the field.\nIf you operate on dark screens and have to sort through a large number of files in order to get the one you want, you will find that these commands, in and of themselves, are quite powerful skills to acquire.\nVarious Methods for Classifying Documents Based on Date and Time\nYou can find a list of instructions to sort based on date and time below.\nCreate a list of files based on the most recent modification time. The following command will display the files in a long listing style and will order the files based on the most recent modification time. To sort in the other direction, use the ‚Äò-r‚Äô option in conjunction with this command.\n# ls -lt List Files According to Last Access Time Listing of files in directory based on last access time, that is, listing of files based on the time the file was last accessed rather than the time the file was last edited.\n# ls -ltu List Files According to Last Modification Time The ‚Äòctime‚Äô is a listing of files in the directory that is based on the most recent modification time of the file‚Äôs status information. The first file that is shown by this command is the one whose status information, including the owner, group, permissions, and size, has been modified within the last several minutes.\n# ls -ltc With the ‚Äò-a‚Äô choice, the above commands may list and sort even hidden files in the current directory, while the ‚Äò-r‚Äô switch shows the result in reverse order.\nFor more in-depth sorting, such as sorting on the result of the search command, ls may also be used, but‚Äôsort‚Äô is more useful since the output may include more than just the file name.\nThe following lines demonstrate how to use the sort command in conjunction with the find command to sort a list of files based on Date and Time.\nSorting Files by Month In this case, we use the search command to locate all of the files in the root (‚Äô/‚Äô) directory, and we then report the result in the following order: the month in which the file was accessed, followed by the filename.\n# find / -type f -printf \"n%Ab %p\" | head -n 5 The following command sorts the output with the key as the first field, as supplied by the ‚Äò-k1‚Äô parameter, and then it sorts on the month, as specified by the ‚ÄòM‚Äô parameter that comes before it.\n# find / -type f -printf \"n%Ab %p\" | head -n 5 | sort -k1M ![output](images/image-390.png) ## **Sorting Files by Date** Here, we use the search command once again to locate all of the files in the root directory; however, this time, we will output the results in the following order: filename, last date the file was accessed, and then last time the file was accessed. [console]# find / -type f -printf \"n%AD %AT %p\" | head -n 5 The sort command that follows first sorts on the basis of the last digit of the year, then sorts on the basis of the final digit of the month in the opposite order, and finally sorts on the basis of the first field. In this case, ‚Äú1.8‚Äù refers to the eighth column of the first field, and the letter ‚Äún‚Äù that comes before it indicates that the data is being sorted numerically. The letter ‚Äúr‚Äù indicates that the data is being sorted in reverse order.\n# find / -type f -printf \"n%AD %AT %p\" | head -n 11 | sort -k1.8n -k1.1nr -k1 Sorting Files by Date Here, we use the find command again to list the top 5¬†files in the root directory. The result is printed in the format ‚Äúlast time file was accessed, then filename.‚Äù\n# find / -type f -printf \"n%AT %p\" | head -n 11 The following command arranges the data based on the first column of the first field, which is the first digit of the hour.\n# find / -type f -printf \"n%AT %p\" | head -n 11 | sort -k1.1n Sorting ls -l output by date The output of the ‚Äôls -l‚Äô command is sorted by this command based on the 6th field in the month-wise order, then based on the 7th field, which is the date, in the numerical order.\n# ls -l | sort -k6M -k7n Conclusion In a similar vein, if you have a working grasp of the sort command, you may sort practically any listing according to any field, as well as any column of that listing. These are some tips that might assist you in sorting files depending on the date or time they were created. You are able to have your own own techniques built on top of them.","create-a-list-of-files-based-on-the-most-recent-modification-time#\u003cstrong\u003eCreate a list of files based on the most recent modification time.\u003c/strong\u003e":"","list-files-according-to-last-access-time#\u003cstrong\u003eList Files According to Last Access Time\u003c/strong\u003e":"","list-files-according-to-last-modification-time#\u003cstrong\u003eList Files According to Last Modification Time\u003c/strong\u003e":"","sort-command#\u003cstrong\u003esort command\u003c/strong\u003e":"","sorting-files-by-date#\u003cstrong\u003eSorting Files by Date\u003c/strong\u003e":"","sorting-files-by-month#\u003cstrong\u003eSorting Files by Month\u003c/strong\u003e":"","sorting-files-in-linux-using-linux-utilities#\u003cstrong\u003eSorting Files in Linux using Linux Utilities\u003c/strong\u003e":"","sorting-ls--l-output-by-date#\u003cstrong\u003eSorting ls -l output by date\u003c/strong\u003e":""},"title":"How to Find and Sort Files in Linux Based on Modification Date and Time"},"/utho-docs/docs/linux/how-to-fix-command-not-found-error-in-centos/":{"data":{"":"\nNew Linux or Unix users often ask this. ‚ÄúCommand not found‚Äù means Linux or UNIX looked everywhere for that application but couldn‚Äôt find it. You may have misspelled the command name (typo) or the system administrator did not install it. Try these recommendations to fix this error:\nEnsure that command is the path\nPATH is a shell environment variable that specifies the folders that your shell will examine to locate commands. The current search path can be viewed with the following echo/printf command:\nStep 1. Log in to your server via SSH.\nstep :2\n#echo \"$PATH\" Most user commands are in the directories /bin, /usr/bin, or /usr/local/bin. These directories are where all of your programmes are put. When you type ‚Äúclear,‚Äù you run the /usr/bin/clear file. So, if it isn‚Äôt in your PATH, try adding directories to your search path as follows (setup Linux or UNIX search path with the following bash export command):\nStep:3\n#export PATH=$PATH:/bin:/usr/local/bin copy this command(export PATH=$PATH:/bin:/usr/local/bin) to .bashrc file\nStep:4\n#vi .bashrc bashrc file is a script, and by source it, the commands contained within it are executed.\nStep:5\n#source .bashrc Why source bashrc\nRunning the bashrc file is one reason to use source. bashrc is a script file executed when launching an interactive shell. It‚Äôs per-user and in your home directory."},"title":"How to fix \"Command not found\" error in CentOS"},"/utho-docs/docs/linux/how-to-host-node-js-application-on-plesk/":{"data":{"":"\nNode.js is an open source server environment. Node.js allows you to run JavaScript on the server.¬†**Step 1. Firstly, download the node.js¬†installer for your platform.\n**You can use the link below to download node.js.\nLINK:¬†https://nodejs.org/en/download/\nStep 2.¬†login to Plesk\nStep 3.¬†Upload the node.js (that you downloaded) in file manager (in Home Directory \u003e httpdocs)\nThen extract it in the same folder.\nStep 4. Go back to ‚ÄúWebsites and Domains‚Äù and click on the node.js\nIf you don‚Äôt find node.js under your domain, then you have to add it manually. Under Plesk panel, go to Tools and Settings \u003e updates \u003e Add/Remove Components \u003e click to install node.js\nStep 5. Now you can see Node.js under¬†‚ÄúWebsites and Domains‚Äù. Click on it.\nStep 6.¬†Set Application Root to Root Directory\nStep 7.¬†Step up¬†Application Startup File to ‚Äúnode.js‚Äù\nStep 8.¬†Click on enable Node.js\nNode.js Enabled.\nStep 9.¬†Go back to ‚ÄúWebsites and Domains‚Äù¬†and click on ‚ÄúNode.js App‚Äô to access your Node.js Application.\nThank you."},"title":"How to host node.js application on Plesk"},"/utho-docs/docs/linux/how-to-import-and-export-databases-in-mysql-or-mariadb/":{"data":{"":"\nIntroduction\nDatabase import and export is a common task in software development. Data dumps can be used to backup and restore your information. They are also useful for migrating data to a new server or development environment.\nIn this tutorial, you will work with MySQL or MariaDB database dumps (the commands are interchangeable). You will specifically export a database and then import that database from the dump file.\nPrerequisites\nYou will need the following tools to import or export a MySQL or MariaDB database:\n*A virtual machine with a sudo user that is not root. If you require a server, visit this page to create a microhost Droplet running your preferred Linux distribution. Following creation, select your distribution from this list and proceed with our Initial Server Setup Guide.\n*MySQL or MariaDB must be installed. Follow our How To Install MySQL tutorial to install MySQL. Follow our How To Install MariaDB tutorial to get started.\n*In your database server, create a sample database. To make one, go to ‚ÄúCreating a Sample Database‚Äù in our ‚ÄúAn Introduction to Queries in MySQL‚Äù tutorial.\nNote: Instead of manually installing MySQL, you can use the DigitalOcean Marketplace‚Äôs MySQL One-Click Application.\nStep-1 The First Step ‚Äî Exporting a MySQL or MariaDB Database\nThe console utility mysqldump exports databases to SQL text files. This facilitates the transfer and relocation of databases. You will need the name of your database as well as credentials for an account with at least full read-only access to the database.\nTo export your database, use mysqldump:\n#mysqldump -u username -p database_name \u003e data-dump.sql _username is the username with which you can access the database.\n_The database to export is called database name.\n*The output is stored in the data-dump.sql file located in the current directory.\nAlthough the command won‚Äôt output anything visually, you can examine the contents of data-dump.sql to determine whether it‚Äôs a genuine SQL dump file.\nRun the command line:\n#head -n 5 data-dump.sql The top of the file should look like this, with a MySQL dump for the database database name.\nSQL dump fragment -- MySQL dump 10.13 Distrib 5.7.16, for Linux (x86_64) -- -- Host: localhost Database: database_name -- ------------------------------------------------------ -- Server version 5.7.16-0ubuntu0.16.04.1 Mysqldump will print any errors to the screen if they arise during the export procedure.\nStep 2 ‚Äî Importing a MySQL or MariaDB Database\nYou must build a new database if you want to import an existing dump file into MySQL or MariaDB. The imported data will be stored in this database.\nIn order to create new databases, you must first log into MySQL as root or another user who has the necessary rights:\n#mysql -u root -p This command will launch the MySQL shell prompt. Then, run the following command to create a new database. The new database in this example is called new database:\nmysql\u003e #CREATE DATABASE new_database; This result verifies the construction of the database.\nOutput Query OK, 1 row affected (0.00 sec) Then, using CTRL+D, exit the MySQL shell. You can import the dump file from the command line using the following command:\n#mysql -u username -p new_database \u003c data-dump.sql _username is the username with which you can access the database.\n_The newly established database is known by the name newdatabase.\n*The data dump file to be imported is called data-dump.sql, and it can be found in the current directory.\nIf the command is executed successfully, no output is produced. If any errors arise during the process, mysql will instead print them to the terminal. Log in to the MySQL shell and inspect the data to see if the import was successful. Selecting the new database with USE new database and then viewing some of the data with SHOW TABLES; or a similar command."},"title":"How To Import and Export Databases in MySQL or MariaDB"},"/utho-docs/docs/linux/how-to-increase-and-decrease-the-lvm-size/":{"data":{"":" How to increase and decrease the LVM size\nIn this tutorial, you will learn how to increase and decrease the LVM size. But before learning the steps to do so, first understand what is LVM and how it works","description#Description":"LVM, which stands for ‚ÄúLogical Volume Management,‚Äù is a framework in the Linux operating system that makes it easier to manage physical storage devices. Logical volume management is a lot like virtualization in that you can set up as many virtual storage volumes as you want on top of a single storage device. The logical storage volumes that are made can be made bigger or smaller depending on how much storage you need.\nFor this feature to work, it‚Äôs important to remember that the filesystem itself must support resizing. The EXT2, 3, and 4 filesystems all let you change the size of a filesystem both offline (when the filesystem is not mounted) and online (when the filesystem is mounted). You should look into the filesystems you want to use to see if they can be resized at all, and especially if they can be resized while you are online.","how-to-increase-the-lvm-partition-size#How to increase the LVM partition size":"In this document, we are using one physical volume( /dev/sdb) from which we have created one volume group named as fedora( /dev/fedora). We have created one logical volume from fedora named as lvol0.\nWe have two methods by which we can increase the size of LVM partition.\nlvextend command lvresize command 1. lvextend Method If we are using this command, we need to run another command just to sync the inodes numbers to make the changes in effects.\nStep 1.1 First let‚Äôs see the status of the logical volume and volume group.\n# lvdisplay lvdisplay output\n# lvs status of logical volume\n# vgs volume group status\nNow let‚Äôs increase the size of the logical volume\nlvextend -L +200M /dev/fedora/lvol0 extend the logical volume\nHere in above command,\n-L 1200 will increase the size of logical volume from 1000M to 1200M, please note that if you want to increase the size without knowing the final size, you can use -L +200M to increase the size of lvm by 200 mb. /dev/fedora/lvol0 is the logical volume, lvm, of which we want to increase the size. Now if this logical volume has ext2/ ext3 or ext4 file system, they we will use resize2fs command. And if your lvm has xfs file system then you need to run xfs_growfs to sync the inode table of the partitions.\n# resize2fs /dev/fedora/lvol0 sync the inode number\nIn this example we used ext4 file system. Therefore we used resize2fs command.\nPlease note that, you can increase the size of lvm partition in either online mode or offline mode. But you cannot decrease the size of any lvm in online mode.\n2. lvresize Method To resize the lvm partition by lvresize command you do not have to use any other command to sync the inode table.\nlvresize -L 1200M -r /dev/fedora/lvol0 \u003cfigure\u003e ![resize the LVM size using lvresize](images/image-43-1024x63.png) \u003cfigcaption\u003e resize the LVM size using lvresize \u003c/figcaption\u003e \u003c/figure\u003e \u003cfigure\u003e ![output of lsblk](images/image-46.png) \u003cfigcaption\u003e output of lsblk \u003c/figcaption\u003e \u003c/figure\u003e ## How to decrease the LVM partition size - Suppored in only offline mode - Xfs based FS cannot be reduced - The above command of lvresize runs to reduce the size \u003cfigure\u003e ![status of Logical volume](images/image-48.png) \u003cfigcaption\u003e status of Logical volume \u003c/figcaption\u003e \u003c/figure\u003e [console]# lvresize -L -100M /dev/fedora/lvol0 Decrease the Logical volume size\nNow after doing everything, please check the size of the lvm partition.\n# lsblk output of lsblk\nor you can check the free space by using df command\n# df -h In this tutorial, you have learnt how to increase and decrease the LVM size\nAlso Read: How to install Drupal on Fedora, How to install Cockpit on Ubuntu server","structure-of-lvm#Structure of LVM":"Logical Volume Management lets you combine several hard drives and/or disc partitions into a single volume group (VG). Then, that volume group can be broken up into logical volumes (LV) or used as one big volume. Then, you can create regular file systems, like EXT3 or EXT4, on a logical volume.\nLVM structure"},"title":"How to increase and decrease the LVM size"},"/utho-docs/docs/linux/how-to-install-a-php-version-in-whm/":{"data":{"":"","#":"","1-connect-to-whm#1. Connect to WHM.":"","2-go-to-the-easyapache-4-page#2. Go to the EasyApache 4 page":"","3-to-customise-the-currently-installed-packages-click-the-customize-button#3. To customise the currently installed packages, click the Customize button.":"","4-clickphp-versions#4. Click¬†PHP Versions.":"","5-click-the-switch-symbol-that-is-located-to-the-far-right-of-the-version-to-indicate-that-the-version-should-be-installed#5. Click the switch symbol that is located to the far right of the version to indicate that the version should be installed.":"","6-when-asked-click-the-option-to-only-install-the-php-version-or-the-php-version-and-extensions#6. When asked, click the option to only install the PHP version or the PHP version and extensions":"","7-click-review-and-then-look-at-the-output-to-make-sure-that-the-php-version-you-chose-will-be-installed#7. Click Review, and then look at the output to make sure that the PHP version you chose will be installed":"","8-when-you-click-provision-the-versions-of-php-that-you-chose-will-be-installed#8. When you click Provision, the versions of PHP that you chose will be installed.":"\nDescription\nPHP, which stands for ‚ÄúHypertext PreProcessor,‚Äù is the most popular open source and general purpose server side scripting language. Its primary application is in web development, namely in the production of dynamic websites and applications. Rasmus Lerdorf came up with the idea for it in the year 1994.\nWHM (WebHost Manager) is a control panel that allows users to manage the backend of numerous cPanel accounts. You may manage individual accounts as well as create server-side limits with WHM. This post will explain why you should utilise WHM and how it may help you manage your\nFollow the below steps to Install a PHP Version in WHM‚Ä¶..\nProcedure Here are the measures you need to do to update PHP using WHM.\n1.Connect to WHM.\n2. Go to the EasyApache 4 page.\n3. To customise the currently installed packages, click the Customize button.\n4. Click¬†PHP Versions.\n5. Click the switch symbol that is located to the far right of the version to indicate that the version should be installed.\n6. When asked, click the option to only install the PHP version or the PHP version and extensions.\n7. Click Review, and then look at the output to make sure that the PHP version you chose will be installed.\n8. When you click Provision, the versions of PHP that you chose will be installed.\n1. Connect to WHM. 2. Go to the EasyApache 4 page 3. To customise the currently installed packages, click the Customize button. 4. Click¬†PHP Versions. 5. Click the switch symbol that is located to the far right of the version to indicate that the version should be installed. 6. When asked, click the option to only install the PHP version or the PHP version and extensions 7. Click Review, and then look at the output to make sure that the PHP version you chose will be installed 8. When you click Provision, the versions of PHP that you chose will be installed. Hope you have understand How To Install a PHP Version in WHM ..continue read..\nMust read :- https://utho.com/docs/tutorial/find-out-all-live-hosts-ip-addresses-connected-on-network-in-linux/\nThank you"},"title":"How To Install a PHP Version in WHM"},"/utho-docs/docs/linux/how-to-install-aapanel-on-centos-7-by-one-click/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install aaPanel on Centos 7 by one click.\nThank You üôÇ","installation#Installation":" # yum install -y wget \u0026\u0026 wget -O install.sh http://www.aapanel.com/script/install_6.0_en.sh \u0026\u0026 sh install.sh 93684c35 Paste the aapanel Internet address in your browser to login into the aapanel, which is shown in the previous screenshot. Username and password are also shown in the screenshot.","introduction#Introduction":"In this article, you will learn how to install aaPanel on Centos 7 by one click.\naaPanel is a control panel for web servers that is an alternative to cPanel and Vesta. It was developed in China. BT.cn is responsible for its development, and it is now on version v6. 8.5 (at the moment of the writing). It is free, has reached an adequate level of maturity, and comes with some really excellent features such as an editor, uploader, file management, backups, and preconfigured Nginx rules.\nNote: If you are setting up a new system that has not yet had other environments such as Apache/Nginx/php/MySQL installed, you will need to install aaPanel.\nThe following components are often included in web hosting control panels:\nWeb server¬†(e.g.¬†Apache HTTP Server,¬†NGINX,¬†Internet Information Services) Domain Name System¬†server Mail server¬†and¬†spam¬†filter File Transfer Protocol¬†server Database File manager System monitor Web log analysis software Firewall phpMyAdmin "},"title":"How to install aaPanel on Centos 7 by one click"},"/utho-docs/docs/linux/how-to-install-aapanel-on-debian-by-one-click/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install aaPanel on Debian by one click.\nThank You üôÇ","installation#Installation":" # wget -O install.sh http://www.aapanel.com/script/install-ubuntu_6.0_en.sh \u0026\u0026 bash install.sh 93684c35 Paste the aapanel Internet address in your browser to login into the aapanel, which is shown in the previous screenshot. Username and password are also shown in the screenshot.","introduction#Introduction":"In this article, you will learn how to install aaPanel on Debian by one click.\naaPanel is a control panel for web servers that is an alternative to cPanel and Vesta. It was developed in China. BT.cn is responsible for its development, and it is now on version v6. 8.5 (at the moment of the writing). It is free, has reached an adequate level of maturity, and comes with some really excellent features such as an editor, uploader, file management, backups, and preconfigured Nginx rules.\nNote: If you are setting up a new system that has not yet had other environments such as Apache/Nginx/php/MySQL installed, you will need to install aaPanel.\nThe following components are often included in web hosting control panels:\nWeb server¬†(e.g.¬†Apache HTTP Server,¬†NGINX,¬†Internet Information Services) Domain Name System¬†server Mail server¬†and¬†spam¬†filter File Transfer Protocol¬†server Database File manager System monitor Web log analysis software Firewall phpMyAdmin "},"title":"How to install aaPanel on Debian by one click"},"/utho-docs/docs/linux/how-to-install-aapanel-on-fedora-by-one-click/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install aaPanel on Fedora by one click.\nThank You üôÇ","installation#Installation":" # wget -O install.sh http://www.aapanel.com/script/install_6.0_en.sh \u0026\u0026 bash install.sh 93684c35 Paste the aapanel Internet address in your browser to login into the aapanel, which is shown in the previous screenshot. Username and password are also shown in the screenshot.","introduction#Introduction":"In this article, you will learn how to install aaPanel on Fedora by one click.\naaPanel is a control panel for web servers that is an alternative to cPanel and Vesta. It was developed in China. BT.cn is responsible for its development, and it is now on version v6. 8.5 (at the moment of the writing). It is free, has reached an adequate level of maturity, and comes with some really excellent features such as an editor, uploader, file management, backups, and preconfigured Nginx rules.\nNote: If you are setting up a new system that has not yet had other environments such as Apache/Nginx/php/MySQL installed, you will need to install aaPanel.\nThe following components are often included in web hosting control panels:\nWeb server¬†(e.g.¬†Apache HTTP Server,¬†NGINX,¬†Internet Information Services) Domain Name System¬†server Mail server¬†and¬†spam¬†filter File Transfer Protocol¬†server Database File manager System monitor Web log analysis software Firewall phpMyAdmin "},"title":"How to install aaPanel on Fedora by one click"},"/utho-docs/docs/linux/how-to-install-aapanel-on-ubuntu-by-one-click/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install aaPanel on Ubuntu by one click.\nThank You üôÇ","installation#Installation":" # wget -O install.sh http://www.aapanel.com/script/install-ubuntu_6.0_en.sh \u0026\u0026 sudo bash install.sh 93684c35 Paste the aapanel Internet address in your browser to login into the aapanel, which is shown in the previous screenshot. Username and password are also shown in the screenshot.","introduction#Introduction":"In this article, you will learn how to install aaPanel on Ubuntu by one click.\naaPanel is a control panel for web servers that is an alternative to cPanel and Vesta. It was developed in China. BT.cn is responsible for its development, and it is now on version v6. 8.5 (at the moment of the writing). It is free, has reached an adequate level of maturity, and comes with some really excellent features such as an editor, uploader, file management, backups, and preconfigured Nginx rules.\nNote: If you are setting up a new system that has not yet had other environments such as Apache/Nginx/php/MySQL installed, you will need to install aaPanel.\nThe following components are often included in web hosting control panels:\nWeb server¬†(e.g.¬†Apache HTTP Server,¬†NGINX,¬†Internet Information Services) Domain Name System¬†server Mail server¬†and¬†spam¬†filter File Transfer Protocol¬†server Database File manager System monitor Web log analysis software Firewall phpMyAdmin "},"title":"How to install aaPanel on Ubuntu by one click"},"/utho-docs/docs/linux/how-to-install-anaconda-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Anaconda on Centos 7.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Anaconda on Centos 7.\nAnaconda is a free and open-source version of the Python and R programming languages designed specifically for use in data science. Its primary goal is to make the administration and deployment of packages more straightforward. Conda, the package management system used by Anaconda, is responsible for managing the various package versions. Conda does an analysis of the present environment before carrying out an installation in order to prevent the installation from interrupting other frameworks and packages.\nThe Anaconda distribution is pre-configured with more than 250 different packages already installed. In addition to the conda package and the virtual environment manager, PyPI allows for the installation of over 7500 more open-source packages that can be used. In addition to the command line interface, it also comes with a graphical user interface called Anaconda Navigator.\nThis interface is a graphical replacement for the command line interface. Users are able to run programmes and manage conda packages, environments, and channels without having to use command-line commands thanks to the Anaconda Navigator, which is part of the Anaconda distribution and comes standard with the package. Navigator is capable of conducting a search for packages, installing those packages in an environment, running those packages, and updating those packages.","step-1-update-your-system#Step 1: Update Your System":" # yum update -y ","step-2-download-anaconda#Step 2: Download Anaconda":" # cd /tmp # curl -O https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh ","step-3-verify-installer-hashes#Step 3: Verify Installer hashes":"After you have downloaded the installer, you can validate the hashes by running the sha256sum command as shown below.\n# sha256sum Anaconda3-5.3.1-Linux-x86_64.sh ","step-4-install-anaconda#Step 4: Install Anaconda":" # yum install bzip2 -y Run the installation script to begin the Anaconda installation process:\n# bash Anaconda3-5.3.1-Linux-x86_64.sh The programme will greet you and welcome you to the installer before requesting that you read over and agree to the licence conditions. After you have done so, use the Enter button to proceed. Then press CTRL+C.\nYou are going to be prompted to confirm that you accept the conditions; if you do, type ‚Äúyes.‚Äù\nYou will be prompted by the system to use the default location for the installation.\nIf you want to continue or specify your location, press the Enter key. In the event that it is necessary, you are free to cancel the installation. Unless you have a specific need to modify the location of the installation, it is advised that you use the location that is specified during the installation.\nDo you wish the installer to initialize Anaconda3\nby running conda init? [yes|no] type yes\nDo you wish the installer to initialize Anaconda3\nin your /root/.bashrc ? [yes|no] type yes\nDo you wish to proceed with the installation of Microsoft VSCode? [yes|no] type no","step-5-activate-the-installation#Step 5: Activate the Installation":"It is not necessary to restart your current terminal session in order for the modifications to take effect. To activate the installation while you‚Äôre still in the same terminal session, all you have to do is execute the command source /.bashrc.\n# source ~/.bashrc # condo info "},"title":"How to Install Anaconda on centos 7"},"/utho-docs/docs/linux/how-to-install-anaconda-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Anaconda on Debian.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Anaconda on Debian.\nAnaconda is a free and open-source version of the Python and R programming languages designed specifically for use in data science. Its primary goal is to make the administration and deployment of packages more straightforward. Conda, the package management system used by Anaconda, is responsible for managing the various package versions. Conda does an analysis of the present environment before carrying out an installation in order to prevent the installation from interrupting other frameworks and packages.\nThe Anaconda distribution is pre-configured with more than 250 different packages already installed. In addition to the conda package and the virtual environment manager, PyPI allows for the installation of over 7500 more open-source packages that can be used. In addition to the command line interface, it also comes with a graphical user interface called Anaconda Navigator.\nThis interface is a graphical replacement for the command line interface. Users are able to run programmes and manage conda packages, environments, and channels without having to use command-line commands thanks to the Anaconda Navigator, which is part of the Anaconda distribution and comes standard with the package. Navigator is capable of conducting a search for packages, installing those packages in an environment, running those packages, and updating those packages.","step-1-update-your-system#Step 1: Update Your System":" # apt update -y ","step-2-download-anaconda#Step 2: Download Anaconda":" # wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh ","step-3-verify-installer-hashes#Step 3: Verify Installer hashes":"After you have downloaded the installer, you can validate the hashes by running the sha256sum command as shown below.\n# sha256sum Anaconda3-2021.11-Linux-x86_64.sh ","step-4-install-anaconda#Step 4: Install Anaconda":"Next the last step, you will now be able to initiate the Anaconda installation by executing the bash Anaconda3-2021.11-Linux-x86 64.sh command, as outlined in the following paragraph. Several times throughout the installation process, it will ask for your confirmation on specific choices.\n# bash Anaconda3-2021.11-Linux-x86_64.sh The programme will greet you and welcome you to the installer before requesting that you read over and agree to the licence conditions. After you have done so, use the Enter button to proceed. Then press CTRL+C.\nYou are going to be prompted to confirm that you accept the conditions; if you do, type ‚Äúyes.‚Äù\nYou will be prompted by the system to use the default location for the installation.\nIf you want to continue or specify your location, press the Enter key. In the event that it is necessary, you are free to cancel the installation. Unless you have a specific need to modify the location of the installation, it is advised that you use the location that is specified during the installation.\nDo you wish the installer to initialize Anaconda3\nby running conda init? [yes|no] type yes","step-5-activate-the-installation#Step 5: Activate the Installation":"It is not necessary to restart your current terminal session in order for the modifications to take effect. To activate the installation while you‚Äôre still in the same terminal session, all you have to do is execute the command source /.bashrc.\n# source ~/.bashrc # condo info "},"title":"How to Install Anaconda on Debian"},"/utho-docs/docs/linux/how-to-install-anaconda-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Anaconda on Fedora.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Anaconda on Fedora.\nAnaconda is a free and open-source version of the Python and R programming languages designed specifically for use in data science. Its primary goal is to make the administration and deployment of packages more straightforward. Conda, the package management system used by Anaconda, is responsible for managing the various package versions. Conda does an analysis of the present environment before carrying out an installation in order to prevent the installation from interrupting other frameworks and packages.\nThe Anaconda distribution is pre-configured with more than 250 different packages already installed. In addition to the conda package and the virtual environment manager, PyPI allows for the installation of over 7500 more open-source packages that can be used. In addition to the command line interface, it also comes with a graphical user interface called Anaconda Navigator.\nThis interface is a graphical replacement for the command line interface. Users are able to run programmes and manage conda packages, environments, and channels without having to use command-line commands thanks to the Anaconda Navigator, which is part of the Anaconda distribution and comes standard with the package. Navigator is capable of conducting a search for packages, installing those packages in an environment, running those packages, and updating those packages.","step-1-update-your-system#Step 1: Update Your System":" # dnf update -y ","step-2-download-anaconda#Step 2: Download Anaconda":" # cd /tmp # curl -O https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh ","step-3-verify-installer-hashes#Step 3: Verify Installer hashes":"After you have downloaded the installer, you can validate the hashes by running the sha256sum command as shown below.\n# sha256sum Anaconda3-5.3.1-Linux-x86_64.sh ","step-4-install-anaconda#Step 4: Install Anaconda":" # dnf install bzip2 -y Run the installation script to begin the Anaconda installation process:\n# bash Anaconda3-5.3.1-Linux-x86_64.sh The programme will greet you and welcome you to the installer before requesting that you read over and agree to the licence conditions. After you have done so, use the Enter button to proceed. Then press CTRL+C.\nYou are going to be prompted to confirm that you accept the conditions; if you do, type ‚Äúyes.‚Äù\nYou will be prompted by the system to use the default location for the installation.\nIf you want to continue or specify your location, press the Enter key. In the event that it is necessary, you are free to cancel the installation. Unless you have a specific need to modify the location of the installation, it is advised that you use the location that is specified during the installation.\nDo you wish the installer to initialize Anaconda3\nin your /root/.bashrc ? [yes|no] type yes\nDo you wish to proceed with the installation of Microsoft VSCode? [yes|no] type no","step-5-activate-the-installation#Step 5: Activate the Installation":"It is not necessary to restart your current terminal session in order for the modifications to take effect. To activate the installation while you‚Äôre still in the same terminal session, all you have to do is execute the command source /.bashrc.\n# source ~/.bashrc # condo info "},"title":"How to Install Anaconda on Fedora"},"/utho-docs/docs/linux/how-to-install-anaconda-on-ubuntu-20-04-lts/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Anaconda on Ubuntu 20.04 LTS.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Anaconda on Ubuntu 20.04 LTS.\nAnaconda is a free and open-source version of the Python and R programming languages designed specifically for use in data science. Its primary goal is to make the administration and deployment of packages more straightforward. Conda, the package management system used by Anaconda, is responsible for managing the various package versions. Conda does an analysis of the present environment before carrying out an installation in order to prevent the installation from interrupting other frameworks and packages.\nThe Anaconda distribution is pre-configured with more than 250 different packages already installed. In addition to the conda package and the virtual environment manager, PyPI allows for the installation of over 7500 more open-source packages that can be used. In addition to the command line interface, it also comes with a graphical user interface called Anaconda Navigator.\nThis interface is a graphical replacement for the command line interface. Users are able to run programmes and manage conda packages, environments, and channels without having to use command-line commands thanks to the Anaconda Navigator, which is part of the Anaconda distribution and comes standard with the package. Navigator is capable of conducting a search for packages, installing those packages in an environment, running those packages, and updating those packages.","step-1-update-your-system#Step 1: Update Your System":" # apt update ","step-2-download-anaconda#Step 2: Download Anaconda":" # wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh ","step-3-verify-installer-hashes#Step 3: Verify Installer hashes":"After you have downloaded the installer, you can validate the hashes by running the sha256sum command as shown below.\n# sha256sum Anaconda3-2021.11-Linux-x86_64.sh ","step-4-install-anaconda#Step 4: Install Anaconda":"Next the last step, you will now be able to initiate the Anaconda installation by executing the bash Anaconda3-2021.11-Linux-x86 64.sh command, as outlined in the following paragraph. Several times throughout the installation process, it will ask for your confirmation on specific choices.\n# bash Anaconda3-2021.11-Linux-x86_64.sh The programme will greet you and welcome you to the installer before requesting that you read over and agree to the licence conditions. After you have done so, use the Enter button to proceed. Then press CTRL+C.\nYou are going to be prompted to confirm that you accept the conditions; if you do, type ‚Äúyes.‚Äù\nYou will be prompted by the system to use the default location for the installation.\nIf you want to continue or specify your location, press the Enter key. In the event that it is necessary, you are free to cancel the installation. Unless you have a specific need to modify the location of the installation, it is advised that you use the location that is specified during the installation.\nDo you wish the installer to initialize Anaconda3\nby running conda init? [yes|no] type yes","step-5-activate-the-installation#Step 5: Activate the Installation":"It is not necessary to restart your current terminal session in order for the modifications to take effect. To activate the installation while you‚Äôre still in the same terminal session, all you have to do is execute the command source /.bashrc.\n# source ~/.bashrc # condo info "},"title":"How to Install Anaconda on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-install-and-configure-pgadmin-4-on-ubuntu-22-04/":{"data":{"":" How To Install and Configure pgAdmin 4 on Ubuntu 22.04\nIntroduction In this tutorial, you will learn how to install and configure PgAdmin 4 on Ubuntu 22.04. pgAdmin is an open-source tool for managing and developing PostgreSQL and other database management systems that work with it. It was made with Python and jQuery, and it has all of the features of PostgreSQL. You can do everything with pgAdmin, from writing simple SQL queries to keeping an eye on your databases and setting up more complex database architectures.\nPrerequisites A super user( root ) or any normal user with SUDO privileges. Configured Nginx reverse proxy for¬†http://unix:/tmp/pgadmin4.sock. PostgreSQL installed on your server. Python 3 and¬†venv¬†installed on your server. Read here for more information about this. 1.¬†Installing pgAdmin and its Dependencies Step 1: As of this writing, pgAdmin 4 is the most recent version, while pgAdmin 3 is the most recent version that can be found in the official Ubuntu repositories. But pgAdmin 3 is no longer supported, and the people who run the project suggest that you use pgAdmin 4. In this step, we‚Äôll talk about how to install the latest version of pgAdmin 4 in a virtual environment (which is what the project‚Äôs development team recommends) and how to use apt to install its dependencies.\napt update Step 2: Next, you need to install the following. These include libgmp3-dev, which is a library for multiprecision arithmetic, libpq-dev, which has header files and a static library that make it easier to talk to a PostgreSQL backend, and so on.\napt install libgmp3-dev libpq-dev Following this, create a few directories where pgAdmin will store its sessions data, storage data, and logs:\nmkdir -p /var/lib/pgadmin4/sessions mkdir /var/lib/pgadmin4/storage mkdir /var/log/pgadmin4 Step 3: Then, change who owns these directories to your user and group that is not root. This is necessary because they are currently owned by your root user, but we will install pgAdmin from a virtual environment owned by your non-root user, and the installation process involves creating some files in these directories. After the installation, we will change the ownership to the www-data user and group so that it can be served to the web:\nNext, open up your virtual environment. Find the directory where your programming environment is and then turn it on. We‚Äôll go to the environments directory and turn on the my env environment by using the same naming rules as the prerequisite Python 3 tutorial:\ncd environments/ source my_env/bin/activate Step 4: After turning on the virtual environment, it would be smart to make sure that your system has the most recent version of pip. Run the following command to bring pip up to the latest version:\npython -m pip install -U pip Step 5: Now install pgadmin using python\npython -m pip install pgadmin4==6.10 Step 6: Next, install Gunicorn, a Python WSGI server that will be used with Nginx to serve the¬†pgadmin¬†web interface later in the tutorial:\npython -m pip install gunicorn That‚Äôs all there is to installing pgAdmin and the things it needs. Before you can connect it to your database, you‚Äôll need to make a few changes to how the programme is set up.\n2\\. Configuring pgAdmin 4 Even though pgAdmin has been installed on your server, you still need to take a few steps to make sure it has the permissions and settings it needs to serve the web interface correctly.\nThe main configuration file for pgAdmin, config.py, is read before any other file. Its contents can be used as a guide for other configuration settings that can be set in pgAdmin‚Äôs other config files, but you shouldn‚Äôt change the config.py file itself to avoid unexpected errors. We‚Äôll make some changes to the configuration in a new file called config local.py. This file will be read after the main one.\nStep 7 Create this file now using your preferred text editor. Here, we will use vi\nvi my_env/lib/python3.10/site-packages/pgadmin4/config_local.py In your editor, add the following content:\nLOG_FILE = '/var/log/pgadmin4/pgadmin4.log' SQLITE_PATH = '/var/lib/pgadmin4/pgadmin4.db' SESSION_DB_PATH = '/var/lib/pgadmin4/sessions' STORAGE_DIR = '/var/lib/pgadmin4/storage' SERVER_MODE = True Notice that each of these file paths point to the directories you created in Step 1.\nAnd now save the file and exit.\nStep 8: With those configurations in place, run the pgAdmin setup script to set your login credentials:\npython my_env/lib/python3.10/site-packages/pgadmin4/setup.py After you run this command, you will see a prompt asking for your email address and a password. When you access pgAdmin later, these will be your login information, so make sure to remember or write down what you put here:\nAfter that, pgAdmin is all set up. But your server isn‚Äôt yet serving the programme, so it‚Äôs still not available. To fix this, we will set up Gunicorn and Nginx to serve pgAdmin so that you can use a web browser to get to its user interface.\nStarting Gunicorn and Configuring Nginx Gunicorn will be used to make pgAdmin work as a web application. But since Gunicorn is an application server, it can only be used locally and can‚Äôt be accessed over the internet. You will need to use Nginx as a reverse proxy to make it available from afar.\nAfter you‚Äôve done what you needed to do to set up Nginx as a reverse proxy, this will be in your Nginx configuration file:\nThis reverse proxy configuration makes it possible for your local browser to connect to your Gunicorn server. With the pgAdmin programme, you can start up your Gunicorn server:\nvi gunicorn --bind unix:/tmp/pgadmin4.sock --workers=1 --threads=25 --chdir ~/environments/my_env/lib/python3.10/site-packages/pgadmin4 pgAdmin4:app With Gunicorn as an application server and your Nginx reverse proxy making it possible to access it, you can now use your web browser to access pgAdmin.\n4\\. Accessing pgAdmin Step 9: Open your favourite web browser on your own computer and go to your server‚Äôs IP address:\nhttp://Server-ip_Or-domain-name In this tutorial, you have learnt How To Install and Configure pgAdmin 4 on Ubuntu 22.04.\nAlso Read: How to install Cockpit on Fedora Server, How to install Apache on CentOS 7"},"title":"How To Install and Configure pgAdmin 4 on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-and-configure-redis-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install and configure Redis on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","configuration-of-redis#Configuration of Redis":"In order to use the Redis server, you will need to configure it such that it can be used on Ubuntu.\nStep 1: Activate the Redis Service Right Away The very first and most important step in dealing with any server is to activate its services so that the service will be enabled regardless of whether or not the server is rebooted:\n# systemctl enable --now redis-server Step 2: Redis should be allowed to listen on all network interfaces Redis does not allow remote connections of any kind, and it will only answer to the requests that come from a local host or the system that it is being used on. Additionally, it is essential to link the database to the same computer as the rest of the system.\nLet‚Äôs say you wish to make it possible for other distant connections to establish a connection to Redis. In this scenario, you will need to adjust the configuration of ‚ÄúRedis‚Äù so that it listens to addresses other than the localhost:\nIn order to accomplish this, we will access the configuration file for Redis by using the vi editor to execute the following command:\n# vi /etc/redis/redis.conf Locate the line in the file that begins with ‚Äúbind‚Äù and replace it with the line that is seen in the figure:\nStep 3: Change the password for Redis. Look for the line that says ‚Äúrequirepass‚Äù in the file ‚Äú/etc/redis/redis.conf‚Äù and then write your password in front of it:\nIn order for the modifications to take effect, the ‚ÄúRedis‚Äù server must be restarted.\n# systemctl restart redis-server Step 4: Check that the Changes were Made Check that Redis is still listening on port ‚Äú6379‚Äù after you‚Äôve made the adjustments described above:\n# ss -an | grep 6379 After this, all TCP traffic should be permitted to pass through port ‚Äú6379‚Äù:\n# ufw allow 6379/tcp Step 5: You can get the Redis CLI here Now that all of the configurations have been finalised, run the following command to access the ‚ÄúRedis CLI‚Äù:\n# redis-cli Authorise your identity by using the ‚ÄúAUTH‚Äù keyword in the configuration file, along with the password that you set in previous steps (step 3):\n# AUTH \u003cpassword\u003e Ping the Redis to ensure that it is operating properly:\n# ping Use the following command to exit the Redis Command Line Interface:\n# quit ","installation-of-redis#Installation of Redis":"Before downloading any package from the default repository, it is recommended that the operating system be brought up to date first:\n# apt update Use the command that is mentioned below in the following manner in order to install the most recent version of Redis-server from Ubuntu‚Äôs repository:\n# apt install redis-server -y Check the status of the ‚ÄúRedis‚Äù server using the following command to determine whether or not the installation was successful:\n# systemctl status redis-server In order to verify that the installation was successful, we will use the following command to check the version of Redis:\n# redis-server -v ","introduction#Introduction":"In this article, you will learn how to install and configure Redis on Ubuntu 22.04.\nRedis is a data structure that is stored in memory and functions similarly to a store. It can be used as a cache, streaming engine, or database. Redis makes it possible for developers to save data in their programmes using straightforward statements rather than the more complicated query language often required.\nRedis has support for a wide variety of data structures, including strings, hashes, sets, and many others. In addition to that, it gives you access to the Replication features as well as the Redis cluster, which gives you automatic partitioning."},"title":"How to install and configure Redis on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-and-manage-supervisor/":{"data":{"":"","#":"\nIntroduction In many VPS environments, you may have a number of small programs that you want to run all the time. These could be small shell scripts, Node.js apps, or large packages.\nMost of the time, external packages come with a unit file that lets an init system like systemd handle them, or they come as docker images that a container engine can handle. But a lightweight alternative is helpful for software that isn‚Äôt well-packaged or for users who don‚Äôt want to interact with a low-level init system on their server.\nSupervisor¬†is a process manager which provides a singular interface for managing and monitoring a number of long-running programs. In this tutorial, you will install Supervisor on a Linux server and learn how to manage Supervisor configurations for multiple applications.\nPrerequisites A Linux server and either a super user ( root) or any non-root user with sudo privileges.¬†apt repository configured server to install packages 1\\. Installation Begin by updating your package sources and installing Supervisor:\napt update \u0026\u0026 sudo apt install supervisor After installation, the supervisor service will run on its own. You can find out how it‚Äôs going:\nsystemctl status supervisor 2 - Adding a Program The best way to use Supervisor is to write a configuration file for each programme it will run.\nAll programmes that run under Supervisor must run in a mode that doesn‚Äôt run in the background. This is sometimes called ‚Äúforeground mode.‚Äù If your programme automatically goes back to the shell after running by default, you may need to look in the program‚Äôs manual to find the option to turn on this mode. If you don‚Äôt, Supervisor won‚Äôt be able to tell what the program‚Äôs status is.\nTo show how Supervisor works, we‚Äôll make a shell script that does nothing but print out a predictable message every second. This script will run in the background until someone manually stops it. Open up a file in your home directory called idle.sh with nano or your favourite text editor:\nvi ~/idle.sh Add the following contents:\n#!/bin/bash while true do # Echo current date to stdout echo `date` # Echo 'error!' to stderr echo 'error!' \u003e\u00262 sleep 1 done Next, make your script executable:\nchmod +x ~/idle.sh Supervisor programmes‚Äô per-program configuration files are in the /etc/supervisor/conf.d directory. Most of the time, one programme runs per file, and the files end in.conf. We‚Äôll make a configuration file for this script called ‚Äú/etc/supervisor/conf.d/idle.conf:\nvi /etc/supervisor/conf.d/idle.conf command=/home/ubuntu/idle.sh autostart=true autorestart=true stderr_logfile=/var/log/idle.err.log stdout_logfile=/var/log/idle.out.log Now review this by\ncommand=/home/ubuntu/idle.sh The configuration begins by defining a program with the name¬†idle¬†and the full path to the program:\nautostart=true autorestart=true The autostart option tells Supervisor to start this programme when the system starts up. If you set this to false, the system will need to be started by hand after being shut down.\nautorestart¬†defines how Supervisor should manage the program in the event that it exits:\nOnce our configuration file is made and saved, we can use the supervisorctl command to tell Supervisor about our new programme. First, we tell Supervisor to look in the /etc/supervisor/conf.d directory for any new or changed programme configurations.\nsupervisorctl reread supervisorctl update Any time you make a change to any program configuration file, running the two previous commands will bring the changes into effect.\n3\\. Managing Programs You won‚Äôt just want to run programmes; you‚Äôll also want to stop, restart, or check the status of them. The supervisorctl programme, which we used in Step 2, also has a mode that lets us control our programmes by talking to it.\nTo enter the interactive mode, run supervisorctl with no arguments:\nsupervisorctl You can¬†start¬†or¬†stop¬†a program with the associated commands followed by the program name:\nsupervisorctl\u003e start idle Using¬†status¬†you can view again the current execution state of each program after making any changes:\nsupervisorctl\u003e status idle STOPPED Nov 21 12:36 PM Finally, you can exit supervisorctl with Ctrl+C or by entering¬†quit¬†into the prompt:\nsupervisorctl\u003e quit "},"title":"How To Install and Manage Supervisor"},"/utho-docs/docs/linux/how-to-install-and-use-aide-on-rhel-centos-7-8/":{"data":{"":"\nDescription\nAIDE, or Advanced Intrusion Detection Environment, is a free and open source software that maintains a database of files on the system and then utilises that database to ensure file integrity and identify system intrusions. To ensure the file‚Äôs integrity, it employs numerous message digest techniques. All of the standard file properties can also be tested for consistency. More on the RHEL Security Guide","aide-database-should-be-updated#AIDE Database should be updated.":"If you choose, you can also use the aide ‚Äîupdate command to update the AIDE database, as seen below.\naide --update ","check-version#¬†Check Version":"After a successful installation, you can check the utility version with the assistance ‚Äîversion command, as shown below.\naide --version ","conduct-integrity-checks#Conduct integrity checks.":"You can run the checks manually or using a cron job. To conduct manual checks, use the aide ‚Äîcheck command as shown below, however if you want to perform checks on a daily, weekly, or monthly basis, it is always a good idea to set up a cron job for that.\naide --check ","create-the-first-database#Create the First Database":"Before completing integrity and intrusion tests, you must initialise the database based on the /etc/aide.conf configuration using the aide ‚Äîinit command, as seen below. If you wish to add additional files and directories to the database, you must modify the /etc/aide.conf configuration file.\naide --init ","examine-every-available-option#Examine Every Available Option":" aide --help ","how-does-aide-work#How Does AIDE Work?":"During initialization, AIDE creates a database of all files based on the rules given in the /etc/aide.conf configuration file. Following database initialization, we may run file integrity checks and system intrusion detection on those files. If you want to add more files and directories, you must alter the configuration in the /etc/aide.conf file and then execute checks against those files and directories.","install-aide#Install AIDE":" yum install aide ","install-and-use-aide-on-rhelcentos-78#Install and use AIDE on RHEL/CentOS 7/8.":"Before installing a new package, use the yum update command to refresh the package cache with all the newest releases from the repository, as shown below. If you need to upgrade a package, use the yum upgrade command.\nyum update ","qualities-of-aide#Qualities of AIDE":" It supports md5, sha1, rmd160, tiger, crc32, sha256, sha512, and whirlpool message digest algorithms, as well as libmhash: gost, haval, and crc32b. File attributes supported include file type, permissions, inode, uid, gid, link name, size, block count, number of links, mtime, ctime, and atime. Posix ACL, SELinux, XAttrs, and extended file system attributes are also supported. It allows you to select which files and directories to monitor and which to ignore using strong regular expressions. If zlib support is compiled in, it also supports gzip database compression. It includes static binary support for standalone client-server monitoring settings. aide.github.io has further information. ","troubleshooting#Troubleshooting":"When doing integrity checks using the aide ‚Äîcheck command, you may receive a ‚ÄúCouldn‚Äôt open file /var/lib/aide/aide.db.gz for reading error.‚Äù\nIf you receive the above error, just navigate to the /var/lib/aide directory and use the ln -s aide.db.new.gz aide.db.gz command to make a soft link of aide.db.gz.\ncd /var/lib/aide/ ln -s aide.db.new.gz aide.db.gz You should now be able to complete the integrity tests if you try again.\nMust Read : How to Install and Use Apache Cassandra on Ubuntu 20.04 LTS\nThankyou"},"title":"How to Install and Use AIDE on RHEL/CentOS 7/8"},"/utho-docs/docs/linux/how-to-install-and-use-apache-cassandra-on-ubuntu-20-04-lts/":{"data":{"":"Description\nI‚Äôll walk you through installing and configuring Apache Cassandra on Ubuntu 20.04 LTS (Focal Fossa). Cassandra is the best NoSQL database to utilise if you need a database that is both efficient and speedy. Cassandra is a NoSQL database management system that is free and open-source, distributed, wide-column store, decentralised, elastically scalable, and highly available. It is designed to handle large amounts of data across many commodity servers while providing high availability with no single point of failure. It is widely utilised to power cloud applications in a variety of sectors.","add-repo#Add Repo":" echo \"deb http://www.apache.org/dist/cassandra/debian 40x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list ","cassandra-service-check#Cassandra service check":"After a successful installation, use the systemctl status cassandra command to check the service‚Äôs status.\nsystemctl status cassandra ","configure-cassandra#Configure Cassandra":"You may have observed that the Cassandra cluster name is set to Test Cluster by default. If you want to modify this name, first perform the update query below and then exit the prompt by typing exit.\nupdate system.local SET cluster_name = 'CyberITHub Cluster' WHERE KEY = 'local'; exit Then, using an editor such as vi, change the cluster name from Test to CyberITHub in the /etc/cassandra/cassandra.yaml configuration file, When you‚Äôre finished, use Ctrl+X to save and close the file.\nvi /etc/cassandra/cassandra.yaml After modifying the configuration, use the systemctl restart cassandra command to restart the service. Use the systemctl status cassandra command to check the status.\nsystemctl restart cassandra systemctl status cassandra Wait a few minutes for the services to start up before testing the cluster name with cqlsh in the terminal again. The cluster name has been changed to CyberITHub.\nNOTE: Please keep in mind that if you get the Connection Problem: Unable to connect to any servers error after running the cqlsh command, you should verify the node status with the nodetool status command. Only use the cqlsh command to login to Cassandra once the node is up and running.\nConclusion: I‚Äôll walk you through installing and configuring Apache Cassandra on Ubuntu 20.04 LTS (Focal Fossa). Cassandra is the best NoSQL database to utilise if you need a database that is both efficient and speedy. Cassandra is a NoSQL database management system that is free and open-source, distributed, wide-column store, decentralised, elastically scalable, and highly available. It is designed to handle large amounts of data across many commodity servers while providing high availability with no single point of failure. It is widely utilised to power cloud applications in a variety of sectors.","gpg-key-import#GPG Key Import":"You may receive the steps to add the GPG key from the Cassandra Download Page. This is necessary to ensure the package‚Äôs integrity.\nwget -q -O - https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - ","install-apache-cassandra#Install Apache Cassandra":"We can now proceed with the installation of Cassandra DB; however, because Apache Cassandra is not available from the default repository, we will need to add the repository first, from which we can directly download and install the packages.\nInstall apt-transport-https\nBut first, we must install the apt-transport-https package using the apt install apt-transport-https command.\napt install apt-transport-https ","install-cassandra-db#Install Cassandra DB":"Now we can use apt install Cassandra to install the Cassandra package and all of its dependencies.\napt install cassandra ","install-java#Install Java":"If Java is not already installed, the next step is to install it. You may verify this by executing the java version command. If it is not installed, you will get results similar to the one below, prompting you to use any of the following instructions to install the Java platform. We‚Äôll install OpenJDK from the Ubuntu Repository because we‚Äôll need JDK.\njava --version Use the apt install openjdk-8-jdk command to install OpenJDK\napt install openjdk-8-jdk If you check the Java version again, it should now appear like this. This validates that Java is now correctly installed.\njava --version ","login-to-cassandra#Login to Cassandra":"Once Cassandra is up and running, use the cqlsh command to connect to the database.\ncqlsh ","nodetool-status-check#Nodetool Status: Check":"If you are operating a single node cluster, you should additionally verify the node state by running nodetool status.\nnodetool status ","prerequisites#Prerequisites":"a) You must have an Ubuntu 20.04 LTS (Focal Fossa) server running.\nb) To run privileged commands, you must have sudo or root access.\nc) The apt, apt-get, curl, and tee utilities should be available on your server.","update-package#Update Package":" apt update ","update-your-server#Update Your Server":" apt update "},"title":"How to Install and Use Apache Cassandra on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-install-apache-on-centos-7/":{"data":{"":"\nApache is a well-known HTTP Server that is free, open source, and runs on Unix-like operating systems like Linux and Windows OS. Since it came out 20 years ago, it has been the most popular web server. Many sites on the Internet use it to run. It is easy to install and set up so that a Linux or Windows server can host one or more websites.\nIn this article, we‚Äôll show you how to use the command line to install, set up, and manage Apache HTTP web server on a CentOS 7 or RHEL 7 server.","apache-important-files-and-directoires#Apache Important Files and Directoires":" The default server root directory (top level directory containing configuration files):¬†/etc/httpd The main Apache configuration file:¬†/etc/httpd/conf/httpd.conf Additional configurations can be added in:¬†/etc/httpd/conf.d/ Configurations for modules:¬†/etc/httpd/conf.modules.d/ Apache default server document root directory (stores web files):¬†/var/www/html Install Apache Web Server First update the system software packages to the latest version.\nyum update -y Next, do the following with the YUM package manager to install Apache HTTP server from the default software repositories.\nyum install httpd -y ","configure-apache-http-server-on-centos-7#Configure Apache HTTP Server on CentOS 7":"After installing Apache web server, you can start it for the first time and set it to start automatically when the system starts up.\nsystemctl start httpd systemctl enable httpd You can confirm the status of apache server by using following command\nsystemctl status httpd Configure firewalld to Allow Apache Traffic The firewall that comes with CentOS 7 is set up to block Apache traffic by default. To let web traffic through on Apache, change the system firewall rules to allow HTTP and HTTPS packets to come in.\nfirewall-cmd --permanent --add-service http firewall-cmd --permanent --add-service https firewall-cmd --reload Test Apache HTTP Server on CentOS 7 If you now go to the following URL, a default Apache page will be shown.\nhttp://server-ip ","prerequisites#Prerequisites":" A CentOS 7 Server Minimal Install Super user ( root ) or any normal user yum repository configured "},"title":"How to install Apache on CentOS 7"},"/utho-docs/docs/linux/how-to-install-atom-text-editor-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Atom Text Editor on Debian 10.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Atom Text Editor on Debian 10.\nAtom is a cross-platform text editor that is built on open source and does not cost anything to use. It is compatible with macOS, Windows, and Linux-based operating systems such as Debian 10. It is also known as the hackable text editor in the technical generation that is currently living in the 21st century.\nBecause it supports a wide variety of add-ons, including dynamic linting, syntax highlighting for many languages and formats, debuggers, video and music player controls, and runtime environments, Atom is regarded as being more user-friendly than its rivals.","method-1-atom-on-debian-10-using-apt#Method 1: Atom on Debian 10 using APT":"In order to successfully install Atom using APT on Debian 10, you will need to follow the step-by-step steps that are provided below.\nStep 1: Update system repositories Open the terminal window on your Debian 10 computer and type in the following instructions to bring the system repositories up to date:\n# apt update # apt upgrade -y Step 2: Get the Atom GPG key here After the system repositories have been brought up to date, download the GPG key for Atom Editor:\n# wget¬†-qO¬†- https://packagecloud.io/AtomEditor/atom/gpgkey¬†|¬†apt-key add¬†- Step 3: Import information from the Atom repository Importing the Atom repository onto your system should be the next step, and the following command should be used to accomplish this:\n# sh¬†-c¬†'echo \"deb \\[arch=amd64\\] https://packagecloud.io/AtomEditor/atom/any/ any main\" \u003e /etc/apt/sources.list.d/atom.list' After you have imported the repository and installed the packages, it is highly advised that you then update your operating system.\n# apt update Step 4: Atom install on Debian 10 Using the command that has been provided below, you may now install Atom on Debian 10:\n# apt¬†install¬†atom -y ","method-2-installing-atom-on-debian-10-with-snap-store#Method 2: Installing Atom on Debian 10 with Snap Store":"The Snap Store provides the second option for installing Atom on Debian 10, which can be found here. You may quickly and easily install Atom in just a few minutes by utilizing a single command provided by Snap Store.\nStep 1: Snap Installation To begin, install Snap on your machine if it has not previously been done so:\n# apt¬†install¬†snapd Step 2: Atom installation on Debian 10 After installing Snap, run this command to install Atom:\n# snap¬†install¬†atom¬†--classic "},"title":"How to install Atom Text Editor on Debian 10"},"/utho-docs/docs/linux/how-to-install-atom-text-editor-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Atom Text Editor on Debian 12.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Atom Text Editor on Debian 12.\nAtom¬†is a cross-platform text editor that is built on open source and does not cost anything to use. It is compatible with macOS, Windows, and Linux-based operating systems such as Debian 12. It is also known as the hackable text editor in the technical generation that is currently living in the 21st century.\nBecause it supports a wide variety of add-ons, including dynamic linting, syntax highlighting for many languages and formats, debuggers, video and music player controls, and runtime environments, Atom is regarded as being more user-friendly than its rivals.","method-1-atom-on-debian-12-using-apt#Method 1: Atom on Debian 12 using APT":"In order to successfully install Atom using APT on Debian 12, you will need to follow the step-by-step steps that are provided below.\nStep 1: Update system repositories Open the terminal window on your Debian 12 computer and type in the following instructions to bring the system repositories up to date:\n# apt update # apt upgrade -y Step 2: Get the Atom GPG key here After the system repositories have been brought up to date, download the GPG key for Atom Editor:\n# wget¬†-qO¬†- https://packagecloud.io/AtomEditor/atom/gpgkey¬†|¬†apt-key add¬†- Step 3: Import information from the Atom repository Importing the Atom repository onto your system should be the next step, and the following command should be used to accomplish this:\n# sh¬†-c¬†'echo \"deb \\[arch=amd64\\] https://packagecloud.io/AtomEditor/atom/any/ any main\" \u003e /etc/apt/sources.list.d/atom.list' After you have imported the repository and installed the packages, it is highly advised that you then update your operating system.\n# apt update Step 4: Atom install on Debian 12 Using the command that has been provided below, you may now install Atom on Debian 12:\n# apt¬†install¬†atom -y ","method-2-installing-atom-on-debian-12-with-snap-store#Method 2: Installing Atom on Debian 12 with Snap Store":"The Snap Store provides the second option for installing Atom on Debian 12, which can be found here. You may quickly and easily install Atom in just a few minutes by utilizing a single command provided by Snap Store.\nStep 1: Snap Installation To begin, install Snap on your machine if it has not previously been done so:\n# apt¬†install¬†snapd Step 2: Atom installation on Debian 12 After installing Snap, run this command to install Atom:\n# snap¬†install¬†atom¬†--classic "},"title":"How to install Atom Text Editor on Debian 12"},"/utho-docs/docs/linux/how-to-install-atom-text-editor-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Atom Text Editor on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Atom Text Editor on Ubuntu 22.04.\nAtom is a cross-platform text editor that is built on open source and does not cost anything to use. It is compatible with macOS, Windows, and Linux-based operating systems such as Ubuntu 22.04. It is also known as the hackable text editor in the technical generation that is currently living in the 21st century.\nBecause it supports a wide variety of add-ons, including dynamic linting, syntax highlighting for many languages and formats, debuggers, video and music player controls, and runtime environments, Atom is regarded as being more user-friendly than its rivals.","method-1-atom-on-ubuntu-2204-using-apt#Method 1: Atom on Ubuntu 22.04 using APT":"In order to successfully install Atom using APT on Ubuntu 22.04, you will need to follow the step-by-step steps that are provided below.\nStep 1: Update system repositories Open the terminal window on your Ubuntu 22.04 computer and type in the following instructions to bring the system repositories up to date:\n# apt update # apt upgrade -y Step 2: Get the Atom GPG key here After the system repositories have been brought up to date, download the GPG key for Atom Editor:\n# wget¬†-qO¬†- https://packagecloud.io/AtomEditor/atom/gpgkey¬†|¬†apt-key add¬†- Step 3: Import information from the Atom repository Importing the Atom repository onto your system should be the next step, and the following command should be used to accomplish this:\n# sh¬†-c¬†'echo \"deb \\[arch=amd64\\] https://packagecloud.io/AtomEditor/atom/any/ any main\" \u003e /etc/apt/sources.list.d/atom.list' After you have imported the repository and installed the packages, it is highly advised that you then update your operating system.\n# apt update Step 4: Atom install on Ubuntu 22.04 Using the command that has been provided below, you may now install Atom on Ubuntu 22.04:\n# apt¬†install¬†atom -y ","method-2-installing-atom-on-ubuntu-2204-with-snap-store#Method 2: Installing Atom on Ubuntu 22.04 with Snap Store":"The Snap Store provides the second option for installing Atom on Ubuntu 22.04, which can be found here. You may quickly and easily install Atom in just a few minutes by utilizing a single command provided by Snap Store.\nStep 1: Snap Installation To begin, install Snap on your machine if it has not previously been done so:\n# apt¬†install¬†snapd Step 2: Atom installation on Ubuntu 22.04 After installing Snap, run this command to install Atom:\n# snap¬†install¬†atom¬†--classic "},"title":"How to install Atom Text Editor on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-certbot-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Certbot on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-certbot-on-almalinux-with-nginx--apache#Install Certbot on AlmaLinux with nginx / apache":"In order to install certbot, we first need to install the epel-release repository (all of the following commands are performed with root privileges):\n# yum install epel-release Installing the client application and the package necessary for communicating with the server (using nginx as an example) is the next step.\n# dnf install certbot python3-certbot-nginx Install certbot along with the appropriate package if you are utilising Apache.\n# dnf install certbot python3-certbot-apache mod\\_ssl The next step is to immediately acquire a free certificate and set up https configuration. If you are using nginx, run:\n# certbot --nginx If Apache is already installed on your system:\n# certbot --apache Run the following command to check the version of Certbot.\n# certbot --version ","introduction#Introduction":"In this article, you will learn how to install Certbot on AlmaLinux 8.\nCertbot¬†is a Python client that automates the process of obtaining free certificates from Let‚Äôs Encrypt and configuring web servers to establish a secure https connection using these certificates. It does this by acquiring the certificates through Let‚Äôs Encrypt.\nAs a starting point, we will look at the process of downloading Cerbot on AlmaLinux, learn how to acquire a free certificate by utilising it, and automatically configure nginx and apache."},"title":"How to install Certbot on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-certbot-on-almalinux-9/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Certbot on AlmaLinux 9.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-certbot-on-almalinux-with-nginx--apache#Install Certbot on AlmaLinux with nginx / apache":"In order to install certbot, we first need to install the epel-release repository (all of the following commands are performed with root privileges):\n# yum install epel-release Installing the client application and the package necessary for communicating with the server (using nginx as an example) is the next step.\n# dnf install certbot python3-certbot-nginx Install certbot along with the appropriate package if you are utilising Apache.\n# dnf install certbot python3-certbot-apache mod\\_ssl The next step is to immediately acquire a free certificate and set up https configuration. If you are using nginx, run:\n# certbot --nginx If Apache is already installed on your system:\n# certbot --apache Run the following command to check the version of Certbot.\n# certbot --version ","introduction#Introduction":"In this article, you will learn how to install Certbot on AlmaLinux 9.\nCertbot¬†is a Python client that automates the process of obtaining free certificates from Let‚Äôs Encrypt and configuring web servers to establish a secure https connection using these certificates. It does this by acquiring the certificates through Let‚Äôs Encrypt.\nAs a starting point, we will look at the process of downloading Cerbot on AlmaLinux, learn how to acquire a free certificate by utilising it, and automatically configure nginx and apache."},"title":"How to install Certbot on AlmaLinux 9"},"/utho-docs/docs/linux/how-to-install-certbot-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Certbot on CentOS.\nAlso Read: How to Use Iperf to Test Network Performance\nThank You üôÇ","install-certbot-on-centos-7-with-nginx--apache#Install Certbot on CentOS 7 with nginx / apache":"In order to install certbot, we first need to install the epel-release repository (all of the following commands are performed with root privileges):\n# yum install epel-release Installing the client application and the package necessary for communicating with the server (using nginx as an example) is the next step.\n# yum install certbot python2-certbot-nginx Install certbot along with the appropriate package if you are utilising Apache.\n# yum install certbot python2-certbot-apache The next step is to immediately acquire a free certificate and set up https configuration. If you are using nginx, run:\n# certbot --nginx If Apache is already installed on your system:\n# certbot --apache When you use Certbot, it will ask for an email account that can be used for critical renewals and notifications related to security. After pointing to your message and pressing Enter, we will:\nFollowing that, you will need to approve the terms of service by entering Y and then pressing Enter:\nAfter the successful completion of the certificate retrieval process, we will be prompted to publicise our email address. We do not accept by typing N and pressing Enter:\nThe nginx and Apache configuration files will be scanned by Certbot for domain names. If they are found, it will display a list of them and offer to indicate which ones you want to acquire certificates for and configure the web server for. If they are not found, it will not display the list. (position numbers are entered in the console, possibly several separated by a space).\nIn the event that you do not yet have any configured domains, you will be required to explicitly provide the name:\nAfter that, we have to wait until the completion of the task for the client:\nIn order for Certbot to function properly, both port 80 and port 443 need to be open.\nRun the following command to check the version of Certbot.\n# certbot --version We navigate to the browser‚Äôs address bar and type in our website. A secure connection will be suggested by the presence of a lock icon to the left of it:\nLet‚Äôs Encrypt certificates are valid for a period of three months after they are granted, so we will set up automatic renewal for them. In order to accomplish this, start any text editor and type cron:\n# vi /etc/crontab In addition, we will assign a recurring action to periodically update all certificates by using certbot:\n0 0,12 * * * root python -c ‚Äòimport random; import time; time.sleep(random.random() * 3600)‚Äô \u0026\u0026 certbot renew -q\nSave the by file pressing Escape‚ÄØ:wq","introduction#Introduction":"In this article, you will learn how to install Certbot on CentOS.\nCertbot is a Python client that automates the process of obtaining free certificates from Let‚Äôs Encrypt and configuring web servers to establish a secure https connection using these certificates. It does this by acquiring the certificates through Let‚Äôs Encrypt.\nAs a starting point, we will look at the process of downloading Cerbot on CentOS, learn how to acquire a free certificate by utilising it, and automatically configure nginx and apache."},"title":"How to install Certbot on CentOS 7"},"/utho-docs/docs/linux/how-to-install-certbot-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Certbot on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-certbot-on-fedora-with-nginx--apache#Install Certbot on Fedora with nginx / apache":"In order to install certbot, we first need to install the epel-release repository (all of the following commands are performed with root privileges):\n# dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm --skip-broken Installing the client application and the package necessary for communicating with the server (using nginx as an example) is the next step.\n# dnf install certbot python3-certbot-nginx Install certbot along with the appropriate package if you are utilising Apache.\n# dnf install certbot python3-certbot-apache mod\\_ssl The next step is to immediately acquire a free certificate and set up https configuration. If you are using nginx, run:\n# certbot --nginx If Apache is already installed on your system:\n# certbot --apache Run the following command to check the version of Certbot.\n# certbot --version ","introduction#Introduction":"In this article, you will learn how to install Certbot on Fedora.\nCertbot¬†is a Python client that automates the process of obtaining free certificates from Let‚Äôs Encrypt and configuring web servers to establish a secure https connection using these certificates. It does this by acquiring the certificates through Let‚Äôs Encrypt.\nAs a starting point, we will look at the process of downloading Cerbot on Fedora, learn how to acquire a free certificate by utilising it, and automatically configure nginx and apache."},"title":"How to install Certbot on Fedora"},"/utho-docs/docs/linux/how-to-install-clipgrab-on-ubuntu-20-04-lts-to-download-youtube-videos/":{"data":{"":"\nDescription\nInstructions on how to install ClipGrab in order to download videos from YouTube using Ubuntu 20.04 LTS. ClipGrab is a free video downloader and converter that works with a wide variety of online video platforms in addition to YouTube, including Vimeo, Facebook, and many more. It only takes the press of a mouse to download videos of a very high quality. You also have the option to convert videos to MPEG-4, MP3, and any other format that is currently supported. ClipGrab can be downloaded for use on virtually every platform. In this section, we will go over the steps necessary to instal on systems based on Ubuntu 20.04 LTS.","add-execute-permission#Add Execute Permission":"To make it executable, use the chmod +x ClipGrab-3.9.7-x86 64.AppImage command, as shown below, to give it permission to run.\nchmod +x ClipGrab-3.9.7-x86_64.AppImage ","download-clipgrab#Download ClipGrab":"Since it‚Äôs open source, you can compile the source code or use the pre-compiled version. Here, we‚Äôll use a pre-compiled package. You can download ClipGrab‚Äôs newest pre-compiled version for Linux via wget. This downloads the current package.\nwget https://download.clipgrab.org/ClipGrab-3.9.7-x86_64.AppImage ","install-ffmpeg#Install ffmpeg":"To convert and download 1080p videos from YouTube with ClipGrab, you need to instal either ffmpeg or avconv. So, we‚Äôre going to instal ffmpeg by typing the command apt-get instal ffmpeg, as shown below. If you try to open ClipGrab without installing ffmpeg or avconv, you will get a message that says no installed version of avconv or ffmpeg could be found.\napt-get install ffmpeg ","open-clipgrab#Open ClipGrab":"You can now navigate to the location where you downloaded the pre-compiled package and double-click on it to begin.\nClipgrab has made it possible for you to download videos from YouTube at this time.\nHopefully Now you know How to Install ClipGrab on Ubuntu 20.04 LTS to Download YouTube Videos.\nMust Read : Determine All IP Addresses of Live Hosts Connected to the Network in Linux\nThankyou","update-server#Update Server":" apt update First, we need to use the apt update or apt-get update command to see if there are any new updates for all the configured and enabled repositories."},"title":"How to Install ClipGrab on Ubuntu 20.04 LTS to Download YouTube Videos"},"/utho-docs/docs/linux/how-to-install-cockpit-on-debian/":{"data":{"":" How to install Cockpit on Debian\nThe Cockpit is a web portal that lets you control the configuration settings of your servers through an easy-to-use web interface. It also works as a web console, so you can use your phone or tablet to access it.\nThe online portal Cockpit gives you access to a wide range of administrative tasks, such as:\nTaking care of service Taking care of accounts System service management and monitoring Setting up network connections and a firewall Reviewing system logs Managing virtual machines Making reports on diagnoses Setting kernel dump configuration Setting up SELinux Changing the software Taking care of subscriptions to systems Because the Cockpit web console and the terminal both use the same application programming interfaces (APIs), any changes made in the terminal are immediately reflected on the web console. You can also change the settings directly through the online portal or the terminal.","installing-cockpit-web-console-in-debian#Installing Cockpit Web Console in Debian":"Using the command given below, you will be able to install it on your system. With this command, the cockpit and all of its dependencies will be installed.\napt update apt install cockpit Next, turn the controls on and start the cockpit. socket service to use the web interface to connect to the system and check the service while also going through the cockpit procedure.\n# systemctl start cockpit ","logging-in-to-the-cockpit-web-console#Logging in to the Cockpit Web Console":"Follow the steps below to learn how to use a user account that only works on the local system to log in to the Cockpit web interface for the first time. You can use the user name and password of any local account on the system to log in to Cockpit. This is because it uses a PAM stack authentication method that can be found in the /etc/pam.d/cockpit file.\nUse your web browser to go to the following URLs to get to the Cockpit‚Äôs web console:\nhttps://server-ip_or_domain-name Now, you will see the browser page like below. This a simple warning page, you can ignore it by clicking on advanced and ‚Äòproceed to *server-ip*(unsafe)‚Äô\nWarning page of domain\nNow you will prompt to login page of the cockpit, here enter the details of the username with you want to login.\nLogin page of Cockpit\nAlso Read: How to install SSL on Ubuntu with Apache2, How to install SSL on Centos-7.3 with Nginx server","systemctl-enable-cockpit#systemctl enable cockpit":" If you are using the ufw firewall on the system, you will need to allow traffic through the cockpit port 9090 in the firewall.\nufw allow 9090 Output:\nRule added\nRule added (v6)"},"title":"How to install Cockpit on Debian"},"/utho-docs/docs/linux/how-to-install-cockpit-on-fedora-server/":{"data":{"":"\nThe Cockpit is a web console with an easy-to-use web-based interface that lets you manage your servers‚Äô settings. Because it is also a web console, you can also use it on your phone or tablet.\nYou can do a wide range of administrative tasks through the Cockpit web console, such as:\nTaking care of service Taking care of accounts System service management and monitoring Setting up network connections and a firewall Reviewing system logs Managing virtual machines Making reports on diagnoses Setting kernel dump configuration Setting up SELinux Changing the software Taking care of subscriptions to systems The Cockpit web console uses the same system APIs as the terminal, and tasks done in the terminal are quickly reflected in the web console. You can also change the settings directly through the web console or the terminal.","installing-cockpit-web-console-in-fedora#Installing Cockpit Web Console in Fedora":"You can install it on your system by using the command below, which will install the cockpit with its required dependencies.\ndnf install cockpit -y Next, enable and start the¬†cockpit.socket¬†service to connect to the system through the web console and verify the service and running the cockpit process\nsystemctl start cockpit systemctl enable cockpit If you are running a¬†firewall¬†on the system, you need to open the cockpit port¬†9090¬†in the firewall.\nfirewall-cmd --permanent --add-service=cockpit firewall-cmd --reload ","logging-in-to-the-cockpit-web-console-in-fedora#Logging in to the Cockpit Web Console in Fedora":"The steps below show how to use a local system user account to log in to the Cockpit web console for the first time. As Cockpit uses a certain PAM stack authentication found at /etc/pam.d/cockpit, you can log in with the user name and password of any local account on the system\nGo to the following URLs in your web browser to open the Cockpit web console:\nWarning page\nAfter clicking ‚Äòproceed to ‚Ä¶(unsafe)‚Äô and that is it. You have installed Cockpit on your Fedora server."},"title":"How to install Cockpit on Fedora Server"},"/utho-docs/docs/linux/how-to-install-cockpit-on-ubuntu-server/":{"data":{"":"\nThe Cockpit is a web console with an easy-to-use web-based interface that lets you manage your servers‚Äô settings. Because it is also a web console, you can also use it on your phone or tablet.\nYou can do a wide range of administrative tasks through the Cockpit web console, such as:\nTaking care of service Taking care of accounts System service management and monitoring Setting up network connections and a firewall Reviewing system logs Managing virtual machines Making reports on diagnoses Setting kernel dump configuration Setting up SELinux Changing the software Taking care of subscriptions to systems The Cockpit web console uses the same system APIs as the terminal, and tasks done in the terminal are quickly reflected in the web console. You can also change the settings directly through the web console or the terminal.","installing-cockpit-web-console-in-ubuntu#Installing Cockpit Web Console in Ubuntu":"You can install it on your system by using the command below, which will install the cockpit with its required dependencies.\n# apt update apt install cockpit completion of installation of cockpit\nNext, enable and start the¬†cockpit.socket¬†service to connect to the system through the web console and verify the service and running the cockpit process\nsystemctl start cockpit systemctl enable cockpit If you are running ufw firewall¬†on the system, you need to open the cockpit port¬†9090¬†in the firewall.\n# ufw allow 9090 Output:\nRule added\nRule added (v6)","logging-in-to-the-cockpit-web-console#Logging in to the Cockpit Web Console":"The steps below show how to use a local system user account to log in to the Cockpit web console for the first time. As Cockpit uses a certain PAM stack authentication found at /etc/pam.d/cockpit, you can log in with the user name and password of any local account on the system\nGo to the following URLs in your web browser to open the Cockpit web console:\nhttps://server-ip Now, you will see the browser page like below. This a simple warning page, you can ignore it by clicking on advanced and ‚Äòproceed to *server-ip*(unsafe)‚Äô\nhttps://server-ip, warning page\nNow you will prompt to login page of the cockpit, here enter the details of the username with you want to login.\ncockpit login page\nAfter successfully login in to your cockpit page, you will see a page similar to below screenshot.\nCongratulations!!! you have successfully installed the cockpit on your ubuntu server\ncockpit homepage"},"title":"How to install Cockpit on Ubuntu server"},"/utho-docs/docs/linux/how-to-install-cockpit-web-console-in-centos-7-7/":{"data":{"":"\nThe Cockpit is a web console with an easy-to-use web-based interface that lets you manage your servers‚Äô settings. Because it is also a web console, you can also use it on your phone or tablet.\nYou can do a wide range of administrative tasks through the Cockpit web console, such as:\nTaking care of service Taking care of accounts System service management and monitoring Setting up network connections and a firewall Reviewing system logs Managing virtual machines Making reports on diagnoses Setting kernel dump configuration Setting up SELinux Changing the software Taking care of subscriptions to systems The Cockpit web console uses the same system APIs as the terminal, and tasks done in the terminal are quickly reflected in the web console. You can also change the settings directly through the web console or the terminal.","installing-cockpit-web-console-in-centos-77#Installing Cockpit Web Console in CentOS 7.7":"You can install it on your system by using the command below, which will install the cockpit with its required dependencies.\nyum install cockpit -y confirmation of installation\nNext, enable and start the¬†cockpit.socket¬†service to connect to the system through the web console and verify the service and running the cockpit process\nsystemctl start cockpit systemctl enable cockpit If you are running a¬†firewalld¬†on the system, you need to open the cockpit port¬†9090¬†in the firewall.\nfirewall-cmd --permanent --add-service=cockpit firewall-cmd --reload ","logging-in-to-the-cockpit-web-console-in-centos-8#Logging in to the Cockpit Web Console in CentOS 8":"The steps below show how to use a local system user account to log in to the Cockpit web console for the first time. As Cockpit uses a certain PAM stack authentication found at /etc/pam.d/cockpit, you can log in with the user name and password of any local account on the system\nGo to the following URLs in your web browser to open the Cockpit web console:\nWarning page\nAfter clicking ‚Äòproceed to ‚Ä¶(unsafe)‚Äô you will see a similar page like below.\nLogin page of cockpit"},"title":"How to Install Cockpit Web Console in CentOS 7.7"},"/utho-docs/docs/linux/how-to-install-composer-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Composer on Almalinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Composer on Almalinux 8.\nComposer is an open source tool for managing PHP dependencies. It was made to make it easier to distribute and maintain PHP files as separate application components. It has changed the PHP ecosystem in a big way by laying the groundwork for current PHP development with applications and frameworks that are built from elements.\nStep 1: Install Dependencies Install the PHP CLI (command-line interface) package along with everything else that needs to be done by:\n# dnf install php-cli php-json php-zip wget unzip -y Step 2: Download and Install Composer Once PHP CLI has been successfully set up, download the Composer installation script:\n# php -r \"copy('https://getcomposer.org/installer', 'composer-setup.php');\" With the above command, a file called ‚Äúcomposer-setup.php‚Äù is downloaded into the working directory, which is the directory that is currently being used.\nThe file‚Äôs SHA-384 hash should be checked against the hash on the Composer Public Keys / Signatures page. This will make sure the information is correct.\nThe following wget command will get the latest Composer installation‚Äôs signature from the Composer Github website and save it in a variable called HASH:\n# HASH=\"$(wget -q -O - https://composer.github.io/installer.sig)\" Run the following command to make sure that the installation script hasn‚Äôt been messed up:\n# php -r \"if (hash\\_file('SHA384', 'composer-setup.php') === '$HASH') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP\\_EOL;\" If the hashes are the same, this output will appear:\nIf the hashes don‚Äôt match, on the other hand, you‚Äôll find that the Installer file is broken. After the authenticity has been proven, move on to the next step.\nFor Composer to be installed in the /usr/local/bin directory, the following command must be run:\n# php composer-setup.php --install-dir=/usr/local/bin --filename=composer When the previous command instals it, all users on the system can use the composer command. This is what the final product will look like.\nWhen the previous command installs it, all users on the system can use the composer command.\nTo test your installation, run:\n# composer "},"title":"How to install Composer on Almalinux 8"},"/utho-docs/docs/linux/how-to-install-composer-on-centos-server/":{"data":{"":" How to install Composer on CentOS server\nIn this tutorial we will learn how to install Composer on Centos 7 or on a more recent version.¬†Composer¬†is a tool for taking care of PHP dependencies. It lets you list the libraries that your project needs, and it will install, update, and manage them for you.\nComposer is not like Yum or Apt in that it is not a package manager. Yes, it works with ‚Äúpackages‚Äù or libraries, but it handles them per project and installs them in a directory (like ‚Äúvendor‚Äù) inside your project. It doesn‚Äôt install anything everywhere by default. So, it is a manager of dependencies. It does, however, make it easy to work on a ‚Äúglobal‚Äù project with the ‚Äúglobal‚Äù command.\nThis is not a new idea, and node‚Äôs npm and ruby‚Äôs bundler were big influences on Composer.","1--install-dependencies#1 ‚Äî Install Dependencies":"Install the PHP CLI (command-line interface) package along with everything else that needs to be done by:\n# yum install php-cli php-json php-zip wget unzip -y ","2--download-and-install-composer#2 ‚Äî Download and Install Composer":"Once PHP CLI has been successfully set up, download the Composer installation script:\n# php -r \"copy('https://getcomposer.org/installer', 'composer-setup.php');\" With the above command, a file called ‚Äúcomposer-setup.php‚Äù is downloaded into the working directory, which is the directory that is currently being used.\nThe file‚Äôs SHA-384 hash should be checked against the hash on the Composer Public Keys / Signatures page. This will make sure the information is correct.\nThe following wget command will get the latest Composer installation‚Äôs signature from the Composer Github website and save it in a variable called HASH:\n# HASH=\"$(wget -q -O - https://composer.github.io/installer.sig)\" Run the following command to make sure that the installation script hasn‚Äôt been messed up:\n# php -r \"if (hash_file('SHA384', 'composer-setup.php') === '$HASH') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\" If the hashes are the same, this output will appear:\nOutput:\nInstaller verified\nIf the hashes don‚Äôt match, on the other hand, you‚Äôll find that the Installer file is broken. After the authenticity has been proven, move on to the next step.\nFor Composer to be installed in the /usr/local/bin directory, the following command must be run:\n# php composer-setup.php --install-dir=/usr/local/bin --filename=composer When the previous command instals it, all users on the system can use the composer command. This is what the final product will look like.\nDownloading of Composer\nWhen the previous command installs it, all users on the system can use the composer command.\nTo test your installation, run:\n# composer Successfully installed Composer on Centos\nNow that Composer has been successfully installed on your CentOS system, you can start using it.\nAlso Read: How To Create Temporary and Permanent Redirects with Apache on Ubuntu, Python 3 Installation and Programming Environment Configuration on an Ubuntu 22.04","prerequisites#Prerequisites":" Yum server configured on CentOS server Super user( root ) or any Normal user with SUDO privileges. "},"title":"How to install Composer on CentOS server"},"/utho-docs/docs/linux/how-to-install-composer-on-debian-servers/":{"data":{"":" How to install Composer on Debian servers\nIn this tutorial we will learn how to install Composer on a Debian server.¬†Composer¬†is a tool for taking care of PHP dependencies. It lets you list the libraries that your project needs, and it will install, update, and manage them for you.\nComposer is not like Yum or Apt in that it is not a package manager. Yes, it works with ‚Äúpackages‚Äù or libraries, but it handles them per project and installs them in a directory (like ‚Äúvendor‚Äù) inside your project. It doesn‚Äôt install anything everywhere by default. So, it is a manager of dependencies. It does, however, make it easy to work on a ‚Äúglobal‚Äù project with the ‚Äúglobal‚Äù command.\nThis is not a new idea, and node‚Äôs npm and ruby‚Äôs bundler were big influences on Composer.","1--install-dependencies#1 ‚Äî Install Dependencies":"First, make sure your package manager‚Äôs cache is up to date and install the necessary dependencies, including php-cli:\napt-get install update apt install php-cli unzip ","2--download-and-install-composer#2 ‚Äî Download and Install Composer":"First, make sure you‚Äôre in the directory where your home files are. Then, use curl to download the Composer installer:\ncurl -sS https://getcomposer.org/installer -o /tmp/composer-setup.php Next, we‚Äôll see if the SHA-384 hash of the installation we downloaded matches the SHA-384 hash of the most recent installer, which can be found on the page with the Composer Public Keys/Signatures.\nUsing curl, you can get the most recent signature, which you can then store in a shell variable:\n# HASH=\\``curl -sS https://composer.github.io/installer.sig`\\` Run the following line of PHP code to see if it‚Äôs a good time to run the installation script:\nphp -r \"if (hash_file('SHA384', '/tmp/composer-setup.php') === '$HASH') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\" Output:\nInstaller verified\nThe next command will download and install Composer as a command called composer that can be used all over the system. This command will be found in the directory /usr/local/bin.\nphp /tmp/composer-setup.php --install-dir=/usr/local/bin --filename=composer Output:\nAll settings correct for using Composer\nDownloading‚Ä¶\nComposer (version 2.3.5) successfully installed to: /usr/local/bin/composer\nUse it: php /usr/local/bin/composer\nAnd now, if you want to verify your installation.\n# composer Confirmation of installation of Composer\nAnd that is how easily you can install the Composer on your Debian Server.\nAlso Read: VirtualHost creation in Tomcat 10/9/8/7, How To Create Redirects with Nginx","prerequisites#Prerequisites":" An Debian server with APT repository configured Super user( root ) or any Normal user with SUDO privileges. "},"title":"How to install Composer on Debian servers"},"/utho-docs/docs/linux/how-to-install-composer-on-ubuntu-20-04/":{"data":{"":"","1--install-dependencies#1 ‚Äî Install Dependencies":"","2--download-and-install-composer#2 ‚Äî Download and Install Composer":"Ensure you‚Äôre in your home directory, and then use curl to download the Composer installer:\n# curl -sS https://getcomposer.org/installer -o /tmp/composer-setup.php Next, we‚Äôll make sure that the installer we downloaded matches the latest installer‚Äôs SHA-384 hash, which can be found on the Composer Public Keys / Signatures page.\nGet the latest signature with curl and put it in a shell variable:\n# HASH=``curl -sS https://composer.github.io/installer.sig`` Now, in order to check whether or not it is okay to run the installation script, execute the following PHP code:\n# php -r \"if (hash_file('SHA384', '/tmp/composer-setup.php') === '$HASH') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\" Output:\nInstaller verified\nThe following command will download and install Composer as a system-wide command called composer in /usr/local/bin:\n# php /tmp/composer-setup.php --install-dir=/usr/local/bin --filename=composer output of above command\nOutput:\nAll settings correct for using Composer\nDownloading‚Ä¶\nComposer (version 2.3.5) successfully installed to: /usr/local/bin/composer\nUse it: php /usr/local/bin/composer\nAnd now, if you want to verify your installation.\n# composer Successful installation of Composer\nAnd That is how you can install the Composer on the Ubuntu server\nAlso Read: How to use Hugo, How to Install Nginx and PHP-FastCGI on Fedora","apt-install-php-cli-unzip#apt install php-cli unzip":" How to install Composer on Ubuntu\nIn this tutorial we will learn how to install Composer on Ubuntu 20.04 or on more latest version. Composer is a tool for taking care of PHP dependencies. It lets you list the libraries that your project needs, and it will install, update, and manage them for you.\nComposer is not like Yum or Apt in that it is not a package manager. Yes, it works with ‚Äúpackages‚Äù or libraries, but it handles them per project and instals them in a directory (like ‚Äúvendor‚Äù) inside your project. It doesn‚Äôt install anything everywhere by default. So, it is a manager of dependencies. It does, however, make it easy to work on a ‚Äúglobal‚Äù project with the ‚Äúglobal‚Äù command.\nThis is not a new idea, and node‚Äôs npm and ruby‚Äôs bundler were big influences on Composer.\nPrerequisites An Ubuntu server with APT repository configured Super user( root ) or any Normal user with SUDO privileges. 1 ‚Äî Install Dependencies First, make sure your package manager‚Äôs cache is up to date and install the necessary dependencies, including php-cli:\n# apt-get install update apt install php-cli unzip ","prerequisites#Prerequisites":""},"title":"How to install Composer on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-csf-in-cpanel/":{"data":{"":"\nConfigServe Firewall (CSF) is a firewall configuration tool that was created to enhance server security by providing a simple yet powerful interface for configuring firewall settings on cPanel servers.\nThis programme is freely downloadable as a WHM plugin. To perform a simple CSF installation, follow these steps.\nInstall CSF:¬†Log into your server as root, using SSH.\nFollow below given command\n# cd /usr/local/src/ You will obtain the following image after running this command:\nAfter changing the directory, use the following command to start the download:\n# wget https://download.configserver.com/csf.tgz After you‚Äôve finished downloading, run the command listed below.\n# tar -xzf csf.tgz # cd csf # sh install.sh # systemctl status csf.service Configure CSF: By logging into WHM as root and going to the bottom left menu. Go to ConfigServer Security Firewall in the Plugins area.\nThank you!!"},"title":"How to install CSF in cPanel"},"/utho-docs/docs/linux/how-to-install-cyberpanel-on-centos-7/":{"data":{"":"\nIntroduction\nCyberPanel is an open-source and free web hosting control panel powered by OpenLiteSpeed. It helps reduce resource usage as a performance benefit. This is why CyberPanel is different from other control panels. Cyberpanel is available both in free and enterprise versions. OpenLiteSpeed is used by the free version, and the enterprise version uses LSWS.\nHow to Install\nStep 1: Login to your server via SSH\n**Step 2: Firstly, update the packages installed on your system by running:\n**\n# yum update ‚Äìy Step 3: Since CyberPanel is free, you do not have to prepare any kind of license, and you can¬†install¬†it using the following command:\n# sh \u003c(curl `[https://cyberpanel.net/install.sh](https://cyberpanel.net/install.sh)` || wget `-O - [https://cyberpanel.net/install.sh](https://cyberpanel.net/install.sh)`) **Step 4: Next you will see ‚ÄúCyberPanel Installer v2.1.1‚Äù.\n**Press¬†1¬†and hit enter.\nStep 5: Now, the installer will show you the RAM and disk requirements to install CyberPanel\nPress¬†1¬†and hit enter to install CyberPanel with¬†OpenLiteSpeed\nStep 6: Next comes a prompt to install the full service for CyberPanel, which includes PowerDNS, Postfix, and Pure-FTPd.\nPress¬†y¬†and hit enter\nStep 7: Next comes a prompt to setup Remote MySQL.\nPress¬†n¬†and hit enter.\nStep 8: Next comes a prompt to select the¬†version of the CyberPanel¬†that you want to install.\nJust hit enter to continue with the latest version.\nStep 9: Next comes a prompt to choose a password.\nYou can choose the default password, a randomly generated password or specify the admin password\nYou can press¬†d¬†to choose the default password (which is¬†1234567)\nOr you can press¬†r¬†to choose a randomly generated password\nOr you can press¬†s¬†to specify the admin password\nIn this article, we‚Äôre choosing the¬†default password. So, we will press¬†d¬†and hit enter\nThe admin password will be set to¬†1234567.\nStep 10: Next comes a prompt to install the Memcached process and its PHP extension.\nPress¬†Y¬†and hit Enter.\nStep 11: Next comes a prompt to install the Redis process and its PHP extensions.\nPress¬†y¬†and hit Enter.\nStep 12: Next it will ask if you want to kill the watchdog.\nPress¬†Y¬†(capital Y)and hit enter\nInstallation will begin.\nStep 13: After the installation is complete, it will ask you to restart the server.\nPress y and hit enter to restart the server\nPutty closes once you restart the server.\n**Step 14: Access your CyberPanel by hitting¬†Your_Server_IP:8090¬†in your browser.\n**\nUse default login credentials.\nEnter username: admin\nEnter password: 1234567\nYou‚Äôve entered your CyberPanel.\nHow to configure Firewall on CyberPanel\nFirewall rules define what type of Internet traffic is allowed or blocked. You can verify the OS firewall and the Vultr firewall are both open for inbound and outbound on the following ports:\n1_ TCP: 8090 for CyberPanel\n2_ TCP: 80, TCP: 443, and UDP: 443 for the webserver\n3_ TCP: 21 and TCP: 40110-40210 for FTP\n4_ TCP: 25, TCP: 587, TCP: 465, TCP: 110, TCP: 143, and TCP: 993 for mail service\n5_ TCP: 53 and UDP: 53 for DNS service\nThank you."},"title":"How to Install CyberPanel on CentOS 7"},"/utho-docs/docs/linux/how-to-install-django-on-debian-server/":{"data":{"":"","1-global-install-from-packages#1. Global Install from Packages":"","2-install-django-with-pip-in-a-virtual-environment#2. Install Django with pip in a Virtual Environment":"","3-creating-a-sample-project-with-newly-installed-django#3. Creating a Sample Project with newly installed Django":"","4-changing-the-allowed-hosts-setting-in-django#4. Changing the ALLOWED HOSTS setting in Django":"","5-testing-out-the-development-server#5. Testing out the Development Server":" How to Install Django on debian\nIn this document, we will learn how to install Django on Debian server. Django is a web framework written in Python that lets you make interactive websites and apps. With Django, it‚Äôs easy to make Python web apps, and the framework takes care of a lot of the hard work for you.\nIt works quickly and is made to help developers get their apps up and running as soon as possible. Django helps developers avoid common security mistakes like SQL injection, XSS, CSRF, and clickjacking.\nAfter you‚Äôve installed it, you‚Äôll make a new project that will be the basis for your website.\nPostgreSQL, MariaDB, MySQL, Oracle, and SQLite are the four main databases that Django supports. Other popular SQL and NoSQL databases are supported at different levels by community libraries. We recommend that you choose the same database for production and development, even though Django‚Äôs Object-Relational Mapper brings out many of the differences in the database (ORM).\nMethods to install Django There are different ways to install Django, depending on your needs and how you want to set up your development environment. There are pros and cons to each of these, and one might work better for you than the others.\nSome of the different ways are:\nGlobal install from packages: The standard apt package manager in Debian can be used to install Django packages from the official repositories. This is simple, but not very adaptable. It‚Äôs also possible that the version in the repositories is behind the official project version. In a virtual environment, you can install with pip:¬†Using tools like venv and virtualenv, you can give each of your projects its own environment. With a virtual environment, you can put Django and other customizations and packages for the project in the project directory. Most of the time, this is the best and most practical way to work with Django. To install the development version with git:¬†If you want to install the latest development version, you can¬†get the code from the Git repository¬†instead of the stable release. This needs to be done in your virtual environment so that you can get the latest fixes and features. But development models aren‚Äôt as stable as versions that have been tested more. (Not covered by this document) Prerequisites: Any browser installed on your machine Super user or any normal user with SUDO privileges. Installation Steps: 1. Global Install from Packages If you want to use the Debian repositories to set up Django, the process is very easy.\nStep 1:¬†First step is to use apt to update your local package index:\napt update Step 2:¬†Now check the version of¬†Python you installed on your Debian server. You can see if it‚Äôs true by:\npython3 ‚ÄìV Step 3:¬†You can now put Django on your computer:\napt install python3-django Step 4:¬†You can make sure the installation is right by:\ndjango-admin version Django version\nThis means that the software will be installed correctly. You might also find that the version of Django you have is not the latest stable version.\n2. Install Django with pip in a Virtual Environment Django can be installed on your system in a virtual environment in a flexible and easy way. We‚Äôll show you how to install Django in a virtual environment, which will be made with the venv module in the standard Python 3 library. With this tool, you can create a virtual Python environment and install Python packages without affecting the rest of the system. So, you can choose Python packages for each project, even if they conflict with the needs of other projects.\nStep 2.1:¬†First refresh the local package index:\napt update Step 2.2:¬†Check the version of Python that is installed:\npython3 -V Step 2.3:¬†Now, install pip using the Debian repositories:\napt install python3-pip python3-venv -y Step 2.4:¬†Now, whenever you start a new project, you can set up a virtual environment for it. Start by making a new project directory and moving it to the new one:\nmkdir ~/testProject cd ~/testProject Step 2.5:¬†Next, use the python command in the project directory to set up a virtual environment for your version of Python. We‚Äôll call my env as our virtual environment, but you should call it descriptive:\npython3.8 - venv my_env Step 2.6:¬†This will install standalone versions of Python and pip in your project directory into a separate directory structure. The file hierarchy for where your packages will be installed will be put in a directory with the name you give it.\nPackages can be install into the isolated environment by:\nsource my_env/bin/activate Prompt after executing above command\nStep 2.7:¬†Your prompt should now say that your virtual environment is up and running. It will look like (my env)username@hostname:/newProject$.\nYou can add Django to your new environment with pip. No matter what version of Python you have, you should only call pip when you are in your virtual environment. Also, because you are installing locally, you won‚Äôt need sudo:\npip install django Keep in mind that this version may be different.\nStep 2.8:¬†To leave your virtual environment, you must send the deactivate command from anywhere on the system:\ndeactivate 3. Creating a Sample Project with newly installed Django With Django, you can start building your project. We will talk about how to make a project and test it on your development server using a virtual environment.\nStep 3.1:¬†After following the above steps, you jest need to follow the below steps to create a sample project with your newly installed Django\nWith startproject, you can use the¬†django-admin¬†command to build your project. You could call this something else, but we‚Äôre going to call our project djangoproject. In the working directory you already have, startproject will create a directory with:\n(myEnv) $ django-admin startproject Step 3.2:¬†Using the manage.py command to migrate the database. Migration takes into account any changes you have made to your database schema in your Django models.\nTo move the database, you¬†can:\n(myEnv) $ python3.8 manage.py migrate Migrate the database of Django project\nLastly, let‚Äôs make an administrative user so you can use the Djano admin interface. Using the createsuperuser command, let‚Äôs do this:\n(myEnv) # python3.8 manage.py createsuperuser You will be asked to give your user a user name, an email address, and a password.\n4. Changing the ALLOWED HOSTS setting in Django You will need to change one of the directives in the Django settings in order to test your app.\nStep 4.1:¬†Type: to open the settings file.\nvi ~/django-test/djangoproject/settings.py Find the ALLOWED HOSTS directive inside. This sets up a list of addresses or domain names that can be used to connect to the Django instance. If a request comes in with a Host header that is not on this list, an exception will be thrown. Django needs you to set this so that a certain type of security hole doesn‚Äôt happen.\nSave the file and close the editor when you are done.\nStep 4.2:¬†Put the IP addresses or domain names that are connected to your Django server in the square brackets. Each item should be written in quotation marks, with a comma between each entry. Add a period to the beginning of the entry to get requests for the whole domain and any subdomains:\nALLOWED_HOSTS = ['your_server_ip_or_domain', 'your_second_ip_or_domain', . . .] Step 4.3:¬†Save the file and close the editor when you are done.\n5. Testing out the Development Server Once you have a user, you can start up the Django development server to see what a new Django project looks like. You should only use this to build something. When you are ready to deploy, make sure to carefully follow Django‚Äôs deployment instructions.\nStep 5.1:¬†Make sure that the right port is open in your firewall before you try the development server. If you set up your server according to the initial setup guide and are using UFW, you can open port 8000 by typing:\nufw allow 8000 Step 5.2:¬†Start the development server:\n(myEnv) # python3.8 manage.py runserver \u003cserver-ip\u003e:8000 Running Django project after installing Django successfully\nVisit your server‚Äôs IP address followed by¬†:8000¬†in your favourite web browser:\nYou should see something that looks like the below screenshot:\nDjango Homepage\nTo access the admin interface, add¬†/admin/¬†to the end of your URL and it will take you to the admin login page.\nUser login page\nCongratulations! You have learnt successfully How to Install Django on Debian server.","installation-steps#Installation Steps:":"","methods-to-install-django#Methods to install Django":"","prerequisites#Prerequisites:":""},"title":"How to install Django on Debian server"},"/utho-docs/docs/linux/how-to-install-django-on-rockylinux-8/":{"data":{"":"","1-global-install-from-packages#1. Global Install from Packages":"","2-install-django-with-pip-in-a-virtual-environment#2. Install Django with pip in a Virtual Environment":"","3-creating-a-sample-project-with-newly-installed-django#3. Creating a Sample Project with newly installed Django":"","4-changing-the-allowed-hosts-setting-in-django#4. Changing the ALLOWED HOSTS setting in Django":"","5-testing-out-the-development-server#5. Testing out the Development Server":" How to install Django on RockyLinux 8\nIn this document, we will learn how to install Django on RockyLinux 8. Django is a web framework written in Python that lets you make interactive websites and apps. With Django, it‚Äôs easy to make Python web apps, and the framework takes care of a lot of the hard work for you.\nIt works quickly and is made to help developers get their apps up and running as soon as possible. Django helps developers avoid common security mistakes like SQL injection, XSS, CSRF, and clickjacking.\nThis guide will show you how to put Django on a server running RockyLinux. After you‚Äôve installed it, you‚Äôll make a new project that will be the basis for your website.\nPostgreSQL, MariaDB, MySQL, Oracle, and SQLite are the four main databases that Django supports. Other popular SQL and NoSQL databases are supported at different levels by community libraries. We recommend that you choose the same database for production and development, even though Django‚Äôs Object-Relational Mapper brings out many of the differences in the database (ORM).\nMethods to install Django There are different ways to install Django, depending on your needs and how you want to set up your development environment. There are pros and cons to each of these, and one might work better for you than the others.\nSome of the different ways are:\nGlobal install from packages: The standard yum package manager in RockyLinux can be used to install Django packages from the official repositories. This is simple, but not very adaptable. It‚Äôs also possible that the version in the repositories is behind the official project version.\nIn a virtual environment, you can install with pip:¬†Using tools like venv and virtualenv, you can give each of your projects its own environment. With a virtual environment, you can put Django and other customizations and packages for the project in the project directory. Most of the time, this is the best and most practical way to work with Django.\nTo install the development version with git:¬†If you want to install the latest development version, you can¬†get the code from the Git repository¬†instead of the stable release. This needs to be done in your virtual environment so that you can get the latest fixes and features. But development models aren‚Äôt as stable as versions that have been tested more. (Not covered by this document)\nPrerequisites: Any browser installed on your machine\nSuper user or any normal user with SUDO privileges.\nInstallation Steps: 1. Global Install from Packages If you want to use the yum repositories to set up Django, the process is very easy.\nStep 1:¬†First step is to install Epel-release:\nyum install epel-release -y Step 2:¬†Now check the version of¬†Python you installed on your RockyLinux server. You can see if it‚Äôs true by:\npython3 ‚ÄìV Step 3:¬†You can now put Django on your computer:\nyum install python3-django Step 4:¬†You can make sure the installation is right by:\ndjango-admin version Django version\nThis means that the software will be installed correctly. You might also find that the version of Django you have is not the latest stable version.\n2. Install Django with pip in a Virtual Environment Django can be installed on your system in a virtual environment in a flexible and easy way. We‚Äôll show you how to install Django in a virtual environment, which will be made with the venv module in the standard Python 3 library. With this tool, you can create a virtual Python environment and install Python packages without affecting the rest of the system. So, you can choose Python packages for each project, even if they conflict with the needs of other projects.\nStep 2.1:¬†First list the local package index:\nyum repolist Step 2.2:¬†Check the version of Python that is installed:\npython3 -V Step 2.3:¬†Now, install pip using the yum repositories:\nyum install python3-pip python3-venv -y Step 2.4:¬†Now, whenever you start a new project, you can set up a virtual environment for it. Start by making a new project directory and moving it to the new one:\nmkdir ~/testProject cd ~/testProject Step 2.5:¬†Next, use the python command in the project directory to set up a virtual environment for your version of Python. We‚Äôll call my env as our virtual environment, but you should call it descriptive:\npython3.8 - venv my_env Step 2.6:¬†This will install standalone versions of Python and pip in your project directory into a separate directory structure. The file hierarchy for where your packages will be installed will be put in a directory with the name you give it.\nPackages can be install into the isolated environment by:\nsource my_env/bin/activate Prompt after executing above command\nStep 2.7:¬†Your prompt should now say that your virtual environment is up and running. It will look like (my env)username@hostnamenewProject$.\nYou can add Django to your new environment with pip. No matter what version of Python you have, you should only call pip when you are in your virtual environment. Also, because you are installing locally, you won‚Äôt need sudo:\npip install django Keep in mind that this version may be different.\nStep 2.8:¬†To leave your virtual environment, you must send the deactivate command from anywhere on the system:\ndeactivate 3. Creating a Sample Project with newly installed Django With Django, you can start building your project. We will talk about how to make a project and test it on your development server using a virtual environment.\nStep 3.1:¬†After following the above steps, you jest need to follow the below steps to create a sample project with your newly installed Django\nWith startproject, you can use the¬†django-admin¬†command to build your project. You could call this something else, but we‚Äôre going to call our project djangoproject. In the working directory you already have, startproject will create a directory with:\n(myEnv) $ django-admin startproject Step 3.2:¬†Using the manage.py command to migrate the database. Migration takes into account any changes you have made to your database schema in your Django models.\nTo move the database, you¬†can:\n(myEnv) $ python3.8 manage.py migrate Migrate the database of Django project\nLastly, let‚Äôs make an administrative user so you can use the Djano admin interface. Using the createsuperuser command, let‚Äôs do this:\n(myEnv) # python3.8 manage.py createsuperuser You will be asked to give your user a user name, an email address, and a password.\n4. Changing the ALLOWED HOSTS setting in Django You will need to change one of the directives in the Django settings in order to test your app.\nStep 4.1:¬†Type: to open the settings file.\nvi ~/django-test/djangoproject/settings.py Find the ALLOWED HOSTS directive inside. This sets up a list of addresses or domain names that can be used to connect to the Django instance. If a request comes in with a Host header that is not on this list, an exception will be thrown. Django needs you to set this so that a certain type of security hole doesn‚Äôt happen.\nSave the file and close the editor when you are done.\nStep 4.2:¬†Put the IP addresses or domain names that are connected to your Django server in the square brackets. Each item should be written in quotation marks, with a comma between each entry. Add a period to the beginning of the entry to get requests for the whole domain and any subdomains:\nALLOWED_HOSTS = ['your_server_ip_or_domain', 'your_second_ip_or_domain', . . .] Step 4.3:¬†Save the file and close the editor when you are done.\n5. Testing out the Development Server Once you have a user, you can start up the Django development server to see what a new Django project looks like. You should only use this to build something. When you are ready to deploy, make sure to carefully follow Django‚Äôs deployment instructions.\nStep 5.1:¬†Make sure that the right port is open in your firewall before you try the development server. If you set up your server according to the initial setup guide and are using UFW, you can open port 8000 by typing:\nfirewall-cmd --permanent --allow-port=8000/tcp firewall-cmd --reload Step 5.2:¬†Start the development server:\n(myEnv) # python3.8 manage.py migrate Running Django project after installing Django successfully\nVisit your server‚Äôs IP address followed by¬†:8000¬†in your favourite web browser:\nYou should see something that looks like the below screenshot:\nNow, you have learnt how to install Django on RockyLinux 8","installation-steps#Installation Steps:":"","methods-to-install-django#Methods to install Django":"","prerequisites#Prerequisites:":""},"title":"How to install Django on RockyLinux 8"},"/utho-docs/docs/linux/how-to-install-django-on-ubuntu-22-04/":{"data":{"":"","1-global-install-from-packages#1. Global Install from Packages":"","2-install-django-with-pip-in-a-virtual-environment#2. Install Django with pip in a Virtual Environment":"","3-creating-a-sample-project-with-newly-installed-django#3. Creating a Sample Project with newly installed Django":"","4-changing-the-allowed-hosts-setting-in-django#4. Changing the ALLOWED HOSTS setting in Django":"","5-testing-out-the-development-server#5. Testing out the Development Server":" How to Install Django on Ubuntu\nIn this document, we will learn how to install Django on Ubuntu 20.04 and 22.04 LTS. Django is a web framework written in Python that lets you make interactive websites and apps. With Django, it‚Äôs easy to make Python web apps, and the framework takes care of a lot of the hard work for you.\nIt works quickly and is made to help developers get their apps up and running as soon as possible. Django helps developers avoid common security mistakes like SQL injection, XSS, CSRF, and clickjacking.\nThis guide will show you how to put Django on a server running Ubuntu 22.04 LTS. After you‚Äôve installed it, you‚Äôll make a new project that will be the basis for your website.\nPostgreSQL, MariaDB, MySQL, Oracle, and SQLite are the four main databases that Django supports. Other popular SQL and NoSQL databases are supported at different levels by community libraries. We recommend that you choose the same database for production and development, even though Django‚Äôs Object-Relational Mapper brings out many of the differences in the database (ORM).\nMethods to install Django There are different ways to install Django, depending on your needs and how you want to set up your development environment. There are pros and cons to each of these, and one might work better for you than the others.\nSome of the different ways are:\nGlobal install from packages: The standard apt package manager in Ubuntu can be used to install Django packages from the official repositories. This is simple, but not very adaptable. It‚Äôs also possible that the version in the repositories is behind the official project version. In a virtual environment, you can install with pip: Using tools like venv and virtualenv, you can give each of your projects its own environment. With a virtual environment, you can put Django and other customizations and packages for the project in the project directory. Most of the time, this is the best and most practical way to work with Django. To install the development version with git: If you want to install the latest development version, you can get the code from the Git repository instead of the stable release. This needs to be done in your virtual environment so that you can get the latest fixes and features. But development models aren‚Äôt as stable as versions that have been tested more. (Not covered by this document) Prerequisites: Any browser installed on your machine Super user or any normal user with SUDO privileges. Installation Steps: 1. Global Install from Packages If you want to use the Ubuntu repositories to set up Django, the process is very easy.\nStep 1: First step is to use apt to update your local package index:\napt update Step 2: Now check the version of Python you installed on your Ubuntu server. You can see if it‚Äôs true by:\npython3 ‚ÄìV Step 3: You can now put Django on your computer:\napt install python3-django Step 4: You can make sure the installation is right by:\ndjango-admin version Django version\nThis means that the software will be installed correctly. You might also find that the version of Django you have is not the latest stable version.\n2. Install Django with pip in a Virtual Environment Django can be installed on your system in a virtual environment in a flexible and easy way. We‚Äôll show you how to install Django in a virtual environment, which will be made with the venv module in the standard Python 3 library. With this tool, you can create a virtual Python environment and install Python packages without affecting the rest of the system. So, you can choose Python packages for each project, even if they conflict with the needs of other projects.\nStep 2.1: First refresh the local package index:\napt update Step 2.2: Check the version of Python that is installed:\npython3 -V Step 2.3: Now, install pip using the Ubuntu repositories:\napt install python3-pip python3-venv -y Step 2.4: Now, whenever you start a new project, you can set up a virtual environment for it. Start by making a new project directory and moving it to the new one:\nmkdir ~/testProject cd ~/testProject Step 2.5: Next, use the python command in the project directory to set up a virtual environment for your version of Python. We‚Äôll call my env as our virtual environment, but you should call it descriptive:\npython3.8 - venv my_env Step 2.6: This will install standalone versions of Python and pip in your project directory into a separate directory structure. The file hierarchy for where your packages will be installed will be put in a directory with the name you give it.\nPackages can be install into the isolated environment by:\nsource my_env/bin/activate Prompt after executing above command\nStep 2.7: Your prompt should now say that your virtual environment is up and running. It will look like (my env)username@hostname:/newProject$.\nYou can add Django to your new environment with pip. No matter what version of Python you have, you should only call pip when you are in your virtual environment. Also, because you are installing locally, you won‚Äôt need sudo:\npip install django Keep in mind that this version may be different.\nStep 2.8: To leave your virtual environment, you must send the deactivate command from anywhere on the system:\ndeactivate 3. Creating a Sample Project with newly installed Django With Django, you can start building your project. We will talk about how to make a project and test it on your development server using a virtual environment.\nStep 3.1: After following the above steps, you jest need to follow the below steps to create a sample project with your newly installed Django\nWith startproject, you can use the django-admin command to build your project. You could call this something else, but we‚Äôre going to call our project djangoproject. In the working directory you already have, startproject will create a directory with:\n(myEnv) $ django-admin startproject Step 3.2: Using the manage.py command to migrate the database. Migration takes into account any changes you have made to your database schema in your Django models.\nTo move the database, you¬†can:\n(myEnv) $ python3.8 manage.py migrate Migrate the database of Django project\nLastly, let‚Äôs make an administrative user so you can use the Djano admin interface. Using the createsuperuser command, let‚Äôs do this:\n(myEnv) # python3.8 manage.py createsuperuser You will be asked to give your user a user name, an email address, and a password.\n4. Changing the ALLOWED HOSTS setting in Django You will need to change one of the directives in the Django settings in order to test your app.\nStep 4.1: Type: to open the settings file.\nvi ~/django-test/djangoproject/settings.py Find the ALLOWED HOSTS directive inside. This sets up a list of addresses or domain names that can be used to connect to the Django instance. If a request comes in with a Host header that is not on this list, an exception will be thrown. Django needs you to set this so that a certain type of security hole doesn‚Äôt happen.\nSave the file and close the editor when you are done.\nStep 4.2: Put the IP addresses or domain names that are connected to your Django server in the square brackets. Each item should be written in quotation marks, with a comma between each entry. Add a period to the beginning of the entry to get requests for the whole domain and any subdomains:\nALLOWED_HOSTS = ['your_server_ip_or_domain', 'your_second_ip_or_domain', . . .] Step 4.3: Save the file and close the editor when you are done.\n5. Testing out the Development Server Once you have a user, you can start up the Django development server to see what a new Django project looks like. You should only use this to build something. When you are ready to deploy, make sure to carefully follow Django‚Äôs deployment instructions.\nStep 5.1: Make sure that the right port is open in your firewall before you try the development server. If you set up your server according to the initial setup guide and are using UFW, you can open port 8000 by typing:\nufw allow 8000 Step 5.2: Start the development server:\n(myEnv) # python3.8 manage.py migrate Running Django project after installing Django successfully\nVisit your server‚Äôs IP address followed by¬†:8000¬†in your favourite web browser:\nYou should see something that looks like the below screenshot:\nDjango Homepage\nTo access the admin interface, add¬†/admin/¬†to the end of your URL and it will take you to the admin login page.\nUser login page\nCongratulations! You have learnt successfully How to Install Django on Ubuntu.","installation-steps#Installation Steps:":"","methods-to-install-django#Methods to install Django":"","prerequisites#Prerequisites:":""},"title":"How to Install Django on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-docker-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Docker on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on AlmaLinux 8.\nDocker¬†is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the¬†(OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly.\nDevelopers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.\nStep 1: Add Docker Repo You will need to add the official Docker CE repository to your AlmaLinux 8 system in order for us to be able to install Docker without having to manually download its docker packages.\n# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo Step 2: Update the system In order for the system to recognise the newly added Docker repository and the packages that are available in it, you will need to execute the system update, which will lead AlmaLinux to rebuild the system‚Äôs repo cache.\n# dnf update -y Using the following command, you may verify the newly added repository in addition to the ones on your system:\n# dnf repolist -v Step 3: Install Docker Now that we have the Docker repository, it is necessary to install the Docker-CE together with its command-line tool and containerd.io so that we can effectively control the container lifecycle of the system on which it is hosted. To do this, we will perform a simple command using the DNF package manager.\n# dnf install docker-ce docker-ce-cli containerd.io Step 4: Start Docker Services After the installation has been finished, you should start the Docker service on your AlmaLinux computer and also make it such that it starts automatically whenever the system does.\n# systemctl enable docker # systemctl start docker Check the Service Status to see if it‚Äôs working accurate.\n# systemctl status docker To get information and details about the docker installer, such as the version, the number of containers that have been loaded, the host kernel version, the architecture, the CPU, the name of the operating system, etc.\n# docker info "},"title":"How to install Docker on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-docker-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Docker on Centos 7.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on Centos 7.\nDocker is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly. Developers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.","step-1-update-the-system#Step 1: Update the system":"The first step you need to do is get the repositories up to date. To accomplish this, use the following command:\n# yum update -y ","step-2-install-the-dependencies#Step 2: Install the Dependencies":"For the installation to go off without a hitch, there are some prerequisites or dependencies that must be met. Therefore, in order to install them, execute the following command:\n# yum install -y yum-utils device-mapper-persistent-data lvm2 ","step-3-add-the-docker-repository-to-centos#Step 3: Add the Docker Repository to CentOS":"You will need to integrate the Docker CE stable repository into your operating system in order to install the edge or test versions of Docker. To accomplish this, use the following command:\n# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo ","step-4-install-docker#Step 4: Install Docker":"After you have completed all of the necessary preparations, you may at long last go on to installing Docker on CentOS 7 by executing the following command.\n# yum install docker -y Step: 5 Manage Docker Service Docker has been installed on CentOS, but the service is not yet operating even though you have done so.\nEnabling a service to run automatically upon system boot will allow it to be started. Carry out each of the commands that follow in the sequence that they are presented.\n# systemctl start docker # systemctl enable docker # systemctl restart docker # systemctl status docker Execute the following command to check that Docker has successfully installed:\n# docker version "},"title":"How To Install Docker on Centos 7"},"/utho-docs/docs/linux/how-to-install-docker-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Docker on Debian.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on Debian.\nDocker is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly. Developers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.","step-1-update-the-system#Step 1: Update the system":"The first step you need to do is get the repositories up to date. To accomplish this, use the following command:\n# apt update -y ","step-2-install-dependencies#Step 2: Install dependencies":"For the installation to go off without a hitch, there are some prerequisites or dependencies that must be met. Therefore, in order to install them, execute the following command:\n# apt¬†install¬†apt-transport-https ca-certificates curl gnupg2 software-properties-common -y ","step-3-install-docker#Step 3: Install Docker":"The Docker packages are now served over the internet via the HTTPS protocol, which was previously used. In order to make use of the Docker package repository, you will need to add the GPG key that is associated with it.\n# curl¬†-fsSL¬†https://download.docker.com/linux/debian/gpg |¬†sudo¬†apt-key add¬†- Now, on your Debian machine, you need to execute the following command in order to add the Docker package repository.\n# echo¬†\"deb [arch=amd64] https://download.docker.com/linux/debian¬†$(lsb_release -cs) stable\"¬†|¬†sudo¬†tee¬†/etc/apt/sources.list.d/docker-ce.list Install Docker CE at this time by running the following command:\n# apt¬†install¬†docker-ce docker-ce-cli containerd.io -y This will install Docker as well as any extra packages, libraries, and dependencies that are necessary for Docker and any associated packages to function properly.","step-4-manage-docker-service#Step 4: Manage Docker Service":" # systemctl start docker # systemctl enable docker # systemctl restart docker # systemctl status docker ","step-5-confirm-that-docker-is-installed#Step 5: Confirm that Docker is installed":"Execute the following command to check that Docker has successfully installed:\n# docker version ","step-6-test-docker#Step 6: Test Docker":"In order to evaluate Docker, we will obtain a ‚Äòhello-world‚Äô image from the Docker Hub repository. A container will be formed from the image that shows a ‚ÄúHello world‚Äù message on the terminal along with the steps of what just transpired after the command has been done. This will take place after the image has been loaded.\n# docker run hello-world Execute the following command to verify the photos that are currently stored on the system:\n# docker images "},"title":"How To Install Docker on Debian"},"/utho-docs/docs/linux/how-to-install-docker-on-fedora-2/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Docker on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on Fedora.\nDocker¬†is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the¬†(OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly.\nDevelopers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.\nStep 1: Add Docker Repo You will need to add the official Docker CE repository to your AlmaLinux 8 system in order for us to be able to install Docker without having to manually download its docker packages.\n# dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo Step 2: Update the system In order for the system to recognise the newly added Docker repository and the packages that are available in it, you will need to execute the system update, which will lead Fedora to rebuild the system‚Äôs repo cache.\n# dnf update -y Using the following command, you may verify the newly added repository in addition to the ones on your system:\n# dnf repolist -v Step 3: Install Docker Now that we have the Docker repository, it is necessary to install the Docker-CE together with its command-line tool and containerd.io so that we can effectively control the container lifecycle of the system on which it is hosted. To do this, we will perform a simple command using the DNF package manager.\n# dnf install docker-ce docker-ce-cli containerd.io Step 4: Start Docker Services After the installation has been finished, you should start the Docker service on your Fedora computer and also make it such that it starts automatically whenever the system does.\n# systemctl enable docker # systemctl start docker Check the Service Status to see if it‚Äôs working accurate.\n# systemctl status docker To get information and details about the docker installer, such as the version, the number of containers that have been loaded, the host kernel version, the architecture, the CPU, the name of the operating system, etc.\n# docker info "},"title":"How to install Docker on Fedora"},"/utho-docs/docs/linux/how-to-install-docker-on-fedora/":{"data":{"":"","conclusion#conclusion":"Hopefully, you have learned how to install Docker on Fedora.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on Fedora.\nDocker is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly. Developers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.","step-1-update-the-system#Step 1: Update the system":"The first step you need to do is get the repositories up to date. To accomplish this, use the following command:\n# dnf update -y ","step-2-enable-docker-ce-official-repository#Step 2: Enable Docker CE Official Repository":" # dnf -y install dnf-plugins-core # dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo ","step-3-install-docker-and-its-dependencies#Step 3: Install Docker and its Dependencies":" # dnf install -y docker-ce docker-ce-cli containerd.io ","step-4-manage-docker-service#Step 4: Manage Docker Service":" # systemctl start docker # systemctl enable docker # systemctl restart docker # systemctl status docker ","step-5-confirm-that-docker-is-installed#Step 5: Confirm that Docker is installed":"Execute the following command to check that Docker has successfully installed:\n# docker version ","step-6-test-docker#Step 6: Test Docker":"In order to evaluate Docker, we will obtain a ‚Äòhello-world‚Äô image from the Docker Hub repository. A container will be formed from the image that shows a ‚ÄúHello world‚Äù message on the terminal along with the steps of what just transpired after the command has been done. This will take place after the image has been loaded.\n# docker run hello-world Execute the following command to verify the photos that are currently stored on the system:\n# docker images "},"title":"How To Install Docker on Fedora"},"/utho-docs/docs/linux/how-to-install-docker-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Docker on Ubuntu 20.04.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on Ubuntu 20.04.\nDocker is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly. Developers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.","step-1-update-the-system#Step 1: Update the system":"The first step you need to do is get the repositories up to date. To accomplish this, use the following command:\n# apt update ","step-2-install-dependencies#Step 2: Install dependencies":"For the installation to go off without a hitch, there are some prerequisites or dependencies that must be met. Therefore, in order to install them, execute the following command:\n# apt install apt-transport-https curl gnupg-agent ca-certificates software-properties-common -y ","step-3-install-docker-on-ubuntu#Step 3: Install Docker on Ubuntu":"The following step, which is installing Docker, follows the installation of the prerequisites. We are going to install the open-source Docker Community Edition, also known as Docker CE, which is completely free to use and download.\n# curl -fsSL [https://download.docker.com/linux/ubuntu/gpg](https://download.docker.com/linux/ubuntu/gpg) | sudo apt-key add - # add-apt-repository \"deb [arch=amd64] [https://download.docker.com/linux/ubuntu](https://download.docker.com/linux/ubuntu) focal stable\" After you have added the GPG key and the repository, you can install Docker and the associated packages by executing the following command.\n# apt install docker-ce docker-ce-cli containerd.io -y This will install Docker as well as any extra packages, libraries, and dependencies that are necessary for Docker and any associated packages to function properly.","step-4-manage-docker-service#Step 4: Manage Docker Service":" # systemctl start docker # systemctl enable docker # systemctl restart docker # systemctl status docker ","step-5-confirm-that-docker-is-installed#Step 5: Confirm that Docker is installed":"Execute the following command to check that Docker has successfully installed:\n# docker version ","step-6-test-docker#Step 6: Test Docker":"In order to evaluate Docker, we will obtain a ‚Äòhello-world‚Äô image from the Docker Hub repository. A container will be formed from the image that shows a ‚ÄúHello world‚Äù message on the terminal along with the steps of what just transpired after the command has been done. This will take place after the image has been loaded.\n# docker run hello-world Execute the following command to verify the photos that are currently stored on the system:\n# docker images "},"title":"How To Install Docker on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-docker-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Docker on Ubuntu 22.04.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Docker on Ubuntu 22.04.\nDocker is a platform that is open source and allows developers to build, deploy, run, update, and manage containers. Containers are standardised, executable components that combine application source code with the operating system (OS) libraries and dependencies necessary to run that code in any environment. Docker enables developers to do all of these things.\nContainers make it easier to design and deliver applications that run on distributed systems. As more and more businesses move their operations to cloud-native development and hybrid multicloud environments, their adoption rates have increased significantly. Developers have the ability to create containers even without the use of Docker by cooperating directly with features that are pre-installed in Linux and other operating systems. Docker, on the other hand, makes containerization more quickly, easily, and securely.","step-1-update-the-system#Step 1: Update the system":"The first step you need to do is get the repositories up to date. To accomplish this, use the following command:\n# apt update ","step-2-install-dependencies#Step 2: Install dependencies":"For the installation to go off without a hitch, there are some prerequisites or dependencies that must be met. Therefore, in order to install them, execute the following command:\n# apt install apt-transport-https curl gnupg-agent ca-certificates software-properties-common -y ","step-3-install-docker-on-ubuntu#Step 3: Install Docker on Ubuntu":"The following step, which is installing Docker, follows the installation of the prerequisites. We are going to install the open-source Docker Community Edition, also known as Docker CE, which is completely free to use and download.\n# curl -fsSL [https://download.docker.com/linux/ubuntu/gpg](https://download.docker.com/linux/ubuntu/gpg) | sudo apt-key add - # add-apt-repository \"deb [arch=amd64] [https://download.docker.com/linux/ubuntu](https://download.docker.com/linux/ubuntu) focal stable\" After you have added the GPG key and the repository, you can install Docker and the associated packages by executing the following command.\n# apt install docker-ce docker-ce-cli containerd.io -y This will install Docker as well as any extra packages, libraries, and dependencies that are necessary for Docker and any associated packages to function properly.","step-4-manage-docker-service#Step 4: Manage Docker Service":" # systemctl start docker # systemctl enable docker # systemctl restart docker # systemctl status docker ","step-5-confirm-that-docker-is-installed#Step 5: Confirm that Docker is installed":"Execute the following command to check that Docker has successfully installed:\n# docker version ","step-6-test-docker#Step 6: Test Docker":"In order to evaluate Docker, we will obtain a ‚Äòhello-world‚Äô image from the Docker Hub repository. A container will be formed from the image that shows a ‚ÄúHello world‚Äù message on the terminal along with the steps of what just transpired after the command has been done. This will take place after the image has been loaded.\n# docker run hello-world Execute the following command to verify the photos that are currently stored on the system:\n# docker images "},"title":"How To Install Docker on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-drupal-on-centos-server/":{"data":{"":"","1--configure-mysql-mariadb-database-for-drupal#1- Configure MySQL/ Mariadb database for Drupal":"","2--install-and-configure-drupal#2- Install and configure Drupal":"","mv-drupal-946-varwwwhtmldrupal#mv drupal-9.4.6 /var/www/html/drupal":"\nThis article will show you how to Install Drupal on CentOS.\nDrupal is an open-source content management system that lets us build and edit websites without having to know how to code. The code for Drupal is written in PHP and is licenced by GNU GPL (General Public License ).\nPrerequesties yum server configured on your centos server Apache 2.x¬†(Recommended) PHP 5.5.9¬†or higher (5.5 recommended). To install the latest version of php, follow this article. MySQL 5.5.3 or MariaDB 5.5.20 A super user or any other normal user with sudo privileges. To meet above requirements, you just need to install LAMP server on your machine. Follow this article to install LAMP on Centos\n1- Configure MySQL/ Mariadb database for Drupal We have to create a database and a user for the Drupal site to manage.\n# mysql -u root -p \u003e CREATE DATABASE microhost_db;\n\u003e CREATE USER microhost_user@localhost IDENTIFIED BY ‚Äò-‚Äî‚Äî‚Äî-‚Äô;\n\u003e GRANT ALL ON microhost_db.* TO microhost_user@localhost;\n\u003e FLUSH PRIVILEGES;\n\u003e EXIT;\n2- Install and configure Drupal We‚Äôll begin by using the wget command to get the most recent Drupal version (i.e. 8.2.6). If you don‚Äôt already have the wget and tar packages installed, use the following command to get them:\n# yum install wget tar -y wget https://ftp.drupal.org/files/projects/drupal-8.0.2.tar.gz Use the below command to unzip the downloaded drupal file. Then move the Drupal folder into the Apache Document Root, which is, /var/www/html\n# tar -zxcf drupal-9.4.6.tar.gz mv drupal-9.4.6 /var/www/html/drupal Then, in the folder (/var/www/html/drupal/sites/default), create the settings file settings.php (based on the example settings file default.settings.php), and then establish the proper permissions on the Drupal site directory, including sub-directories and files as follows:\ncd /var/www/html/drupal/sites/default/ cp default.settings.php settings.php chown -R apache:apache /var/www/html/drupal/ Finally, at this point, go to the URL: http://server IP/drupal/ to launch the online installer, choose your chosen installation language, and click Save to proceed.\nhttp://server-ip/drupal Note: if your server have php version lower than 5.6, you will encounter the below error when you hit your server-ip on your browser\nError of having lower version than php 5.6\nIf you have higher than php5.6 you must see the below page. Here, just select save and continue after selecting favourite language\nselect language\nOn the next page, you will be asked to select the installation profile. We have selected the standard option for the sake of this tutorial. After making the choice, just again click on save and continue\nsecond page of the installation\nOn the third screen, you will be headed to fill the database information you created for the drupal to save the data. After filling up the details, just continue to your path.\nThird page to enter the database details\nAfter clicking on the save and continue option on the last page, you have just started the installation of the drupal on your server\nInstallation process\nAfter installation, you will be asked to enter the details of your site. Just fill up according to your site.\nEnter the domain details on the fifth page -1\nFinally!!! You have installed Drupal on CentOS server and configured the drupal on your server successfully. Enjoy\nWelcome page of drupal","prerequesties#Prerequesties":""},"title":"How to install Drupal on CentOS server"},"/utho-docs/docs/linux/how-to-install-drupal-on-debian/":{"data":{"":"\nIn this post, we‚Äôll talk more about how to install Drupal on Debian.\nDrupal is a free and open-source content management system that lets us make and change websites without having to learn how to code. The source code for Drupal is written in PHP and is shared under the GNU General Public License (General Public License ).\nPrerequisites A apt repository configure debian server A super user( root) or any normal user with SUDO privileges Drupal runs on the front end and is powered by a database server on the back end, just like any other CMS. So, you need to install a¬†LAMP stack¬†before you install anything else. LAMP is made up of the Apache web server, the MariaDB or MySQL database, and PHP, which is a scripting language that runs on the server","1--create-a-drupal-database#1- Create a Drupal Database":"First, we‚Äôll make a database that Drupal will use to store information during and after the installation. Sign in to the MariaDB database server first.\nOnce you‚Äôre in the MariaDB shell, we‚Äôll make a database called drupal db.\n\u003e create DATABASE Microhost_DB; Next, we‚Äôll make a database user with a strong password and give that user full access to the Drupal database, as shown.\nCREATE USER ‚ÄòMicrohost_user‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚ÄúPassword‚Äù;\n\u003e¬†GRANT ALL ON Microhost_DB.* TO ‚ÄòMicrohost_user‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äúpassword‚Äù;\n\u003e FLUSH PRIVILEGES;\n\u003e¬†EXIT;\nNow that we have the Apache web server, the Drupal database, and all the PHP extensions set up, we will move on and download the Drupal installation file.\nIf you face any PHP error, try installing the below php dependencies too\napt install php libapache2-mod-php php-cli php-fpm php-json php-common php-mysql php-zip php-gd php-intl php-mbstring php-curl php-xml php-pear php-tidy php-soap php-bcmath php-xmlrpc ","2--download-and-install-drupal-in-debian#2- Download and Install Drupal in Debian":"We are going to use the wget command to download the compressed file from¬†Drupal‚Äôs official¬†site as shown.\nwget https://www.drupal.org/download-latest/tar.gz -O drupal.tar.gz Once the download is done, unzip it in the current directory, move the unzipped Drupal folder to the /var/www/html path, and list the directory‚Äôs contents as shown:\ntar -xvf drupal.tar.gz# mv drupal-9.0.7 /var/www/html/drupal # ls -l /var/www/html/drupal Next, change the directory permissions so that anyone can use Drupal.\nchown -R www-data:www-data /var/www/html/drupal/ # chmod -R 755 /var/www/html/drupal/ ","3--create-an-apache-virtual-host-for-drupal#3- Create an Apache Virtual Host for Drupal":"We need to make an Apache virtual host file to serve Drupal‚Äôs site on the front end. Make the file as shown using the text editor you like best. The vim editor is being used here.\nvim /etc/apache2/sites-available/drupal.conf Now fill the opened file with below content\n\u003cVirtualHost *:80\u003e ServerAdmin Microhost.document.com DocumentRoot /var/www/html/drupal/ ServerName document.com ServerAlias www.document.com ErrorLog ${APACHE_LOG_DIR}/drupal_error.log CustomLog ${APACHE_LOG_DIR}/drupal_access.log combined \u003cDirectory /var/www/html/drupal/\u003e; Options FollowSymlinks AllowOverride All Require all granted \u003c/Directory\u003e \u003cDirectory /var/www/html/\u003e RewriteEngine on RewriteBase / RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteRule ^(.*)$ index.php?q=$1 [L,QSA] \u003c/Directory\u003e \u003c/VirtualHost\u003e Save the changes and close the file when you‚Äôre done.\nUp until this point, a browser could only get to the Apache Welcome page. We need to change this so that the Drupal site is served by Apache. We need to turn on Drupal‚Äôs virtual host in order to do this. So, follow these instructions:\na2ensite drupal.conf # a2enmod rewrite And now just restart the apache service by below command\nsystemctl restart apache2 Now you just to open your favourite browser and hit for your server_ip\nhttp://server-ip_or_domain-name The below screen will be the first page you will see after hitting your server ip on the browser.\nHere, just click on save and continue if you do not want to change the default language.\nHomepage of your drupal site\nOn the next page, you will be asked to select the installation profile. We have selected the standard option for the sake of this tutorial. After making the choice, just again click on save and continue to install Drupal on Debian.\nsecond page of the installation\nOn the third screen, you will be headed to fill the database information you created for the drupal to save the data. After filling up the details, just continue to your path.\nThird page to enter the database details\nAfter clicking on the save and continue option on the last page, you have just started the installation of the drupal on your server\nInstallation process\nAfter installation, you will be asked to enter the details of your site. Just fill up according to your site.\nEnter the domain details on the fifth page\nFinally!!! You have Installed Drupal on Debian and configured the drupal on your server successfully. Enjoy\nWelcome page of drupal"},"title":"How to Install Drupal on Debian"},"/utho-docs/docs/linux/how-to-install-drupal-on-fedora/":{"data":{"":"\nIn this post, we‚Äôll talk more about how to install Drupal on Fedora.\nDrupal is a free and open-source content management system that lets us make and change websites without having to learn how to code. The source code for Drupal is written in PHP and is shared under the GNU General Public License (General Public License ).","1--configure-mysql-mariadb-database-for-drupal#1- Configure MySQL/ Mariadb database for Drupal":"We need to set up both a database and a user for the Drupal site we will be running.\nmysql -u root -p \u003e CREATE DATABASE¬†microhost_db;\n\u003e CREATE USER¬†microhost_user@localhost IDENTIFIED BY ‚Äò-‚Äî‚Äî‚Äî-‚Äô;\n\u003e GRANT ALL ON¬†microhost_db.* TO¬†microhost_user@localhost;\n\u003e FLUSH PRIVILEGES;\n\u003e EXIT;","2--install-and-configure-drupal#2- Install and configure Drupal":"First, we‚Äôll use the wget function to get the most recent version of Drupal (i.e. 8.2.6). If you don‚Äôt already have the wget and tar packages on your computer, use the following command to get them:\ndnf install wget tar -y wget https://ftp.drupal.org/files/projects/drupal-8.0.2.tar.gz Use the command given below to get the file you downloaded from Drupal out of its ZIP format. After that, move the folder with Drupal into the /var/www/html directory, which is the Apache Document Root.\ntar -zxcf drupal-9.4.6.tar.gz # mv drupal-9.4.6 /var/www/html/drupal Then, in the directory (/var/www/html/drupal/sites/default), create the settings file settings.php based on the example settings file default.settings.php. After that, set the correct permissions on the Drupal site directory, including its subdirectories and files, as shown below:\ncd /var/www/html/drupal/sites/default/ cp default.settings.php settings.php chown -R apache:apache /var/www/html/drupal/ Finally, at this point, go to the URL: http://server IP/drupal/ to launch the online installer, choose your chosen installation language, and click Save to proceed.\nhttp://server-ip/drupal Note: if your server have php version lower than 5.6, you will encounter the below error when you hit your server-ip on your browser\nError of having lower version than php 5.6\nIf you have higher than php5.6 you must see the below page. Here, just select save and continue after selecting favourite language\nselect language\nOn the next page, you will be asked to select the installation profile. We have selected the standard option for the sake of this tutorial. After making the choice, just again click on save and continue\nsecond page of the installation\nOn the third screen, you will be headed to fill the database information you created for the drupal to save the data. After filling up the details, just continue to your path.\nThird page to enter the database details\nAfter clicking on the save and continue option on the last page, you have just started the installation of the drupal on your server\nInstallation process\nAfter installation, you will be asked to enter the details of your site. Just fill up according to your site.\nEnter the domain details on the fifth page -1\nFinally!!! You have installed Drupal on Fedora server and configured the drupal on your server successfully. Enjoy\nWelcome page of drupal","prerequesties#Prerequesties":" dnf server configured on your Fedora server Apache 2.x¬†(Recommended) PHP 5.5.9¬†or higher (5.5 recommended). To install the latest version of php,¬†follow this article. MySQL 5.5.3 or MariaDB 5.5.20 A super user or any other normal user with sudo privileges. To meet above requirements, you just need to install LAMP server on your machine. Follow this article to¬†install LAMP on Fedora"},"title":"How to install Drupal on Fedora"},"/utho-docs/docs/linux/how-to-install-elasticsearch-on-centos-7/":{"data":{"":"\nElasticsearch¬†is flexible and powerful open-source, distributed real-time search and analytics engine. Using a simple set of APIs provides the ability for full-text search.\nElasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface.\nStep 1.¬†Login to your server via SSH Putty\nStep 2.¬†Installing/Checking java\nJava is the primary requirement for installing Elasticsearch on any system. You can check the installed version of Java by executing the following command:\n# java -version If it returns an error, install Java on your system by using the following commands one after another:\n#¬†sudo dnf search openjdk #¬†sudo dnf install java-11-openjdk Now check the java version on your system.\n#¬†java -version Step 3.¬†First of all, install GPG key for the elasticsearch rpm packages.\n#¬†sudo rpm --import¬†[https://artifacts.elastic.co/GPG-KEY-elasticsearch](https://artifacts.elastic.co/GPG-KEY-elasticsearch) Step 4.¬†Then create yum repository file for the elasticsearch. Edit /etc/yum.repos.d/elasticsearch.repo file:\n#¬†sudo vi /etc/yum.repos.d/elasticsearch.repo Add below content to the file:\n[Elasticsearch-7] name=Elasticsearch repository for 7.xpackages baseurl=[https://artifacts.elastic.co/packages/7.x/yum](https://artifacts.elastic.co/packages/7.x/yum) gpgcheck= gpgkey=[https://artifacts.elastic.co/GPG-KEY-elasticsearch](https://artifacts.elastic.co/GPG-KEY-elasticsearch) enabled= autorefresh= type=rpm-md Save and exit. (:wq)\nStep 5.¬†Install ElasticSearch by running the command:\n#¬†sudo yum install elasticsearch Step 6.¬†After successful installation edit Elasticsearch configuration file ‚Äú/etc/elasticsearch/elasticsearch.yml‚Äù and set the¬†network.host to localhost.\n#¬†vim /etc/elasticsearch/elasticsearch.yml Now¬†set the¬†network.host to localhost\nStep 7.¬†Then enable the elasticsearch service and start it.\n#¬†sudo systemctl enable elasticsearch¬†#¬†sudo systemctl start elasticsearch¬†Step 8.¬†The ElasticSearch has been successfully installed and running on your CentOS¬†system.\nRun the following command to verify service:\n#¬†curl -X GET \"localhost:9200/?pretty\" You will see results like below:\nElasticSearch successfully installed.\nThank You"},"title":"How to Install Elasticsearch on CentOS 7"},"/utho-docs/docs/linux/how-to-install-elinks-on-almalinux/":{"data":{"":"","almalinux-elinks-web-browser-installation#AlmaLinux Elinks Web browser installation":"Step 1: We will use the following command to update our operating system.\n# dnf update -y Step 2: The Elinks package can be found in the AlmaLinux repository. It is not only the easiest way but also the best way to load a package from a distribution‚Äôs main repository. To start the installation, use the command below.\n# dnf install elinks -y Step 3: Start the Internet browser. Now that we‚Äôve installed the browser, we‚Äôll run this command and see what happens.\n# elinks The following screen will show up on your terminal when the link is made. You will get a browser instead of an actual computer. It will be fully functional and show all of the text on a page.\nStep 4: Press ‚ÄúOK‚Äù\nNow we have successfully installed the Elinks web browser on our AlmaLinux operating system.\nStep 5: Type the URL and then press ‚ÄúOK‚Äù.\nAfter pressing OK, below content will show in your terminal.","conclusion#Conclusion":"Hopefully, now you have learned how to install Elinks on AlmaLinux.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Elinks on AlmaLinux.\nElinks¬†is a free text-based web browser that is compatible with operating systems that are similar to Unix. In the latter half of 2001, Petr Baudi created a fork of the Links Web browser that he referred to as the E branch for its focus on experimentation. Since then, the E has come to represent either Enhanced or Extended in many contexts.\nElinks is a text-mode Web browser that can show colours, render tables, download in the background, be set up with a menu, search with tabs, and use thin code. Frames are okay. Various file types can be linked to external readers. Mail-to and telnet are supported by viewers on the outside."},"title":"How to install Elinks on AlmaLinux"},"/utho-docs/docs/linux/how-to-install-elinks-on-centos/":{"data":{"":"","centos-elinks-web-browser-installation#CentOS Elinks Web browser installation":"Step 1: We will use the following command to update our operating system.\n# yum update -y Step 2: The Elinks package can be found in the CentOS repository. It is not only the easiest way but also the best way to load a package from a distribution‚Äôs main repository. To start the installation, use the command below.\n# yum install elinks -y Step 3: Start the Internet browser. Now that we‚Äôve installed the browser, we‚Äôll run this command and see what happens.\n# elinks The following screen will show up on your terminal when the link is made. You will get a browser instead of an actual computer. It will be fully functional and show all of the text on a page.\nStep 4: Press ‚ÄúOK‚Äù\nNow we have successfully installed the Elinks web browser on our CentOS operating system.\nStep 5: Type the URL and then press ‚ÄúOK‚Äù.\nAfter pressing OK, below content will show in your terminal.","conclusion#Conclusion":"Hopefully, now you have learned how to install Elinks on CentOS.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Elinks on CentOS.\nElinks¬†is a free text-based web browser that is compatible with operating systems that are similar to Unix. In the latter half of 2001, Petr Baudi created a fork of the Links Web browser that he referred to as the E branch for its focus on experimentation. Since then, the E has come to represent either Enhanced or Extended in many contexts.\nElinks is a text-mode Web browser that can show colours, render tables, download in the background, be set up with a menu, search with tabs, and use thin code. Frames are okay. Various file types can be linked to external readers. Mail-to and telnet are supported by viewers on the outside."},"title":"How to install Elinks on CentOS"},"/utho-docs/docs/linux/how-to-install-elinks-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Elinks on Debian.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","debian-elinks-web-browser-installation#Debian Elinks Web browser installation":"Step 1: We will use the following command to update our operating system.\n# apt update Step 2: The Elinks package can be found in the Debian repository. It is not only the easiest way but also the best way to load a package from a distribution‚Äôs main repository. To start the installation, use the command below.\n# apt install elinks Step 3: Start the Internet browser. Now that we‚Äôve installed the browser, we‚Äôll run this command and see what happens.\n# elinks The following screen will show up on your terminal when the link is made. You will get a browser instead of an actual computer. It will be fully functional and show all of the text on a page.\nStep 4: Press ‚ÄúOK‚Äù\nNow we have successfully installed the Elinks web browser on our Debian operating system.\nStep 5: Type the URL and then press ‚ÄúOK‚Äù.\nAfter pressing OK, below content will show in your terminal.","introduction#Introduction":"In this article, you will learn how to install Elinks on Debian.\nElinks¬†is a free text-based web browser that is compatible with operating systems that are similar to Unix. In the latter half of 2001, Petr Baudi created a fork of the Links Web browser that he referred to as the E branch for its focus on experimentation. Since then, the E has come to represent either Enhanced or Extended in many contexts.\nElinks is a text-mode Web browser that can show colours, render tables, download in the background, be set up with a menu, search with tabs, and use thin code. Frames are okay. Various file types can be linked to external readers. Mail-to and telnet are supported by viewers on the outside."},"title":"How to install Elinks on Debian"},"/utho-docs/docs/linux/how-to-install-elinks-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Elinks on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","fedora-elinks-web-browser-installation#Fedora Elinks Web browser installation":"Step 1: We will use the following command to update our operating system.\n# dnf update -y Step 2: The Elinks package can be found in the Fedora repository. It is not only the easiest way but also the best way to load a package from a distribution‚Äôs main repository. To start the installation, use the command below.\n# dnf install elinks -y Step 3: Start the Internet browser. Now that we‚Äôve installed the browser, we‚Äôll run this command and see what happens.\n# elinks The following screen will show up on your terminal when the link is made. You will get a browser instead of an actual computer. It will be fully functional and show all of the text on a page.\nStep 4: Press ‚ÄúOK‚Äù\nNow we have successfully installed the Elinks web browser on our Fedora operating system.\nStep 5: Type the URL and then press ‚ÄúOK‚Äù.\nAfter pressing OK, below content will show in your terminal.","introduction#Introduction":"In this article, you will learn how to install Elinks on Fedora.\nElinks¬†is a free text-based web browser that is compatible with operating systems that are similar to Unix. In the latter half of 2001, Petr Baudi created a fork of the Links Web browser that he referred to as the E branch for its focus on experimentation. Since then, the E has come to represent either Enhanced or Extended in many contexts.\nElinks is a text-mode Web browser that can show colours, render tables, download in the background, be set up with a menu, search with tabs, and use thin code. Frames are okay. Various file types can be linked to external readers. Mail-to and telnet are supported by viewers on the outside."},"title":"How to install Elinks on Fedora"},"/utho-docs/docs/linux/how-to-install-elinks-on-ubuntu/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Elinks on Ubuntu.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Elinks on Ubuntu.\nElinks is a free text-based web browser that is compatible with operating systems that are similar to Unix. In the latter half of 2001, Petr Baudi created a fork of the Links Web browser that he referred to as the E branch for its focus on experimentation. Since then, the E has come to represent either Enhanced or Extended in many contexts.\nElinks is a text-mode Web browser that can show colours, render tables, download in the background, be set up with a menu, search with tabs, and use thin code. Frames are okay. Various file types can be linked to external readers. Mail-to and telnet are supported by viewers on the outside.","ubuntu-elinks-web-browser-installation#Ubuntu Elinks Web browser installation":"Step 1: We will use the following command to update our operating system.\n# apt update Step 2: The Elinks package can be found in the Ubuntu repository. It is not only the easiest way but also the best way to load a package from a distribution‚Äôs main repository. To start the installation, use the command below.\n# apt install elinks Step 3: Start the Internet browser. Now that we‚Äôve installed the browser, we‚Äôll run this command and see what happens.\n# elinks The following screen will show up on your terminal when the link is made. You will get a browser instead of an actual computer. It will be fully functional and show all of the text on a page.\nStep 4: Press ‚ÄúOK‚Äù\nNow we have successfully installed the Elinks web browser on our Ubuntu operating system.\nStep 5: Type the URL and then press ‚ÄúOK‚Äù.\nAfter pressing OK, below content will show in your terminal."},"title":"How to install Elinks on Ubuntu"},"/utho-docs/docs/linux/how-to-install-flatpak-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Flatpak on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Flatpak on Debian 12.\nFlatpak¬†is a package management utility that lets you distribute, install and manage software without needing to worry about dependencies, runtime, or the Linux distribution. Since you can install software without any issues irrespective on the Linux distribution (be it a Debian-based distro or an Arch-based distro),¬†Flatpak¬†is called universal package.\nFlatpak is developed to be as technique as is humanly possible when it comes to the construction of desktop applications, yet it may be accessed by all sorts of desktop applications. There are no prerequisites or restrictions imposed on the programming languages, develop tools, toolkits, or frameworks that can be implemented.\nEven though Flatpak can only be run on Linux, it can still be usable by apps that are designed for other operating systems in addition to those that are designed specifically for Linux. There are two types of software: open source and licensed (although some distribution services, like Flathub, can have restrictions in this respect).When compared to other universal methods of delivering software on Linux, Flatpak has a number of significant benefits.\n1. Install Flatpak Debian Buster and newer editions come with a flatpak package already installed. Execute the following commands as root to install it:\n# apt install flatpak -y In order to identify the version of flatpak, use the following command.\n# flatpak --version 2. Install the Software Flatpak plugin Installing the Flatpak plugin for GNOME Software is a smart move to make if you are using GNOME as your desktop environment. To achieve this, hurry:\n# apt install gnome-software-plugin-flatpak -y 3. Add the Flathub repository The best location to obtain Flatpak applications is Flathub. To enable it, run:\n# flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo "},"title":"How to install Flatpak on Debian 12"},"/utho-docs/docs/linux/how-to-install-flatpak-on-debian/":{"data":{"":"","1-install-flatpak#1. Install Flatpak":"","2-install-the-software-flatpak-plugin#2. Install the Software Flatpak plugin":"","3-add-the-flathub-repository#3. Add the Flathub repository":"\nIntroduction In this article, you will learn how to install Flatpak on Debian.\nFlatpak¬†is a package management utility that lets you distribute, install and manage software without needing to worry about dependencies, runtime, or the Linux distribution. Since you can install software without any issues irrespective on the Linux distribution (be it a Debian-based distro or an Arch-based distro), Flatpak is called universal package.\nFlatpak is developed to be as technique as is humanly possible when it comes to the construction of desktop applications, yet it may be accessed by all sorts of desktop applications. There are no prerequisites or restrictions imposed on the programming languages, develop tools, toolkits, or frameworks that can be implemented.\nEven though Flatpak can only be run on Linux, it can still be usable by apps that are designed for other operating systems in addition to those that are designed specifically for Linux. There are two types of software: open source and licensed (although some distribution services, like Flathub, can have restrictions in this respect).When compared to other universal methods of delivering software on Linux, Flatpak has a number of significant benefits.\n1. Install Flatpak Debian Buster and newer editions come with a flatpak package already installed. Execute the following commands as root to install it:\n# apt install flatpak -y In order to identify the version of flatpak, use the following command.\n# flatpak --version 2. Install the Software Flatpak plugin Installing the Flatpak plugin for GNOME Software is a smart move to make if you are using GNOME as your desktop environment. To achieve this, hurry:\n# apt install gnome-software-plugin-flatpak -y 3. Add the Flathub repository The best location to obtain Flatpak applications is Flathub. To enable it, run:\n# flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo ","conclusion#Conclusion":"Hopefully, you have learned how to install Flatpak on Debian.\nThank You üôÇ","introduction#Introduction":""},"title":"How to install Flatpak on Debian"},"/utho-docs/docs/linux/how-to-install-flatpak-on-fedora-35/":{"data":{"":"","1-install-updates#1. Install updates":"","2-install-flatpak-dependencies#2. Install Flatpak dependencies":"","3-install-flatpak-on-fedora-35#3. Install Flatpak on Fedora 35":"","4-add-user-to-mock-group#4. Add user to mock group":"","5-install-flatpak-runtime-environment#5. Install Flatpak runtime environment":"","6-enable-flathub#6. Enable Flathub":"\nIntroduction In this tutorial, we are going to learn how to install Flatpak on Fedora 35.\nFlatpak is a Linux tool used for managing software packages and deploying applications. With Flatpak, users can execute applications independently from the rest of the system in a sandbox.\nFlatpak tries to be as environment- and framework-neutral as possible, making it compatible with a wide variety of desktop environments.\nAdvantages of using Flatpaks on Fedora 1. Updating applications is simple and does not require a system restart.\n2. Fedora silverblue has a simple application installation process.\n3. All officially approved Fedora releases are suitable with Flatpaks.\n4. In other words, users of any distribution can use Flatpaks.\nInstall Flatpak on Fedora 35\n1. Install updates The first thing for any system is to update its repositories in order to make them up to date.\n# dnf update -y 2. Install Flatpak dependencies # dnf install fedmod 3. Install Flatpak on Fedora 35 # dnf install flatpak-module-tools Check the version of installed Flatpak\n# flatpak --version 4. Add user to mock group Mock is a tool for creating RPM packages in a repeatable manner. Fedora‚Äôs build system uses it to create a chroot environment for compiling RPM source packages.\nTo add the user into mock group use the following command\n# usermod -a -G mock $USER Restart your system for the changes to take effect.\n5. Install Flatpak runtime environment Since Flatpak is a container on the Fedora platform, we need to allow remote testing on our machine.\nAdd remote testing\n# flatpak remote-add fedora-testing oci+https://registry.fedoraproject.org#testing Enable remote testing\n# flatpak remote-modify --enable fedora-testing Test Flatpak\nYou can test its functionality by running the following comman\n# flatpak install fedora-testing org.fedoraproject.Platform/x86_64/f35 6. Enable Flathub Flatpak‚Äôs app repository is called Flathub. Basically, it‚Äôs a mirror of Dockerhub, but for docker images. Simply enter this command to install any programme you like.\n# wget https://flathub.org/repo/flathub.flatpakrepo The following command, for some reason, can be used independently on both GNOME and KDE Fedora installations. Add the Fathub remote manually with the following command if that fails to work.\n# flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo Install app with Flathub\nLet‚Äôs have a look at a real-world application installation using Flathub as an example. Okay, let‚Äôs set up steam.\n# flatpak install flathub com.valvesoftware.Steam To¬†run steam¬†do the following\n# flatpak run com.valvesoftware.Steam ","advantages-of-using-flatpaks-on-fedora#\u003cstrong\u003eAdvantages of using Flatpaks on Fedora\u003c/strong\u003e":"","conclusion#Conclusion":"On Fedora 35, we were able to install Flatpak without any problems.\nThank You üôÇ","introduction#Introduction":""},"title":"How to install Flatpak on Fedora 35"},"/utho-docs/docs/linux/how-to-install-flutter-on-debian/":{"data":{"":" How to install Flutter on Debian server\nIn this article, you will learn how to install Flutter on Debian. Snaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nInternet enabled server with updated security patches\nAlthough, we have covered the installation of Snap on Debian server. but if you face any issue, you can follow this guide","steps-to-install-snap-on-debian-server#Steps to install Snap on Debian server":"Step 1: Update the APT repository to install the latest packages.\napt update Step 2: Install the snap package using the below command.\napt install snapd Step 3: Now, verify the installation of your snap package.\nsnap version Snap version\nStep 4: Install The flutter gdk on your server.\nsnap install flutter --classic Step 5: Add the flutter tool to your path:\nexport PATH=\"$PATH:`pwd`/flutter/bin\" Step 6: Install the pre-defined binaries of flutter\nflutter precache Flutter installed on server\nAnd this is how you will install Flutter SDK on Debian server"},"title":"How to install Flutter on Debian"},"/utho-docs/docs/linux/how-to-install-flutter-on-rockylinux-8/":{"data":{"":" How to install Flutter on RockyLinux 8\nIn this article, you will learn how to install Flutter on RockyLinux 8. The Google Flutter Software Development Kit (SDK) is a free and open-source tool for creating cross-platform mobile applications. Flutter enables programmers to create high-performance, scalable applications for Android or iOS that have aesthetically pleasing and useful user interfaces using a single platform-independent codebase. With the help of a library of pre-made widgets, Flutter enables even non-programmers and non-developers to swiftly launch their own mobile applications.\nFlutter, which was developed by Google in 2015 and formally released in 2018, has swiftly taken over as the preferred toolkit for developers. Flutter recently eclipsed React Native to take the top spot among mobile app development frameworks, according to Statista.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nyum repositories configured with RocyLinux server.","steps-to-install-flutter-on-rockylinux-server#Steps to install Flutter on RockyLinux server":"Step 1: Before installing the Snap on your server, you need to install the extra packages for enterprises linux( EPEL) repsitories\nyum install epel-release Installing EPEL repo\nStep 2: Install the Snap by executing the below command\nyum install snapd Install the Snap on RockyLinux\nStep 3: Start and enable the snapd socket to start working with snapd\nsystemctl enable --now snapd.socket Output- Created symlink from /etc/systemd/system/sockets.target.wants/snapd.socket to /usr/lib/systemd/system/snapd.socket. Step 4: Now, you must enter the following to establish a symbolic link between /var/lib/snapd/snap and /snap in order to enable support for traditional snaps: ln -s /var/lib/snapd/snap /snap Step 5: Now, either set the PATH varialbe using the below command or restart another terminal\necho export PATH=$PATH:/snap/bin \u003e\u003e ~/.bashrc export .bashrc Step 6: Check the snapd version.\nsnap version Version of Snap\nStep 7: Install flutter on your server .\nsnap install flutter --classic Step 8: Add the flutter tool to your path:\nexport PATH=\"$PATH:`pwd`/flutter/bin\" Step 9: Install the pre-defined binaries of flutter\nflutter precache And this is how you will install Flutter SDK on RockyLinux 8."},"title":"How to install Flutter on RockyLinux 8"},"/utho-docs/docs/linux/how-to-install-flutter-on-ubuntu-20-04-lts-focal-fossa/":{"data":{"":"\nDescription\nIn this article you will know how to Install Flutter on Ubuntu. Flutter is a Google open source framework used to create cross-platform applications for Android, iOS, Mac, Windows, and other platforms. Anyone picks this framework because it completely transforms the way application development occurs. You can now create, test, and develop outstanding mobile, desktop, web, and embedded apps from a single codebase.","install-flutter#Install Flutter":"There are several methods for installing Flutter on Linux-based systems. We‚Äôll see the installation in two ways here. You can install using either of them.","prerequisites#Prerequisites":"a) You should have an Ubuntu 20.04 LTS server running.\nb) To run privileged commands, you must have sudo or root access.\nc) The snap, apt, wget, and tar programmes should be available on your server.","update-server#¬†Update server":" apt update ","with-snapd#With Snapd":"The snap install flutter ‚Äîclassic command, as displayed below, will install flutter if your system has the snap utility.\nsnap install flutter --classic ","with-tar-file#With Tar File":"You can find the latest version to download on the official download page. The most recent version for us right now is 2.8.1, so we use the wget command below to download this version.\nwget https://storage.googleapis.com/flutter_infra_release/releases/stable/linux/flutter_linux_2.8.1-stable.tar.xz Then, use the tar -xvf flutter linux 2.8.1-stable.tar.xz command to extract the package. This will extract the package into a local directory called ‚ÄúFlutter.‚Äù\ntar -xvf flutter_linux_2.8.1-stable.tar.xz Then, use the command export PATH=\"$PATH:‚Äòpwd‚Äô/flutter/bin\" to export the bin directory. Be aware that this is just a temporary export that won‚Äôt work if you close your current session. So, if you want it to last, you should always export your path from /.profile or /.bashrc.\nexport PATH=\"$PATH:`pwd`/flutter/bin\" Using the flutter ‚Äîversion command, as shown below, you can check the installed version after a successful installation.\nflutter --version Flutter is a Google open source framework used to create cross-platform applications for Android, iOS, Mac, Windows, and other platforms. Anyone picks this framework because it completely transforms the way application development occurs. You can now create, test, and develop outstanding mobile, desktop, web, and embedded apps from a single codebase.\nMust Read : Find multiple Ways to User Account Info and Login Details in Linux\nThankyou"},"title":"How to install Flutter on Ubuntu 20.04 LTS (Focal Fossa)"},"/utho-docs/docs/linux/how-to-install-ftp-on-centos-7-and-access-server-via-filezilla-client/":{"data":{"":"\nStep 1.¬†Login to your server via SSH Putty\nStep 2.¬†Install the vsftp (Very Secure File Trasnfer Protocol) Package :\n#¬†yum install vsftpd -y Step 3.¬†Open the configuration file of vsftp server and disable anonymous access by default ftp is set to anonymous access:\n#¬†vi /etc/vsftpd/vsftpd.conf Change the following values to the value shown below:\n**anonymous_enable=NO** Add the following parameters in the end of the file. these two parameters will provide security to your ftp server.\n**chroot_local_user=YES** **allow_writeable_chroot=YES** Save and Exit :wq Step 4.¬†Create user for ftp access\n#¬†**useradd ftpuser** #¬†**passwd ftpuser** Step 5.¬†Start and enable the service\n#¬†**systemctl start vsftpd** #¬†**systemctl enable vsftpd** Step 6.¬†Apply the firewall rule\n#¬†firewall-cmd --permanent --zone=public --add-service=ftp #¬†firewall-cmd --reload Step 7. Set selinux boolean on ‚Äúftpd_full_access‚Äù\n#¬†**setsebool -P ftpd_full_access on** Step 8.¬†Open Filezilla Client¬†Step 9.¬†type in your login details as follows:\nYour_server_ip in ‚ÄòHost‚Äô\nftp_username in ‚ÄòUsername‚Äô\nftp_password in ‚ÄòPassword‚Äô\nSSH_port in ‚ÄòPort‚Äô\nAnd hit ‚ÄòQuickconnect‚Äô\nStep 10.¬†You have now connected to your server.\nThank You"},"title":"How to Install FTP on CentOS 7 and access server via Filezilla Client"},"/utho-docs/docs/linux/how-to-install-gawk-on-centos/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Gawk on CentOS.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install Gawk on CentOS.\nGNU¬†GAWK¬†is a free and open source utility that interprets a special-purpose programming language that makes it feasible to do some data formatting tasks with only a few lines of code. GAWK is part of GNU, which is an acronym for the Free and Open Source Software Foundation.\nWhen it comes to making specific changes in text files or extracting some data using only a few lines of code, it offers a clear advantage over some of the more renowned programming languages such as C and Pascal. This is because it provides an obvious advantage. It is possible to install it quickly in practically all of the well-known distributions of Linux.\nStep 1: Update the Packages Before continuing with the steps to install Gawk on your system, it is recommended that you first bring all of the currently installed packages up to date with the newest version by using the command sudo apt update, as shown further down in this article.\n# yum update -y Step 2: Installation of Gawk In the following stage, you will be able to install the gawk software from the default CentOS repository by using the command yum install gawk, as shown in the following section. The package, as well as any dependencies it requires, will be downloaded and installed at this point.\n# yum install gawk Step 3: Check the version of Gawk After installing gawk, you can use the gawk ‚Äìversion command, as shown below, to see what version of gawk you have.\n# gawk --version Step 4: Check up all the options If you want to see all of the options that are available with the gawk command, use the ‚Äìhelp option, as shown below.\n# gawk --help "},"title":"How to install Gawk on CentOS"},"/utho-docs/docs/linux/how-to-install-gawk-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Gawk on Debian 10.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install Gawk on Debian 10.\nGNU¬†GAWK¬†is a free and open source utility that interprets a special-purpose programming language that makes it feasible to do some data formatting tasks with only a few lines of code. GAWK is part of GNU, which is an acronym for the Free and Open Source Software Foundation.\nWhen it comes to making specific changes in text files or extracting some data using only a few lines of code, it offers a clear advantage over some of the more renowned programming languages such as C and Pascal. This is because it provides an obvious advantage. It is possible to install it quickly in practically all of the well-known distributions of Linux.\nStep 1: Update the Packages Before continuing with the steps to install Gawk on your system, it is recommended that you first bring all of the currently installed packages up to date with the newest version by using the command sudo apt update, as shown further down in this article.\n# apt update Step 2: Installation of Gawk In the following stage, you will be able to install the gawk software from the default Debian repository by using the command sudo apt install gawk, as shown in the following section. The package, as well as any dependencies it requires, will be downloaded and installed at this point.\n# apt install gawk Step 3: Check the version of Gawk After installing gawk, you can use the gawk ‚Äìversion command, as shown below, to see what version of gawk you have.\n# gawk --version Step 4: Check up all the options If you want to see all of the options that are available with the gawk command, use the ‚Äìhelp option, as shown below.\n# gawk --help "},"title":"How to install Gawk on Debian 10"},"/utho-docs/docs/linux/how-to-install-gawk-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Gawk on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install Gawk on Debian 12.\nGNU¬†GAWK¬†is a free and open source utility that interprets a special-purpose programming language that makes it feasible to do some data formatting tasks with only a few lines of code. GAWK is part of GNU, which is an acronym for the Free and Open Source Software Foundation.\nWhen it comes to making specific changes in text files or extracting some data using only a few lines of code, it offers a clear advantage over some of the more renowned programming languages such as C and Pascal. This is because it provides an obvious advantage. It is possible to install it quickly in practically all of the well-known distributions of Linux.\nStep 1: Update the Packages Before continuing with the steps to install Gawk on your system, it is recommended that you first bring all of the currently installed packages up to date with the newest version by using the command sudo apt update, as shown further down in this article.\n# apt update Step 2: Installation of Gawk In the following stage, you will be able to install the gawk software from the default Debian repository by using the command sudo apt install gawk, as shown in the following section. The package, as well as any dependencies it requires, will be downloaded and installed at this point.\n# apt install gawk Step 3: Check the version of Gawk After installing gawk, you can use the gawk ‚Äìversion command, as shown below, to see what version of gawk you have.\n# gawk --version Step 4: Check up all the options If you want to see all of the options that are available with the gawk command, use the ‚Äìhelp option, as shown below.\n# gawk --help "},"title":"How to install Gawk on Debian 12"},"/utho-docs/docs/linux/how-to-install-gawk-on-debian-9/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Gawk on Debian 9.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install Gawk on Debian 9.\nGNU¬†GAWK¬†is a free and open source utility that interprets a special-purpose programming language that makes it feasible to do some data formatting tasks with only a few lines of code. GAWK is part of GNU, which is an acronym for the Free and Open Source Software Foundation.\nWhen it comes to making specific changes in text files or extracting some data using only a few lines of code, it offers a clear advantage over some of the more renowned programming languages such as C and Pascal. This is because it provides an obvious advantage. It is possible to install it quickly in practically all of the well-known distributions of Linux.\nStep 1: Update the Packages Before continuing with the steps to install Gawk on your system, it is recommended that you first bring all of the currently installed packages up to date with the newest version by using the command sudo apt update, as shown further down in this article.\n# apt update Step 2: Installation of Gawk In the following stage, you will be able to install the gawk software from the default Debian repository by using the command sudo apt install gawk, as shown in the following section. The package, as well as any dependencies it requires, will be downloaded and installed at this point.\n# apt install gawk Step 3: Check the version of Gawk After installing gawk, you can use the gawk ‚Äìversion command, as shown below, to see what version of gawk you have.\n# gawk --version Step 4: Check up all the options If you want to see all of the options that are available with the gawk command, use the ‚Äìhelp option, as shown below.\n# gawk --help "},"title":"How to install Gawk on Debian 9"},"/utho-docs/docs/linux/how-to-install-gawk-on-ubuntu-20-04/":{"data":{"":"\nDescription\nIn this lesson, we are going to learn how to Install Gawk on Ubuntu 20.04. Gawk is a free and open-source GNU utility that reads a special-purpose computer language that lets you format data with just a few lines of code. It has a clear advantage over popular computer languages like C and Pascal when it comes to making changes to text files or getting data out of them with only a few lines of code. It is easy to set up on almost every popular Linux version. Here are the steps to install Gawk on computers that run Ubuntu 20.04.\nFollow the below steps to How to Install Gawk on Ubuntu 20.04.","step-1-update-server#Step 1: Update Server":"It is recommended that you begin by bringing all of the currently installed packages up to date and upgrading them to the most recent version possible by utilising the commands apt update and sudo apt upgrade.\napt update \u0026\u0026 sudo apt upgrade ","step-2-install-gawk-package#Step 2: Install Gawk package":"As you can see in the following example, you can install the gawk utility from the default repository that Ubuntu provides by running the command apt install gawk. The program, as well as all of its dependencies, will be downloaded and installed as a result of this action andAs you can see in the screenshot below, I have already installed that package.¬†apt install gawk ","step-3-verify-your-version#Step 3: Verify your Version":"Using the gawk ‚Äìversion command, you can determine the version of gawk that was successfully installed once the installation had completed.\ngawk --version ","step-4-for-help#Step 4: For Help":"If you want to see all of the options that are accessible with the gawk command, you can do so by utilising the gawk ‚Äìhelp command, which is demonstrated below.\ngawk --help I really hope that you have been able to grasp each of the aforementioned processes to how to Install Gawk on Ubuntu 20.04.\nMust Read :- https://utho.com/docs/tutorial/how-to-install-wmclock-on-ubuntu-20-04/"},"title":"How to Install Gawk on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-gawk-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Gawk on Ubuntu 22.04.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install Gawk on Ubuntu 22.04.\nGNU GAWK is a free and open source utility that interprets a special-purpose programming language that makes it feasible to do some data formatting tasks with only a few lines of code. GAWK is part of GNU, which is an acronym for the Free and Open Source Software Foundation.\nWhen it comes to making specific changes in text files or extracting some data using only a few lines of code, it offers a clear advantage over some of the more renowned programming languages such as C and Pascal. This is because it provides an obvious advantage. It is possible to install it quickly in practically all of the well-known distributions of Linux.\nStep 1: Update the Packages Before continuing with the steps to install Gawk on your system, it is recommended that you first bring all of the currently installed packages up to date with the newest version by using the command sudo apt update, as shown further down in this article.\n# apt update Step 2: Installation of Gawk In the following stage, you will be able to install the gawk software from the default Ubuntu repository by using the command sudo apt install gawk, as shown in the following section. The package, as well as any dependencies it requires, will be downloaded and installed at this point.\n# apt install gawk Step 3: Check the version of Gawk After installing gawk, you can use the gawk ‚Äìversion command, as shown below, to see what version of gawk you have.\n# gawk --version Step 4: Check up all the options If you want to see all of the options that are available with the gawk command, use the ‚Äìhelp option, as shown below.\n# gawk --help "},"title":"How to install Gawk on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-git-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Git on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Git on AlmaLinux 8.\nGit¬†is a piece of software that allows for the tracking of changes in any set of files, and it is most commonly used for the purpose of coordinating work among a group of programmers who are jointly producing source code during the process of software development.\nGit is both free and open source. Its objectives include enhancing speed and data integrity while also providing support for dispersed and non-linear workflows (thousands of parallel branches running on different systems).\nGit is a tool that can be used to monitor the history of all the changes that have been made to a project and the files that are associated with it. Git stores all of this information in a central location known as a repository. A repository is a central storage site that houses all of the files associated with a project as well as the revision histories of those files.\nStep 1: Install Git To installed Git, type the following command.\n# dnf install git -y Check that Git was properly installed.\n# git --version Step 2: Configure Git Setting up your name and email address in Git is a necessary procedure.\n1. Set your name.\n# git config --global user.name \"utho\" 2. Set your email address.\n# git config --global user.email \"abc@utho.com\" Step 3: Verify the settings # git config --list "},"title":"How to install Git on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-git-on-centos-7/":{"data":{"":"","1-install-git#1. Install Git":"","2-configure-git#2. Configure Git":"","3-verify-the-settings#3. Verify the settings.":"\nIntroduction In this article, you will learn how to install Git on Centos 7.\nGit is a piece of software that allows for the tracking of changes in any set of files, and it is most commonly used for the purpose of coordinating work among a group of programmers who are jointly producing source code during the process of software development. Git is both free and open source. Its objectives include enhancing speed and data integrity while also providing support for dispersed and non-linear workflows (thousands of parallel branches running on different systems)\nGit is a tool that can be used to monitor the history of all the changes that have been made to a project and the files that are associated with it. Git stores all of this information in a central location known as a repository. A repository is a central storage site that houses all of the files associated with a project as well as the revision histories of those files. Git is a version control system (VCS) that allows programmers working on the same project to collaborate more easily by granting them the ability to monitor changes, revert to a previous revision, and perform other similar actions.\n1. Install Git To installed Git, type the following command.\n# yum install git -y Check that Git was properly installed.\n# git --version The Git version number should be displayed.\n2. Configure Git Setting up your name and email address in Git is a necessary procedure.\n1. Set your name.\n# git config --global user.name \"microhost\" 2. Set your email address.\n# git config --global user.email \"abc@microhost.com\" 3. Verify the settings. # git config --list The following is an example of what you might see:\nuser.name=microhost\nuser.email=abc@microhost.com","conclusion#Conclusion":"Hopefully, you have learned how to install Git Centos 7.\nAlso read: How To Install MariaDB 10.7 on CentOS 7\nThank You üôÇ","introduction#Introduction":""},"title":"How to install Git on CentOS 7"},"/utho-docs/docs/linux/how-to-install-git-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Git on Debian.\nAlso read: How to Install TinyCP on Debian\nThank You üôÇ","getting-started#Getting Started":"Updating your system packages to the most recent version is recommended before getting started. The following command will update them all:\n# apt-get update -y ","install-git-from-repository-on-debian-10#Install Git From Repository on Debian 10":"Installing Git from Debian‚Äôs default repository is the quickest and easiest method. You should be aware that the repository version you install may be out of date compared to the most recent one.\nYou can install the Git by just running the following command:\n# apt-get install git -y Once the installation is complete, you can use the following command to confirm the version of Git that was downloaded and installed:\n# git --version ","introduction#Introduction":"In this article, you will learn how to install Git on Debian.\nGit¬†is¬†free and open source software¬†for¬†distributed version control: tracking changes in any set of¬†files, usually used for coordinating work among¬†programmers¬†collaboratively developing source code¬†during software development. Its goals include speed,¬†data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems)\nGit is used to track the history of changes made to a project and its associated files. Git keeps all of this data in a repository. All of a project‚Äôs files and their respective revision histories are stored in a central location called a repository. Git is a version control system (VCS) that facilitates collaboration between programmers working on the same project by letting them monitor changes, roll back to a prior revision, and so on.","once-git-has-been-set-up-the-setup-can-be-tested-with-the-following-command#Once Git has been set up, the setup can be tested with the following command:":" # git config --list The following is an example of what you might see:\nuser.name=microhost\nuser.email=abc@microhost.com\nThe above tutorial showed you how to compile Git from source code and install it from the Debian 10 repository.","setting-up-git#Setting Up Git":"The next step is to set up Git such that the commit messages it creates accurately reflect who you are.\nIn order to customize Git, you must provide your name and email address in the appropriate fields, as shown below:\n# git config --global user.name \"microhost\" # git config --global user.email \"abc@microhost.com\" ","to-uninstall-git-type-the-following-command-into-your-terminal#To uninstall Git, type the following command into your terminal:":" # apt-get remove git -y "},"title":"How to install Git on Debian 10"},"/utho-docs/docs/linux/how-to-install-git-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Git on Debian 12.\nAlso read:¬†How to Install TinyCP on Debian\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Git on Debian 12.\nGit¬†is¬†free and open source software¬†for¬†distributed version control: tracking changes in any set of¬†files, usually used for coordinating work among¬†programmers¬†collaboratively developing source code¬†during software development. Its goals include speed,¬†data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems)\nGit is used to track the history of changes made to a project and its associated files. Git keeps all of this data in a repository. All of a project‚Äôs files and their respective revision histories are stored in a central location called a repository. Git is a version control system (VCS) that facilitates collaboration between programmers working on the same project by letting them monitor changes, roll back to a prior revision, and so on.\nUpdating your system packages to the most recent version is recommended before getting started. The following command will update them all:\n# apt-get update -y Install Git From Repository on Debian 12 Installing Git from Debian‚Äôs default repository is the quickest and easiest method. You should be aware that the repository version you install may be out of date compared to the most recent one.\nYou can install the Git by just running the following command:\n# apt-get install git -y Once the installation is complete, you can use the following command to confirm the version of Git that was downloaded and installed:\n# git --version Setting Up Git The next step is to set up Git such that the commit messages it creates accurately reflect who you are.\nIn order to customize Git, you must provide your name and email address in the appropriate fields, as shown below:\n# git config --global user.name \"utho\" # git config --global user.email \"abc@utho.com\" Once Git has been set up, the setup can be tested with the following command: # git config --list The following is an example of what you might see:\nThe above tutorial showed you how to compile Git from source code and install it from the Debian 10 repository.\nTo uninstall Git, type the following command into your terminal:\n# apt-get remove git -y "},"title":"How to install Git on Debian 12"},"/utho-docs/docs/linux/how-to-install-git-on-fedora-2/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Git on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Git on Fedora.\nGit¬†is a piece of software that allows for the tracking of changes in any set of files, and it is most commonly used for the purpose of coordinating work among a group of programmers who are jointly producing source code during the process of software development.\nGit is both free and open source. Its objectives include enhancing speed and data integrity while also providing support for dispersed and non-linear workflows (thousands of parallel branches running on different systems).\nGit is a tool that can be used to monitor the history of all the changes that have been made to a project and the files that are associated with it. Git stores all of this information in a central location known as a repository. A repository is a central storage site that houses all of the files associated with a project as well as the revision histories of those files.\nStep 1: Install Git To installed Git, type the following command.\n# dnf install git -y Check that Git was properly installed.\n# git --version Step 2: Configure Git Setting up your name and email address in Git is a necessary procedure.\n1. Set your name.\n# git config --global user.name \"utho\" 2. Set your email address.\n# git config --global user.email \"abc@utho.com\" Step 3: Verify the settings # git config --list "},"title":"How to install Git on Fedora"},"/utho-docs/docs/linux/how-to-install-git-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Git on Fedora.\nAlso read: How to Install MongoDB on¬†Fedora 36/35/34\nThank You üôÇ","installing-git-using-dnf#Installing Git using DNF":"Git packages are available in the standard Fedora repositories. A previous version, however, is included. Use the following command to install git for the Fedora operating system.\n# sudo dnf install git -y When you‚Äôve finished the preceding, Git will be properly installed on your computer. Use the following command to see what version of git you‚Äôre working with:\n# git --version ","introduction#Introduction":"In this article, you will learn how to install Git on Fedora.\nGit¬†is¬†free and open source software¬†for¬†distributed version control: tracking changes in any set of¬†files, usually used for coordinating work among¬†programmers¬†collaboratively developing source code¬†during software development. Its goals include speed,¬†data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems)\nGit is used to track the history of changes made to a project and its associated files. Git keeps all of this data in a repository. All of a project‚Äôs files and their respective revision histories are stored in a central location called a repository. Git is a version control system (VCS) that facilitates collaboration between programmers working on the same project by letting them monitor changes, roll back to a prior revision, and so on.","once-git-has-been-set-up-the-setup-can-be-tested-with-the-following-command#Once Git has been set up, the setup can be tested with the following command:":" # git config --list The following is an example of what you might see:\nuser.name=microhost\nuser.email=abc@microhost.com\nThis guide helped you set up the most recent version of Git on a Fedora Linux machine.","setting-up-git#Setting Up Git":"The next step is to set up Git such that the commit messages it creates accurately reflect who you are.\nIn order to customize Git, you must provide your name and email address in the appropriate fields, as shown below:\n# git config --global user.name \"microhost\" # git config --global user.email \"abc@microhost.com\" "},"title":"How To Install Git on Fedora"},"/utho-docs/docs/linux/how-to-install-git-on-ubuntu-20-04/":{"data":{"":"","configuring-git#Configuring Git":"Setting up your git username and email address is one of the first steps after installing Git. Each commit you make in Git is linked to your personal information.\nUse the following commands to establish your global commit name and email address:\n# git config --global user.name \"Your Name\" # git config --global user.email \"youremail@microhost.com\" Typing will check the settings modifications you made:\n# git config --list The configuration settings are stored in the ~/.gitconfig file:\n# vi ~/.gitconfig The result should look this:\nEither use the git config command (preferred) or manually edit the /.gitconfig file to make additional adjustments to your Git configuration.\nIf you are Looking to Install Streamlit on Ubuntu 20.04 I have already posted an article.\nThanks for reading this article on how to install Git on Ubuntu 20.04. I hope it helped. üôÇ","installing-git-with-apt#Installing Git with Apt":" # sudo apt update # sudo apt install git Execute the following command, which will print the version of Git, to ensure proper installation:\n# git --version Now that you‚Äôve installed Git on Ubuntu, you can begin using it.","introduction#Introduction":"This article will explain how to install Git on Ubuntu 20.04. Git is free and open source software for distributed version control: tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development\nGit is a piece of software that allows for the tracking of changes in any set of files, and it is typically used for the purpose of coordinating the work of multiple programmers who are working together on the development of source code for software. Git is both free and open source software. Speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches operating on various systems) are some of its goals."},"title":"How to Install Git on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-git-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Git on Ubuntu 22.04.\nThanks for reading this article on how to install Git on Ubuntu 22.04.\nRead Also:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","configuring-git#Configuring Git":"Setting up your git username and email address is one of the first steps after installing Git. Each commit you make in Git is linked to your personal information.\nUse the following commands to establish your global commit name and email address:\n# git config --global user.name \"Your Name\" # git config --global user.email \"youremail@utho.com\" Typing will check the settings modifications you made:\n# git config --list The configuration settings are stored in the ~/.gitconfig file:\n# vi ~/.gitconfig The result should look this:\nEither use the git config command (preferred) or manually edit the /.gitconfig file to make additional adjustments to your Git configuration.","introduction#Introduction":"In this article, you will learn how to install Git on Ubuntu 22.04.\nGit¬†is free and open source software for distributed version control: tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development.\nGit is a piece of software that allows for the tracking of changes in any set of files, and it is typically used for the purpose of coordinating the work of multiple programmers who are working together on the development of source code for software. Git is both free and open source software. Speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches operating on various systems) are some of its goals.\nInstalling Git with Apt # apt update # apt install git Execute the following command, which will print the version of Git, to ensure proper installation:\n# git --version Now that you‚Äôve installed Git on Ubuntu, you can begin using it."},"title":"How to install Git on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-gitlab-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install GitLab on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install GitLab on AlmaLinux 8.\nGitLab¬†is an open-source tool that makes it easy to manage repositories, issues, CI/CD pipelines, and much more. If you use AlmaLinux 8 and want to set up your own GitLab instance to streamline your DevOps process, you‚Äôre in the right place.\nThis step-by-step tutorial will show you how to install GitLab on AlmaLinux 8. There are two versions of GitLab: Enterprise Edition (EE) and Community Edition (CE). We‚Äôll talk about the community version in this post.\nStep 1: Update the System Let‚Äôs start by updating the list of packages and upgrading any packages that are already installed to their most recent versions.\n# dnf update -y # dnf upgrade -y Step 2: Install Dependencies GitLab needs some other things to work properly. Use the following instructions to set them up:\n# dnf install -y curl openssh-server ca-certificates postfix Step 3: Add GitLab Apt Repository Run the following curl command to add the GitLab project. It will instantly figure out what version of AlmaLinux you have and set the repository to match.\n# curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | bash Step 4: Install Gitlab Run the command below to quickly install and set up gitlab-ce on your AlmaLinux system. Replace the server‚Äôs hostname with the name of your setup.\n# EXTERNAL\\_URL=\"http://gitlab.utho.net\" dnf install gitlab-ce Once the command above has been run properly, the output will look something like this:\nThe output shown above shows that GitLab has been set up correctly. The user name for the gitlab web interface is ‚Äúroot,‚Äù and the password is saved at ‚Äú/etc/gitlab/initial_root_password.‚Äù\nStep 5: Access GitLab Web Interface Once GitLab is loaded and set up, open your web browser and type in the IP address or hostname of your server."},"title":"How to install GitLab on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-gitlab-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install GitLab on CentOS 7.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install GitLab on CentOS 7.\nGitLab¬†is an open-source tool that makes it easy to manage repositories, issues, CI/CD pipelines, and much more. If you use CentOS 7 and want to set up your own GitLab instance to streamline your DevOps process, you‚Äôre in the right place.\nThis step-by-step tutorial will show you how to install GitLab on CentOS 7. There are two versions of GitLab: Enterprise Edition (EE) and Community Edition (CE). We‚Äôll talk about the community version in this post.\nStep 1: Update the System Let‚Äôs start by updating the list of packages and upgrading any packages that are already installed to their most recent versions.\n# yum update -y # yum upgrade -y Step 2: Install Dependencies GitLab needs some other things to work properly. Use the following instructions to set them up:\n# yum install -y curl openssh-server ca-certificates postfix Step 3: Add GitLab Apt Repository Run the following curl command to add the GitLab project. It will instantly figure out what version of CentOS 7 you have and set the repository to match.\n# curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | bash Step 4: Install Gitlab Run the command below to quickly install and set up gitlab-ce on your CentOS 7 system. Replace the server‚Äôs hostname with the name of your setup.\n# EXTERNAL\\_URL=\"http://gitlab.utho.net\" yum install gitlab-ce Once the command above has been run properly, the output will look something like this:\nThe output shown above shows that GitLab has been set up correctly. The user name for the gitlab web interface is ‚Äúroot,‚Äù and the password is saved at ‚Äú/etc/gitlab/initial_root_password.‚Äù\nStep 5: Access GitLab Web Interface Once GitLab is loaded and set up, open your web browser and type in the IP address or hostname of your server."},"title":"How to install GitLab on CentOS 7"},"/utho-docs/docs/linux/how-to-install-gitlab-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install GitLab on Debian 10.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install GitLab on Debian 10.\nGitLab¬†is an open-source tool that makes it easy to manage repositories, issues, CI/CD pipelines, and much more. If you use Debian 10 and want to set up your own GitLab instance to streamline your DevOps process, you‚Äôre in the right place.\nThis step-by-step tutorial will show you how to install GitLab on Debian 10. There are two versions of GitLab: Enterprise Edition (EE) and Community Edition (CE). We‚Äôll talk about the community version in this post.\nStep 1: Update the System Let‚Äôs start by updating the list of packages and upgrading any packages that are already installed to their most recent versions.\n# apt update -y # apt upgrade -y Step 2: Install Dependencies GitLab needs some other things to work properly. Use the following instructions to set them up:\n# apt install -y curl openssh-server ca-certificates postfix During the postfix installation, there will be a configuration box. Select ‚ÄúInternet Site‚Äù and type the hostname of your machine as the mail server name. This will make it possible for GitLab to send emails.\nStep 3: Add GitLab Apt Repository Run the following curl command to add the GitLab project. It will instantly figure out what version of Debian you have and set the repository to match.\n# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | bash Step 4: Install Gitlab Run the command below to quickly install and set up gitlab-ce on your Debian system. Replace the server‚Äôs hostname with the name of your setup.\n# EXTERNAL\\_URL=\"http://gitlab.utho.net\" apt install gitlab-ce Once the command above has been run properly, the output will look something like this:\nThe output shown above shows that GitLab has been set up correctly. The user name for the gitlab web interface is ‚Äúroot,‚Äù and the password is saved at ‚Äú/etc/gitlab/initial_root_password.‚Äù\nStep 5: Access GitLab Web Interface Once GitLab is loaded and set up, open your web browser and type in the IP address or hostname of your server."},"title":"How to install GitLab on Debian 10"},"/utho-docs/docs/linux/how-to-install-gitlab-on-debian-11/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install GitLab on Debian 11.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install GitLab on Debian 11.\nGitLab¬†is an open-source tool that makes it easy to manage repositories, issues, CI/CD pipelines, and much more. If you use Debian 11 and want to set up your own GitLab instance to streamline your DevOps process, you‚Äôre in the right place.\nThis step-by-step tutorial will show you how to install GitLab on Debian 11. There are two versions of GitLab: Enterprise Edition (EE) and Community Edition (CE). We‚Äôll talk about the community version in this post.\nStep 1: Update the System Let‚Äôs start by updating the list of packages and upgrading any packages that are already installed to their most recent versions.\n# apt update -y # apt upgrade -y Step 2: Install Dependencies GitLab needs some other things to work properly. Use the following instructions to set them up:\n# apt-get install curl ca-certificates apt-transport-https gnupg2 -y Step 3: Add GitLab Apt Repository Run the following curl command to add the GitLab project. It will instantly figure out what version of Debian you have and set the repository to match.\n# curl -s https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | bash At the time this guide was written, there was no GitLab package for Debian 11. You will need to change the code name for Debian 11 (Bullseye) in the GitLab source file to Debian 10 (Buster).\n# vi /etc/apt/sources.list.d/gitlab\\_gitlab-ce.list Find the lines below.\ndeb https://packages.gitlab.com/gitlab/gitlab-ce/debian/ bullseye main deb-src https://packages.gitlab.com/gitlab/gitlab-ce/debian/ bullseye main Replaced them with the lines below.\ndeb https://packages.gitlab.com/gitlab/gitlab-ce/debian/ buster main deb-src https://packages.gitlab.com/gitlab/gitlab-ce/debian/ buster main Save the file, close it, and use the instructions below to update the repository.\n# apt-get update Step 4: Install Gitlab Run the command below to quickly install and set up gitlab-ce on your Debian system.\n# apt-get install gitlab-ce -y Once the command above has been run properly, the output will look something like this:\nThe output shown above shows that GitLab has been set up correctly. The user name for the gitlab web interface is ‚Äúroot,‚Äù and the password is saved at ‚Äú/etc/gitlab/initial_root_password‚Äù\nStep 5: Configure GitLab After the installation is done, we will set up GitLab to meet our needs. Here are the changes you can make.\n# vi /etc/gitlab/gitlab.rb Change your website name in the next line.\nexternal_url 'https://Your_Domain_Name' Save and close the file.\nLet‚Äôs now use the command below to update the repository.\n# gitlab-ctl reconfigure Step 5: Access GitLab Web Interface Once GitLab is loaded and set up, open your web browser and type in the IP address or hostname of your server."},"title":"How to install GitLab on Debian 11"},"/utho-docs/docs/linux/how-to-install-gitlab-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install GitLab on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install GitLab on Debian 12.\nGitLab¬†is an open-source tool that makes it easy to manage repositories, issues, CI/CD pipelines, and much more. If you use Debian 12 and want to set up your own GitLab instance to streamline your DevOps process, you‚Äôre in the right place.\nThis step-by-step tutorial will show you how to install GitLab on Debian 12. There are two versions of GitLab: Enterprise Edition (EE) and Community Edition (CE). We‚Äôll talk about the community version in this post.\nStep 1: Update the System Let‚Äôs start by updating the list of packages and upgrading any packages that are already installed to their most recent versions.\n# apt update -y # apt upgrade -y Step 2: Install Dependencies GitLab needs some other things to work properly. Use the following instructions to set them up:\n# apt-get install curl ca-certificates apt-transport-https gnupg2 -y Step 3: Add GitLab Apt Repository Run the following curl command to add the GitLab project. It will instantly figure out what version of Debian you have and set the repository to match.\n# curl -s https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | bash Step 4: Install Gitlab Run the command below to quickly install and set up gitlab-ce on your Debian system.\n# apt-get install gitlab-ce -y Once the command above has been run properly, the output will look something like this:\nThe output shown above shows that GitLab has been set up correctly. The user name for the gitlab web interface is ‚Äúroot,‚Äù and the password is saved at ‚Äú/etc/gitlab/initial_root_password‚Äù\nStep 5: Configure GitLab After the installation is done, we will set up GitLab to meet our needs. Here are the changes you can make.\n# vi /etc/gitlab/gitlab.rb Change your website name in the next line.\nexternal_url 'https://Your_Domain_Name' Save and close the file.\nLet‚Äôs now use the command below to update the repository.\n# gitlab-ctl reconfigure Step 5: Access GitLab Web Interface Once GitLab is loaded and set up, open your web browser and type in the IP address or hostname of your server."},"title":"How to install GitLab on Debian 12"},"/utho-docs/docs/linux/how-to-install-gitlab-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install GitLab on Ubuntu 22.04.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install GitLab on Ubuntu 22.04.\nGitLab is an open-source tool that makes it easy to manage repositories, issues, CI/CD pipelines, and much more. If you use Ubuntu 22.04 or 20.04 and want to set up your own GitLab instance to streamline your DevOps process, you‚Äôre in the right place.\nThis step-by-step tutorial will show you how to install GitLab on Ubuntu 22.04 or 20.04. There are two versions of GitLab: Enterprise Edition (EE) and Community Edition (CE). We‚Äôll talk about the community version in this post.\nStep 1: Update the System Let‚Äôs start by updating the list of packages and upgrading any packages that are already installed to their most recent versions.\n# apt update -y # apt upgrade -y Step 2: Install Dependencies GitLab needs some other things to work properly. Use the following instructions to set them up:\n# apt install -y curl openssh-server ca-certificates postfix During the postfix installation, there will be a configuration box. Select ‚ÄúInternet Site‚Äù and type the hostname of your machine as the mail server name. This will make it possible for GitLab to send emails.\nStep 3: Add GitLab Apt Repository Run the following curl command to add the GitLab project. It will instantly figure out what version of Ubuntu you have and set the repository to match.\n# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | bash Step 4: Install Gitlab Run the command below to quickly install and set up gitlab-ce on your Ubuntu system. Replace the server‚Äôs hostname with the name of your setup.\n# EXTERNAL\\_URL=\"http://gitlab.utho.net\" apt install gitlab-ce Once the command above has been run properly, the output will look something like this:\nThe output shown above shows that GitLab has been set up correctly. The user name for the gitlab web interface is ‚Äúroot,‚Äù and the password is saved at ‚Äú/etc/gitlab/initial_root_password.‚Äù\nStep 5: Access GitLab Web Interface Once GitLab is loaded and set up, open your web browser and type in the IP address or hostname of your server."},"title":"How to install GitLab on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-gnome-desktop-gui-on-centos-7/":{"data":{"":"Description\nI will walk you through the process of installing GNOME Desktop (GUI) on CentOS 7, starting with the very first step. GNOME is a desktop environment that features a graphical user interface that is open-source and free to use. It is compatible with the vast majority of Linux distributions. It was initially made available to the public in 2011 and had fundamental features such as the ability to start applications and navigate between windows. GNOME was originally developed for people who used desktop computers, but it has since been modified to work with a wide range of different systems. Using the Yum package manager, this graphical user interface (GUI) environment may be quickly downloaded and set up. We are going to go over each stage in great depth. Additional information from the official documentation for GNOME","enable-gnome-gui#Enable Gnome (GUI)":"","examine-the-package-groups-the-package-groups#Examine the package groups¬† the package groups¬†":"Using the yum grouplist or yum group list command, you can view all of the available package groups. As you can see from the output below, we simply need to install the GNOME Desktop and Graphical Administration Tools package groups.\nyum grouplist ","install-gnome-desktop-gui#Install GNOME Desktop (GUI).":"To install the GNOME Desktop and Graphical Administration Tools package groups, run the yum groupinstall ‚ÄúGNOME Desktop‚Äù or yum groupinstall ‚ÄúGNOME Desktop‚Äù and ‚ÄúGraphical Administration Tools‚Äù commands, as shown below.\nyum groupinstall \"GNOME Desktop\" \"Graphical Administration Tools\" ","prerequisites#Prerequisites":"a) You must have a RHEL/CentOS 7 system operating.\nb) To run privileged commands, you must have sudo or root access.\nc) The Yum utility should be installed on your system.","reboot-server#Reboot Server":" reboot ","set-up-gnome-desktop-gui#Set up GNOME Desktop (GUI).¬†":"After rebooting, you should be able to see the GNOME Desktop environment, as shown below.\nYou have completed the installation of that package effectively.\nGNOME Desktop (GUI) on CentOS 7. GNOME has an open-source, free graphical user interface. It supports most Linux distributions. It was first released in 2011 and had basic features like application launch and window navigation. GNOME was originally designed for desktop computers, but it now works with many others. Yum package manager can quickly download and install this GUI environment.\nThankyou","update-server#Update server":"Before installing a new package, it is always a good idea to use the yum update command to update the system with the most recent available versions, as shown below. If any of the packages require upgrading, use the yum upgrade command to do so.\nyum update "},"title":"How to install GNOME Desktop (GUI) on CentOS 7"},"/utho-docs/docs/linux/how-to-install-go-on-alma-linux/":{"data":{"":" How to install Go on Alma Linux","introduction#Introduction":"In this article, you will learn how to install Go on Alma Linux.\nGolang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.\nPython‚Äôs productivity and comparatively simple design served as an inspiration for the Go language. For effective dependency management, it makes use of goroutines, or lightweight processes, and a selection of packages. It was created to address a number of issues, such as long build times, unmanaged dependencies, effort duplication, challenges creating automated tools, and cross-language development.","prerequisites#Prerequisites":" Yum repository configured on your server.\nAny normal user with Sudo privileges or Super user to download packages","steps-to-install-go-on-alma-linux#Steps to Install Go on Alma Linux":"Step 1: First, download the latest Go package from the official page using wget command( Click here, to learn more about wget command).\nwget https://go.dev/dl/go1.20.5.linux-amd64.tar.gz Step 2: Now, Create a new Go tree in /usr/local/go by extracting the archive you just downloaded into /usr/local:\ntar -C /usr/loca/ -xzvf go1.20.5.linux-amd64.tar.gz Step 3: Set the Environmental variable PATH on your server.\nexport PATH=$PATH:/usr/local/go/bin Step 4: That is it. you have successfully install Go on your machine. However, to check whether is it installed correctly or not, execute the following command.\ngo version And, this is how you have learnt how to install Go on your Alma Linux."},"title":"How to install GO on Alma Linux"},"/utho-docs/docs/linux/how-to-install-go-on-centos/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install Go on CentOS 7 and Centos 8.\nGolang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.\nPython‚Äôs productivity and comparatively simple design served as an inspiration for the Go language. For effective dependency management, it makes use of goroutines, or lightweight processes, and a selection of packages. It was created to address a number of issues, such as long build times, unmanaged dependencies, effort duplication, challenges creating automated tools, and cross-language development.","prerequisites#Prerequisites":" Yum repository configured on your server.\nAny normal user with Sudo privileges or Super user to download packages","steps-to-install-go-on-centos#Steps to Install Go on CentOS":"Step 1: First, download the latest Go package from the official page using wget command( Click here, to learn more about wget command).\nwget https://go.dev/dl/go1.20.5.linux-amd64.tar.gz Step 2: Now, Create a new Go tree in /usr/local/go by extracting the archive you just downloaded into /usr/local:\ntar -C /usr/loca/ -xzvf go1.20.5.linux-amd64.tar.gz Step 3: Set the Environmental variable PATH on your server.\nexport PATH=$PATH:/usr/local/go/bin Step 4: That is it. you have successfully install Go on your machine. However, to check whether is it installed correctly or not, execute the following command.\ngo version And, this is how you have learnt how to install Go on your Centos 7 and Centos 8."},"title":"How to install Go on CentOS"},"/utho-docs/docs/linux/how-to-install-go-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Go on Debian 10.\nAlso Read: How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Go on Debian 10.\nGolang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.","method-1-using-apt#Method 1: Using APT":"Apt is a free user interface for software that works with core libraries to install, update, and remove software on Linux operating systems. Here are the steps to install ‚ÄúGo‚Äù on Debian 10 using the apt command.\nStep 1: Replace outdated software packages\n# apt update Step 2: Upgrade system packages\n# apt upgrade -y Step 3: Setting up Go on Debian 10\n# apt install golang-go -y Step 4: Test the Setup\n# go version Step 5: The process of removing Go on Debian using apt\nIf you want to uninstall Go for whatever reason, here is the apt command to help you do it:\n# apt-get remove golang-go ","method-2-using-snap#Method 2: Using snap":"Snap store is a way to find, install, and manage software. It is a package management system. Most Linux operating systems, like Debian 10, come with it already set up. But you can use these steps if, for whatever reason, it is not already installed on your Debian system.\nStep 1: Use the following command to set up Snap:\n# apt install snapd Step 2: Next, run the ‚Äúsnap‚Äù command given below to install ‚Äúgo,‚Äù as shown below:\n# snap install go --classic Step 3: How to delete Go through snap\nUse the following command to delete Go:\n# snap remove go "},"title":"How to install Go on Debian 10"},"/utho-docs/docs/linux/how-to-install-go-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Go on Debian 12.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Go on Debian 12.\nGolang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.","method-1-using-apt#Method 1: Using APT":"Apt is a free user interface for software that works with core libraries to install, update, and remove software on Linux operating systems. Here are the steps to install ‚ÄúGo‚Äù on Debian 12 using the apt command.\nStep 1: Replace outdated software packages\n# apt update Step 2: Upgrade system packages\n# apt upgrade -y Step 3: Setting up Go on Debian 12\n# apt install golang-go -y Step 4: Test the Setup\n# go version Step 5: The process of removing Go on Debian using apt\nIf you want to uninstall Go for whatever reason, here is the apt command to help you do it:\n# apt-get remove golang-go ","method-2-using-snap#Method 2: Using snap":"Snap store is a way to find, install, and manage software. It is a package management system. Most Linux operating systems, like Debian 12, come with it already set up. But you can use these steps if, for whatever reason, it is not already installed on your Debian system.\nStep 1: Use the following command to set up Snap:\n# apt install snapd Step 2: Next, run the ‚Äúsnap‚Äù command given below to install ‚Äúgo,‚Äù as shown below:\n# snap install go --classic Step 3: How to delete Go through snap\nUse the following command to delete Go:\n# snap remove go "},"title":"How to install Go on Debian 12"},"/utho-docs/docs/linux/how-to-install-go-on-fedora/":{"data":{"":" How to install Go on Fedora","introduction#Introduction":"In this article, you will learn how to install Go on Fedora\nGolang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.\nPython‚Äôs productivity and comparatively simple design served as an inspiration for the Go language. For effective dependency management, it makes use of goroutines, or lightweight processes, and a selection of packages. It was created to address a number of issues, such as long build times, unmanaged dependencies, effort duplication, challenges creating automated tools, and cross-language development.","prerequisites#Prerequisites":" Yum repository configured on your server.\nAny normal user with Sudo privileges or Super user to download packages","steps-to-install-go-on-fedora#Steps to Install Go on Fedora":"Step 1: First, download the latest Go package from the official page using wget command( Click here, to learn more about wget command).\nwget https://go.dev/dl/go1.20.5.linux-amd64.tar.gz Step 2: Now, Create a new Go tree in /usr/local/go by extracting the archive you just downloaded into /usr/local:\ntar -C /usr/loca/ -xzvf go1.20.5.linux-amd64.tar.gz Step 3: Set the Environmental variable PATH on your server.\nexport PATH=$PATH:/usr/local/go/bin Step 4: That is it. you have successfully install Go on your machine. However, to check whether is it installed correctly or not, execute the following command.\ngo version And, this is how you have learnt how to install Go on your Fedora 35"},"title":"How to install GO on Fedora"},"/utho-docs/docs/linux/how-to-install-go-on-rocky-linux/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install Go on Rocky Linux 8. Golang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.\nPython‚Äôs productivity and comparatively simple design served as an inspiration for the Go language. For effective dependency management, it makes use of goroutines, or lightweight processes, and a selection of packages. It was created to address a number of issues, such as long build times, unmanaged dependencies, effort duplication, challenges creating automated tools, and cross-language development.","prerequisites#Prerequisites":" Yum repository configured on your server.\nAny normal user with Sudo privileges or Super user to download packages","steps-to-install-go-on-rocky-linux#Steps to Install Go on Rocky Linux":"Step 1: First, download the latest Go package from the official page using wget command( Click here, to learn more about wget command).\nwget https://go.dev/dl/go1.20.5.linux-amd64.tar.gz Step 2: Now, Create a new Go tree in /usr/local/go by extracting the archive you just downloaded into /usr/local:\ntar -C /usr/loca/ -xzvf go1.20.5.linux-amd64.tar.gz Step 3: Set the Environmental variable PATH on your server.\nexport PATH=$PATH:/usr/local/go/bin Step 4: That is it. you have successfully install Go on your machine. However, to check whether is it installed correctly or not, execute the following command.\ngo version And, this is how you have learnt how to install Go on your Rocky linux"},"title":"How to install GO on Rocky Linux"},"/utho-docs/docs/linux/how-to-install-go-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Go on Ubuntu 22.04.\nAlso Read: How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Go on Ubuntu 22.04.\nGolang, also known as Go, is an open-source and cross-platform programming language that can be set up on Linux, Windows, and macOS. The language is well-built so that professionals can use it to build applications. Go is easy to build and run, which makes it a great programming language for making software that works well. It is reliable, builds quickly, and has software that works well and can grow quickly.","method-1-using-apt#Method 1: Using APT":"Apt is a free user interface for software that works with core libraries to install, update, and remove software on Linux operating systems. Here are the steps to install ‚ÄúGo‚Äù on Ubuntu 22.04 using the apt command.\nStep 1: Replace outdated software packages\n# apt update Step 2: Upgrade system packages\n# apt upgrade -y Step 3: Setting up Go on Ubuntu 22.04\n# apt install golang-go -y Step 4: Test the Setup\n# go version Step 5: The process of removing Go on Ubuntu using apt\nIf you want to uninstall Go for whatever reason, here is the apt command to help you do it:\n# apt-get remove golang-go ","method-2-using-snap#Method 2: Using snap":"Snap store is a way to find, install, and manage software. It is a package management system. Most Linux operating systems, like Ubuntu 22.04, come with it already set up. But you can use these steps if, for whatever reason, it is not already installed on your Ubuntu system.\nStep 1: Use the following command to set up Snap:\n# apt install snapd Step 2: Next, run the ‚Äúsnap‚Äù command given below to install ‚Äúgo,‚Äù as shown below:\n# snap install go --classic Step 3: How to delete Go through snap\nUse the following command to delete Go:\n# snap remove go "},"title":"How to install Go on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-gogs-on-debian-12/":{"data":{"":"","before-you-begin#Before You Begin":"All of Gogs‚Äô data can be stored in either a SQLite, PostgreSQL, or MySQL/MariaDB database¬†depending on the user‚Äôs preference.\nWe will be utilising SQLite as our database of choice throughout this course. You can install SQLite on your computer by putting in the following command, if it has not already been done so.\n# apt install sqlite3 -y ","conclusion#Conclusion":"Hopefully, you have learned how to install Gogs on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","create-a-systemd-unit-file#Create a systemd Unit File":"Gogs includes a Systemd unit file that, out of the box, is already configured to work with our infrastructure.\nSimply type in the following command to copy the file into the /etc/systemd/system/ directory:\n# cp /home/git/gogs/scripts/systemd/gogs.service /etc/systemd/system/ After that, run the Gogs service and make sure it‚Äôs enabled:\n# systemctl enable gogs # systemctl start gogs # systemctl status gogs ","download-gogs-binary#Download Gogs binary":"Go to the Gogs Download page to get the most recent version of the binary that is compatible with your architecture. The most recent version, as of this writing, is 0.11.86; however, if there is a newer version available, the VERSION variable in the command that follows should be updated accordingly.\nThe following wget command must be used in order to download the Gogs archive into the /tmp directory:\n# wget https://dl.gogs.io/${VERSION}/gogs\\_${VERSION}\\_linux\\_amd64.tar.gz -P /tmp As soon as the download is finished, extract the Gogs tar.gz file, and then move it into the /home/git directory:\n# tar xf /tmp/gogs\\_\\*\\_linux\\_amd64.tar.gz -C /home/git Execute the following command to transfer ownership of the directory containing the Gogs installation to the user and group git:\n# chown -R git: /home/git/gogs ","install-gogs-using-the-web-installer#Install Gogs using the web installer":"It is time to finalise the installation using the web interface now that Gogs has been downloaded and is operating successfully.\nOpen your browser, type¬†http://your_ip:3000¬†and a screen similar to the following will appear:\nDatabase Settings: Database Type: SQLite3\nPath: Use an absolute path,¬†/home/git/gogs/gogs.db\nApplication General Settings\nApplication Name: Enter your organization name\nRepository Root Path: Leave the default¬†/home/git/gogs-repositories\nRun User: git\nDomain: Enter your domain or server IP address.\nSSH Port: 22, change it if SSH is¬†listening on other Port\nHTTP Port: 3000\nApplication URL: Use http and your domain or server IP address.\nLog Path: Leave the default¬†/home/git/gogs/log\nOnce done hit the ‚ÄúInstall Gogs‚Äù button. The installation is instant and when completed you will be redirected to the login page.","installing-gogs#Installing Gogs":"The Gogs binary will be used for the installation. The process of installation is pretty easy to understand and follow.\nInstalling Git on your server is the first thing you need to do. In order to accomplish this, you must first update the local package index and then install the git package by executing the command below.\n# apt update # apt install git Displaying the Git version will allow you to validate the installation.\n# git --version Simply typing in the following command will create a new user account for the Gogs service.\n# adduser --system --group --disabled-password --shell /bin/bash --home /home/git --gecos 'Git Version Control' git ","introduction#Introduction":"In this article, you will learn how to install Gogs on Debian 12.\nGogs is a git server that is self-hosted and open source. It was created in Go. It comes with a file editor for the repository, tracking for the project issues, and a wiki that is built right in.\nGogs is a lightweight application that may be loaded on low-powered devices because of its low memory and CPU requirements. You should give Gogs a shot if you are looking for an alternative to Gitlab that has a significantly smaller memory footprint and if you do not want all of the bells and whistles that Gitlab provides. The steps necessary to install Gogs on Debian are outlined in this tutorial."},"title":"How to install Gogs on Debian 12"},"/utho-docs/docs/linux/how-to-install-gogs-on-debian/":{"data":{"":"","before-you-begin#Before You Begin":"All of Gogs‚Äô data can be stored in either a SQLite, PostgreSQL, or MySQL/MariaDB database depending on the user‚Äôs preference.\nWe will be utilising SQLite as our database of choice throughout this course. You can install SQLite on your computer by putting in the following command, if it has not already been done so.\n# apt install sqlite3 -y ","conclusion#Conclusion":"Hopefully, you have learned how to install Gogs on Debian.\nThank You üôÇ","create-a-systemd-unit-file#Create a systemd Unit File":"Gogs includes a Systemd unit file that, out of the box, is already configured to work with our infrastructure.\nSimply type in the following command to copy the file into the /etc/systemd/system/ directory:\n# cp /home/git/gogs/scripts/systemd/gogs.service /etc/systemd/system/ After that, run the Gogs service and make sure it‚Äôs enabled:\n# systemctl enable gogs # systemctl start gogs # systemctl status gogs ","download-gogs-binary#Download Gogs binary":"Go to the Gogs Download page to get the most recent version of the binary that is compatible with your architecture. The most recent version, as of this writing, is 0.11.86; however, if there is a newer version available, the VERSION variable in the command that follows should be updated accordingly.\n# VERSION=0.11.86 The following wget command must be used in order to download the Gogs archive into the /tmp directory:\n# wget https://dl.gogs.io/${VERSION}/gogs_${VERSION}_linux_amd64.tar.gz -P /tmp As soon as the download is finished, extract the Gogs tar.gz file, and then move it into the /home/git directory:\n# tar xf /tmp/gogs_*_linux_amd64.tar.gz -C /home/git Execute the following command to transfer ownership of the directory containing the Gogs installation to the user and group git:\n# chown -R git: /home/git/gogs ","install-gogs-using-the-web-installer#Install Gogs using the web installer":"It is time to finalise the installation using the web interface now that Gogs has been downloaded and is operating successfully.\nOpen your browser, type¬†http://your_ip:3000¬†and a screen similar to the following will appear:\nDatabase Settings: Database Type: SQLite3\nPath: Use an absolute path,¬†/home/git/gogs/gogs.db\nApplication General Settings\nApplication Name: Enter your organization name\nRepository Root Path: Leave the default¬†/home/git/gogs-repositories\nRun User: git\nDomain: Enter your domain or server IP address.\nSSH Port: 22, change it if SSH is¬†listening on other Port\nHTTP Port: 3000\nApplication URL: Use http and your domain or server IP address.\nLog Path: Leave the default¬†/home/git/gogs/log\nOnce done hit the ‚ÄúInstall Gogs‚Äù button. The installation is instant and when completed you will be redirected to the login page.","installing-gogs#Installing Gogs":"The Gogs binary will be used for the installation. The process of installation is pretty easy to understand and follow.\nInstalling Git on your server is the first thing you need to do. In order to accomplish this, you must first update the local package index and then install the git package by executing the command below.\n# apt update # apt install git Displaying the Git version will allow you to validate the installation.\n# git --version Simply typing in the following command will create a new user account for the Gogs service.\n# adduser --system --group --disabled-password --shell /bin/bash --home /home/git --gecos 'Git Version Control' git ","introduction#Introduction":"In this article, you will learn how to install Gogs on Debian.\nGogs is a git server that is self-hosted and open source. It was created in Go. It comes with a file editor for the repository, tracking for the project issues, and a wiki that is built right in.\nGogs is a lightweight application that may be loaded on low-powered devices because of its low memory and CPU requirements. You should give Gogs a shot if you are looking for an alternative to Gitlab that has a significantly smaller memory footprint and if you do not want all of the bells and whistles that Gitlab provides. The steps necessary to install Gogs on Debian are outlined in this tutorial."},"title":"How to install Gogs on Debian"},"/utho-docs/docs/linux/how-to-install-gogs-on-ubuntu-20-04/":{"data":{"":"","before-you-begin#Before You Begin":"All of Gogs‚Äô data can be stored in either a SQLite, PostgreSQL, or MySQL/MariaDB database depending on the user‚Äôs preference.\nWe will be utilising SQLite as our database of choice throughout this course. You can install SQLite on your computer by putting in the following command, if it has not already been done so.\n# apt install sqlite3 -y ","conclusion#Conclusion":"Hopefully, you have learned how to install Gogs on Ubuntu 20.04.\nThank You üôÇ","create-a-systemd-unit-file#Create a systemd Unit File":"Gogs includes a Systemd unit file that, out of the box, is already configured to work with our infrastructure.\nSimply type in the following command to copy the file into the /etc/systemd/system/ directory:\n# cp /home/git/gogs/scripts/systemd/gogs.service /etc/systemd/system/ After that, run the Gogs service and make sure it‚Äôs enabled:\n# systemctl enable gogs # systemctl start gogs # systemctl status gogs ","download-gogs-binary#Download Gogs binary":"Go to the Gogs Download page to get the most recent version of the binary that is compatible with your architecture. The most recent version, as of this writing, is 0.11.86; however, if there is a newer version available, the VERSION variable in the command that follows should be updated accordingly.\n# VERSION=0.11.86 The following wget command must be used in order to download the Gogs archive into the /tmp directory:\n# wget https://dl.gogs.io/${VERSION}/gogs_${VERSION}_linux_amd64.tar.gz -P /tmp As soon as the download is finished, extract the Gogs tar.gz file, and then move it into the /home/git directory:\n# tar xf /tmp/gogs_*_linux_amd64.tar.gz -C /home/git Execute the following command to transfer ownership of the directory containing the Gogs installation to the user and group git:\n# chown -R git: /home/git/gogs ","install-gogs-using-the-web-installer#Install Gogs using the web installer":"It is time to finalise the installation using the web interface now that Gogs has been downloaded and is operating successfully.\nOpen your browser, type¬†http://your_ip:3000¬†and a screen similar to the following will appear:\nDatabase Settings: Database Type: SQLite3 Path: Use an absolute path,¬†/home/git/gogs/gogs.db Application General Settings\nApplication Name: Enter your organization name Repository Root Path: Leave the default¬†/home/git/gogs-repositories Run User: git Domain: Enter your domain or server IP address. SSH Port: 22, change it if SSH is¬†listening on other Port HTTP Port: 3000 Application URL: Use http and your domain or server IP address. Log Path: Leave the default¬†/home/git/gogs/log Once done hit the ‚ÄúInstall Gogs‚Äù button. The installation is instant and when completed you will be redirected to the login page.","installing-gogs#Installing Gogs":"The Gogs binary will be used for the installation. The process of installation is pretty easy to understand and follow.\nInstalling Git on your server is the first thing you need to do. In order to accomplish this, you must first update the local package index and then install the git package by executing the command below.\n# apt update # apt install git Displaying the Git version will allow you to validate the installation.\n# git --version Simply typing in the following command will create a new user account for the Gogs service.\n# adduser --system --group --disabled-password --shell /bin/bash --home /home/git --gecos 'Git Version Control' git ","introduction#Introduction":"In this article, you will learn how to install Gogs on Ubuntu 20.04.\nGogs is a git server that is self-hosted and open source. It was created in Go. It comes with a file editor for the repository, tracking for the project issues, and a wiki that is built right in.\nGogs is a lightweight application that may be loaded on low-powered devices because of its low memory and CPU requirements. You should give Gogs a shot if you are looking for an alternative to Gitlab that has a significantly smaller memory footprint and if you do not want all of the bells and whistles that Gitlab provides. The steps necessary to install Gogs on Ubuntu 20.04 are outlined in this tutorial."},"title":"How to install Gogs on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-gogs-on-ubuntu-22-04/":{"data":{"":"","before-you-begin#Before You Begin":"All of Gogs‚Äô data can be stored in either a SQLite, PostgreSQL, or MySQL/MariaDB database depending on the user‚Äôs preference.\nWe will be utilising SQLite as our database of choice throughout this course. You can install SQLite on your computer by putting in the following command, if it has not already been done so.\n# apt install sqlite3 -y ","conclusion#Conclusion":"Hopefully, you have learned how to install Gogs on Ubuntu 22.04.\nThank You üôÇ","create-a-systemd-unit-file#Create a systemd Unit File":"Gogs includes a Systemd unit file that, out of the box, is already configured to work with our infrastructure.\nSimply type in the following command to copy the file into the /etc/systemd/system/ directory:\n# cp /home/git/gogs/scripts/systemd/gogs.service /etc/systemd/system/ After that, run the Gogs service and make sure it‚Äôs enabled:\n# systemctl enable gogs # systemctl start gogs # systemctl status gogs ","download-gogs-binary#Download Gogs binary":"Go to the Gogs Download page to get the most recent version of the binary that is compatible with your architecture. The most recent version, as of this writing, is 0.11.86; however, if there is a newer version available, the VERSION variable in the command that follows should be updated accordingly.\n# VERSION=0.11.86 The following wget command must be used in order to download the Gogs archive into the /tmp directory:\n# wget https://dl.gogs.io/${VERSION}/gogs_${VERSION}_linux_amd64.tar.gz -P /tmp As soon as the download is finished, extract the Gogs tar.gz file, and then move it into the /home/git directory:\n# tar xf /tmp/gogs_*_linux_amd64.tar.gz -C /home/git Execute the following command to transfer ownership of the directory containing the Gogs installation to the user and group git:\n# chown -R git: /home/git/gogs ","install-gogs-using-the-web-installer#Install Gogs using the web installer":"It is time to finalise the installation using the web interface now that Gogs has been downloaded and is operating successfully.\nOpen your browser, type¬†http://your_ip:3000¬†and a screen similar to the following will appear:\nDatabase Settings: Database Type: SQLite3 Path: Use an absolute path,¬†/home/git/gogs/gogs.db Application General Settings\nApplication Name: Enter your organization name Repository Root Path: Leave the default¬†/home/git/gogs-repositories Run User: git Domain: Enter your domain or server IP address. SSH Port: 22, change it if SSH is¬†listening on other Port HTTP Port: 3000 Application URL: Use http and your domain or server IP address. Log Path: Leave the default¬†/home/git/gogs/log Once done hit the ‚ÄúInstall Gogs‚Äù button. The installation is instant and when completed you will be redirected to the login page.","installing-gogs#Installing Gogs":"The Gogs binary will be used for the installation. The process of installation is pretty easy to understand and follow.\nInstalling Git on your server is the first thing you need to do. In order to accomplish this, you must first update the local package index and then install the git package by executing the command below.\n# apt update # apt install git Displaying the Git version will allow you to validate the installation.\n# git --version Simply typing in the following command will create a new user account for the Gogs service.\n# adduser --system --group --disabled-password --shell /bin/bash --home /home/git --gecos 'Git Version Control' git ","introduction#Introduction":"In this article, you will learn how to install Gogs on Ubuntu 22.04.\nGogs is a git server that is self-hosted and open source. It was created in Go. It comes with a file editor for the repository, tracking for the project issues, and a wiki that is built right in.\nGogs is a lightweight application that may be loaded on low-powered devices because of its low memory and CPU requirements. You should give Gogs a shot if you are looking for an alternative to Gitlab that has a significantly smaller memory footprint and if you do not want all of the bells and whistles that Gitlab provides. The steps necessary to install Gogs on Ubuntu 22.04 are outlined in this tutorial."},"title":"How to install Gogs on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-gradle-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Gradle on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Gradle on AlmaLinux 8.\nGradle is a build automation tool for making software in more than one language. It handles the whole development process, to gather and packaging to testing, deploying, and publishing. Java, C/C++, and JavaScript are among the programmes that can be used.","step-1-system-update#Step 1: System update":"It is actually recommended that you update the system on the AlmaLinux 8 server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# dnf -y install epel-release # dnf -y update ","step-2-install-jdk#Step 2: Install JDK":"In order to function, Gradle needs a version of the Java Development Kit (JDK) that is 7 or above. The installation of JDK 8 will be covered in this guide. To install JDK 8 on your server, you will need to run the following command.\n# dnf -y install java-1.8.0-openjdk wget unzip Using the above command will also install the wget and unzip programmes. Make sure the installation is correct.\n# java -version ","step-3-download-gradle#Step 3: Download Gradle":"There are two different varieties of the Gradle distribution archive: ‚Äúbinary-only‚Äù and ‚Äúcomplete.‚Äù The ‚Äúfull‚Äù package includes not only the binary but also the documentation and the source code for the Gradle programme. The ‚Äúbinary-only‚Äù archive just contains the Gradle software itself. In order to download Gradle to your computer, use the following command.\n# wget https://services.gradle.org/distributions/gradle-3.4.1-bin.zip ","step-4-install-gradle#Step 4: Install Gradle":"Create a directory for the Gradle installation to take place in.\n# mkdir /opt/gradle Extract the archive that you downloaded into the directory that you just established.\n# unzip -d /opt/gradle gradle-3.4.1-bin.zip Set the value of the PATH environment variable so that the gradle executable may be run in any directory on the system.\n# export PATH=$PATH:/opt/gradle/gradle-3.4.1/bin To determine whether or not the Gradle installation was successful, you may check by using the following command.\n# gradle -v "},"title":"How to install Gradle on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-gradle-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Gradle on CentOS 7.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Gradle on CentOS 7.\nThe build automation toolkit known as Gradle is available for free and under an open source license. It is modelled around Apache Ant and Apache Maven. A software project‚Äôs full development lifecycle may be supported by the Gradle platform, which is provided by the tool.","step-1-system-update#Step 1: System update":"It is actually recommended that you update the system on the CentOS server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# yum -y install epel-release # yum -y update ","step-2-install-jdk#Step 2: Install JDK":"In order to function, Gradle needs a version of the Java Development Kit (JDK) that is 7 or above. The installation of JDK 8 will be covered in this guide. To install JDK 8 on your server, you will need to run the following command.\n# yum -y install java-1.8.0-openjdk wget unzip Using the above command will also install the wget and unzip programmes. Make sure the installation is correct.\n# java -version ","step-3-download-gradle#Step 3: Download Gradle":"There are two different varieties of the Gradle distribution archive: ‚Äúbinary-only‚Äù and ‚Äúcomplete.‚Äù The ‚Äúfull‚Äù package includes not only the binary but also the documentation and the source code for the Gradle programme. The ‚Äúbinary-only‚Äù archive just contains the Gradle software itself. In order to download Gradle to your computer, use the following command.\n# wget https://services.gradle.org/distributions/gradle-3.4.1-bin.zip ","step-4-install-gradle#Step 4: Install Gradle":"Create a directory for the Gradle installation to take place in.\n# mkdir /opt/gradle Extract the archive that you downloaded into the directory that you just established.\n# unzip -d /opt/gradle gradle-3.4.1-bin.zip Set the value of the PATH environment variable so that the gradle executable may be run in any directory on the system.\n# export PATH=$PATH:/opt/gradle/gradle-3.4.1/bin To determine whether or not the Gradle installation was successful, you may check by using the following command.\n# gradle -v "},"title":"How to install Gradle on CentOS 7"},"/utho-docs/docs/linux/how-to-install-gradle-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install¬†Gradle on Debian 10.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install¬†Gradle on Debian 10.\nThe build automation toolkit known as Gradle is available for free and under an open source license. It is modelled around Apache Ant and Apache Maven. A software project‚Äôs full development lifecycle may be supported by the Gradle platform, which is provided by the tool.","step-1-system-update#Step 1: System update":"It is actually recommended that you update the system on the Debian server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# apt update # apt install default-jdk -y # java -version ","step-2-downloading-gradle#Step 2: Downloading Gradle":"Downloading the Gradle binary file requires the following wget command to be executed:\n# wget https://services.gradle.org/distributions/gradle-6.3-bin.zip -P /tmp When the download is finished, extract the contents of the zip file into the /opt/gradle directory as follows:\n# unzip -d /opt/gradle /tmp/gradle-*.zip Check to see whether the Gradle files were successfully extracted:\n# ls /opt/gradle/gradle-* output: bin init.d lib LICENSE NOTICE README ","step-3-setting-up-the-environment-variables#Step 3: Setting up the Environment Variables":"Next, we will need to ensure that the Gradle bin directory is included in the PATH environment variable by configuring it. To do this, use your preferred text editor and create a new file with the name gradle.sh inside the /etc/profile.d directory on your system.\n# vi /etc/profile.d/gradle.sh Paste the following configuration:\nexport GRADLE_HOME=/opt/gradle/gradle-6.3 export PATH=${GRADLE_HOME}/bin:${PATH} Please save the file and then close it. During the initialization of the shell, this script will be sourced.\nExecutable permissions may be granted to the script by using the chmod command as follows:\n# chmod +x /etc/profile.d/gradle.sh Through the use of the source command, load the environment variables:\n# source /etc/profile.d/gradle.sh ","step-4-verifying-the-gradle-installation#Step 4: Verifying the Gradle Installation":"Executing the following command, which will indicate the version of Gradle, will verify that the Gradle installation was performed correctly:\n# gradle -v "},"title":"How to install Gradle on Debian 10"},"/utho-docs/docs/linux/how-to-install-gradle-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install¬†Gradle on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install¬†Gradle on Debian 12.\nThe build automation toolkit known as¬†Gradle¬†is available for free and under an open source license. It is modelled around Apache Ant and Apache Maven. A software project‚Äôs full development lifecycle may be supported by the Gradle platform, which is provided by the tool.\nStep 1: System update It is actually recommended that you update the system on the Debian server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# apt update # apt install default-jdk -y # java -version Step 2: Downloading Gradle Downloading the Gradle binary file requires the following wget command to be executed:\n# wget https://services.gradle.org/distributions/gradle-6.3-bin.zip -P /tmp When the download is finished, extract the contents of the zip file into the /opt/gradle directory as follows:\n# unzip -d /opt/gradle /tmp/gradle-\\*.zip Check to see whether the Gradle files were successfully extracted:\n# ls /opt/gradle/gradle-\\* output: bin init.d lib LICENSE NOTICE README Step 3: Setting up the Environment Variables Next, we will need to ensure that the Gradle bin directory is included in the PATH environment variable by configuring it. To do this, use your preferred text editor and create a new file with the name gradle.sh inside the /etc/profile.d directory on your system.\n# vi /etc/profile.d/gradle.sh Paste the following configuration:\nexport GRADLE_HOME=/opt/gradle/gradle-6.3 export PATH=${GRADLE_HOME}/bin:${PATH} Please save the file and then close it. During the initialization of the shell, this script will be sourced.\nExecutable permissions may be granted to the script by using the chmod command as follows:\n# chmod +x /etc/profile.d/gradle.sh Through the use of the source command, load the environment variables:\n# source /etc/profile.d/gradle.sh Step 4: Verifying the Gradle Installation Executing the following command, which will indicate the version of Gradle, will verify that the Gradle installation was performed correctly:\n# gradle -v "},"title":"How to install Gradle on Debian 12"},"/utho-docs/docs/linux/how-to-install-gradle-on-debian-9/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install¬†Gradle on Debian 9.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install¬†Gradle on Debian 9.\nThe build automation toolkit known as Gradle is available for free and under an open source license. It is modelled around Apache Ant and Apache Maven. A software project‚Äôs full development lifecycle may be supported by the Gradle platform, which is provided by the tool.","step-1-system-update#Step 1: System update":"It is actually recommended that you update the system on the Debian server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# apt update # apt install default-jdk -y # java -version ","step-2-downloading-gradle#Step 2: Downloading Gradle":"Downloading the Gradle binary file requires the following wget command to be executed:\n# wget https://services.gradle.org/distributions/gradle-6.3-bin.zip -P /tmp When the download is finished, extract the contents of the zip file into the /opt/gradle directory as follows:\n# apt install unzip # unzip -d /opt/gradle /tmp/gradle-*.zip Check to see whether the Gradle files were successfully extracted:\n# ls /opt/gradle/gradle-* output: bin init.d lib LICENSE NOTICE README ","step-3-setting-up-the-environment-variables#Step 3: Setting up the Environment Variables":"Next, we will need to ensure that the Gradle bin directory is included in the PATH environment variable by configuring it. To do this, use your preferred text editor and create a new file with the name gradle.sh inside the /etc/profile.d directory on your system.\n# vi /etc/profile.d/gradle.sh Paste the following configuration:\nexport GRADLE_HOME=/opt/gradle/gradle-6.3 export PATH=${GRADLE_HOME}/bin:${PATH} Please save the file and then close it. During the initialization of the shell, this script will be sourced.\nExecutable permissions may be granted to the script by using the chmod command as follows:\n# chmod +x /etc/profile.d/gradle.sh Through the use of the source command, load the environment variables:\n# source /etc/profile.d/gradle.sh ","step-4-verifying-the-gradle-installation#Step 4: Verifying the Gradle Installation":"Executing the following command, which will indicate the version of Gradle, will verify that the Gradle installation was performed correctly:\n# gradle -v "},"title":"How to install Gradle on Debian 9"},"/utho-docs/docs/linux/how-to-install-gradle-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Gradle on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Gradle on Fedora.\nGradle¬†is a build automation tool for making software in more than one language. It handles the whole development process, to gather and packaging to testing, deploying, and publishing. Java, C/C++, and JavaScript are among the programmes that can be used.","step-1-system-update#Step 1: System update":"It is actually recommended that you update the system on the Fedora server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# dnf install¬†[https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm](https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm)¬†--skip-broken # dnf -y update ","step-2-install-jdk#Step 2: Install JDK":"In order to function, Gradle needs a version of the Java Development Kit (JDK) that is 7 or above. The installation of JDK 8 will be covered in this guide. To install JDK 8 on your server, you will need to run the following command.\n# dnf -y install java-1.8.0-openjdk wget unzip Using the above command will also install the wget and unzip programmes. Make sure the installation is correct.\n# java -version ","step-3-download-gradle#Step 3: Download Gradle":"There are two different varieties of the Gradle distribution archive: ‚Äúbinary-only‚Äù and ‚Äúcomplete.‚Äù The ‚Äúfull‚Äù package includes not only the binary but also the documentation and the source code for the Gradle programme. The ‚Äúbinary-only‚Äù archive just contains the Gradle software itself. In order to download Gradle to your computer, use the following command.\n# wget https://services.gradle.org/distributions/gradle-3.4.1-bin.zip ","step-4-install-gradle#Step 4: Install Gradle":"Create a directory for the Gradle installation to take place in.\n# mkdir /opt/gradle Extract the archive that you downloaded into the directory that you just established.\n# unzip -d /opt/gradle gradle-3.4.1-bin.zip Set the value of the PATH environment variable so that the gradle executable may be run in any directory on the system.\n# export PATH=$PATH:/opt/gradle/gradle-3.4.1/bin To determine whether or not the Gradle installation was successful, you may check by using the following command.\n# gradle -v "},"title":"How to install Gradle on Fedora"},"/utho-docs/docs/linux/how-to-install-gradle-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install¬†Gradle on Ubuntu 20.04.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install¬†Gradle on Ubuntu 20.04.\nThe build automation toolkit known as Gradle is available for free and under an open source license. It is modelled around Apache Ant and Apache Maven. A software project‚Äôs full development lifecycle may be supported by the Gradle platform, which is provided by the tool.","step-1-system-update#Step 1: System update":"It is actually recommended that you update the system on the Ubuntu server instance you are using before installing any packages. To bring the system up to date, you will need to log in as the sudo user and then perform the following instructions.\n# apt update # apt install default-jdk -y # java -version ","step-2-downloading-gradle#Step 2: Downloading Gradle":"Downloading the Gradle binary file requires the following wget command to be executed:\n# wget https://services.gradle.org/distributions/gradle-6.3-bin.zip -P /tmp When the download is finished, extract the contents of the zip file into the /opt/gradle directory as follows:\n# unzip -d /opt/gradle /tmp/gradle-*.zip Check to see whether the Gradle files were successfully extracted:\n# ls /opt/gradle/gradle-* output: bin init.d lib LICENSE NOTICE README ","step-3-setting-up-the-environment-variables#Step 3: Setting up the Environment Variables":"Next, we will need to ensure that the Gradle bin directory is included in the PATH environment variable by configuring it. To do this, use your preferred text editor and create a new file with the name gradle.sh inside the /etc/profile.d directory on your system.\n# vi /etc/profile.d/gradle.sh Paste the following configuration:\nexport GRADLE_HOME=/opt/gradle/gradle-6.3 export PATH=${GRADLE_HOME}/bin:${PATH} Please save the file and then close it. During the initialization of the shell, this script will be sourced.\nExecutable permissions may be granted to the script by using the chmod command as follows:\n# chmod +x /etc/profile.d/gradle.sh Through the use of the source command, load the environment variables:\n# source /etc/profile.d/gradle.sh ","step-4-verifying-the-gradle-installation#Step 4: Verifying the Gradle Installation":"Executing the following command, which will indicate the version of Gradle, will verify that the Gradle installation was performed correctly:\n# gradle -v "},"title":"How to install Gradle on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-grafana-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Grafana on Almalinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Grafana on Almalinux 8.\nGrafana¬†is a free monitoring and data visualisation application that works across multiple platforms. It provides insightful analytics by rendering your data in graphical form and is cross-platform. It is possible to create dynamic dashboards that can be reused, to explore metrics by means of ad hoc queries, to set up alert rules for critical metrics that are constantly evaluated and notify relevant parties of any changes, and to collaborate with members of the team by means of in-built sharing.\nStep 1: Add Grafana Repository: Open a terminal session or log in via SSH to access your AlmaLinux 8 server.\nRun the following command to add the Grafana repository:\n# dnf install https://dl.grafana.com/oss/release/grafana-release.rpm Step 2: Install Grafana Once the repository is added, run the following command to install Grafana:\n# dnf install grafana Step 3: Start and Enable Grafana Service: Start the Grafana service using the following command:\n# systemctl start grafana-server To ensure that Grafana starts automatically at system boot, enable it with the following command:\n# systemctl enable grafana-server Step 4: Adjust Firewall If you have a firewall enabled on your AlmaLinux server, you need to allow incoming traffic to the Grafana port (default is TCP 3000) to access the Grafana web interface. You can use the following command to open the port:\n# firewall-cmd --add-port=3000/tcp --permanent # firewall-cmd --reload Step 5: Access Grafana Web Interface: Open a web browser on your local machine and enter the IP address or hostname of your AlmaLinux server, followed by port 3000 (e.g., http://server_IP:3000).\nStep 6: Log in to Grafana: The default username and password for Grafana is ‚Äúadmin‚Äù.\nEnter ‚Äúadmin‚Äù as the username and ‚Äúadmin‚Äù as the password.\nWhen you first log in, you‚Äôll be asked to change the password."},"title":"How to install Grafana On Almalinux 8"},"/utho-docs/docs/linux/how-to-install-grafana-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"You have Grafana installed on your server successfully. A screen prompting you to log in will appear. Please use admin as both your username and your password. Now that you have access, you can go ahead and configure the Dashboard so that you can start controlling and analysing your data.\nI hope you have learned how to Install Grafana on Centos 7.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Install Grafana on Centos 7.\nGrafana is a free monitoring and data visualisation application that works across multiple platforms. It provides insightful analytics by rendering your data in graphical form and is cross-platform. It is possible to create dynamic dashboards that can be reused, to explore metrics by means of ad hoc queries, to set up alert rules for critical metrics that are constantly evaluated and notify relevant parties of any changes, and to collaborate with members of the team by means of in-built sharing. These are just some of the many features that this software possesses. In addition to that, it enables access to other systems like Graphite, Prometheus, Elasticsearch, and InfluxDB. This guide will walk you through the process of installing Grafana on a server that is running Centos 7","new-grafana-features#New Grafana features":"Panels in a user‚Äôs library can be used on several dashboards thanks to this feature.\nThe Prometheus metrics browser enables you to locate metrics and relevant labels in a short amount of time, which you can then use to construct fundamental queries.\nAlerts from Grafana managed alerts and Prometheus-compatible data sources are consolidated into a single user interface (UI) and application programming interface (API).\nIt is now possible for data sources to update dashboards in real time using a websocket connection, a technology known as ‚Äúreal-time streaming.‚Äù\nAn alternative to the traditional bar chart for representing categorical data.\nHistograms, previously buried in the Graph panel, are now available in their own representation.\nThe State timeline display tracks the emergence of new conditions throughout time.\nThe ability to view time series data has passed from beta and into a stable phase.\nThe log results from inspecting a panel are now downloadable as a text (.txt) file.","step-1-install-grafana-on-centos-7#Step 1: Install Grafana on CentOS 7¬†":"The newest version of Grafana, 6, is ready for download and installation. Complement your computer with the Grafana RPM repository.\n# vi /etc/yum.repos.d/grafana.repo Add the following content to the file:\n[grafana]name=grafanabaseurl=https://packages.grafana.com/oss/rpmrepo_gpgcheck=1enabled=1gpgcheck=1gpgkey=https://packages.grafana.com/gpg.keysslverify=1sslcacert=/etc/pki/tls/certs/ca-bundle.crt save and exit the file with escape: wq¬†To get started with grafana, add the repository and then install the grafana rpm package.\n# yum -y install grafana View additional information regarding the package you have installed.\n# rpm -qi grafana ","step-2-start-grafana-service-on-centos-7#Step 2: Start Grafana service on CentOS 7":"Using the systemctl service management command, Grafana may be launched and set to regularly start with CentOS 7:\n# systemctl enable --now grafana-server It‚Äôs believed that the service will be in a ‚Äúrunning‚Äù state.\n# systemctl status grafana-server ","step-3-access-grafana-dashboard#Step 3: Access Grafana Dashboard":"Simply open your web browser, navigate to http://Server IP:3000/, and you will have access to the Grafana Web Interface without having to use a reverse proxy. Take, for instance:\n# http://192.0.2.10:3000 "},"title":"How To Install Grafana on Centos 7"},"/utho-docs/docs/linux/how-to-install-grafana-on-fedora-35-34-33-32-31/":{"data":{"":"","conclusion#Conclusion":"You have Grafana installed on your server successfully. A screen prompting you to log in will appear. Please use admin as both your username and your password. Now that you have access, you can go ahead and configure the Dashboard so that you can start controlling and analysing your data.\nI hope you have learned how to install Grafana on Fedora 35/34/33/32/31.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Grafana on Fedora 35/34/33/32/31.\nGrafana is a free monitoring and data visualisation application that works across multiple platforms. It provides insightful analytics by rendering your data in graphical form and is cross-platform. It is possible to create dynamic dashboards that can be reused, to explore metrics by means of ad hoc queries, to set up alert rules for critical metrics that are constantly evaluated and notify relevant parties of any changes, and to collaborate with members of the team by means of in-built sharing. These are just some of the many features that this software possesses. In addition to that, it enables access to other systems like Graphite, Prometheus, Elasticsearch, and InfluxDB. This guide will walk you through the process of installing Grafana on a server that is running Fedora.","new-grafana-features#New Grafana features":"Panels in a user‚Äôs library can be used on several dashboards thanks to this feature.\nThe Prometheus metrics browser enables you to locate metrics and relevant labels in a short amount of time, which you can then use to construct fundamental queries.\nAlerts from Grafana managed alerts and Prometheus-compatible data sources are consolidated into a single user interface (UI) and application programming interface (API).\nIt is now possible for data sources to update dashboards in real time using a websocket connection, a technology known as ‚Äúreal-time streaming.‚Äù\nAn alternative to the traditional bar chart for representing categorical data.\nHistograms, previously buried in the Graph panel, are now available in their own representation.\nThe State timeline display tracks the emergence of new conditions throughout time.\nThe ability to view time series data has passed from beta and into a stable phase.\nThe log results from inspecting a panel are now downloadable as a text (.txt) file.","step-1-install-grafana-on-fedora#Step 1: Install Grafana on Fedora":"The newest version of Grafana, 6, is ready for download and installation. Complement your computer with the Grafana RPM repository.\n# vi /etc/yum.repos.d/grafana.repo Add the following content to the file:\n[grafana]name=grafanabaseurl=https://packages.grafana.com/oss/rpmrepo_gpgcheck=1enabled=1gpgcheck=1gpgkey=https://packages.grafana.com/gpg.keysslverify=1sslcacert=/etc/pki/tls/certs/ca-bundle.crt save and exit the file with escape: wq¬†To get started with grafana, add the repository and then install the grafana rpm package.\n# dnf -y install grafana ","step-2-start-grafana-service-on-fedora#Step 2: Start Grafana service on Fedora":"Using the systemctl service management command, Grafana may be launched and set to regularly start with Fedora:\n# systemctl start grafana-server # systemctl enable grafana-server # systemctl status grafana-server Simply open your web browser, navigate to http://Server IP:3000/, and you will have access to the Grafana Web Interface without having to use a reverse proxy. Take, for instance:\n# http://192.0.2.10:3000 "},"title":"How To Install Grafana on Fedora 35/34/33/32/31"},"/utho-docs/docs/linux/how-to-install-grafana-on-ubuntu-20-04/":{"data":{"":"","1-install-grafana#1. Install Grafana":"","2-install-and-configure-nginx-reverse-proxy-optional#2. Install and Configure Nginx Reverse Proxy (optional)":"","3-access-grafana-dashboard#3. Access Grafana Dashboard":"\nIntroduction In this article, you will learn how to install Grafana on ubuntu 20.04.\nGrafana is a free monitoring and data visualisation application that works across multiple platforms. It provides insightful analytics by rendering your data in graphical form and is cross-platform. It is possible to create dynamic dashboards that can be reused, to explore metrics by means of ad hoc queries, to set up alert rules for critical metrics that are constantly evaluated and notify relevant parties of any changes, and to collaborate with members of the team by means of in-built sharing. These are just some of the many features that this software possesses. In addition to that, it enables access to other systems like Graphite, Prometheus, Elasticsearch, and InfluxDB. This guide will walk you through the process of installing Grafana on a server that is running Ubuntu 18.04 or 20.04.\n1. Install Grafana The system packages are required to be updated.\n# apt update Install the necessary packages for the system.\n# apt-get install -y gnupg2 curl software-properties-common Add Grafana GPG key.\n# curl https://packages.grafana.com/gpg.key | sudo apt-key add - Install the Grafana repository on your machine.\n# add-apt-repository \"deb https://packages.grafana.com/oss/deb stable main\" Update the system‚Äôs packages up to date.\n# apt update Install Grafana\n# apt -y install grafana Start Grafana service.\n# systemctl start grafana-server Enable the Grafana service to start automatically when the system boots up.\n# systemctl enable grafana-server Check the service status.\n# systemctl status grafana-server The default port for Grafana is 3000, which means you can access it there. It is possible to utilise a reverse proxy to reroute all traffic from port 3000 to port 80, which is the location where Grafana is hosted. Continue reading to find out what to do. The Grafana web interface can be accessed through port 3000 in any other circumstance.\n2. Install and Configure Nginx Reverse Proxy (optional) Install Nginx.\n# apt install nginx -y Start Nginx service.\n# systemctl start nginx Allow the Nginx service to launch automatically.\n# systemctl enable nginx Check the current status of the Nginx service.\n# systemctl status nginx Unlink the file that is used as the default configuration.\n# unlink /etc/nginx/sites-enabled/default Create a new configuration file.\n# vi /etc/nginx/sites-available/grafana.conf In the new file, insert the code shown below, then save the changes and close the file by escape :wq\nserver { listen 80; location / { proxy_pass http://localhost:3000; } } Link and activate the new configuration file.\n# ln -s /etc/nginx/sites-available/grafana.conf /etc/nginx/sites-enabled/grafana.conf Test the configuration file.\n# service nginx configtest Restart Nginx service.\n# systemctl restart nginx 3. Access Grafana Dashboard Simply open your web browser, navigate to http://Server IP:3000/, and you will have access to the Grafana Web Interface without having to use a reverse proxy. Take, for instance:\n# http://192.0.2.10:3000 ","conclusion#Conclusion":"You have Grafana installed on your server successfully. A screen prompting you to log in will appear. Please use admin as both your username and your password. Now that you have access, you can go ahead and configure the Dashboard so that you can start controlling and analysing your data.\nHopefully, you have learned how to install Grafana on ubuntu 20.04.\nThank You üôÇ","introduction#Introduction":""},"title":"How to Install Grafana on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-hastebin-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully you have learned how to install Hastebin on Debian 10.\nThank You üôÇ","configure-hastebin-on-debian#Configure Hastebin on Debian":"Now that Hastebin has been installed, the Debian setup process may begin. To begin, just swap ports (from 7777 to 80). Modifying the config.js file with your preferred text editor and the following command will allow you to alter the port:\n# vi config.js Then replace port 7777 with port 80:\n\"port\": \"7777\", \"port\": \"80\", Then you should save the changes and close the text editor by escape :wq\nInstalling PM2 is the best option for Hastebin execution management. You can do this by typing in the following command:\n# apt install npm # npm install pm2 -g To begin using Hastebin, type in the following command:\n# pm2 start server.js At this point, you need run the following command to save the default settings:\n# pm2 save Note:¬†After executing the above command, the configuration file will be saved in the\n/root/.pm2/dump.pm2¬†directory.\nSimply type in this command to have pm2 begin loading:\n# pm2 startup If you‚Äôre ready to give Hastebin a try, all you need to do is point your browser to HTTP: / your-server-address.","introduction#Introduction":"In this article, you will learn How to install Hastebin on Debian 10.\nHastebin is a web software with basic Pastebin and Hacker-style features that makes it simple to keep track of notes and snippets. Hastebin is a website that allows you to store and distribute little pieces of text or code. You can generate a new snippet, save the current one, and change any previously created ones with the help of the open-source service Hastebin. The Just Text option allows you to edit the sample in its native plain-text format. Once you‚Äôve saved the snippet, you can send the link to other people who could benefit from it.\nHastebin uses Node.js for its server software. You can create and edit snippets via the online interface. The snippets can be uploaded to the server via the console by using the Hastebin command-line programme. Ruby is the language used to create Hastebin. Hastebin‚Äôs designers aimed to make the service as easy as possible to set up and begin using.","setup-hastebin-on-debian-10#Setup Hastebin on Debian 10":"Now that you know what Hastebin is, we‚Äôll show you how to get it up and running. First, run this command to update your system:\n# apt update It‚Äôs time to get Snaps set up on Debian 10. Snaps are a type of software package that can run on any Linux distribution, are automatically updated, and roll back if necessary. Snapd can be activated by typing in the following command:\n# apt install snapd -y The snap core should now be installed using the following command:\n# snap install core Finally, the following command must be entered to set up Haste Server:\n# snap install haste-server --beta "},"title":"How to install Hastebin on Debian 10"},"/utho-docs/docs/linux/how-to-install-hastebin-on-ubuntu-20-04/":{"data":{"":" How to Install Hastebin on Ubuntu 20.04","conclusion#Conclusion":"Hopefully, you have learned how to install Hastebin on Ubuntu 20.04.\nThank You üôÇ","install-hastebin-server#Install Hastebin Server":"1. Use a root SSH login to access your server.\n2. Get started using haste-server by cloning its GitHub project.\n# git clone https://github.com/seejohnrun/haste-server.git 3. Change to the¬†haste-server¬†directory.\n# cd haste-server 4. Install the required packages with npm.\n# apt install npm # npm install # npm update 5. Port 7777 is the default for Hastebin. In order to access the internet, you must switch to port 80 for HTTP.\nEdit¬†config.js.\n# vi config.js Change this line from 7777 to 80:\n\"port\": \"7777\", This is how it should appear once you‚Äôre done:\n\"port\": \"80\", You can use the escape:wq command to save your work and close the file.","install-pm2#Install PM2":"Node.JS applications can make use of PM2, which is a process manager. If your programme goes down, PM2 will detect it and restart it automatically.\n1. Install PM2.\n# npm install pm2 -g 2. Start your Hastebin server.\n# pm2 start server.js 3. You can save your PM2 settings and have it launch automatically.\n# pm2 save # pm2 startup ","introduction#Introduction":"In this article, you will learn how to install Hastebin on Ubuntu 20.04.\nHastebin is the most visually beautiful and user-friendly pastebin that has ever been created.The act of sharing one‚Äôs source code is beneficial, and doing so ought to be a very simple process. We find ourselves in a variety of scenarios where we need to exchange some code with another person, and in those cases, we use pastebins.\nSince Node.js forms the foundation of the Hastebin server software, setting up a self-hosted instance of Hastebin on your own server is a simple process. You can add and manage snippets through the web interface; however, the unique Hastebin command-line application makes it feasible to submit snippets to the server from the terminal. This is the case even though the web interface is available.","test-the-hastebin-server#Test the Hastebin Server":"To access an empty page where you can insert code, simply point your browser to the IP address of your server."},"title":"How to Install Hastebin on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-htmldoc-on-centos-7/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install HTMLDoc on Centos 7.\nIf you write your hypertext in the appropriate format, the HTMLDoc application will be able to dynamically convert it into a postscript (PDF 1.6) document (HTML 3.2). You will learn how to install HTMLDoc on Centos 7 by following the instructions in this guide.\nAfter the HTMLDoc environment has been established, we will create a simple document consisting of only a single page and excluding any further formatting such as headers, footers, borders, or other embellishments. In its most basic form, it is an HTML template that may be scaled up or down, depending on the dimensions of the sheet of paper that will be used to print the PDF.","making-your-first-pdf-document-from-html#Making Your First PDF Document from HTML":"Let‚Äôs put this new skill through its paces from the command line. To conduct experiments, navigate to the /tmp/ folder.\n# cd /tmp/ Now, let‚Äôs begin by generating a straightforward HTML document, which we can then apply to the production of a PDF file. We may refer to it as the microhost-source html:\n# vi microhost-source.html ","preparing-fedorax64-for-htmldoc#Preparing Fedora(x64) for HTMLDoc":"For the sake of this guide, the IPv4 protocol will be the only one used on the Centos 7 (x64) server. Keep in mind that this also applies to servers that only support IPv6. The first thing that should be done, given that the vast majority of Linux distributions are not configured to automatically install system updates or security patches, is to look for new versions of the programmes that are currently set up and running. In addition, it is often suggested, particularly when dealing with a new installation, to apply any necessary updates to the software and the kernel.\nThis is the time to use the YUM package management to look for available updates to the currently installed packages. With the YUM package manager, you may check for updates to all installed packages with the check-update command.\n# yum check-update We have completed our update scan and will now update all of the software on our system, including any dependencies. The following order will be executed to execute this:\n# yum update -y Now that all of our software packages and the kernel have been brought up to date, we can proceed with the installation of HTMLDoc:\n# yum install htmldoc You may now create PDF files directly from HTML code.","simply-insert-the-following-html-code#Simply insert the following HTML code:":" \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy first PDF from HTML\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e This is the body of my first PDF document made from HTML. \u003c/body\u003e \u003c/html\u003e Currently, you can save the file by typing escape:wq.\nYou can now instruct HTMLDoc to parse a PDF document from your microhost-source.html file by using the following command:\n# htmldoc --webpage -f postscript-output.pdf microhost-source.html You will now have a new file that has been given the name postscript-output.pdf. The title of this file will read ‚ÄúMy first PDF from HTML,‚Äù and the body of the file will have the text ‚ÄúThis is the body of my first PDF document that was generated from HTML.‚Äù Congratulations, you have successfully learnt how to convert straightforward HTML markup into documents that can be exported as PostScript or PDF files.\nHopefully, you have learned how to install HTMLDoc on Centos 7.\nThank You üôÇ"},"title":"How to Install HTMLDoc on Centos 7"},"/utho-docs/docs/linux/how-to-install-htmldoc-on-debian-10/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install HTMLDoc on Debian 10.\nIf you write your hypertext in the appropriate format, the HTMLDoc application will be able to dynamically convert it into a postscript (PDF 1.6) document (HTML 3.2). You will learn how to install HTMLDoc on Debian 10 by following the instructions in this guide.\nAfter the HTMLDoc environment has been established, we will create a simple document consisting of only a single page and excluding any further formatting such as headers, footers, borders, or other embellishments. In its most basic form, it is an HTML template that may be scaled up or down, depending on the dimensions of the sheet of paper that will be used to print the PDF.","making-your-first-pdf-document-from-html#Making Your First PDF Document from HTML":"Let‚Äôs put this new skill through its paces from the command line. To conduct experiments, navigate to the /tmp/ folder.\n# cd /tmp/ Now, let‚Äôs begin by generating a straightforward HTML document, which we can then apply to the production of a PDF file. We may refer to it as the microhost-source html:\n# vi microhost-source.html ","preparing-debian-10x64-for-htmldoc#Preparing Debian 10(x64) for HTMLDoc":"For the sake of this guide, the IPv4 protocol will be the only one used on the Debian 10 (x64) server. Keep in mind that this also applies to servers that only support IPv6. The first thing that should be done, given that the vast majority of Linux distributions are not configured to automatically install system updates or security patches, is to look for new versions of the programmes that are currently set up and running. In addition, it is often suggested, particularly when dealing with a new installation, to apply any necessary updates to the software and the kernel.\nNow is the time for us to figure out whether or not there are any new versions or updates for.\n# apt-get update Now that HTMLDoc is available, we can install:\n# apt-get install htmldoc -y You may now create PDF files directly from HTML code.","simply-insert-the-following-html-code#Simply insert the following HTML code:":" \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy first PDF from HTML\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e This is the body of my first PDF document made from HTML. \u003c/body\u003e \u003c/html\u003e Currently, you can save the file by typing escape:wq.\nYou can now instruct HTMLDoc to parse a PDF document from your microhost-source.html file by using the following command:\n# htmldoc --webpage -f postscript-output.pdf microhost-source.html You will now have a new file that has been given the name postscript-output.pdf. The title of this file will read ‚ÄúMy first PDF from HTML,‚Äù and the body of the file will have the text ‚ÄúThis is the body of my first PDF document that was generated from HTML.‚Äù Congratulations, you have successfully learnt how to convert straightforward HTML markup into documents that can be exported as PostScript or PDF files.\nHopefully, you have learned how to install HTMLDoc on Debian 10.\nThank You üôÇ"},"title":"How to Install HTMLDoc on Debian 10"},"/utho-docs/docs/linux/how-to-install-htmldoc-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install HTMLDoc on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install HTMLDoc on Debian 12.\nIf you write your hypertext in the appropriate format, the HTMLDoc application will be able to dynamically convert it into a postscript (PDF 1.6) document (HTML 3.2). You will learn how to install HTMLDoc on Debian 10 by following the instructions in this guide.\nAfter the HTMLDoc environment has been established, we will create a simple document consisting of only a single page and excluding any further formatting such as headers, footers, borders, or other embellishments. In its most basic form, it is an HTML template that may be scaled¬†up or down, depending on the dimensions of the sheet of paper that will be used to print the PDF.\nPreparing Debian 12(x64) for HTMLDoc For the sake of this guide, the IPv4 protocol will be the only one used on the Debian 10 (x64) server. Keep in mind that this also applies to servers that only support IPv6. The first thing that should be done, given that the vast majority of Linux distributions are not configured to automatically install system updates or security patches, is to look for new versions of the programmes that are currently set up and running. In addition, it is often suggested, particularly when dealing with a new installation, to apply any necessary updates to the software and the kernel.\nNow is the time for us to figure out whether or not there are any new versions or updates for.\n# apt-get update Now that HTMLDoc is available, we can install:\n# apt-get install htmldoc -y You may now create PDF files directly from¬†HTML¬†code.\nMaking Your First PDF Document from HTML Let‚Äôs put this new skill through its paces from the command line. To conduct experiments, navigate to the /tmp/ folder.\n# cd /tmp/ Now, let‚Äôs begin by generating a straightforward HTML document, which we can then apply to the production of a PDF file. We may refer to it as the microhost-source html:\n# vi utho-source.html Simply insert the following HTML code: \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy first PDF from HTML\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e This is the body of my first PDF document made from HTML. \u003c/body\u003e \u003c/html\u003e Currently, you can save the file by typing escape:wq.\nYou can now instruct HTMLDoc to parse a PDF document from your microhost-source.html file by using the following command:\n# htmldoc --webpage -f postscript-output.pdf utho-source.html You will now have a new file that has been given the name postscript-output.pdf. The title of this file will read ‚ÄúMy first PDF from HTML,‚Äù and the body of the file will have the text ‚ÄúThis is the body of my first PDF document that was generated from HTML.‚Äù Congratulations, you have successfully learnt how to convert straightforward HTML markup into documents that can be exported as PostScript or PDF files."},"title":"How to install HTMLDoc on Debian 12"},"/utho-docs/docs/linux/how-to-install-htmldoc-on-debian-9/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install HTMLDoc on Debian 9.\nIf you write your hypertext in the appropriate format, the HTMLDoc application will be able to dynamically convert it into a postscript (PDF 1.6) document (HTML 3.2). You will learn how to install HTMLDoc on Debian 9 by following the instructions in this guide.\nAfter the HTMLDoc environment has been established, we will create a simple document consisting of only a single page and excluding any further formatting such as headers, footers, borders, or other embellishments. In its most basic form, it is an HTML template that may be scaled up or down, depending on the dimensions of the sheet of paper that will be used to print the PDF.","making-your-first-pdf-document-from-html#Making Your First PDF Document from HTML":"Let‚Äôs put this new skill through its paces from the command line. To conduct experiments, navigate to the /tmp/ folder.\n# cd /tmp/ Now, let‚Äôs begin by generating a straightforward HTML document, which we can then apply to the production of a PDF file. We may refer to it as the microhost-source html:\n# vi microhost-source.html ","preparing-debian-9-for-htmldoc#Preparing Debian 9 for HTMLDoc":"For the sake of this guide, the IPv4 protocol will be the only one used on the Debian 10 (x64) server. Keep in mind that this also applies to servers that only support IPv6. The first thing that should be done, given that the vast majority of Linux distributions are not configured to automatically install system updates or security patches, is to look for new versions of the programmes that are currently set up and running. In addition, it is often suggested, particularly when dealing with a new installation, to apply any necessary updates to the software and the kernel.\nNow is the time for us to figure out whether or not there are any new versions or updates for.\n# apt-get update Now that HTMLDoc is available, we can install:\n# apt-get install htmldoc -y You may now create PDF files directly from HTML code.","simply-insert-the-following-html-code#Simply insert the following HTML code:":" \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy first PDF from HTML\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e This is the body of my first PDF document made from HTML. \u003c/body\u003e \u003c/html\u003e Currently, you can save the file by typing escape:wq.\nYou can now instruct HTMLDoc to parse a PDF document from your microhost-source.html file by using the following command:\n# htmldoc --webpage -f postscript-output.pdf microhost-source.html You will now have a new file that has been given the name postscript-output.pdf. The title of this file will read ‚ÄúMy first PDF from HTML,‚Äù and the body of the file will have the text ‚ÄúThis is the body of my first PDF document that was generated from HTML.‚Äù Congratulations, you have successfully learnt how to convert straightforward HTML markup into documents that can be exported as PostScript or PDF files.\nHopefully, you have learned how to install HTMLDoc on Debian 9.\nThank You üôÇ"},"title":"How to Install HTMLDoc on Debian 9"},"/utho-docs/docs/linux/how-to-install-htmldoc-on-fedora/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install HTMLDoc on Fedora.\nIf you write your hypertext in the appropriate format, the HTMLDoc application will be able to dynamically convert it into a postscript (PDF 1.6) document (HTML 3.2). You will learn how to install HTMLDoc on Fedora by following the instructions in this guide.\nAfter the HTMLDoc environment has been established, we will create a simple document consisting of only a single page and excluding any further formatting such as headers, footers, borders, or other embellishments. In its most basic form, it is an HTML template that may be scaled up or down, depending on the dimensions of the sheet of paper that will be used to print the PDF.","making-your-first-pdf-document-from-html#Making Your First PDF Document from HTML":"Let‚Äôs put this new skill through its paces from the command line. To conduct experiments, navigate to the /tmp/ folder.\n# cd /tmp/ Now, let‚Äôs begin by generating a straightforward HTML document, which we can then apply to the production of a PDF file. We may refer to it as the microhost-source html:\n# vi microhost-source.html ","preparing-fedorax64-for-htmldoc#Preparing Fedora(x64) for HTMLDoc":"For the sake of this guide, the IPv4 protocol will be the only one used on the Fedora (x64) server. Keep in mind that this also applies to servers that only support IPv6. The first thing that should be done, given that the vast majority of Linux distributions are not configured to automatically install system updates or security patches, is to look for new versions of the programmes that are currently set up and running. In addition, it is often suggested, particularly when dealing with a new installation, to apply any necessary updates to the software and the kernel.\nNow that HTMLDoc is available, we can install:\n# dnf install htmldoc -y You may now create PDF files directly from HTML code.","simply-insert-the-following-html-code#Simply insert the following HTML code:":" \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy first PDF from HTML\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e This is the body of my first PDF document made from HTML. \u003c/body\u003e \u003c/html\u003e Currently, you can save the file by typing escape:wq.\nYou can now instruct HTMLDoc to parse a PDF document from your microhost-source.html file by using the following command:\n# htmldoc --webpage -f postscript-output.pdf microhost-source.html You will now have a new file that has been given the name postscript-output.pdf. The title of this file will read ‚ÄúMy first PDF from HTML,‚Äù and the body of the file will have the text ‚ÄúThis is the body of my first PDF document that was generated from HTML.‚Äù Congratulations, you have successfully learnt how to convert straightforward HTML markup into documents that can be exported as PostScript or PDF files.\nHopefully, you have learned how to install HTMLDoc on Fedora.\nThank You üôÇ"},"title":"How to Install HTMLDoc on Fedora"},"/utho-docs/docs/linux/how-to-install-htmldoc-on-ubuntu-20-04/":{"data":{"":"","introduction#Introduction":"In this article, you will learn how to install HTMLDoc on Ubuntu 20.04.\nIf you write your hypertext in the appropriate format, the HTMLDoc application will be able to dynamically convert it into a postscript (PDF 1.6) document (HTML 3.2). You will learn how to install HTMLDoc on Ubuntu 20.04 by following the instructions in this guide.\nAfter the HTMLDoc environment has been established, we will create a simple document consisting of only a single page and excluding any further formatting such as headers, footers, borders, or other embellishments. In its most basic form, it is an HTML template that may be scaled up or down, depending on the dimensions of the sheet of paper that will be used to print the PDF.","making-your-first-pdf-document-from-html#Making Your First PDF Document from HTML":"Let‚Äôs put this new skill through its paces from the command line. To conduct experiments, navigate to the /tmp/ folder.\n# cd /tmp/ Now, let‚Äôs begin by generating a straightforward HTML document, which we can then apply to the production of a PDF file. We may refer to it as the microhost-source html:\n# vi microhost-source.html ","preparing-ubuntu-2004-x64-for-htmldoc#Preparing Ubuntu 20.04 (x64) for HTMLDoc":"For the sake of this guide, the IPv4 protocol will be the only one used on the Ubuntu 20.04 (x64) server. Keep in mind that this also applies to servers that only support IPv6. The first thing that should be done, given that the vast majority of Linux distributions are not configured to automatically install system updates or security patches, is to look for new versions of the programmes that are currently set up and running. In addition, it is often suggested, particularly when dealing with a new installation, to apply any necessary updates to the software and the kernel.\nNow is the time for us to figure out whether or not there are any new versions or updates for.\n# apt-get update Now that HTMLDoc is available, we can install:\n# apt-get install htmldoc -y You may now create PDF files directly from HTML code.","simply-insert-the-following-html-code#Simply insert the following HTML code:":" \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy first PDF from HTML\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e This is the body of my first PDF document made from HTML. \u003c/body\u003e \u003c/html\u003e Currently, you can save the file by typing escape:wq.\nYou can now instruct HTMLDoc to parse a PDF document from your microhost-source.html file by using the following command:\n# htmldoc --webpage -f postscript-output.pdf microhost-source.html You will now have a new file that has been given the name postscript-output.pdf. The title of this file will read ‚ÄúMy first PDF from HTML,‚Äù and the body of the file will have the text ‚ÄúThis is the body of my first PDF document that was generated from HTML.‚Äù Congratulations, you have successfully learnt how to convert straightforward HTML markup into documents that can be exported as PostScript or PDF files.\nHopefully, you have learned how to install HTMLDoc on Ubuntu 20.04.\nThank You üôÇ"},"title":"How to Install HTMLDoc on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-ibm-http-server-in-linux/":{"data":{"":" How to install IBM Http server\nIn this tutorial, we will learn how to install IBM http server in linux, hereafter referred to as the IHS,¬†using command line interface. AIX¬Æ, HP-UX, Linux, Solaris, Windows, and z/OS¬Æ all work with IBM HTTP Server, Version 8.5.5. This information applies to Version 8.5.5 and all releases and changes that come after it until new editions say otherwise.","prerequisites#Prerequisites":" Need to have IBM id to install IBM WAS Super User or any normal user with SUDO privileges. IBM Installation Manager installed machine. You can¬†install IBM installation manager¬†using¬†this guide. IBM WAS installed machine. You can install IBM WAS( Websphere Application Server) using this guide. ","steps-to-install-ibm#Steps to install IBM":"Step 1. Installation of Installation manager, creates a new directory in /opt named as IBM. Now go to this directory and list all available files\ncd /opt/IBM/InstallationManager/eclipse/tools/ Step 2. Now find and save or copy either the¬†online BASE repository¬†or the¬†composite repository¬†to install WAS. At this time, following are the above mentioned repositories. In this tutorial we will use composite repositories to install IBM WAS.\nRepositories to install IBM WAS\nStep 3. List all the available packages in the given repository. As mentioned, we will use the composite repository. And at the time of writing this tutorial,¬†https://www.ibm.com/software/repositorymanager/V9WASBase¬†is the composite repository.\n./imcl listAvailablePackages -repositories https://www.ibm.com/software/repositorymanager/V9WASBase -prompt Note that, here you will be asked to enter the IBM login credentials, such as email id and password. To enter the same, press ‚ÄòP‚Äô then ‚Äòemail-id‚Äô and then password\nLists of available packages in repo\nIf you are using composite repositories, you will see java jdk, websphere appClient, websphere base, websphere IHS etc. and in base repo, you will see only java jdk and websphere IHS packages.\nStep 4. Now copy the name of the latest IHS and JDK package and execute the below command\n./imcl install com.ibm.websphere.IHS.v90_9.0.5012.20220513_1431 com.ibm.java.jdk.v8_8.0.7016.20220915_1446¬†-repositories https://www.ibm.com/software/repositorymanager/V9WASBase -prompt -showProgress -acceptLicense Step 5 Now install the best HTTP plugins. Before that, create a directory in which you will install it.\nmkdir /opt/IBM/WebSphere/Plugins ./imcl install com.ibm.websphere.PLG.v90_9.0.5012.20220513_1431 com.ibm.java.jdk.v8_8.0.7016.20220915_1446 -repositories https://www.ibm.com/software/repositorymanager/V9WASBase -prompt -showProgress -acceptLicense -installationDirectory /opt/IBM/WebSphere/Plugins Step 6. After installing plugins, client can make necessary changes in http configuration file( In IBM http server, config file exists as- /opt/IBM/HTTPServer/conf/httpd.conf) to enable the http plugins\nStep 7. You can check the right installation of http server by¬†cd /opt/IBM/HTTPServer/bin ./apachectl start // and to stop, use ./apachectl stop Now go to browser and hit your server IP. You will see something like this.\nSuccessfull installation of IBM HTTP server\nAnd this is how you will install IBM Http server in linux using command line interface."},"title":"Install IBM HTTP server in Linux"},"/utho-docs/docs/linux/how-to-install-java-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Java on Almalinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how install Java on Almalinux 8.\nThe Java Platform comes in three different versions: Standard Edition (SE), Enterprise Edition (EE), and Micro Edition (ME). This guide is all about Java SE (Java Platform, Standard Edition). Almost all Java software that is free and open source is made to work with Java SE.\nThe Java Runtime Environment (JRE) and the Java Development Kit (JDK) are both Java SE packages that can be installed (JDK). JRE is a version of the Java Virtual Machine (JVM), which lets you run Java programmes and applets that have already been built. The JDK comes with the JRE and other software needed to write, develop, and compile Java programmes and applets.\nStep 1: Installing OpenJDK To install the OpenJDK using yum, you can run dnf install java.\n# dnf install java If you don‚Äôt specify a version when you try to install Java, the most common stable version of the OpenJDK JRE will be used. As you can see from this output, that is java-1.8.0-openjdk as of this writing.\nYou should now have a Java setup that works. To make sure, run java -version to see what version of Java is currently installed in your environment:\n# java -version To install the full OpenJDK JDK, you need to install the package with the name -devel added to it. Java follows this common practise for development packages for other programming environments.\n# dnf install java-devel Step 2: Installing Other OpenJDK Releases OpenJDK¬†has changed the way it numbers its versions so that they are more in line with Oracle Java releases. Like with 1.8.0, you can put the version number in the package name to install a newer version of OpenJDK. To install OpenJDK 17, for example, you can type yum install java-17-openjdk:\n# dnf install java-11-openjdk-devel Step 3: Setting Your Default Java Version If you have more than one version of Java installed, you may want to make one the default (i.e. the one that will run when a user runs the java command). Also, some programmes need certain environment variables to be set in order to find out which version of Java to use.\nThe default Java version can be set by using the alternatives command, which uses symbolic links to manage the default commands. Use alternatives ‚Äìconfig java: to see a list of the versions of Java that alternatives can handle.\n# alternatives --config java Enter the number of the a selection to choose which Java executable to use by default. It will change the symbolic links on your system so that the java command points to the right set of libraries. You can run this command as many times as you want, and each time the output of java -version should change:\n# java -version "},"title":"How to install Java on Almalinux 8"},"/utho-docs/docs/linux/how-to-install-java-on-fedora-based-linux/":{"data":{"":"","1--installing-openjdk#1 ‚Äì Installing OpenJDK":"The Java Platform comes in three different versions: Standard Edition (SE), Enterprise Edition (EE), and Micro Edition (ME). This guide is all about Java SE (Java Platform, Standard Edition). Almost all Java software that is free and open source is made to work with Java SE.\nThe Java Runtime Environment (JRE) and the Java Development Kit (JDK) are both Java SE packages that can be installed (JDK). JRE is a version of the Java Virtual Machine (JVM), which lets you run Java programmes and applets that have already been built. The JDK comes with the JRE and other software needed to write, develop, and compile Java programmes and applets.\nJava can also be run in two different ways: OpenJDK and Oracle Java. Both implementations are mostly based on the same code, but OpenJDK, which is the reference implementation of Java, is completely open source while Oracle Java has some code that is only available to Oracle. Most Java programmes will work fine with either implementation, but you should use the one that your software needs.\nJava can be installed in different versions and releases, but most people only need one installation. So, try to only install the version of Java you need to run or build your app (s).\nThis section will show you how to install the prebuilt OpenJDK JRE and JDK packages using the¬†yum¬†package manager.\nTo install the OpenJDK using yum, you can run yum install java\nyum install java If you don‚Äôt specify a version when you try to install Java, the most common stable version of the OpenJDK JRE will be used. aAs you can see from this output, that is java-1.8.0-openjdk as of this writing.\nInstalled java on Fedora server\nYou should now have a Java setup that works. To make sure, run java -version to see what version of Java is currently installed in your environment:\njava -version Current version of Java\nTo install the full OpenJDK JDK, you need to install the package with the name -devel added to it. Java follows this common practise for development packages for other programming environments.\nyum install java-devel Package size to install java","2-installing-other-openjdk-releases#2-¬†Installing Other OpenJDK Releases":"OpenJDK has changed the way it numbers its versions so that they are more in line with Oracle Java releases. Like with 1.8.0, you can put the version number in the package name to install a newer version of OpenJDK. To install OpenJDK 17, for example, you can type yum install java-17-openjdk:\nyum install java-11-openjdk-devel ","3--setting-your-default-java-version#3- Setting Your Default Java Version":"If you have more than one version of Java installed, you may want to make one the default (i.e. the one that will run when a user runs the java command). Also, some programmes need certain environment variables to be set in order to find out which version of Java to use.\nThe default Java version can be set by using the alternatives command, which uses symbolic links to manage the default commands. Use alternatives ‚Äìconfig java: to see a list of the versions of Java that alternatives can handle.\nalternatives --config java Alternatives of java version\nEnter the number of the a selection to choose which Java executable to use by default. It will change the symbolic links on your system so that the java command points to the right set of libraries. You can run this command as many times as you want, and each time the output of java -version should change:\njava -version Latest version of java\nHopefully now you can Install Java on CentOS server.\nMust Read : How to install Gradle on CentOS 7","introduction#Introduction":"In this Article you will know How to Install Java on CentOS Server. Red Hat Enterprise Linux, CentOS, Fedora, and Rocky Linux are all types of Linux. Java is a popular programming language and software platform that lets you run many applications that run on the server.","prerequisites#Prerequisites":" Any super user( root ) or any normal user with SUDO privileges yum repository configured server "},"title":"How To Install Java on CentOS server"},"/utho-docs/docs/linux/how-to-install-java-on-fedora-server/":{"data":{"":" How to install Java on Fedora server\nIn this tutorial, we will learn how to install Java on Fedora server. Linux comes in many different flavours, such as Red Hat Enterprise Linux, CentOS, Fedora, and Rocky Linux. Java is a popular programming language and software platform that lets users run many different programmes that are meant to run on a server.","1--installing-java--openjdk#1 ‚Äì Installing Java- OpenJDK":"Users can choose from Standard Edition (SE), Enterprise Edition (EE), and Micro Edition, which are all different versions of the Java Platform (ME). This whole book is all about Java SE (Java Platform, Standard Edition). Java software that is both open source and free is usually made to work with Java SE.\nThe Java Runtime Environment (JRE) and the Java Development Kit (JDK) are both Java SE packages that can be downloaded and installed on a computer (JDK). The Java Runtime Environment (JRE) is a version of the Java Virtual Machine (JVM) that lets you run already-made applets and Java programmes. The Java Development Kit (JDK) comes with the Java Runtime Environment (JRE) and other software needed to design, build, and develop Java programmes and applets.\nOpenJDK and Oracle Java are two more good choices for running Java programmes. OpenJDK, which is the reference version of Java, is completely open source. On the other hand, Oracle Java has some code that only Oracle can access and that no one else can. But most of the code for both implementations is the same. Either Java implementation should be able to run the vast majority of Java programmes without problems, but you should choose the implementation that your software needs.\nJava can be installed in a number of different versions and releases, but most users only need to do a single installation. So, it‚Äôs important to make sure you only install the version of Java that your application needs to create or run (s).\nUsing the dnf package manager, you can use the command dnf install java to install the OpenJDK.\n# dnf install java If you try to install Java without choosing a version, the OpenJDK JRE version that is the most popular stable version will be used. a From this output, you can see that the version of Java that is being used right now is 1.8.0-openjdk.\nInstalled java on Fedora server\nYou should now have a properly working Java installation. To be sure, you can use the command java -version: to find out which version of Java is installed on your computer.\njava -version Current version of Java\nTo get the full OpenJDK JDK, you need to install the package with the name ‚Äú-devel‚Äù added to it. This is a common way to make development packages for other programming environments, and Java does the same thing.\ndnf install java-devel ","2---install-other-openjdk-releases#2 - Install Other OpenJDK Releases":"OpenJDK has changed how it numbers its versions so that they are more in line with Oracle Java releases. Like with 1.8.0, you can install a newer version of OpenJDK by putting the version number in the package name. For example, to install OpenJDK 17, type dnf install java-17-openjdk:\n# dnf install java-11-openjdk-devel ","3---setting-your-default-java-version#3 - Setting Your Default Java Version":"If you have more than one version of Java installed, you should probably make one of them the default version (i.e. the one that will run when a user runs the java command). Also, some applications need certain environment variables to be set in order to figure out which version of Java should be used.\nTo change the default Java version, you need to use the alternatives command, which uses symbolic links to control the default commands. Use the alternatives ‚Äìconfig java: command to see a list of the different versions of Java that alternatives can work with.\nalternatives --config java Alternatives of java version\nEnter the number that corresponds to the option to choose the Java executable that will be used by default. It will change the symbolic links on your computer so that the java command will point to the right set of libraries. You can run this command as many times as you want, and each time java -version should show a different value:\njava -version latest version of Java\nSo this is how you will learn how to install Java on Fedora server","prerequisites#Prerequisites":" Any super user( root ) or any normal user with SUDO privileges dnf repository configured server "},"title":"How To Install Java on Fedora server"},"/utho-docs/docs/linux/how-to-install-jenkins-on-centos-7/":{"data":{"":"","1-install-openjdk-8-package-on-centos-7#1: Install OpenJDK 8 package on Centos 7":"","2-install-jenkins-repository#2. Install Jenkins repository":"","3-adjust-the-firewall#3. Adjust the Firewall¬†":"","4-setting-up-jenkins#4. Setting Up Jenkins¬†":" How to install Jenkins on CentOS 7\nYou will learn how to install Jenkins on CentOS 7 or RHEL 7 in this article. Jenkins is a software platform that supports both Continuous Delivery (CD) and Continuous Integration (CI). It is used to automate software testing, development, delivery, and deployment. Ubuntu‚Äôs Jenkins software generates a powerful management tool that enhances the development process.\nPrerequisites: A super user or any normal user with SUDO privileges\nYum repository configured CentOS server\n1: Install OpenJDK 8 package on Centos 7 Java Runtime Environment is necessary for Jenkins (JRE). For the Java environment, OpenJDK is used in this manual. The Java Runtime Environment is part of the development kit known as OpenJDK. Java may be installed in many versions on Ubuntu. Make sure Java 8 or Java 11 is specified as the default version if you decide to do this. For more information on how to install JDK on Centos server, you can refer this.\nStep 1.1: Verify that Java is already installed on your CentOS server\njava -version Java binary not found\nSince Java isn‚Äôt already installed on our PC, we‚Äôll use OpenJDK to do it.\ninfo ! Step 2 may be skipped if Java is already installed on your Ubuntu system\n2. Install Jenkins repository Step 2.1: The next step is to turn on the repository for Jenkins. To do this, use the following curl command to bring in the GPG key:\ncurl --silent --location http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo | sudo tee /etc/yum.repos.d/jenkins.repo And here‚Äôs how to add the repository to your system:\nrpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key Step 2.2: And here‚Äôs how to add the repository to your system: Once the repository is turned on, type: yum install latest stable Jenkins to install it.\nyum install jenkins Once the installation is done, you can start the Jenkins service by:\nsystemctl enable --now jenkins 3. Adjust the Firewall¬†Step 3.1: You need to port 8080 if you are installing Jenkins on a remote CentOS server that is protected by a firewall.\nUse these commands to open the port you need:\nfirewall-cmd --permanent --zone=public --add-port=8080/tcp firewall-cmd --reload 4. Setting Up Jenkins¬†Open your browser and type your domain or IP address followed by port 8080 to set up your new Jenkins installation:\nhttp://your_ip_or_domain:8080 You will see a screen like the one below, which will ask you to enter the Administrator password that was made during the installation:\nHomepage of Jenkins after installation\nUse the following command to get your terminal to show the password:\ncat /var/lib/jenkins/secrets/initialAdminPassword You should see an alphanumeric password with 32 characters.\n9836b548f4e99a203ee98s68232a32 Copy the password from your terminal, paste it into the Administrator password field, and then click Continue.\nNext step of installation\nOn the next screen, you‚Äôll be asked if you want to install the recommended plugins or choose your own. If you click on the box that says Install suggested plugins, the process of installing them will start right away.\nInstalling required packages\nAfter the installation is done, you will be asked to set up the first administrative user. Fill in all the required information and click Save and Continue.\nFirst Admin user created\nYou will be asked to set the URL for the Jenkins instance on the next page. A URL will be automatically made and put in the URL field.\nFinal step of installing\nTo finish setting up, click the Save and Finish button to confirm the URL.\nInstalled Jenkins\nLastly, click on the button that says ‚ÄúStart using Jenkins.‚Äù This will take you to the Jenkins dashboard, where you can log in as the admin user you made in one of the earlier steps.\nIf you‚Äôve reached this point, you‚Äôve successfully installed Jenkins on your CentOS7 system.","prerequisites#Prerequisites:":""},"title":"How to install Jenkins on CentOS 7"},"/utho-docs/docs/linux/how-to-install-jenkins-on-debian-10/":{"data":{"":"","1-install-java#1: Install Java:":"","2-add-the-jenkins-repository-and-install-jenkins#2: Add the Jenkins Repository and install Jenkins":"","3-start-the-jenkins-services#3. Start the Jenkins services":"","4-setup-you-jenkins#4. Setup you Jenkins":" How to install Jenkins on Debian 10\nLearn how to install Jenkins on Debian 10 in this post. Jenkins is a software platform that supports both Continuous Delivery (CD) and Continuous Integration (CI) (CD). It is used to automate software testing, development, delivery, and deployment. Ubuntu‚Äôs Jenkins software generates a powerful management tool that enhances the development process.\nPrerequisites: A Debian 10 or Debian 11 installed server A super user( root) or any user with SUDO privileges. 1: Install Java: Java Runtime Environment is necessary for Jenkins (JRE). For the Java environment, OpenJDK is used in this manual. The Java Runtime Environment is part of the development kit known as OpenJDK. Java may be installed in many versions on Ubuntu. Make sure Java 8 or Java 11 is specified as the default version if you decide to do this.\nStep 1.1 Verify that Java is already installed on your Ubuntu system.\njava -version Java binary not found\nSince Java isn‚Äôt already installed on our PC, we‚Äôll use OpenJDK to do it.\ninfo ! Step 2 may be skipped if Java is already installed on your Ubuntu system\nStep 1.2: Open a terminal window first, then execute the following command to update the system package repository:\napt update Step 1.3: Run one of the following instructions depending on whether Java 8 or 11 is the version you wish to instal:\napt install openjdk-8-jdk -y or\napt install openjdk-11-jdk -y 2: Add the Jenkins Repository and install Jenkins Step 2.1: By importing and applying the GPG keys to the system, Jenkins may be quickly deployed on Ubuntu.\ncurl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee /usr/share/keyrings/jenkins-keyring.asc \u003e /dev/null Step 2.2: After adding GPG keys, use the following command to add the Jenkins package URL to the sources list:\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ ``` /etc/apt/sources.list.d/jenkins.list \u003e /dev/null Step 2.3: The system‚Äôs APT cache only has to be updated once when the Jenkins repository has been enabled.\napt update Step 2.4: Let‚Äôs proceed and instal Jenkins in its entirety.\napt install jenkins -y 3. Start the Jenkins services Step 3.1: When Jenkins is installed, the service ought to start automatically. Type the command shown below to check the status of the Jenkins service.\nsystemctl status jenkins Step 3.2: In my instance, it is active; however, if it is not in your case, start by running the following command:\nsystemctl enable --now jenkins 4. Setup you Jenkins Step 4.1: When you enter your domain name or IP address and port 8080 into your browser‚Äôs address bar to set up Jenkins, the Unlock Jenkins screen that prompts for a password should appear, as illustrated in the example below.\nHomepage of Jenkins\nStep 4.2: Using the cat command on the terminal, you may get the password from the specified place. The following would be the command to get the password:\ncat /var/lib/jenkins/secrets/initialAdminPassword In this article, we have learnt how to install Jenkins on debian 10 using OpenJDK 8 and OpenJDK 11.\nAlso Read:¬†How to install Tomcat 10 on Ubuntu server,¬†How To Install Java on Fedora server","prerequisites#Prerequisites:":""},"title":"How to install Jenkins on Debian 10"},"/utho-docs/docs/linux/how-to-install-jenkins-on-fedora-server/":{"data":{"":"","1-install-openjdk-8-package-on-fedora#1: Install OpenJDK 8 package on Fedora":"","2-install-jenkins-repository#2. Install Jenkins repository":"","3-adjust-the-firewall#3. Adjust the Firewall¬†":"","4-setting-up-jenkins#4. Setting Up Jenkins¬†":" How to install Jenkins on Fedora\nYou will learn how to install Jenkins on Fedora server or on RHEL 7/8/9 in this article. Jenkins is a software platform that supports both Continuous Delivery (CD) and Continuous Integration (CI). It is used to automate software testing, development, delivery, and deployment. Ubuntu‚Äôs Jenkins software generates a powerful management tool that enhances the development process.\nPrerequisites: A super user or any normal user with SUDO privileges\nYum repository configured Fedora server\n1: Install OpenJDK 8 package on Fedora Java Runtime Environment¬†is necessary for Jenkins (JRE). For the Java environment, OpenJDK is used in this manual. The Java Runtime Environment is part of the development kit known as OpenJDK. Java may be installed in many versions on Ubuntu. Make sure Java 8 or Java 11 is specified as the default version if you decide to do this. For more information on how to install JDK on Fedora server, you can¬†refer this.\nStep 1.1:¬†Verify that Java is already installed on your Fedora server\njava -version Java binary not found\nSince Java isn‚Äôt already installed on our PC, we‚Äôll use OpenJDK to do it.\nInfo! Step 2 may be skipped if Java is already installed on your Ubuntu system\n2. Install Jenkins repository Step 2.1:¬†The next step is to turn on the repository for Jenkins. To do this, use the following curl command to bring in the GPG key:\ncurl --silent --location http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo | sudo tee /etc/yum.repos.d/jenkins.repo And here‚Äôs how to add the repository to your system:\nrpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key Step 2.2:¬†And here‚Äôs how to add the repository to your system: Once the repository is turned on, type: yum install latest stable Jenkins to install it.\nyum install jenkins Once the installation is done, you can start the Jenkins service by:\nsystemctl enable --now jenkins 3. Adjust the Firewall¬†Step 3.1:¬†You need to port 8080 if you are installing Jenkins on a remote Fedora server that is protected by a firewall.\nUse these commands to open the port you need:\nfirewall-cmd --permanent --zone=public --add-port=8080/tcp firewall-cmd --reload 4. Setting Up Jenkins¬†Open your browser and type your domain or IP address followed by port 8080 to set up your new Jenkins installation:\nhttp://your_ip_or_domain:8080 You will see a screen like the one below, which will ask you to enter the Administrator password that was made during the installation:\nHomepage of Jenkins after installation\nUse the following command to get your terminal to show the password:\ncat /var/lib/jenkins/secrets/initialAdminPassword You should see an alphanumeric password with 32 characters.\n9836b548f4e99a203ee98s68232a32 Copy the password from your terminal, paste it into the Administrator password field, and then click Continue.\nNext step of installation\nOn the next screen, you‚Äôll be asked if you want to install the recommended plugins or choose your own. If you click on the box that says Install suggested plugins, the process of installing them will start right away.\nInstalling required packages\nAfter the installation is done, you will be asked to set up the first administrative user. Fill in all the required information and click Save and Continue.\nFirst Admin user created\nYou will be asked to set the URL for the Jenkins instance on the next page. A URL will be automatically made and put in the URL field.\nFinal step of installing\nTo finish setting up, click the Save and Finish button to confirm the URL.\nInstalled Jenkins\nLastly, click on the button that says ‚ÄúStart using Jenkins.‚Äù This will take you to the Jenkins dashboard, where you can log in as the admin user you made in one of the earlier steps.\nIf you‚Äôve reached this point, you‚Äôve successfully installed Jenkins on your Fedora system.","prerequisites#Prerequisites:":""},"title":"How to install Jenkins on Fedora server"},"/utho-docs/docs/linux/how-to-install-jshon-on-ubuntu-20-04/":{"data":{"":"\nDescription\nIn this article, we will acquire new knowledge. How to Install Jshon on Ubuntu 20.04 JSON is a free and open-source command-line utility that can be used to read, generate, and parse JSON documents. It is designed to be a replacement for all of the unstable JSON parsers that are made from grep, sed, and awk, in addition to the heavyweight one-line parsers that are made from Python and Perl. You can load data into Jshon from a file or from the standard input, then execute actions on the data, and finally display the result on the standard output. It is very simple to use and simple to operate with. Additionally, it can be installed on virtually all of the most common operating systems, including Linux. In this section, we are going to examine the procedures required to install Jshon on computers running Ubuntu 20.04 LTS.\nLearn more by following the instructions below How to Install Jshon on Ubuntu 20.04.","step-1-update-server#Step 1: Update Server":"install all the available updates and then update the packages to the current version by using the sudo apt update and sudo apt upgrade commands, as shown below.\napt update \u0026\u0026 sudo apt upgrade ","step-2-install-jshon#Step 2: Install Jshon":"Using the instruction sudo apt install jshon, which will be demonstrated below, you will be able to install the jshon utility from the default repository that comes with Ubuntu. The program, as well as all of its dependencies, will be downloaded and installed as a result of this action.\nsudo apt install jshon ","step-3-check-version#Step 3: Check Version":"After the installation has been completed successfully, you will be able to verify the utility version by utilising the jshon ‚Äìversion program, as is demonstrated below.\njshon --version ","step-4-verify-installation#Step 4: Verify Installation":"You can examine and double-check the installed file path by using the dpkg -L jshon command, as is demonstrated in the following example.\nI really trust that you have followed each step in this process very carefully (How to Install Jshon on Ubuntu 20.04).\nMust read :- https://utho.com/docs/tutorial/how-to-add-a-user-and-grant-root-privileges-on-ubuntu-18-04/\nThankYou"},"title":"How to Install Jshon on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-kde-desktopgui-on-centos-7/":{"data":{"":" How to Install KDE Desktop(GUI) on CentOS 7\nDescription\nIn this article we will learn How to Install KDE Desktop(GUI) on CentOS 7 I‚Äôll show you how to install KDE Desktop (GUI) on CentOS 7 step by step. KDE is powerful desktop software that works on more than 200 different platforms and can run more than 200 programmes. It is used a lot on Linux desktops and other digital platforms. KDE makes it easy to find your way around and gives you a lot to do, like play music, surf the web, talk to family and friends, manage your files and data, and do other important work. Find out more on the website. It has a strong community behind it that builds and maintains all of the desktop software‚Äôs apps.\nProceed in the following manner: How to Install KDE Desktop(GUI) on CentOS 7\nStep 1: Update Server\nBefore installing packages, it is highly recommended to first install all available updates with the yum update command and then upgrade the packages to the latest version with the yum upgrade command, i have already updated my server as shown below.\nyum update \u0026\u0026 yum upgrade ","step-3-checking-groups-of-packages#Step 3: Checking Groups Of Packages":"With the yum grouplist command, you can see a list of all the package groups that can be installed right now. In the output, you should see a group called ‚ÄúKDE Plasma Workspaces‚Äù in the list of package groups.\nyum grouplist ","step-3-install-kde-desktopgui-package#Step 3: Install KDE Desktop(GUI) package":"To install KDE Desktop (GUI), use the command yum groupinstall ‚ÄúKDE Plasma Workspaces‚Äù to install the KDE Plasma Workspaces package group.\nyum groupinstall \"KDE Plasma Workspaces\" Step 4: Enable KDE Desktop (GUI)\nFollowing a successful installation, you will need to enable the graphical user interface (GUI) mode to start automatically by executing the sudo systemctl set-default graphical.target command, as demonstrated below.\nsudo systemctl set-default graphical.target ","step-5-reboot-server#Step 5: Reboot Server":"To enable KDE Desktop, use the reboot command to restart your machine, as illustrated below.\nreboot I truly hope that you will be able to follow each step in the process carefully. How to Install KDE Desktop(GUI) on CentOS 7\nMust read :- https://utho.com/docs/tutorial/2-methods-for-re-running-last-executed-commands-in-linux/\nThankyou"},"title":"How to Install KDE Desktop(GUI) on CentOS 7"},"/utho-docs/docs/linux/how-to-install-kubesphere-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to Install KubeSphere on Ubuntu 22.04.\nCheck out the Kubesphere webpage to learn more.\nAlso Read: How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Install KubeSphere on Ubuntu 22.04.\nKubeSphere is an open-source enterprise-grade Kubernetes container platform that streamlines DevOps operations and automates the whole stack of software development. It provides developers with an easy-to-navigate and welcoming online interface that makes it possible for them to construct and monitor feature-rich platforms for enterprise Kubernetes environments.","step-1-preparing-your-linux-system#Step 1: Preparing your Linux System":"The first thing you need to do is select the operating system that will be used to install Kubesphere. The following computer operating systems are the only ones that are currently being supported at the time when this article was written.\nUbuntu 16.04, 18.04, 20.04, 22.04 Debian 9, 10 Red Hat Enterprise Linux 7 SUSE Linux Enterprise Server 15 OpenSUSE Leap 15.2 Make sure that the following minimum system requirements are met by the system you choose, regardless of the system you choose:\n2 vCPU /4096 MB Memory / 80 GB SSD Disk\nNOTE: To prevent future issues, it is highly recommended that you use an operating system that has no other software installed on it.","step-2-installing-the-necessary-pre-requisite-packages#Step 2: Installing the necessary pre-requisite packages":"Before proceeding with the installation, there are a few prerequisite packages that must be installed. Therefore, set them up in the manner shown.\n# apt update # apt install curl socat conntrack ebtables ipset -y ","step-3-download-the-kubekey-now#Step 3: Download the KubeKey now":"The next step is to download KubeKey from its release page on Github, or to run the following command:\n# curl -sfL https://get-kk.kubesphere.io | VERSION=v3.0.2 sh - After that, use the chmod command to make the kk file executable.\n# chmod +x kk ","step-4-start-the-task-of-installing-kubesphere#Step 4: Start the task of installing Kubesphere":"Due to the fact that this installation is an all-in-one package, you will not be required to make any adjustments or configurations to your system.\n# sudo ./kk create cluster --with-kubernetes v1.24.2 --with-kubesphere v3.3.1 --container-manager containerd NOTE: In this stage, if you don‚Äôt add the ‚Äìwith-kubesphere flag to the command, KubeSphere won‚Äôt be set up. Only Kubernetes will be set up via KubeKey. If you use the same flag without saying which version of KubeSphere to install, the most recent version will be used. Also, you must pass the ‚Äìcontainer-manager flag if you want to install Kubernetes 1.24.x.\nOnce the command is performed, a pre-check is done on your node to make sure that the necessary packages are installed. If everything is in order, you should see the screenshot below.\nTo continue the process of installation, type ‚Äúyes‚Äù and press the Enter key.\nThe installation takes a long time‚Äîabout 20 minutes in our instance. You can now take a break while Kubernetes and Kubesphere are being installed.\nWhen it‚Äôs done, you‚Äôll get some information about how to use the Kubesphere online interface.\nGo to the URL given to obtain to the web console on your browser. This will take you to the page below where you can log in. Use the default username and password provided in the previous screenshot to sign in.\nThen you‚Äôll have to update your password to something different and stronger.\nYou are eventually directed to the dashboard shown below.\nClick on ‚ÄúPlatform‚Äù in the top left corner to see how your cluster‚Äôs resources are being used.\nWhen the menu comes up, choose ‚ÄúCluster Management‚Äù from the list.\nThis brings up the UI that shows how the cluster‚Äôs resources are being used.\nOn the left, there is a menu with a range of options for getting to and seeing different parts of your cluster. For example, go to Nodes \u003e Cluster nodes to see the nodes in the cluster.\nYou will only see one node, which is obvious as you just installed Kubernetes and Kubesphere on one node.\nClick ‚ÄúSystem Components‚Äù to see the system features.\nBack at the machine, perform the following commands to interact with cluster. (or kubectl commands)\n# mkdir -p $HOME/.kube # sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config # sudo chown $(id -u):$(id -g) $HOME/.kube/config Run the following command to find out about a node:\n# kubectl describe node Run the following command to find out about all the pods in all the namespaces on your cluster:\n# kubectl get pods --all-namespaces "},"title":"How to Install KubeSphere on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-latest-mysql-5-7-on-centos-7/":{"data":{"":"","#":"MySQL is an open-source relational database management system . Its name consists a combination of ‚ÄòMy‚Äô and ‚ÄòSQL‚Äô as the name for the Structured Query Language of the co-founder Michael Widenius‚Äôs daughter.\nAdding the MySQL Yum Repository [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]These instructions are only used when MySQL is newly installed on the server.If you have already a MySQL installed using a third-party RPM package on the server , please upgrade or replace the installed MySQL Paket with the MySQL Yum Repository.\nTo download the latest version of MySQL, add the MySQL Yum repository to the repository list of your respective Linux distribution system.[/ht_message]\n[root@Microhost ~]# wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm You have downloaded the package with above command now for installation of those packages please use the following command:\n[root@Microhost ~]# yum localinstall mysql57-community-release-el7-11.noarch.rpm By using the following command, you can verify that MySQL Yum has been successfully added.\n[root@Microhost ~]# yum repolist enabled | grep \"mysql._-community._\" Installing Latest MySQL Version Please use the following command for the installation of MySql server.\n[root@Microhost ~]# yum install mysql-community-server -y The MySQL has been successfully installed. We will start the service of MySQL with the following command.\n[root@Microhost ~]# systemctl start mysqld We can check the status of MySQL with following command\n[root@Microhost ~]# systemctl status mysqld Securing the MySQL Installation Before proceeding further, we will copy the temporary password with the following command.\n[root@Microhost ~]# grep 'temporary password' /var/log/mysqld.log We will copy this password as it will be used while changing the MySql password further.\nThe output will be shown as following:\nWe have to update a new password for MySQL root. Please update the complex password for security purposes.\nYou will get the prompt for changing the root password. You need to press ‚Äún‚Äù as per the screenshot.\nNow , it will show the prompt of removal of anonymous users. You need to press ‚ÄúY‚Äù as per the screenshot.\nYou will get the prompt for disabling the remote root login for security concerns. You need to press ‚Äúy‚Äù as per the screenshot.\nFurther, you will be shown the prompt of removing the test database. You need to press ‚Äúy‚Äù as per the screenshot.\nAt last , you will be shown the prompt of reloading the table privilege. You need to press ‚Äúy‚Äù as per the screenshot.\nWe have done the configuration of Mysql . We can login with the root password using the below command:\n[root@Microhost ~]# mysql -u root -p We have successfully completed the installation and configuration of Mysql.\nThank You :)"},"title":"How to Install Latest MySQL 5.7 on CentOS 7"},"/utho-docs/docs/linux/how-to-install-lemp-on-ubuntu-18-04/":{"data":{"php--v#php -v":"","sudo-apt-update#sudo apt update":"","sudoaptinstallmysql-server#¬†sudo¬†apt¬†install¬†mysql-server":"","sudonginx--t#sudo¬†nginx -t":"\nIntroduction\nThe LEMP software stack is a group of software that can be used to serve dynamic web pages and web applications. This is an acronym that describes a¬†Linux operating system, with an nginx (pronounced like ‚ÄúEngine-X‚Äù) web server. The backend data is stored in the¬†MySQL database and the dynamic processing is handled by¬†PHP.\nLogin to the server with the with the root credential on the putty. After login make sure to update the packages, to do so type the command bellow\n# sudo¬†apt¬†update After successfully update, type the following command for installation.\n# sudo¬†apt¬†install¬†nginx After successfully installation we need to start the service for that type the following command.\n# Systemctl start nginx After start the service you can check the status, with the following command.\n# Systemctl status nginx Installing MySQL to Manage Site Data\nNow that you have a web server, you need to install MySQL (a database management system) to store and manage the data for your site.\nInstall MySQL by typing:\nsudo¬†apt¬†install¬†mysql-server After successfully installation we need to start the service for that type the following command. ``` # Systemctl start mysql After start the service you can check the status, with the following command. ``` Systemctl status mysql ![](images/image-28.png) The MySQL database software is now installed, but its configuration is not yet complete. To secure the installation, MySQL comes with a script that will ask whether we want to modify some insecure defaults. Initiate the script by typing. ``` # sudo¬†mysql_secure_installation This script will ask if you want to configure the¬†`VALIDATE PASSWORD PLUGIN`. Answer¬†`Y`¬†for yes, or anything else to continue without enabling. VALIDATE PASSWORD PLUGIN can be used to test passwords and improve security. It checks the strength of password and allows the users to set only those passwords which are secure enough. Would you like to setup VALIDATE PASSWORD plugin? Press y|Y for Yes, any other key for No. Next, you‚Äôll be asked to submit and confirm a root password. For the rest of the questions, you should press¬†`Y`¬†and hit the¬†`ENTER`¬†key at each prompt. This will remove some anonymous users and the test database, disable remote root logins, and load these new rules so that MySQL immediately respects the changes we have made. 3. Installing PHP You now have Nginx installed to serve your pages and MySQL installed to store and manage your data. However, you still don‚Äôt have anything that can generate dynamic content. This is where PHP comes into play, for installation type the following command. ``` sudo apt update ``` # sudo apt install php-fpm php-mysql Once installed, check the PHP version. ``` php -v f PHP was installed correctly, you should see something similar to below. Above we are using PHP version¬†**8.0,**¬†though this may be a later version for you depending on which package was in the repository. ## Configure Nginx for PHP We now need to make some changes to our Nginx server block. For the moment, we will assume you are using the default. Edit the file in nano. ``` # sudo¬†nano¬†/etc/nginx/sites-available/default Press¬†CTRL¬†+¬†W¬†and search for¬†index.html. Now add¬†index.php¬†before¬†index.html Now check the config file to make sure there are no syntax errors.¬†Any errors could crash the web server on restart. ``` sudo¬†nginx -t Thank You :) ","systemctl-status-mysql#Systemctl status mysql":""},"title":"How to install LEMP on ubuntu 18.04"},"/utho-docs/docs/linux/how-to-install-lemp-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install LEMP on Ubuntu 22.04.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install LEMP on Ubuntu 22.04.\nA software stack is a collection of software tools that are bundled together and LEMP (Nginx, MariaDB, and PHP 8.1) is one such software stack. Linux, Nginx (Engine-X), MariaDB/MySQL, and PHP are all programmes that are open source and free to use. LEMP is an acronym for these programmes.\nIt is the software stack that is used most frequently to power dynamic websites and online apps.\nThe operating system that is being used is Linux.\nThe web server that we use is called Nginx.\nThe database server that we use is MariaDB/MySQL.\nThe generation of dynamic web pages is accomplished through the use of the server-side programming language known as PHP.","step-1-upgrade-software-packages#Step 1: Upgrade software packages":"Before we install the LEMP stack, we should run the following commands to update the repository and software files.\n# apt update # apt upgrade -y ","step-2-install-nginx#Step 2: Install Nginx":"Nginx is a web server that works well and is used by a lot of people these days. It can also be used as a caching server and a reverse proxy. To set up Nginx Web server, type the following command.\n# apt install nginx By using the following command after the installation of Nginx, we can make it so that it starts automatically whenever the system boots up.\n# systemctl enable nginx Then start Nginx by typing in the following command:\n# systemctl start nginx Now check out its status.\n# systemctl status nginx Check the version of Nginx.\n# nginx -v Now, in the address box of your browser, type in the public IP address of the Ubuntu 22.04 server you are using. If you are able to view the ‚ÄúWelcome to Nginx‚Äù webpage, this indicates that the Nginx web server is functioning as expected.\nThere may be a firewall blocking inbound requests to TCP port 80, which would explain why the connection was either rejected or failed to finish. In order to make TCP port 80 available, you will need to execute the following command if your machine is protected by an iptables firewall.\n# iptables -I INPUT -p tcp --dport 80 -j ACCEPT If you are protecting your network with the UFW firewall, you will need to execute this command in order to open TCP port 80.\n# ufw allow http Last but not least, we have to change the owner of the web directory to be the Nginx user www-data. It is the responsibility of the root user by default.\n# chown www-data:www-data /usr/share/nginx/html -R ","step-3-install-mariadb#Step 3: Install MariaDB":"MariaDB can be used in place of MySQL without any modification. It is being created by individuals who formerly worked on the MySQL team who are afraid that Oracle may decide to make MySQL a closed-source product. To install MariaDB on Ubuntu 22.04, type the following command into the terminal.\n# apt install mariadb-server mariadb-client -y You can use the command to check the version of MariaDB that is currently installed;\n# mysql -V Now, using the following command, check the current status of mariadb.\n# systemctl status mariadb Executing the following command will enable MariaDB to start automatically whenever the system boots.\n# systemctl enable mariadb Execute the post-installation security script¬†at this time.\n# mysql\\_secure\\_installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nRemove test database and access to it? [Y/n] Press y\nReload privilege tables now? [Y/n] : Press y\nA username and password combination should be required in order to access the MariaDB shell.\n# mysql -u root -p To exit, run the following command\n# exit ","step-4-install-php#Step 4: Install PHP":"PHP8.1 is a component of the Ubuntu 22.04 repository and offers a marginal increase in overall performance when compared to PHP8.0. In order to install PHP 8.1 and some of the more common extensions, execute the following command.\n# apt install php8.1 php8.1-fpm php8.1-mysql php-common php8.1-cli php8.1-common php8.1-opcache php8.1-readline php8.1-mbstring php8.1-xml php8.1-gd php8.1-curl Installing these PHP extensions will ensure that your content management system operates without any problems. Now begin operating php8.1-fpm.\n# systemctl start php8.1-fpm Turn on the auto-start feature during boot up.\n# systemctl enable php8.1-fpm Verify the status of PHP:\n# systemctl status php8.1-fpm ","step-5-create-a-nginx-server-block#Step 5: Create a Nginx Server Block":"When working with Nginx, a server block is similar to a virtual host. We are not going to use the default server block because it is not capable of running PHP code and if we try to edit it, it turns into a tangled mess. Executing the following command will get rid of the default symlink that is located in the sites-enabled directory. (It‚Äôs still available as /etc/nginx/sites-available/default.)\n# rm /etc/nginx/sites-enabled/default Then, build a new server block file in the directory located at /etc/nginx/conf.d/ using a text editor for the command line such as vi.\n# vi /etc/nginx/conf.d/default.conf Copy and paste the content that is provided below into the file. The code snippet that is presented below will cause Nginx to start listening on port 80 of IPv4 and port 80 of IPv6 with a catch-all server name.\nserver { listen 80; listen [::]:80; server_name _; root /usr/share/nginx/html/; index index.php index.html index.htm index.nginx-debian.html; location / { try_files $uri $uri/ /index.php; } location ~ \\.php$ { fastcgi_pass unix:/run/php/php8.1-fpm.sock; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; include snippets/fastcgi-php.conf; } # A long browser cache lifetime can speed up repeat visits to your page location ~* \\.(jpg|jpeg|gif|png|webp|svg|woff|woff2|ttf|css|js|ico|xml)$ { access_log off; log_not_found off; expires 360d; } # disable access to hidden files location ~ /\\.ht { access_log off; log_not_found off; deny all; } } Save and exit from the file by pressing escape :wq\nThen test Nginx‚Äôs settings.\n# nginx -t If the test works, you will need to reload Nginx.\n# systemctl reload nginx ","step-6-test-php#Step 6: Test PHP":"In order to test PHP-FPM with Nginx as a web server, we will need to create a file named info.php in the webroot directory.\n# vi /usr/share/nginx/html/info.php Copy and paste the PHP code that is shown below into the file.\n\u003c?php phpinfo(); ?\u003e Save and exit from the file by pressing escape :wq\nNow, within the address bar of your web browser, type server-ip-address/info.php. Simply change sever-ip-address to your own real IP address.\n# server-ip-address/info.php "},"title":"How to install LEMP on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-lemp-stack-on-centos-7/":{"data":{"":"\nThe term LEMP is an acronym of the names of its four open-source components:\nL¬†- Linux operating system E¬†- Nginx [engine x] M¬†- MySQL or MariaDB relational database management system P¬†- PHP programming language Login into the server using root credentials on putty.\nStep 1. Installing Nginx\nNginx is not available in the default CentOS 7 repository so we will use the EPEL repositories. To add the¬†EPEL repository¬†to your system, use the following command.\n# yum install epel-release Now that the EPEL repository is enabled, install the Nginx package with below command.\n# yum install nginx Once it is installed, start and enable the Nginx service by typing:\n# systemctl start nginx # systemctl enable nginx # systemctl status nginx Step 2. Installing MariaDB\nThe next step is to install the MariaDB packages. To do so type:\nyum install mariadb-server\nOnce MariaDB server is installed, start and enable the service with\n# systemctl start mariadb.service # systemctl enable mariadb.service Step 3. Installing PHP\nCentOS 7 ships with PHP version 5.4 which is EOL-ed for quite some time so we‚Äôll use the Remi repository to install PHP 7.2.\nRun the following command to add the Remi repository to your system.\n#yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm Once it is added, install the¬†yum-utils¬†package and enable the¬†remi-php72¬†repository.\n# yum install yum-utils # yum-config-manager --enable remi-php72 Now that we have Remi repository enabled, we can install PHP FPM and several most common PHP modules with.\n# yum install php-fpm php-opcache php-cli php-gd php-curl php-mysql By default PHP FPM will run as user¬†apache¬†on port 9000. We‚Äôll change the user to¬†nginx¬†and switch from TCP socket to Unix socket. To do so edit the lines highlighted in yellow.\n# vi /etc/php-fpm.d/[www.conf](http://www.conf/) Make sure the¬†/var/lib/php¬†directory has the correct permissions\n# chown -R root:nginx /var/lib/php Save the file, enable and start the PHP FPM service with.\n# systemctl enable php-fpm # systemctl start php-fpm Thank you!!"},"title":"How to install LEMP stack on centOS 7"},"/utho-docs/docs/linux/how-to-install-mailcatcher-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MailCatcher On Ubuntu 22.04\nAlso Read: How to install PHP 8.2 on Ubuntu 22.04\nThank You üôÇ","installation#Installation":"Using the following commands, the first thing you need to do is install the dependencies that are necessary for installing Mailcatcher:\n# apt-get update # apt-get install -y build-essential software-properties-common # apt-get install -y libsqlite3-dev ruby-dev After that, in the next step, you will be able to quickly install MailCatcher by using the following command:\n# gem install mailcatcher Once you‚Äôve followed the steps above to install MailCatcher, you‚Äôre ready to get started.\nRunning the following command¬†will start MailCatcher. Note that you need to enter your chosen IP address instead of Public IP address in the next command.\n# mailcatcher --ip \u003cPublic IP address\u003e Now you can access your mailcatcher by browsing the following command in your browser:\n# http://server\\_ip:1080 ","introduction#Introduction":"In this article, you will learn how to install MailCatcher On Ubuntu 22.04.\nMailCatcher is a free programme that can monitor e-mails sent from any online or mobile application. It can do this by tracking the sender and the recipient of the messages. This programme acts as a fake Simple Mail Transfer Protocol (SMTP) server to which you can direct your messages rather than sending them to a real SMTP server.\nIt is important to keep in mind that emails sent in this manner will only be received by a local server and can be seen through a web interface. On the other hand, it‚Äôs possible that these emails are simply never received.","mailcatcher-features#MailCatcher Features":"Mailcatcher is able to assist you in conducting a comprehensive review of the contents as well as the titles of your email messages. You will be able to see the HTML version of an email and check to see if all of the headers, including the address for the Return-Path field, are correctly set using this tool. Before sending an email, you have the ability to test links, examine attachments, and pinpoint areas in which there is room for improvement.\nBecause using Mailcatcher is completely free, it might be appealing to you if you‚Äôre operating on a very tight budget. Mailcatcher offers the capability to stop spam from reaching actual users of your application. You will have an easier time determining what actually works and what does not with the help of this tool."},"title":"How to install MailCatcher On Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-mariadb-10-3-on-centos-7/":{"data":{"":"","#":"The MariaDB server is a MySQL server fork developed by the community. MariaDB is developed as a drop-in replacement for MySQL(r), with new features , new storage engines, fewer bugs and better performance by core members of the original MySQL team. For databases professionals who want a robust , scalable and reliable SQL server, MariaDB can be a better choice.\nAdd MariaDB Yum Repository [root@Microhost ~]# vi /etc/yum.repos.d/mariadb.repo The above command will open a blank file . Please copy and paste the content given below:\n[mariadb] name=MariaDB baseurl=http://yum.mariadb.org/10.3/centos7-amd64 gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB gpgcheck=1 enabled=1 Press the Esc button and exit with :wq\nWe need to import the public GPG key to verify the digital signatures of the packages in this repository before using the MariaDB andum repository.\nHowever, the GPG public key does not need to be manually imported. When we first install a package from MariaDB yum repository, the public GPG key will be automatically installed by yum.\nHere, for the sake of demonstration, we manually import the GPG public key.\n[root@Microhost ~]# rpm --import https://yum.mariadb.org/RPM-GPG-KEY-MariaDB We will move to the installation section. You can use the following command to install mariaDB.\n[root@Microhost ~]# yum install -y mariadb-server Now, MariaDB has been installed successfully. We can start and enable the service of MariaDB.\n[root@Microhost ~]# systemctl start mariadb [root@Microhost ~]# systemctl enable mariadb We will move to the secure installation of MariaDB. The command is given below to initiate the process.\n[root@Microhost ~]# mysql_secure_installation Press ‚Äúenter‚Äù to set new password for MariaDB root, as shown in the above screenshot.\nNow, You will get the prompt of the removal of anonymous User. You need to press ‚Äúy‚Äù as per the image given below:\nIn the next prompt, you have to disable the root login of mariadb. You need to press ‚Äúy‚Äù as per the image given below:\nNow, the prompt would be shown for the removal of test databases. You need to press ‚Äúy‚Äù as per the image given below:\nIn the last prompt, You will press ‚Äúy‚Äù to reload the table privileges as shown in image below:\nWe have done with the configuration and secure installation portion of MariaDB.\nWe will use the below command for login into database:\n[root@Microhost ~]# mysql -u root -p We have completed the section for installation and configuration of MariaDB.\nThank You :)"},"title":"HOW TO INSTALL MARIADB 10.3 ON CENTOS 7"},"/utho-docs/docs/linux/how-to-install-mariadb-10-3-on-ubuntu-20-04/":{"data":{"":"","a-username-and-password-combination-should-be-required-in-order-to-access-the-mariadb-shell#A username and password combination should be required in order to access the MariaDB shell.":" # mysql -u root -p # CREATE DATABASE microhost; # SHOW DATABASES; Executed the following command To make changes take effect without reloading or restarting mysql service, you can use the flush privileges command to reload the grant tables in the database.\n# FLUSH PRIVILEGES; # exit ","conclusion#Conclusion":"Hopefully now you have Installed MariaDB 10.3\nThank You üôÇ","install-mariadb-103-on-ubuntu-2004#Install MariaDB 10.3 on Ubuntu 20.04":" # apt install mariadb-server mariadb-client -y You can use the command to check the version of MariaDB that is currently installed;\n# mysql -V Now, using the following command, check the current status of mariadb.\n# systemctl status mariadb ","install-mariadb-apt-repos-on-your-ubuntu-2004-server-by-following-the-instructions-below-and-executing-the-commands#Install MariaDB APT repos on your Ubuntu 20.04 server by following the instructions below and executing the commands.":" # apt update # apt install software-properties-common dirmngr apt-transport-https ca-certificates -y # wget -qO- https://mariadb.org/mariadb_release_signing_key.asc | gpg --dearmor \u003e /etc/apt/trusted.gpg.d/mariadb.gpg # echo 'deb [arch=amd64,arm64,ppc64el,s390x] https://atl.mirrors.knownhost.com/mariadb/repo/10.7/ubuntu focal main' \u003e /etc/apt/sources.list.d/mariadb.list ","introduction#Introduction":"In this article, you will learn how to install MariaDB 10.3 on ubuntu 20.04.\nThis is to Install MariaDB 10.3 on Ubuntu 20.04. MariaDB is a database management system that is a fork of MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.","run-the-mariadb-secure-installation-script-which-assists-you-in-protecting-your-mariadb-database-server#Run the mariadb-secure-installation script, which assists you in protecting your MariaDB database server.":" # sudo mariadb-secure-installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nReload privilege tables now? [Y/n] : Press y"},"title":"How to Install MariaDB 10.3 on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-mariadb-10-7-on-centos-7/":{"data":{"":"","conclusion#conclusion":"Hopefully, you have learned how to install how to install MariaDB 10.7 on centos 7.\nAlso Read: How To Install Docker on Centos 7\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install MariaDB 10.7 on Centos 7\nThis is to install the latest MariaDB on a CentOS server. MariaDB is a database management system that is a fork of MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.","step-1-add-mariadb-yum-repository#Step 1: Add MariaDB YUM repository":"There is a MariaDB YUM repository available for systems that are based on RHEL, and it may be added to the system by executing the following commands:\n# curl -LsS -O https://downloads.mariadb.com/MariaD B/mariadb_repo_setup # sudo bash mariadb_repo_setup --mariadb-server-ver sion=10.7 The script that was run automates the process of configuring MariaDB repositories for YUM as well as the importation of the necessary GPG keys.\nBy showing all of the accessible repositories, we will be able to determine whether or not the repository was successfully added.\n# sudo yum makecache # sudo yum repolist ","step-2-install-mariadb-107-on-centos-7#Step 2: Install MariaDB 10.7 on CentOS 7":"Following the installation of the repository, we are able to move forward with the installation of MariaDB 10.7 on CentOS 7.\n# sudo yum install MariaDB-server -y Using the rpm command, you are able to obtain more information about the currently installed version of the MariaDB database.\n# rpm -qi MariaDB-server ","step-3-start-and-secure-mariadb-107-on-centos-7#Step 3: Start and Secure MariaDB 10.7 on CentOS 7":"After the success of a successful installation, let‚Äôs set the database service up and running:\n# sudo systemctl enable --now mariadb Confirm status is running;\n# systemctl status mariadb It is useful to verify the version of the Client by using the following command:\n# mariadb -V Run the mariadb-secure-installation script, which assists you in protecting your MariaDB database server.\n# sudo mariadb-secure-installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nRemove test database and access to it? [Y/n] : Press y\nReload privilege tables now? [Y/n] : Press y\nA username and password combination should be required in order to access the MariaDB shell.\n# mysql -u root -p # CREATE DATABASE microhost; # SHOW DATABASES; Executed the following command To make changes take effect without reloading or restarting mysql service, you can use the flush privileges command to reload the grant tables in the database.\n# FLUSH PRIVILEGES; # exit "},"title":"How To Install MariaDB 10.7 on CentOS 7"},"/utho-docs/docs/linux/how-to-install-mariadb-10-7-on-fedora/":{"data":{"":"","a-username-and-password-combination-should-be-required-in-order-to-access-the-mariadb-shell#A username and password combination should be required in order to access the MariaDB shell.":" # mysql -u root -p # CREATE DATABASE microhost; # SHOW DATABASES; Executed the following command To make changes take effect without reloading or restarting mysql service, you can use the flush privileges command to reload the grant tables in the database.\n# FLUSH PRIVILEGES; # exit ","check-mariadb-107-service-status#Check MariaDB 10.7 Service Status":"MariaDB is not enabled or active when the system boots up by default since it is deactivated. The following command can be used to begin the process of activating MariaDB.\n# sudo systemctl enable mariadb --now Check the status with the following command.\n# systemctl status mariadb ","conclusion#Conclusion":"Hopefully, you have learned how to install MariaDB 10.7 on Fedora.\nAlso read: How To Install Git on Fedora\nThank You üôÇ","install-dependency-required#Install Dependency Required":"Execute the following command in order to install or verify that the package dnf-plugins-core is installed on your Fedora desktop computer before continuing with the installation process.\n# sudo dnf install dnf-plugins-core -y This function should be installed automatically.","install-mariadb-107-on-fedora#Install MariaDB 10.7 on Fedora":"This need to be the default setting. There is a default installation of MariaDB 10.7 in the Fedora repository. Having said that, you will have to enable it. To achieve this, you must first print out a list of the different versions of MariaDB that are available. installed.\n# sudo dnf module list mariadb As was previously mentioned, the default repository version is 10.5. On the other hand, versions 10.6 and 10.7 are accessible. The following command must be executed in order to enable version 10.7 of the MariaDB series.\n# sudo dnf module enable mariadb:10.7 -y Execute the following commands in order to continue with the installation of MariaDB in your terminal.\n# yum install mariadb-server -y ","introduction#Introduction":"In this article you will learn how to install MariaDB 10.7 on Fedora.\nThis is to install the latest MariaDB on a Fedora server. MariaDB is a database management system that is a fork of MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.","run-the-mariadb-secure-installation-script-which-assists-you-in-protecting-your-mariadb-database-server#Run the mariadb-secure-installation script, which assists you in protecting your MariaDB database server.":" # sudo mariadb-secure-installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nRemove test database and access to it? [Y/n] : Press y\nReload privilege tables now? [Y/n] : Press y","verify-that-mariadb-was-successfully-installed-by-checking-both-the-version-and-the-build#Verify that MariaDB was successfully installed by checking both the version and the build:":" # mariadb --version ","you-should-confirm-that-your-fedora-operating-system-is-up-to-date-by-performing-an-update-on-all-of-the-existing-packages#You should confirm that your Fedora operating system is up to date by performing an update on all of the existing packages:":" # sudo dnf upgrade --refresh -y "},"title":"How to Install MariaDB 10.7 on Fedora"},"/utho-docs/docs/linux/how-to-install-mariadb-11-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MariaDB 11 on Debian 10.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install MariaDB 11 on Debian 10.\nThis is to install MariaDB 11 on Debian 10. MariaDB is a database management system that is a fork of¬†MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.\nAdd MariaDB 11 APT repository # apt update # curl -LsS https://downloads.mariadb.com/MariaDB/mariadb\\_repo\\_setup | bash -s -- --mariadb-server-version=11.0 You need to update the system after adding the repo.\n# apt update Install MariaDB 11 on Debian 10 # apt install mariadb-server mariadb-client -y You can use the command to check the version of MariaDB that is currently installed;\n# mariadb -V Now, using the following command, check the current status of mariadb.\n# systemctl status mariadb Run the mariadb-secure-installation script, which assists you in protecting your MariaDB database server # mysql\\_secure\\_installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nRemove test database and access to it? [Y/n] Press y\nReload privilege tables now? [Y/n] : Press y\nA username and password combination should be required in order to access the MariaDB shell.\n# mysql -u root -p To exit, run the following command\n# exit "},"title":"How to install MariaDB 11 on Debian 10"},"/utho-docs/docs/linux/how-to-install-mariadb-11-on-debian-9/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MariaDB 11 on Debian 9.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install MariaDB 11 on Debian 9.\nThis is to install MariaDB 11 on Debian 9. MariaDB is a database management system that is a fork of¬†MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.\nAdd MariaDB 11 APT repository # apt update # curl -LsS https://downloads.mariadb.com/MariaDB/mariadb\\_repo\\_setup | bash -s -- --mariadb-server-version=11.0 You need to update the system after adding the repo.\n# apt update Install MariaDB 11 on Debian 9 # apt install mariadb-server mariadb-client -y You can use the command to check the version of MariaDB that is currently installed;\n# mariadb -V Now, using the following command, check the current status of mariadb.\n# systemctl status mariadb Run the mariadb-secure-installation script, which assists you in protecting your MariaDB database server # mysql\\_secure\\_installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nRemove test database and access to it? [Y/n] Press y\nReload privilege tables now? [Y/n] : Press y\nA username and password combination should be required in order to access the MariaDB shell.\n# mysql -u root -p To exit, run the following command\n# exit "},"title":"How to install MariaDB 11 on Debian 9"},"/utho-docs/docs/linux/how-to-install-mariadb-11-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MariaDB 11 on Ubuntu 22.04.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install MariaDB 11 on Ubuntu 22.04.\nThis is to install MariaDB 11 on Ubuntu 22.04. MariaDB is a database management system that is a fork of MySQL. It is extremely similar to MySQL, which is a database management system. Several different applications, including data warehousing, e-commerce, enterprise-level features, and logging programmes, all make use of the MariaDB database.\nMariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge.\nA database is a repository for information that can be easily retrieved and applied in the context in which it is required. When compared to recording information on a piece of paper or in a Word document, storing all of your information in a database allows it to be organized into tables, making it simple to retrieve each individual entry in a manner that is both systematic and accurate.\nAdd MariaDB 11 APT repository # apt update # curl -LsS https://downloads.mariadb.com/MariaDB/mariadb\\_repo\\_setup | bash -s -- --mariadb-server-version=11.0 You need to update the system after adding the repo.\n# apt update Install MariaDB 11 on Ubuntu 22.04 # apt install mariadb-server mariadb-client -y You can use the command to check the version of MariaDB that is currently installed;\n# mariadb -V Now, using the following command, check the current status of mariadb.\n# systemctl status mariadb Run the mariadb-secure-installation script, which assists you in protecting your MariaDB database server. # mysql\\_secure\\_installation Enter current password for root (enter for none): Press Enter\nSwitch to unix_socket authentication [Y/n] : Press y\nChange the root password? [Y/n] : Press y\nRemove anonymous users? [Y/n] : Press y\nIf you choose you wish to let root login remotely, then press the y key; otherwise, press the n key.\nRemove test database and access to it? [Y/n] Press y\nReload privilege tables now? [Y/n] : Press y\nA username and password combination should be required in order to access the MariaDB shell.\n# mysql -u root -p To exit, run the following command\n# exit "},"title":"How to install MariaDB 11 on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-mariadb-on-debian-10/":{"data":{"":"\nIntroduction\nOpen-source database management system (DBMS) MariaDB is frequently used as a MySQL replacement in the well-known LAMP (Linux, Apache, MySQL, PHP/Python/Perl) stack. Designed to replace MySQL seamlessly, Debian now only includes MariaDB packages out of the box. The compatible MariaDB replacement versions will be installed in their place if you try to install packages related to the MySQL server.\nThe following three stages make up the abbreviated installation guide:\n_Using apt, bring your package index up to date.\n_Using apt, install the mariadb-server package. Additionally, the package includes associated tools for interacting with MariaDB.\n*To restrict access to the server, run the mysql secure installation security script that is included in the package.\n#sudo apt update #sudo apt install mariadb-server #sudo mysql_secure_installation This guide will show you how to install MariaDB version 10.3 on a Debian 10 server, then check to make sure it is up and running with a secure initial setup.\nPrerequisites\nYou will require one Debian 10 server configured with a firewall and a non-root user with sudo rights in order to finish this tutorial. By following our basic server setup guide, you may set this up.\nStep 1 ‚Äî Installing MariaDB\nMariaDB version 10.3 is by default available in the APT package repositories for Debian 10. The MySQL/MariaDB packaging team for Debian has designated it as the default MySQL variation.\nUpdate your server‚Äôs package index with apt to install it:\n#sudo apt update install the package:\n#sudo apt install mariadb-server These scripts will install MariaDB, but they won‚Äôt ask you to modify any configuration settings or set a password. Because MariaDB‚Äôs default settings make your installation insecure, you‚Äôll use a script provided by the mariadb-server package to limit server access and delete unused users.\nStep 2 ‚Äî Configuring MariaDB\nRunning the provided security script is the next step for fresh MariaDB installations. The default settings that are less secure are changed by this script. It will be put to use in order to eliminate unused database users and prevent remote root logins.\nActivate the upcoming security script:\n#sudo mysql_secure_installation This prompts you to adjust MariaDB‚Äôs security options. First, enter the database root password. Press ENTER to signify ‚Äúnone‚Äù since you haven‚Äôt set one up.\nNext, you‚Äôre asked to set a database root password. Enter N. In Debian, the MariaDB root account is tied to automated system maintenance, therefore don‚Äôt change its authentication methods. By removing access to the administrative user, a package update could disrupt the database system. If socket authentication isn‚Äôt acceptable for your use case, we‚Äôll cover setting up a second administrative account for password access.\nFrom there, press Y and then ENTER to accept all defaults. This removes anonymous users and the test database, disables remote root logins, and loads new rules so MariaDB respects your changes.\nStep-3 User Authentication and Privileges Changing\nIn Debian 10.3 systems, the root MariaDB user authenticates using the unix socket plugin rather than a password. This increases security and usability in many circumstances, but it might complicate matters when granting administrative access to an external software (e.g., phpMyAdmin).\nBecause the server uses the root account for log rotation and starting and stopping, don‚Äôt modify its authentication information. Changing credentials in /etc/mysql/debian.cnf may work initially, but package updates may overwrite them. Instead of altering root, package maintainers advocate creating a password-protected administrator account.\nTo demonstrate, we‚Äôll create an admin account with the same permissions as root, but password authentication. Open your terminal‚Äôs MariaDB prompt:\n#sudo mysql Create a new user now with root access and password-based authentication. To suit your tastes, modify the username and password:\n[consiole]maridb\u003e #GRANT ALL ON . TO ‚Äòadmin‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;[/console]\nTo make sure that the privileges are saved and accessible for the current session, flush them:\nmariadb\u003e #FLUSH PRIVILEGES; exit the MariaDB shell:\nmariadb\u003e exit Output Bye Let‚Äôs make sure the MariaDB installation is working properly.\nStep 4 ‚Äî Testing MariaDB\nMariaDB ought should launch automatically after installation from the default repository. Check its status to see if this works:\n#sudo systemctl status mariadb Output ‚óè mariadb.service - MariaDB 10.3.31 database server Loaded: loaded (/lib/systemd/system/mariadb.service; enabled; vendor preset: Active: active (running) since Mon 2022-03-14 18:33:32 UTC; 2min 2s ago Docs: man:mysqld(8) https://mariadb.com/kb/en/library/systemd/ Main PID: 3229 (mysqld) Status: \"Taking your SQL requests now...\" Tasks: 31 (limit: 4915) Memory: 74.4M CGroup: /system.slice/mariadb.service ‚îî‚îÄ3229 /usr/sbin/mysqld Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: performance_schema Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: Phase 6/7: Checking and u Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: Running 'mysqlcheck' with Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: # Connecting to localhost Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: # Disconnecting from loca Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: Processing databases Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: information_schema Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: performance_schema Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: Phase 7/7: Running 'FLUSH Mar 14 18:33:32 mariadb /etc/mysql/debian-start[3267]: OK Start MariaDB with sudo systemctl start mariadb.\nTry connecting to the database using the mysqladmin client to run administrative commands. This command connects as root to MariaDB and returns the Unix socket version:\n#sudo mysqladmin version It should look like this:\nOutput mysqladmin Ver 9.1 Distrib 10.3.31-MariaDB, for debian-linux-gnu on x86_64 Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Server version 10.3.31-MariaDB-0+deb10u1 Protocol version 10 Connection Localhost via UNIX socket UNIX socket /var/run/mysqld/mysqld.sock Uptime: 3 min 6 sec Threads: 6 Questions: 473 Slow queries: 0 Opens: 175 Flush tables: 1 Open tables: 31 Queries per second avg: 2.543 You can carry out the same action if a different administrative user with password authentication was set up by executing the following command:\n#mysqladmin -u admin -p version Output Ver 9.1 Distrib 10.3.31-MariaDB, for debian-linux-gnu on x86_64 Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Server version 10.3.31-MariaDB-0+deb10u1 Protocol version 10 Connection Localhost via UNIX socket UNIX socket /var/run/mysqld/mysqld.sock Uptime: 7 min 11 sec Threads: 6 Questions: 474 Slow queries: 0 Opens: 175 Flush tables: 1 Open tables: 31 Queries per second avg: 1.099 This output indicates that MariaDB is up and running, as well as that the person in question may successfully authenticate themselves."},"title":"How To Install MariaDB on Debian 10"},"/utho-docs/docs/linux/how-to-install-mariadb-on-debian-11/":{"data":{"":"\nHow To Install MariaDB on Debian 11\nIntroduction\nThe popular LAMP (Linux, Apache, MySQL, PHP/Python, Perl) stack, which consists of MariaDB as the database component, is an open-source relational database management system. It is designed to replace MySQL seamlessly.\nThe three steps below make up the condensed version of this installation guide:\n_Using apt, bring your package index up to date.\n_Using apt, install the mariadb-server package. Additionally, the package includes associated tools for interacting with MariaDB.\n*Run the included mysql_secure_installation security script to restrict access to the server\n#sudo apt update #sudo apt install mariadb-server #sudo mysql_secure_installation This tutorial will show you how to install MariaDB on a Debian 11 server and ensure that it is up and running with a secure initial configuration.\nPrerequisites\n*You will need a server running Debian 11 to complete this tutorial. This server should have a non-root administrative user and a UFW-enabled firewall. Set this up by following our Debian 11 initial server setup guide.\nStep 1 ‚Äî Installing MariaDB\nAs of this writing, MariaDB version 10.5.15 is available in Debian 11‚Äôs default software repositories. The Debian MySQL/MariaDB packaging team has designated it as the default MySQL variant.\nTo install it, use apt to update your server‚Äôs package index:\n#sudo apt update Then proceed to install the package:\n#sudo apt install mariadb-server These commands will install MariaDB but will not prompt you to set a password or change any other settings. Because the default configuration makes your MariaDB installation insecure, you will use a script provided by the mariadb-server package to restrict server access and remove unused accounts.\nStep 2 ‚Äî Configuring MariaDB\nThe following step for new MariaDB installations is to run the included security script. This script modifies some of the less secure default options for things like remote root logins and sample users.\nRun the security script:\n#sudo mysql_secure_installation This will take you through a series of prompts where you can modify the security settings for your MariaDB installation. The first prompt will request your current database root password. Because you haven‚Äôt yet created one, press ENTER to indicate ‚Äúnone.‚Äù\nOutput NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MariaDB to secure it, you'll need the current password for the root user. If you've just installed MariaDB, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): You‚Äôll be asked if you want to use unix socket authentication. You can skip this step if you already have a protected root account. Enter n and then press ENTER.\nOutput . . . Setting the root password or using the unix_socket ensures that nobody can log into the MariaDB root user without the proper authorisation. You already have your root account protected, so you can safely answer 'n'. Switch to unix_socket authentication [Y/n] n The next prompt asks if you want to change the root password. Because the root account for MariaDB is closely linked to automated system maintenance in Debian 11, you should not change the configured authentication methods for that account.\nBy doing so, a package update could break the database system by removing access to the administrative account. Enter n followed by ENTER.\nOutput Change the root password? [Y/n] If socket authentication isn‚Äôt appropriate for your use case, you‚Äôll learn how to create an additional administrative account for password access later.\nFrom there, you can accept the defaults for all subsequent questions by pressing Y and then ENTER. This will remove some anonymous users and the test database, disable remote root logins, and load these new rules so that MariaDB can immediately apply your changes.\nYou‚Äôve now completed MariaDB‚Äôs initial security configuration. The next step is optional, but you should do it if you prefer to use a password to connect to your MariaDB server.\nStep.3 Establishing a New Administrative User Who Will Require a Password for Authentication\nOn Debian systems running MariaDB 10.5, the root MariaDB user authenticates via the unix socket plugin by default. This increases security and usability in many circumstances, but it might complicate matters when granting administrative access to an external software (e.g., phpMyAdmin).\nBecause the server uses the root account for log rotation and starting and stopping, don‚Äôt modify its authentication information. Changing credentials in /etc/mysql/debian.cnf may work initially, but package updates may overwrite them. Instead of altering root, package maintainers advocate creating a password-protected administrator account.\nTo do this, we‚Äôll create an admin account with the same permissions as root but password authentication. Terminal MariaDB prompt:\n#sudo mariadb Then, make a new user with root privileges and password access. Change the username and password to reflect your preferences:\nmariadb \u003e #GRANT ALL ON *.* TO 'admin'@'localhost' IDENTIFIED BY 'password' WITH GRANT OPTION; To ensure that the privileges are saved and available in the current session, flush them:\nmariadb \u003e #FLUSH PRIVILEGES; exit the MariaDB shell:\nmariadb \u003e #exit Now that everything is in place, let‚Äôs check out MariaDB.\nStep 4 ‚Äî Testing MariaDB\nMariaDB will start automatically when installed from the default repositories. Check its status to see if it works.\n#sudo systemctl status mariadb You‚Äôll get output that looks somewhat like this:\nOutput ‚óè mariadb.service - MariaDB 10.5.15 database server Loaded: loaded (/lib/systemd/system/mariadb.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2022-03-11 22:01:33 UTC; 14min ago Docs: man:mariadbd(8) https://mariadb.com/kb/en/library/systemd/ . . . If MariaDB isn‚Äôt already running, use the command sudo systemctl start mariadb to get it going.\nYou can also connect to the database using the mysqladmin tool, which is a client that allows you to run administrative commands. For example, this command instructs the server to connect to MariaDB as root via a Unix socket and return the version:\n#sudo mysqladmin version You will see output that looks somewhat like this:\nOutput mysqladmin Ver 9.1 Distrib 10.5.15-MariaDB, for debian-linux-gnu on x86_64 Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Server version 10.5.15-MariaDB-0+deb11u1 Protocol version 10 Connection Localhost via UNIX socket UNIX socket /run/mysqld/mysqld.sock Uptime: 4 min 20 sec Threads: 1 Questions: 72 Slow queries: 0 Opens: 32 Open tables: 25 Queries per second avg: 0.276 "},"title":"How To Install MariaDB on Debian 11"},"/utho-docs/docs/linux/how-to-install-mariadb-on-ubuntu-18-04/":{"data":{"":"\nDescription\nA wide variety of applications, including data warehousing, e-commerce, enterprise-level functionality, and logging programmes, make use of the MariaDB database. MariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge. What exactly is a database, then?\nNote\nThis walkthrough is intended for users who do not have root privileges. The prefix sudo is added to commands that need to be run with elevated privileges. Check out our Users and Groups guide if you aren‚Äôt familiar with the sudo command and want to learn more about it.","create-a-new-user-and-database-in-mariadb#Create a New User and Database in MariaDB":"log in in again to the database. If you set a password above, type it in when asked.\n#sudo mysql -u root -p Below is an example in which testdb is the name of the database, testuser is the name of the user, and password is the password for the user. You are strongly encouraged to switch to a more secure password:\n#CREATE DATABASE testdb; #CREATE user 'testuser'@localhost IDENTIFIED BY 'password'; #GRANT ALL ON testdb.* TO 'testuser' IDENTIFIED BY 'password'; This procedure can be streamlined by creating the user and assigning database permissions simultaneously:\n#CREATE DATABASE testdb; #GRANT ALL ON testdb.* TO 'testuser' IDENTIFIED BY 'password'; exit; ","reset-the-mariadb-root-password#Reset the MariaDB Root Password":"If you forget your root password for MariaDB, you can change it.\nStop the MariaDB server instance currently running.\n#sudo systemctl stop mariadb Then run the following command, which will let the database start up without loading the grant tables or networking.\n#sudo systemctl set-environment MYSQLD_OPTS=\"--skip-grant-tables --skip-networking\" ","restart-mariadb#Restart MariaDB:":" #sudo systemctl start mariadb Sign in to the MariaDB server with the root account, but don‚Äôt give a password this time:\n#sudo mysql -u root To change root‚Äôs password, enter the following commands. Swap out your weak password for a secure one:\n#FLUSH PRIVILEGES; #UPDATE mysql.user SET password = PASSWORD('password') WHERE user = 'root'; Make necessary adjustments to the authentication procedures for the root password:\n#UPDATE mysql.user SET authentication_string = '' WHERE user = 'root'; #UPDATE mysql.user SET plugin = '' WHERE user = 'root'; #exit; Change the environment settings back to the way they were so that the database can start with grant tables and networking:\n#sudo systemctl unset-environment MYSQLD_OPTS Then restart MariaDB:\n#sudo systemctl restart mariadb You should now be able to use your new root password to get into the database:\n#sudo mysql -u root -ps We hope these are helpful,\nThank you","set-up-and-install-mariadb#Set up and install MariaDB":" #sudo apt install mariadb-server Root Login\n#sudo mysql -u root -p then provide the password for mariadb\nMariaDB [(none)]\u003e Providing Protection for the Setup\nAfter you have logged into MariaDB as the root user of your database, activate the mysql native password plugin to enable root password authentication.\n#USE mysql; #UPDATE user SET plugin='mysql_native_password' WHERE user='root'; #FLUSH PRIVILEGES; #exit; Execute the mysql secure installation script in order to address a number of security concerns that are present in a default MariaDB installation:\n#sudo mysql_secure_installation You can change MariaDB‚Äôs root password, remove anonymous user accounts, disable root logins outside localhost, and delete test databases. Yes is recommended. MariaDB‚Äôs Knowledge Base has more on the script."},"title":"How To Install MariaDB On Ubuntu 18.04"},"/utho-docs/docs/linux/how-to-install-mariadb-on-ubuntu-22-04/":{"data":{"":"\nHow To Install MariaDB on Ubuntu 22.04\nIntroduction\nMariaDB is a relational database management system that is available as open source software. It is frequently utilised as a substitute for MySQL as the database component of the widely used LAMP (Linux, Apache, MySQL, PHP/Python/Perl) stack of software. It is designed to be a direct replacement for MySQL in all respects.\nThe abridged version of this tutorial to the installation process is comprised of the following three steps:\nUsing apt, bring your package index up to date.\nThe mariadb-server package should be installed using apt. Additionally, associated tools that interface with MariaDB are fetched and included by the package.\nRun the included mysql_secure_installation security script to restrict access to the server\n#sudo apt update #sudo apt install mariadb-server #sudo mysql_secure_installation This guide will show you how to install MariaDB on an Ubuntu 22.04 server, check that it is up and running, and set it up securely.\nPrerequisites\nFor this tutorial to work, a server running Ubuntu 22.04 is required. This server needs a firewall set up with UFW and a non-root administrative user. Follow our basic server setup guide for Ubuntu 22.04 to set this up.","step-1--installing-mariadb#Step 1 ‚Äî Installing MariaDB":"Ubuntu 22.04‚Äôs APT repositories feature MariaDB 10.5.12.\nTo install, update your server‚Äôs apt package index:\n#sudo apt update Then install the package:\n#sudo apt install mariadb-server The MariaDB server will be installed after you execute these commands; however, you will not be prompted to choose a password or make any other configuration modifications. You will use a script that is provided by the mariadb-server package in order to restrict access to the server and remove unnecessary accounts. This is necessary since your installation of MariaDB would be insecure if it is left with the default settings.","step-2--configuring-mariadb#Step 2 ‚Äî Configuring MariaDB":"Running the security script that is included is the next step for fresh MariaDB instals. This script modifies a few of the less secure default settings for things like sample users and remote root logins.\nActivate the security script:\n#sudo mysql_secure_installation This will lead you through a series of prompts where you will have the opportunity to make some modifications to the security parameters that are associated with your MariaDB installation. At the first step, you will be required to enter the root password for the database that is currently being used. Because you have not established one of these yet, use the ENTER key to indicate that there is none.\noutput NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB Output NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MariaDB to secure it, you'll need the current password for the root user. If you've just installed MariaDB, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): If you want to switch to unix socket authentication, you will be prompted. You can skip this step since you already have a protected root account. After entering n, hit ENTER.\noutput . . . Setting the root password or using the unix_socket ensures that nobody can log into the MariaDB root user without the proper authorisation. You already have your root account protected, so you can safely answer 'n'. Switch to unix_socket authentication [Y/n] n You are prompted to create a database root password in the following window. You should avoid changing the configured authentication methods for the root account for MariaDB on Ubuntu because it is tightly linked to automated system upkeep.\nThe removal of access to the administrative account would enable a package update to destroy the database system. Press ENTER after you type ‚Äún‚Äù.\noutput . . . OK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MariaDB root user without the proper authorisation. Set root password? [Y/n] n If socket authentication isn‚Äôt acceptable for your use case, you‚Äôll set up a second administrative account for password access.\nFrom there, press Y and then ENTER to accept all defaults. This removes anonymous users and the test database, disables remote root logins, and loads new rules so MariaDB applies your modifications.\nYou‚Äôve finished configuring MariaDB‚Äôs security. The next step is optional, but you should do it if you want to password-protect MariaDB.","step-3--optional-creating-an-administrative-user-that-employs-password-authentication#Step 3 ‚Äî (Optional) Creating an Administrative User that Employs Password Authentication":"The root MariaDB user is configured by default to authenticate using the unix socket plugin rather than a password on Ubuntu systems running MariaDB 10.5. In many cases, this improves security and usability, but it can also make things more difficult when you need to grant administrative rights to an outside programme, like phpMyAdmin.\nIt is preferable to avoid changing the root account‚Äôs login information because the server uses it for operations like log rotation and starting and terminating the server. Changing the password in the /etc/mysql/debian.cnf configuration file might initially succeed, but future package updates might overwrite those modifications. The package maintainers advise making a different administrative account with password-based access in instead of changing the root account.\nTo do this, a new account called admin will be created and given the same permissions as the root account but with password authentication set up. Open the MariaDB command window from your\n#sudo mariadb Create a new user with root capabilities and restrict access to that user with a password. Ensure that the username and password are updated to reflect your preferences by doing the following:\n#GRANT ALL ON *.* TO 'admin'@'localhost' IDENTIFIED BY 'password' WITH GRANT OPTION; flush the privileges so that they can be saved and used for the current session:\n#FLUSH PRIVILEGES; After that, close the MariaDB shell:\n#exit ","step-4--testing-mariadb#Step 4 ‚Äî Testing MariaDB":"MariaDB will launch automatically after installation from the default repositories. Check the status of that to test this.\n#sudo systemctl status mariadb output ‚óè mariadb.service - MariaDB 10.5.12 database server Loaded: loaded (/lib/systemd/system/mariadb.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2022-03-11 22:01:33 UTC; 14min ago Docs: man:mariadbd(8) https://mariadb.com/kb/en/library/systemd/ . . . With the command sudo systemctl start mariadb, you can start MariaDB if it isn‚Äôt already running.\nThe mysqladmin tool, a client that enables you to issue administrative commands, can be used to establish a connection to the database as an additional check. As an illustration, the following command instructs you to connect to MariaDB as root using a Unix socket and return the version:\n#sudo mysqladmin version output mysqladmin Ver 9.1 Distrib 10.5.12-MariaDB, for debian-linux-gnu on x86_64 Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Server version 10.5.12-MariaDB-1build1 Protocol version 10 Connection Localhost via UNIX socket UNIX socket /run/mysqld/mysqld.sock Uptime: 15 min 53 sec Threads: 1 Questions: 482 Slow queries: 0 Opens: 171 Open tables: 28 Queries per second avg: 0.505 "},"title":"How To Install MariaDB on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-maven-on-debian/":{"data":{"":" How to install Maven on Debian\nIn this article, you will learn how to install Maven on Debian 11 and Debian 10.\nThe Maven lesson explains both the basics and more advanced ideas of the Apache Maven technology. Our maven lesson is made for both newbies and experts. Maven is a strong tool for managing projects. It is built on POM, which stands for ‚Äúproject object model.‚Äù It is used to make, keep track of, and record projects. Like ANT, it makes the building process easier. But it‚Äôs too far ahead of ANT.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges. An updated APT repositories to install java and maven. Java installed on server. If you have did not installed Java on your server, you can follow this guide- How to install Jenkins. ","steps-to-install-maven-on-debian#Steps to install Maven on Debian":"We have two Methods to install the Maven on Linux.\nUsing apt repositories Using wget to download the latest version and work on it. 1. Using apt command to install it Step 1: Update the apt repositories before installing the Maven.\napt-get update Step 2: Now, just install the maven using the below command.\napt-get install maven -y Installing maven on Debian\nStep 3: Now, Check the installed maven version on your server.\nmvn -version Step 4: Even if you did not installed the java yet, you will note that, installing the maven on your server already installed latest available version of the Java on your server.\njava -version Installed java version on your server\n2. Install the maven using wget command Step 1: Visit the page where you can download Maven and choose the version you want to install. In the Files section, you can see the most recent version, and in the Previous Releases part, you can use the backup link to see older versions.\nStep 2: Now, download the file using wget command.As this is a third party application, it would be best if we install it on any other directory than any user‚Äôs home directory. Therefore, we will use the /opt directory, which is in linux for these types of situation\ncd /opt ; wget \u003cdownload-link-of-maven\u003e or\ncd /opt ; wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz Step 3: Now, unzip the file and rename it to make it more suitable.\ntar -zxvf apache-maven-3.9.1-bin.tar.gz mv apache-maven-3.9.1 maven Step 4: Now, setup the Environment variable for your maven to make its binary executable from anywhere on your machine.\nvi /etc/profile.d/maven.sh And, now paste the below lines to this file.\nFile content:\nexport M2_HOME=/opt/maven # path to your maven folder\nexport MAVEN_HOME=/opt/maven # again path to your maven folder\nexport PATH=$M2_HOME/bin/:$PATH\nStep 5: Now make this file executable and source this file.\nchmod +x /etc/profile.d/maven.sh source /etc/profile.d/maven.sh Step 6: By this step, you have installed the maven on your system successfully. To check this, use the below command.\nmvn -version latest version of your Maven\nAnd this is how you have installed maven on Debian machine."},"title":"How to install Maven on Debian"},"/utho-docs/docs/linux/how-to-install-maven-on-ubuntu/":{"data":{"":" How to install Maven on Ubuntu\nIn this article, you will learn how to install Maven on Ubuntu 22.04.\nThe Maven lesson explains both the basics and more advanced ideas of the Apache Maven technology. Our maven lesson is made for both newbies and experts. Maven is a strong tool for managing projects. It is built on POM, which stands for ‚Äúproject object model.‚Äù It is used to make, keep track of, and record projects. Like ANT, it makes the building process easier. But it‚Äôs too far ahead of ANT.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges. An updated APT repositories to install java and maven. Java installed on server. If you have did not installed Java on your server, you can follow this guide- How to install Jenkins. ","steps-to-install-maven-on-ubuntu#Steps to install Maven on Ubuntu":"We have two Methods to install the Maven on Linux.\nUsing apt repositories Using wget to download the latest version and work on it. 1. Using apt command to install it Step 1: Update the apt repositories before installing the Maven.\napt-get update Step 2: Now, just install the maven using the below command.\napt-get install maven -y Step 3: Now, Check the installed maven version on your server.\nmvn -version Step 4: Even if you did not installed the java yet, you will note that, installing the maven on your server already installed latest available version of the Java on your server.\njava -version Installed java version on your server\n2. Install the maven using wget command Step 1: Visit the page where you can download Maven and choose the version you want to install. In the Files section, you can see the most recent version, and in the Previous Releases part, you can use the backup link to see older versions.\nStep 2: Now, download the file using wget command.As this is a third party application, it would be best if we install it on any other directory than any user‚Äôs home directory. Therefore, we will use the /opt directory, which is in linux for these types of situation\ncd /opt ; wget \u003cdownload-link-of-maven\u003e or\ncd /opt ; wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz Step 3: Now, unzip the file and rename it to make it more suitable.\ntar -zxvf apache-maven-3.9.1-bin.tar.gz mv apache-maven-3.9.1 maven Step 4: Now, setup the Environment variable for your maven to make its binary executable from anywhere on your machine.\nvi /etc/profile.d/maven.sh And, now paste the below lines to this file.\nFile content:\nexport M2_HOME=/opt/maven # path to your maven folder\nexport MAVEN_HOME=/opt/maven # again path to your maven folder\nexport PATH=$M2_HOME/bin/:$PATH\nStep 5: Now make this file executable and source this file.\nchmod +x /etc/profile.d/maven.sh source /etc/profile.d/maven.sh Step 6: By this step, you have installed the maven on your system successfully. To check this, use the below command.\nmvn -version latest version of your Maven\nAnd this is how you have installed maven on Ubuntu machine."},"title":"How to install Maven on Ubuntu"},"/utho-docs/docs/linux/how-to-install-minikube-on-centos-7-and-8/":{"data":{"":" How to install Minikube on CentOS 7 and 8\nIn this tutorial, we will learn How to install Minikube on CentOS 7 and 8. Minikube is free software that lets you set up a small Kubernetes cluster with a single computer. The software starts up a virtual machine and runs a Kubernetes cluster inside of it. This lets you test directly in a Kubernetes environment.","prerequisites#Prerequisites":" A normal user with SUDO privileges or Super user Yum repositories configured to install packages in your CentOS server. ","steps-to-install-minikube-on-centos#Steps to install Minikube on Centos":"Step 1: Update your machine\nyum -y update Step 2: Install EPEL repositories using below command.\nyum -y install epel-release Step 3: Install libvirt packages, dependencies to run Minikube\nyum -y install libvirt qemu-kvm virt-install virt-top libguestfs-tools bridge-utils Step 4: Now run, enable and check the status of libvirt daemon to ensure it is running and installed successfully.\nsystemctl enable --now libvirtd systemctl status libvirtd Step 5: Now, add your any normal user to libvirt group by using usermod command.\nusermod -a -G libvirt \u003cnormal-username\u003e usermod -a -G libvirt prabhu # my normal user Step 6: Now, append the below mentioned details to the below file.\nvi /etc/libvirt/libvirtd.conf Append contend to above file::\nunix_sock_group = ‚Äúlibvirt‚Äù\nunix_sock_rw_perms = ‚Äú0770‚Äù\nStep 7: Restart the libvirtd daemon/ service to reflcet the changes.\nsystemctl restart libvirtd.service Download Minikube binary Step 8: Download and install the minikube binary using the curl and install command as shown below.\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Step 9: Check the version installed of your minikube.\nminikube version Minikube install on CentOS\nDownload Kubectl binary and installed it Step 10: Download the latest kubectl binary using the below command\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl Step 11: Now, make the binary executable, so the it can be run\nchmod +x kubectl Step 12: Move the binary to /usr/loca/bin directory so that it can be run from anywhere of your machine.\nmv kubectl /usr/local/bin Step 13: Check the version installed on your centos machine\nkubectl version --client -o json Version of installed kubectl\nInstall Docker container Step 14: Now, without, docker environment, you cannot run minikube. Therefore, run the below command to add the repolist to your machine to install Docker.\nyum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Step 15: Install the docker related dependencies to run your minikube.\nyum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Step 16: Now, run the below command to enable and start the docker service instantly\nsystemctl enable --now docker Step 17: Check the docker running version.\ndocker version Docker installed on Centos\nStep 18: Now, add your normal user( user which is added to libvirt group in earlier step) to the docker group as well.\nusermod -aG docker prabhu Step 19: Now, switch to that user and run the minikube with driver docker as a default driver.\nsu - prabhu minikube start --driver=docker Minikube installed and running on centos\nAnd, that‚Äôs how, you have learnt how to install Minikube on CentOS 7 and 8."},"title":"How to install Minikube on CentOS 7 and 8"},"/utho-docs/docs/linux/how-to-install-minikube-on-debian/":{"data":{"":"","install-kubectl-binary#Install Kubectl binary":"","mv-kubectl-usrlocalbin#mv kubectl /usr/local/bin/":" How to install Minikube on Debian\nThis post will show you how to install Minikube on Debian. Minikube is a single-node Kubernetes (k8s) cluster. If you‚Äôre new to Kubernetes and want to learn about it and test deploying apps on it, minikube is the way to do it. It gives you a command line interface for managing a Kubernetes (k8s) cluster and all of its parts.\nPrerequisites Super user or any normal user with SUDO privileges. Debian server with up to date security patches Machine with pre-installed docker containers. If you did not installed Docker in you machine yet, kindly follow this article to install Docker on Debian. Steps to install Minikube on Debian Step 1: Update your apt repositories and make sure you have installed all security patches\n# apt update \u0026\u0026 apt upgrade -y **Step 2:**Run the following command to install the necessary requirements for Minikube,\n# apt install -y curl wget apt-transport-https Step 3: Use the curl command below to get the newest minikube binaries,\n# curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 Step 4: Once the binary has been downloaded, transfer it to /usr/local/bin and make sure it can be run.\n# install minikube-linux-amd64 /usr/local/bin/minikube Step 5: Now verify the installed minikube version\n# minikube version Installed Minikube version\nInstall Kubectl binary Step 6: Kubectl is a command line tool that lets you talk to a Kubernetes cluster. It is used to control deployments, services, pods, and other things. Use the curl command shown below to get the most recent version of kubectl.\n# curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl Step 7: Once kubectl is downloaded, configure the kubectl binary to be executable and transfer it to /usr/local/bin.\n# chmod +x kubectl mv kubectl /usr/local/bin/ Step 8: Now, let‚Äôs check the installed version of kubectl.\n# kubectl version -o yaml Installed version of kubectl","prerequisites#Prerequisites":"","start-the-minikube#Start the Minikube":"Step 9: As we said in the beginning, we would be using docker as the foundation for minikue, so start the minikube with the docker driver, execute\n$ minikube start --driver=docker Note:: If you run the above command with root privileges, you will encounter the below error. To overcome this, either use ‚Äìforce argument or execute the above command with any normal user.\nError while starting Minikube with super user\nminikube start --drive=docker --force Minikube started on Ubuntu server\nNote:¬†If you wish to start minikube with custom resources and have the installer choose the driver automatically, you may execute the following command:\nminikube start --addons=ingress --cpus=3 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=4g Step 10:¬†Run the minikube command below to see what‚Äôs going on.\nminikube status Output: minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Step 11:¬†Run the following kubectl command to check the version of Kubernetes, the status of each node, and cluster information.\nkubectl cluster-info \u0026\u0026 kubectl get nodes Cluster information after installing minikube","steps-to-install-minikube-on-debian#Steps to install Minikube on Debian":""},"title":"How to install Minikube on Debian"},"/utho-docs/docs/linux/how-to-install-minikube-on-fedora-server/":{"data":{"":" How to install Minikube on Fedora server\nIn this tutorial, we will learn How to install Minikube on Fedora server. Minikube is free software that lets you set up a small Kubernetes cluster with a single computer. The software starts up a virtual machine and runs a Kubernetes cluster inside of it. This lets you test directly in a Kubernetes environment.","prerequisites#Prerequisites":" A normal user with SUDO privileges or Super user Yum repositories configured to install packages. ","steps-to-install-minikube-on-fedora#Steps to install Minikube on Fedora.":"Step 1: Update your machine\nyum -y update Step 2: Install EPEL repositories using below command.\nyum -y install epel-release Step 3: Install libvirt packages, dependencies to run Minikube\nyum -y install libvirt qemu-kvm virt-install virt-top libguestfs-tools bridge-utils Step 4: Now run, enable and check the status of libvirt daemon to ensure it is running and installed successfully.\nsystemctl enable --now libvirtd systemctl status libvirtd Step 5: Now, add your any normal user to libvirt group by using usermod command.\nusermod -a -G libvirt \u003cnormal-username\u003e usermod -a -G libvirt prabhu # my normal user Step 6: Now, append the below mentioned details to the below file.\nvi /etc/libvirt/libvirtd.conf Append contend to above file::\nunix_sock_group = ‚Äúlibvirt‚Äù\nunix_sock_rw_perms = ‚Äú0770‚Äù\nStep 7: Restart the libvirtd daemon/ service to reflect the changes.\nsystemctl restart libvirtd.service Download Minikube binary Step 8: Download and install the minikube binary using the curl and install using the command as shown below.\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Step 9: Check the version installed of your minikube.\nminikube version Minikube install on Fedora\nDownload Kubectl binary and installed it Step 10: Download the latest kubectl binary using the below command\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl Step 11: Now, make the binary executable, so the it can be run\nchmod +x kubectl Step 12: Move the binary to /usr/loca/bin directory so that it can be run from anywhere of your machine.\nmv kubectl /usr/local/bin Step 13: Check the version installed on your Fedora machine\nkubectl version --client -o json Version of installed kubectl\nInstall Docker container Step 14: Now, without, docker environment, you cannot run minikube. Therefore, run the below command to add the repolist to your machine to install Docker.\nyum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo Step 15: Install the docker related dependencies to run your minikube.\nyum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Step 16: Now, run the below command to enable and start the docker service instantly\nsystemctl enable --now docker Step 17: Check the docker running version.\ndocker version Docker installed on Fedora\nStep 18: Now, add your normal user( user which is added to libvirt group in earlier step) to the docker group as well.\nusermod -aG docker prabhu Step 19: Now, switch to that user and run the minikube with driver docker as a default driver.\nsu - prabhu minikube start --driver=docker Minikube installed and running on Fedora\nAnd, that‚Äôs how, you have learnt how to install Minikube on Fedora server."},"title":"How to install Minikube on Fedora server"},"/utho-docs/docs/linux/how-to-install-minikube-on-ubuntu-server/":{"data":{"":"","install-kubectl-binary#Install Kubectl binary":"","mv-kubectl-usrlocalbin#mv kubectl /usr/local/bin/":" How to Install Minikube on Ubuntu server\nThis post will show you how to install Minikube on Ubuntu 22.04 or 20.04 LTS. Minikube is a single-node Kubernetes (k8s) cluster. If you‚Äôre new to Kubernetes and want to learn about it and test deploying apps on it, minikube is the way to do it. It gives you a command line interface for managing a Kubernetes (k8s) cluster and all of its parts.\nPrerequisites Super user or any normal user with SUDO privileges. Ubuntu server with up to date security patches Machine with pre-installed docker containers. If you did not installed Docker in you machine yet, kindly follow this article to install Docker on Ubuntu. Steps to install Minikube on Ubuntu Step 1: Update your apt repositories and make sure you have installed all security patches\n# apt update \u0026\u0026 apt upgrade -y **Step 2:**Run the following command to install the necessary requirements for Minikube,\n# apt install -y curl wget apt-transport-https Step 3: Use the curl command below to get the newest minikube binaries,\n# curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 Step 4: Once the binary has been downloaded, transfer it to /usr/local/bin and make sure it can be run.\n# install minikube-linux-amd64 /usr/local/bin/minikube Step 5: Now verify the installed minikube version\n# minikube version Installed Minikube version\nInstall Kubectl binary Step 6: Kubectl is a command line tool that lets you talk to a Kubernetes cluster. It is used to control deployments, services, pods, and other things. Use the curl command shown below to get the most recent version of kubectl.\n# curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl Step 7: Once kubectl is downloaded, configure the kubectl binary to be executable and transfer it to /usr/local/bin.\n# chmod +x kubectl mv kubectl /usr/local/bin/ Step 8: Now, let‚Äôs check the installed version of kubectl.\n# kubectl version -o yaml Installed version of kubectl","prerequisites#Prerequisites":"","start-the-minikube#Start the Minikube":"Step 9: As we said in the beginning, we would be using docker as the foundation for minikube, so start the minikube with the docker driver, execute\n$ minikube start --driver=docker Note:: If you run the above command with root privileges, you will encounter the below error. To overcome this, either use ‚Äìforce argument or execute the above command with any normal user.\nError while starting minikube with super user\n# minikube start --drive=docker --force start the minikube with the docker driver\nNote: If you wish to start minikube with custom resources and have the installer choose the driver automatically, you may execute the following command:\nminikube start --addons=ingress --cpus=3 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=4g Step 10: Run the minikube command below to see what‚Äôs going on.\n# minikube status Output: minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Step 11: Run the following kubectl command to check the version of Kubernetes, the status of each node, and cluster information.\nkubectl cluster-info \u0026\u0026 kubectl get nodes Cluster information after installing minikube","steps-to-install-minikube-on-ubuntu#Steps to install Minikube on Ubuntu":""},"title":"How to Install Minikube on Ubuntu server"},"/utho-docs/docs/linux/how-to-install-mongodb-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MongoDB on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install MongoDB on AlmaLinux 8.\nMongoDB is a non-relational document database that provides support for¬†JSON-like storage. The MongoDB database has a flexible data model that enables you to store unstructured data, and it provides full indexing support, and replication with rich and intuitive APIs.\nStep 1: Update the system # dnf update -y Step 2: Install MongoDB For MongoDB to work, we need to make one source file. We‚Äôll use vi editor to make a repo file.\n# vi /etc/yum.repos.d/mongodb-org-4.4.repo Add following lines:\n[mongodb-org-4.4]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.4/x86_64/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-4.4.asc baseurl: This points to the URL of a directory where the repository‚Äôs repodata directory, which holds the repository‚Äôs information, can be found.\ngpgcheck: Setting this directive to 1 tells the DNF package manager to allow GPG signature checking on this repository. This requires it to check whether any packages you want to install from this repository have been corrupted or tampered with.\ngpgkey: This option gives the URL of the GPG key that should be imported to check the signatures of packages from this repository.\nSave and exit.\nUse the following command to install the most stable version of MongoDB:\n# dnf install -y mongodb-org Step 3: Start and enable the services # systemctl start mongod # systemctl enable mongod Step 4: Use MongoDB # mongo "},"title":"How to install MongoDB on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-mongodb-on-centos/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MongoDB on CentOS.\nAlso read: How To Install MariaDB 10.7 on CentOS 7\nThank You üôÇ","introduction#Introduction":"","step-1--adding-the-mongodb-repository#\u003cstrong\u003eStep 1 ‚Äì Adding the MongoDB Repository\u003c/strong\u003e":"","step-2--installing-mongodb#\u003cstrong\u003e\u003cstrong\u003eStep 2 ‚Äì Installing MongoDB\u003c/strong\u003e\u003c/strong\u003e":"","step-3--verifying-startup#\u003cstrong\u003eStep 3 ‚Äì Verifying Startup\u003c/strong\u003e":"","step-4--importing-an-example-dataset-optional#\u003cstrong\u003eStep 4 ‚Äì Importing an Example Dataset (Optional)\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to install MongoDB on CentOS.\nMongoDB is a non-relational document database that provides support for JSON-like storage. The MongoDB database has a flexible data model that enables you to store unstructured data, and it provides full indexing support, and replication with rich and intuitive APIs.\nStep 1 ‚Äì Adding the MongoDB Repository The CentOS default repositories do not contain the mongodb-org package. MongoDB does, however, keep a separate repository. Add it to our server now.\nCreate a.repo file for yum, the package management utility for CentOS, using the vi editor by following these steps:\n# vi /etc/yum.repos.d/mongodb-org.repo Then add the following context in file:\n[filecode][mongodb-org-3.4]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.4/x86_64/\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc[/filecode]\nSave the file by escape colon wq\nBefore we continue, we need to make sure that the yum utility contains a MongoDB repository. If it does, then we can carry on. The repolist command will give a list of the repositories that are currently enabled:\n# yum repolist With the MongoDB Repository in place, let‚Äôs proceed with the installation.\nStep 2 ‚Äì Installing MongoDB We can install the mongodb-org package from the third-party repository using the yum utility.\n# yum install mongodb-org -y # systemctl start mongod # systemctl status mongod After performing the start command, the systemctl utility did not produce a result; however, we can verify that the service started by examining the mongod.log file using the tail command and looking at the end of the file:\n# sudo tail /var/log/mongodb/mongod.log The database server, MongoDB, has started properly if the line waiting for a connection appears in the output of the MongoDB Shell:\n# mango The db.help() method returns a set of methods for the db object, which can be used to understand how to interact with MongoDB from the shell.\n# db.help() While exiting the shell, leave the mongod process running in the background:\n# exit Step 3 ‚Äì Verifying Startup Because a database-driven application can‚Äôt work without a database, we‚Äôll make sure that the MongoDB daemon, mongod, starts up with the system.\nCheck its startup status with the systemctl command:\n# systemctl is-enabled mongod; echo $? A result of 0 means that the daemon is running, which is what we want. A one, on the other hand, means that the daemon is disabled and will not start.\nIf a daemon is turned off, use the systemctl command to turn it on:\n# sudo systemctl enable mongod We now have an instance of MongoDB that is running and will start up automatically when the system reboots.\nStep 4 ‚Äì Importing an Example Dataset (Optional) There is no sample data available in the MongoDB test database like there is in other database servers. Instead of risking the integrity of our production data by testing out new software on it, we‚Äôll import a sample dataset from the ‚ÄúGetting Started with MongoDB‚Äù documentation‚Äôs ‚ÄúImport Example Dataset‚Äù section. To get comfortable with working with MongoDB without risking any important information, we‚Äôll be practicing on a JSON document containing a list of restaurants.\nTo begin, please change to a directory that can be modified:\n# cd /tmp To obtain the JSON file, simply use the curl command and the corresponding link from MongoDB:\n# curl -LO [https://raw.githubusercontent.com/mongodb/docs-assets/primer-dataset/primer-dataset.json](https://raw.githubusercontent.com/mongodb/docs-assets/primer-dataset/primer-dataset.json) The data will be imported into the demo database using the mongoimport command. Using the ‚Äîdb parameter, you can provide the database to use; the ‚Äîcollection flag can define the collection within the database; and the ‚Äîfile flag can specify the file to import from.\n# mongoimport --db test --collection restaurants --file /tmp/primer-dataset.json Following the installation of the sample dataset, we will carry out a query on it.\nRelaunch the MongoDB Shell:\n#¬†mongo By default, the shell will use the test database, which is where we transferred our information.\nTo see a complete listing of the restaurants in the dataset, we can use the find() method to query the restaurants collection. Since there are more than 25,000 items in the collection, you can use the limit() method to restrict the results to a smaller size. By inserting newlines and indents, the pretty() technique also improves readability.\n# db.restaurants.find().limit( 1 ).pretty() You can either keep working with the example data to learn more about MongoDB, or you can remove it entirely with the db.restaurants.drop() command.\n# db.restaurants.drop() Finally, use the exit command to exit the shell:\n# exit "},"title":"How to Install MongoDB on CentOS"},"/utho-docs/docs/linux/how-to-install-mongodb-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MongoDB on Debian.\nThank You üôÇ","creating-administrative-mongodb-user#Creating Administrative MongoDB User":"If you chose to enable the MongoDB authentication, you will be required to generate an administrative user who is authorised to access and control the MongoDB instance. To accomplish this, log into the Mongo shell using:\n# mongo In order to connect to the administrative database, enter the following command while you are within the MongoDB shell:\n# use admin Execute the following command in order to create a new user with the name mongoAdmin who will have the role of userAdminAnyDatabase:\ndb.createUser( { user: \"mongoAdmin\", pwd: \"changeMe\", roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } ) output:\nExit the mongo shell with the following:\n# quit() ","installing-mongodb#Installing MongoDB":"Install the necessary packages for creating a new repository, which are as follows:\n# apt install dirmngr gnupg apt-transport-https software-properties-common ca-certificates curl -y Include the GPG key for MongoDB within your system:\n# curl -fsSL https://www.mongodb.org/static/pgp/server-4.2.asc | apt-key add - Make the MongoDB repository available:\n# add-apt-repository 'deb https://repo.mongodb.org/apt/debian buster/mongodb-org/4.2 main' The package list should be brought up to date, and the mongodb-org meta-package should be installed.\n# apt update # apt install mongodb-org -y Launch the MongoDB service and set it to automatically launch at system startup:\n# systemctl enable mongod --now Connecting to the MongoDB database server with the help of the mongo tool and printing the connection status allows one to determine whether or not the installation was carried out without incident.\n# mongo --eval 'db.runCommand({ connectionStatus: 1 })' If the ok field has the value 1, then the operation was successful.","introduction#Introduction":"In this article, you will learn how to install MongoDB on Debian.\nMongoDB is a non-relational document database that provides support for¬†JSON-like storage. The MongoDB database has a flexible data model that enables you to store unstructured data, and it provides full indexing support, and replication with rich and intuitive APIs.\nThe MongoDB database was first made available to the public in February 2009 and is managed and developed by MongoDB.Inc. It is distributed under the Server Side Public License. In addition to this, it offers official driver support for all of the most common programming languages, including C, C++, C#, etc. Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid are the programming languages that are supported. In order to enable the development of applications utilising any one of these languages. These days, there are a great number of businesses, such as Facebook, Nokia, eBay, Adobe, Google, and others, who have chosen to store their massive amounts of data using MongoDB."},"title":"How to Install MongoDB on Debian"},"/utho-docs/docs/linux/how-to-install-mongodb-on-fedora-36-35-34/":{"data":{"":"","introduction#Introduction":"","step-1-update-system#Step 1: Update System":"","step-2-add-dnfyum-mongodb-repositories#Step 2: Add DNF/YUM MongoDB Repositories":"","step-3-install-mongodb-44-on-fedora-363534#Step 3: Install MongoDB 4.4 on Fedora 36/35/34":"","step-4start--enable-mongodb-service#Step 4:¬†Start \u0026amp; Enable MongoDB Service":"\nIntroduction In this article, you will learn how to install MongoDB on Fedora 36/35/34.\nMongoDB is a non-relational document database that provides support for¬†JSON-like storage. The MongoDB database has a flexible data model that enables you to store unstructured data, and it provides full indexing support, and replication with rich and intuitive APIs.\nThe MongoDB database was first made available to the public in February 2009 and is managed and developed by MongoDB.Inc. It is distributed under the Server Side Public License. In addition to this, it offers official driver support for all of the most common programming languages, including C, C++, C#, etc. Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid are the programming languages that are supported. In order to enable the development of applications utilising any one of these languages. These days, there are a great number of businesses, such as Facebook, Nokia, eBay, Adobe, Google, and others, who have chosen to store their massive amounts of data using MongoDB.\nStep 1: Update System # dnf -y update Step 2: Add DNF/YUM MongoDB Repositories The addition of the repositories to the system is the initial step in installing MongoDB 4 on Fedora 36/35/34. This step may take a while.\n# cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/mongodb.repo [mongodb-org-4.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/8/mongodb-org/4.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.4.asc EOF Bringing the Yum cache index up to date:\n# yum clean all # yum makecache Provide a list of YUM mirrors:\n# yum repolist Step 3: Install MongoDB 4.4 on Fedora 36/35/34 The next step is to deploy the mongodb-org package on Fedora.\n# dnf -y install mongodb-org The following command will allow you to determine the version of MongoDB that is currently installed:\n# mongo -version Step 4:¬†Start \u0026 Enable MongoDB Service The final step is to start the MongoDB service and enable it to start automatically on boot.\n# systemctl start mongod.service # systemctl enable mongod.service Perform the following to check the status:\n# systemctl status mongod.service Hopefully, now you have learned how to install MongoDB on Fedora 36/35/34.\nThank You üôÇ"},"title":"How to Install MongoDB on¬†Fedora 36/35/34"},"/utho-docs/docs/linux/how-to-install-mongodb-on-ubuntu-18-10/":{"data":{"":"\nMongoDB is¬†a document database built on a scale-out architecture¬†that has become popular with developers of all kinds who are building scalable applications using agile methodologies. The document data model is a powerful way to store and retrieve data that allows developers to move fast.\nit provides high performance and great scalability features, it is being used for building modern applications that require powerful, mission-critical and high-availability databases.\n1. Login the server with the root credential on the putty.¬†After successfully login you will get looking like bellow mentioned screenshot.\nAfter login first update the system software package cache to have the most latest version of the repository listings. Command given below\n# sudo apt update Next, install MongoDB package that includes several other packages such as¬†mongo-tools,¬†mongodb-clients,¬†mongodb-server¬†and¬†mongodb-server-core.\n# sudo apt install mongodb To start a MongoDB service, type the following command.\n# sudo systemctl stop mongodb To enable again MongoDB service, type the following command.\n# sudo systemctl enable mongodb Once you have successfully installed it, the MongoDB service will start automatically via systemd and the process listens on port¬†27017. You can verify its status using the systemctl command as shown.\n# sudo systemctl status mongodb By default¬†MongoDB¬†comes with user authentication disabled, its therefore started without access control. To launch the¬†mongo shell, run the following command.\n# mongo Once you have connected to the¬†mongo shell, you can list all available databases with the following command.\n# show dbs switch to the¬†admin¬†database, then create the¬†root user¬†using following commands.\nNow exit the mongo shell to enable authentication as explained next.\nThe¬†mongodb¬†instance was started without the¬†--auth¬†command line option. You need to enable authentication of users by editing¬†/lib/systemd/system/mongod.service¬†file, first open the file for editing like so.\nsudo vim /lib/systemd/system/mongodb.service¬†Under the¬†[Service]¬†config section, find the parameter¬†ExecStart.\nChange it to the following.\nSave the file and exit it.\nAfter making changes to configuration file, run ‚Äòsystemctl daemon-reload‚Äò to reload units and restart the MongoDB service and check its status as follows.\n# systemctl daemon-reload # sudo systemctl restart mongodbb # sudo systemctl status mongodbb Now when you try to connect to¬†mongodb, you must authenticate yourself as a MongoDB user. For example:\nThat‚Äôs all!¬†MongoDB¬†is an open-source, modern No-SQL database management system that provides high performance, high availability, and automatic scaling.\nThank you :)"},"title":"How to install mongodb on Ubuntu 18.10"},"/utho-docs/docs/linux/how-to-install-mongodb-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install MongoDB on Ubuntu 20.04.\nThank You üôÇ","creating-administrative-mongodb-user#Creating Administrative MongoDB User":"If you chose to enable the MongoDB authentication, you will be required to generate an administrative user who is authorised to access and control the MongoDB instance. To accomplish this, log into the Mongo shell using:\n# mongo In order to connect to the administrative database, enter the following command while you are within the MongoDB shell:\n# use admin Execute the following command in order to create a new user with the name mongoAdmin who will have the role of userAdminAnyDatabase:\ndb.createUser( { user: \"mongoAdmin\", pwd: \"changeMe\", roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } ) output:\nExit the mongo shell with the following:\n# quit() ","installing-mongodb#Installing MongoDB":" # curl -fsSL https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - # apt-key list # echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-4.4.list The package list should be brought up to date, and the mongodb-org meta-package should be installed.\n# apt update # apt install mongodb-org ","introduction#Introduction":"In this article, you will learn how to install MongoDB on Ubuntu 20.04.\nMongoDB is a non-relational document database that provides support for¬†JSON-like storage. The MongoDB database has a flexible data model that enables you to store unstructured data, and it provides full indexing support, and replication with rich and intuitive APIs.\nThe MongoDB database was first made available to the public in February 2009 and is managed and developed by MongoDB.Inc. It is distributed under the Server Side Public License. In addition to this, it offers official driver support for all of the most common programming languages, including C, C++, C#, etc. Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid are the programming languages that are supported.\nIn order to enable the development of applications utilising any one of these languages. These days, there are a great number of businesses, such as Facebook, Nokia, eBay, Adobe, Google, and others, who have chosen to store their massive amounts of data using MongoDB.","starting-the-mongodb-service#Starting the MongoDB Service":" # systemctl start mongod.service # systemctl status mongod # systemctl enable mongod Connecting to the MongoDB database server with the help of the mongo tool and printing the connection status allows one to determine whether or not the installation was carried out without incident.\n# mongo --eval 'db.runCommand({ connectionStatus: 1 })' If the ok field has the value 1, then the operation was successful."},"title":"How to Install MongoDB on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-multicraft-on-ubuntu-20-04/":{"data":{"":"","1-install-the-dependencies#1. Install the Dependencies":"","2-install-multicraft#2. Install Multicraft":"","3-configure-the-control-panel#3. Configure the Control Panel":"","4-change-the-administrator-account-password#4. Change the Administrator Account Password":"\nIntroduction In this article, you will learn how to install Multicraft on ubuntu 20.04.\nMulticraft is a powerful admin tool for running many Minecraft servers from the same computer. The purpose of this article is to provide instructions for installing Multicraft on an Ubuntu 20.04 server. So start with update.\n# apt-get update 1. Install the Dependencies Install Apache2 and SQLite.\n# apt-get install apache2 sqlite -y Install PHP and the required PHP extensions.\n# apt-get install php7.4 php7.4-sqlite php7.4-gd -y Install Java.\n# apt-get install openjdk-8-jdk -y Edit the Apache configuration file¬†/etc/apache2/apache2.conf¬†in your text editor.\n# vi /etc/apache2/apache2.conf Apache‚Äôs Directory /var/www/\u003e step needs the AllowOverride option to be set to all.\nSave and exit the file, by using escape :wq\nReload the Apache configuration.\n# service apache2 reload 2. Install Multicraft Download the Multicraft installer.\n# wget -O multicraft.tar.gz http://www.multicraft.org/download/index?arch=linux64 Extract the installer.\n# tar -xzf multicraft.tar.gz Navigate to the extracted directory.\n# cd multicraft/ Run the installation script.\n# ./setup.sh During installation, the script will prompt you to answer a series of questions to customise the process. Press ENTER to accept the system‚Äôs suggested responses if you‚Äôre at a loss for what to type. If prompted to enable the in-built FTP server, you should say no.\n3. Configure the Control Panel In a web browser, go to the server‚Äôs IP address and then the subdirectory /multicraft/install.php.\nPress¬†Start Installation.\nIf you‚Äôve completed the steps above without an error, the page shows your server meets the requirements. Click¬†Continue.\nMulticraft asks you for a confirmation to copy the configuration file. Press¬†Continue.\nClick¬†Initialize Database. Multicraft has initialized the database for use. Press¬†Continue¬†once again.\nClick¬†Login¬†in the black navigation bar at the top of the page.\nA sign in form shows up on your screen. Input the name¬†microhost¬†and password¬†microhost. Click¬†Login.\nPress¬†Continue¬†again.\nClick¬†Initialize Database¬†again and then¬†Continue.\nFill in your email address into the¬†Administrator contact Email¬†field.\nClick the¬†Save¬†button.\nStart the Multicraft daemon.\n# /home/minecraft/multicraft/bin/multicraft -v start To resume browsing, simply reopen your current browser. To continue, please refresh the page. Select Multicraft and hit the Next button.\nFor safety reasons, you should remove the Multicraft setup file.\n# rm /var/www/html/multicraft/install.php 4. Change the Administrator Account Password To keep your Multicraft control panel safe from hackers, you should update the password for the administrator account.\n1. Choose Users from the top menu.\n2. In the Users menu, click on My Profile.\n3. To change your current password, click the Change button and enter admin.\n4. Fill out the New Password and Confirm Password sections with a new password.\n5. Press¬†Save.","conclusion#Conclusion":"Following reading this article, you should now be able to successfully set up Multicraft.\nThank You üôÇ","introduction#Introduction":""},"title":"How to install Multicraft on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-mysql-on-ubuntu-20-04/":{"data":{"":"\nMySQL is one of the most popular and widely used open-source relational database management systems.\nHosting MySQL databases on Ubuntu 20.04 requires installing the¬†MySQL Server¬†package.\nStep 1. Login to your server via SSH Putty\nStep 2.¬†Update/Upgrade Package Repository by running the command\n# sudo apt update Step 3. Install MySQL by running the following command\n# sudo apt install mysql-server When asked if you want to continue with the installation, answer¬†Y¬†and hit¬†ENTER.\nThe system downloads MySQL packages and installs them on your machine.\nStep 4. Check if MySQL was successfully installed by running:\n# mysql --version The output shows which version of MySQL is installed on the machine.\nStep 5.¬†Secure your MySQL user account with password authentication by running the included security script:\n#¬†sudo mysql_secure_installation Enter your password and answer¬†Y¬†when asked if you want to continue setting up the¬†VALIDATE PASSWORD¬†component. The component checks to see if the new password is strong enough.\nChoose one of the¬†three levels¬†of password validation:\n0¬†- Low. A password containing at least 8 characters. 1¬†- Medium. A password containing at least 8 characters, including numeric, mixed case characters, and special characters. 2¬†- Strong. A password containing at least 8 characters, including numeric, mixed case characters, and special characters, and compares the password to a dictionary file. Enter¬†0,¬†1, or¬†2¬†depending on the password strength you want to set. The script then instructs you to enter your password and re-enter it afterward to confirm.\nAny subsequent MySQL user passwords¬†need to match your selected password strength.\nThe program estimates the strength of your password and requires confirmation to continue.\nPress¬†Y¬†if you are happy with the password or any other key if you want a different one.\nStep 6.¬†The script then prompts for the following security features:\nRemove anonymous users? Disallow root login remotely? Remove test database and access to it? Reload privilege tables now? The recommended answer to all these questions is¬†Y. However, if you want a different setting for any reason, enter any other key.\nStep 7.¬†Upon successfully installing MySQL, the MySQL service starts automatically.\nVerify that the MySQL server is running by running:\n#¬†sudo systemctl status mysql The output above shows that the service is operational and running.\nStep 8.¬†Finally, to log in to the MySQL interface, run the following command:\n#¬†sudo mysql -u root Now you can execute queries, create databases, and test out your new MySQL setup.\nPlease find a link below to have a look at some important¬†MySQL commands.LINK:¬†https://phoenixnap.com/kb/mysql-commands-cheat-sheet\nYou now have a fully functioning MySQL server installed on your machine.\nThank You."},"title":"How to Install MySQL on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-mysql-relational-databases-on-fedora-12/":{"data":{"":"\nMany web and server programmes rely on MySQL as their database management system. Following this tutorial will make working with MySQL on Fedora 12 Instance much easier for newcomers. It is assumed that you have updated your system, followed the instructions in our Setting Up and Securing a Compute Instance guide, and logged into your Instance as root via SSH before proceeding with this tutorial.","configure-mysql#Configure mysql":"Mysql secure installation is a software that may be executed after MySQL has been installed to assist strengthen security. In addition to changing the MySQL root password, removing anonymous users, disabling root logins from hosts other than localhost, and erasing test databases are also options provided while executing mysql secure installation. You should select ‚Äúyes‚Äù for these questions. When asked, choose ‚Äúyes‚Äù to reload the privilege tables. To begin using the application, use the following command:\n#mysql_secure_installation #service mysqld restart ","installing-mysql#Installing MySQL":" #yum update #yum install mysql-server #service mysqld start ","resetting-the-mysql-root-password#Resetting the MySQL Root Password":"If you have forgotten your MySQL root password, you can retrieve it using the following commands:\n#service mysqld stop #mysqld_safe --skip-grant-tables #mysql -u root The MySQL client programme will now handle the next part of the password reset:\n#USE mysql; #UPDATE user SET PASSWORD=PASSWORD(\"CHANGEME\") WHERE User='root'; #FLUSH PRIVILEGES; #exit; Final step, restart MySQL with this command:\n#service mysqld restart Thank you","system-configuration#System Configuration":"please ensure that your /etc/hosts file has complete entries, similar to the given shown below\nFile: /etc/hosts 1.127.0.0.1 localhost.yourdoamin localhost 2.12.34.56.78 servername.yourdomain.com servername "},"title":"How to install MySQL Relational Databases on Fedora 12"},"/utho-docs/docs/linux/how-to-install-ncurses-library-on-ubuntu-20-04/":{"data":{"":"\nDescription\nIn this article, we will acquire new knowledge how to Install Ncurses Library on Ubuntu 20.04. Ncurses, also known as the new curses library, is an emulation of curses that can be run on System V Release 4.0 (SVr4) as well as a great number of other systems. Ncurses is a free and open-source piece of software. It makes use of the terminfo format, supports pads and colour and multiple highlights, forms characters, and function-key mapping, and it possesses all of the additional advancements that SVr4-curses offers over BSD curses. The SVr4 curses served as the foundation for the X/Open curses. In this section, we will go over the procedures required to install Ncurses on computers running Ubuntu 20.04 LTS.Now we will follow the steps given below to how to Install Ncurses Library on Ubuntu 20.04","step-1-update-your-server#Step 1: Update Your Server":"Ncurses library package on your system, it‚Äôs always a good idea to sync your system with the latest available changes on the default Ubuntu repo by using the commands sudo apt update \u0026\u0026 sudo apt upgrade, as shown below.\napt update \u0026\u0026 sudo apt upgrade ","step-2-install-package-ncurses-library#Step 2: Install Package Ncurses Library":"Use the command apt-get install libncurses5-dev libncursesw5-dev to install the Ncurses library. This will get the package and all of its dependencies from the usual Ubuntu repository and install them.\napt-get install libncurses5-dev libncursesw5-dev ","step-3-verify-package#Step 3: Verify Package":"You will be able to confirm that the correct library package was installed by using the dpkg -L libncurses-dev, libncurses5-dev, and libncursesw5-dev commands, as demonstrated in the following example.\ndpkg -L libncurses-dev libncurses5-dev libncursesw5-dev ","step-4-uninstall-package-ncurses-library#Step 4: Uninstall Package Ncurses Library":"You have the option to remove it from your system by using the command sudo apt-get delete libncurses5-dev libncursesw5-dev, which is demonstrated in the following paragraph.\nImportant:- Be careful when using the below command, since it has the potential to delete any of the dependant packages that are needed by other applications that are now running on the system.\napt-get remove libncurses5-dev libncursesw5-dev I hope you found this post helpful How to Install Ncurses Library on Ubuntu 20.04 to read more knowledge base articles, go to the microhost website.\nMust Read:- https://utho.com/docs/tutorial/how-to-install-the-opengl-library-on-ubuntu-20-04/\nThankYou"},"title":"How to Install Ncurses Library on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-neofetch-on-ubuntu-20-04-lts/":{"data":{"":" How to Install Neofetch on Ubuntu 20.04 LTS\nDescription\nIn this article, we will learn how to install Neofetch on Ubuntu 20.04 LTS. Neofetch is a free command-line tool written in Bash 3.2+ that is open source. to make it easy for you to see information about your operating system, software, and hardware. Neofetch lets you set up and show information about the operating system the way you want. You can set up your computer to use an image, your wallpaper, a custom ASCII file, or nothing at all. Neofetch works on more than 150 operating systems, including Windows, Linux, and Mac OS X. It‚Äôs also easy to set up on almost every operating system. Here are the steps to install Neofetch on systems running Ubuntu 20.04 LTS.\nFollow the below steps to learn How to Install Neofetch on Ubuntu 20.04 LTS","step-1-update-server#Step 1: Update Server":"You must install all available updates from the default Ubuntu repository using the command sudo apt update and upgrade all packages to the most recent version.\nsudo apt update ","step-2-install-neofetch-package#Step 2: Install Neofetch package":"You can use the sudo apt install neofetch command to install neofetch from the default Ubuntu repository, as shown below. This will download and install the package, as well as any dependencies it may have.\nsudo apt install neofetch ","step-3-check-installation#Step 3: Check Installation":"After the installation is done, you can use the dpkg -L newfetch command, as shown below, to check the installed file path.\ndpkg -L neofetch ","step-4-version#Step 4: Version":"With the neofetch ‚Äîversion command, as demonstrated below, you can also determine the current installed version.\nneofetch --version ","step-5-launch-neofetch#Step 5: Launch Neofetch":"To utilise the neofetch utility, start a command-line terminal and type neofetch to obtain all system-related information.\nneofetch I sincerely hope that each and every one of these things was clear to you. How to Install Neofetch on Ubuntu 20.04 LTS\nMust read :- https://utho.com/docs/tutorial/find-multiple-ways-to-user-account-info-and-login-details-in-linux/"},"title":"How to Install Neofetch on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-install-netstat-on-ubuntu-20-04-lts/":{"data":{"":" How to Install netstat on Ubuntu 20.04 LTS\nDescription\ninstructions on how to instal netstat on a computer running Ubuntu 20.04 LTS. netstat is a utility that can be run from the command line that provides statistics about ports and protocols. It is possible to use it to display the current state of TCP and UDP endpoints in table format, as well as information regarding routing tables and interfaces. It is a highly well-known programme that is predominantly utilised by Unix and Linux administrators all over the world to troubleshoot any port and network interface problems that may arise.\nBecause it is distributed as a component of the Net Tools package, the installation of this tool requires that the Net Tools package itself be installed first. However, for the purpose of this tutorial, we will focus on installing netstat on Ubuntu 20.04 LTS because its installation is quite straightforward across the majority of Linux distributions.","check-installation#Check Installation":"Using the dpkg -L net-tools command, you can also check the installed file path.\ndpkg -L net-tools ","check-version#Check Version":"If the installation went well, you can use the netstat ‚Äîversion command to check the version that was installed.\nnetstat --version ","install-netstat#Install Netstat":"Using the command sudo apt instal net-tools, you can download and instal the netstat programme that is included in the default Ubuntu repository. The package, as well as any and all of its dependencies, will be downloaded and installed as a result of this action.\nsudo apt install net-tools ","more-info-of-netstat#More info of netstat":" netstat --help I sincerely hope that you have a complete understanding of everything\nMust Read : Convert rwx permissions to octal format in Linux","update-your-server#Update Your Server":"Use sudo apt update and sudo apt upgrade before installing a new package in the system to sync the system packages with the most recent available updates from the Ubuntu repo.\nsudo apt update \u0026\u0026 sudo apt upgrade ","use-of-netstat#Use of netstat":"Let‚Äôs make use of the fact that the netstat tool has been successfully installed by listing the current status of TCP ports using it.\nsudo netstat -ntlp "},"title":"How to Install netstat on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-install-nginx-web-server-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install NGINX Web Server on Debian 10.\nAlso Read:¬†How to host a domain on centos 7\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install NGINX Web Server on Debian 10.\nNGINX¬†is free software that can be used for serving web pages, reverse proxying, caching, balancing the load, streaming media, and more. It began out as a web server meant to be as fast and stable as possible. In addition to being an HTTP server, NGINX may also act as a proxy server for email (IMAP, POP3, and SMTP) and as a reverse proxy and load balancer for HTTP, TCP, and UDP servers.","step-1-install-nginx#Step 1: Install NGINX":"1. Install NGINX with the use of the package manager.\n# apt install nginx -y 2. The NGINX service immediately begins its normal operation. You can use the following command to check the current state of it:\n# systemctl status nginx 3. You can start using NGINX another time with the following command:\n# systemctl enable nginx 4. To check out how well your installation is working, go to the default NGINX website. You‚Äôll locate it by going to the server‚Äôs IP address in your browser.\n# http://server\\_ip Use NGINX Through the use of NGINX, this section will guide you through the process of putting up your own website. This explains not only how to set up an NGINX proxy to deliver static content but also how to do so.","step-2-nginx-configuration#Step 2: NGINX Configuration":"1. Disable the NGINX configuration file that is provided by default.\n# unlink /etc/nginx/sites-enabled/default 2. Generate an NGINX configuration file for the website you‚Äôre working on. In this example, replace both the filename and the contents of the file with your site‚Äôs domain. Do the same thing from now on anytime you see example.com.\n# vi /etc/nginx/sites-available/example.com Paste the following content in this file:\nserver { listen 80; listen [::]:80; server_name example.com; root /var/www/example.com; index index.html; location / { try_files $uri $uri/ =404; } } 3. Bring your NGINX site live.\n# ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ 4. Run the following command to check if the NGINX configuration file is correct or not:\n# nginx -t 5. In order for changes to take effect, you will need to restart NGINX.\n# systemctl restart nginx ","step-3-set-up-the-website#Step 3: Set Up the Website":"1. Make a directory to store the content of your NGINX website.\n# mkdir /var/www/example.com 2. In the new NGINX site directory, you must create an index.html page.\n# vi /var/www/example.com/index.html Paste the following content in this file:\n\u003c!doctype html\u003e \u003chtml\u003e \u003cbody\u003e \u003ch1\u003eHello, Guys!\u003c/h1\u003e \u003cp\u003eThis is an example website running on NGINX.\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 3. Just type your site‚Äôs domain name into a browser to check it out. There should be a ‚ÄúHello, Guys!‚Äù page shown on your domain."},"title":"How to install NGINX Web Server on Debian 10"},"/utho-docs/docs/linux/how-to-install-nginx-web-server-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install NGINX Web Server on Debian 12.\nAlso Read:¬†How to host a domain on centos 7\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install NGINX Web Server on Debian 12.\nNGINX¬†is free software that can be used for serving web pages, reverse proxying, caching, balancing the load, streaming media, and more. It began out as a web server meant to be as fast and stable as possible. In addition to being an HTTP server, NGINX may also act as a proxy server for email (IMAP, POP3, and SMTP) and as a reverse proxy and load balancer for HTTP, TCP, and UDP servers.","step-1-install-nginx#Step 1: Install NGINX":"1. Install NGINX with the use of the package manager.\n# apt install nginx -y 2. The NGINX service immediately begins its normal operation. You can use the following command to check the current state of it:\n# systemctl status nginx 3. You can start using NGINX another time with the following command:\n# systemctl enable nginx 4. To check out how well your installation is working, go to the default NGINX website. You‚Äôll locate it by going to the server‚Äôs IP address in your browser.\n# http://server\\_ip Use NGINX Through the use of NGINX, this section will guide you through the process of putting up your own website. This explains not only how to set up an NGINX proxy to deliver static content but also how to do so.","step-2-nginx-configuration#Step 2: NGINX Configuration":"1. Disable the NGINX configuration file that is provided by default.\n# unlink /etc/nginx/sites-enabled/default 2. Generate an NGINX configuration file for the website you‚Äôre working on. In this example, replace both the filename and the contents of the file with your site‚Äôs domain. Do the same thing from now on anytime you see example.com.\n# vi /etc/nginx/sites-available/example.com Paste the following content in this file:\nserver { listen 80; listen [::]:80; server_name example.com; root /var/www/example.com; index index.html; location / { try_files $uri $uri/ =404; } } 3. Bring your NGINX site live.\n# ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ 4. Run the following command to check if the NGINX configuration file is correct or not:\n# nginx -t 5. In order for changes to take effect, you will need to restart NGINX.\n# systemctl restart nginx ","step-3-set-up-the-website#Step 3: Set Up the Website":"1. Make a directory to store the content of your NGINX website.\n# mkdir /var/www/example.com 2. In the new NGINX site directory, you must create an index.html page.\n# vi /var/www/example.com/index.html Paste the following content in this file:\n\u003c!doctype html\u003e \u003chtml\u003e \u003cbody\u003e \u003ch1\u003eHello, Guys!\u003c/h1\u003e \u003cp\u003eThis is an example website running on NGINX.\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 3. Just type your site‚Äôs domain name into a browser to check it out. There should be a ‚ÄúHello, Guys!‚Äù page shown on your domain."},"title":"How to install NGINX Web Server on Debian 12"},"/utho-docs/docs/linux/how-to-install-node-js-and-npm-on-ubuntu-20-04/":{"data":{"":"","1-install-nodejs-and-npm-from-ubuntu-repository#1. Install Node.js and npm from Ubuntu Repository":"","2-install-nodejs-and-npm-from-nodesource#2. Install Node.js and npm from NodeSource":" How to Install Node.js and npm on Ubuntu 20.04\nThis tutorial is to learn the method of how to Install Node.js and npm on Ubuntu 20.04. Based on Chrome‚Äôs V8 JavaScript engine, Node.js is an open-source, cross-platform JavaScript runtime environment. It is typically used for conventional web pages and back-end API services and is built for non-blocking, event-driven servers. You can rapidly create network apps thanks to it. By using JavaScript on both the front and backend, it improves development consistency. The Node.js registry and package management is called npm. It is used to create, distribute, find, and install node applications.\nIn this article, we will show you a few different ways to install Node.js and npm on an Ubuntu 20.04 server.\nPrerequisites Apt repository ready and available to install packages Super user or any normal user with SUDO privileges 1. Install Node.js and npm from Ubuntu Repository Node.js and npm may be installed from the Ubuntu default repository in the simplest and most convenient method possible. It does not, however, include the most recent Node.js version. The most recent Node.js version that is compatible with Ubuntu 20.04 at the time this guide was written is 10.19.0.\nFirst, execute the following command to update the system packages:\napt-get update -y Use the following command to install Node.js and npm after all the packages have been updated:\napt-get install nodejs npm -y Once both packages are installed, verify the Node.js and npm version using the following command:\nnode -v installed versions of nodejs and npm\n2. Install Node.js and npm from NodeSource Numerous Node.js versions are available in the APT repository that is maintained by NodeSource. You may use it to install a certain Node.js version on your computer.\nWe will install Node.js version 14.x from the NodeSource in this step.\nStart by running the following command to install curl:\napt-get install curl -y Run the below command to download and launch the Node.js installation script:\ncurl -sL https://deb.nodesource.com/setup_14.x | bash - The GPG key and Node.js repository will now be added to the APT.\nThe next step is to install Node.js 14.x by executing the following command:\napt-get install nodejs -y Once Node.js is set up, use the following command to check its version:\nnode -v latest version of nodejs","install-nodejs-and-npm-with-nvm#Install Node.js and npm with NVM":"A script called NVM, commonly referred to as ‚ÄúNode Version Manager,‚Äù enables you to handle numerous Node.js versions.\nYou must first download and install NVM on your computer. The following command may be used to manually download and execute the script:\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.37.2/install.sh | bash After installation, you ought to see the following result:\nOutput of successfully installation of nvm\nOf add the path to the nvm script to the current shell session, you must next close and reopen the terminal.\nNext, use the following command to confirm the NVM version:\nnvm --version NVM‚Äôs version\nThe following command will list every Node.js version that can be installed with NVM:\nnvm list-remote You will get a lengthy list of every Node.js version when you run this command.\nNext, run the command to install Node.js‚Äô most recent stable version:\nnvm install node status of node installation\nRun the following command to install the most recent LTS version:\nnvm install --lts Output of above command\nThe following command will now list every Node.js version that is currently installed:\nnvm ls Lists of installed nodejs version\nYou learned three distinct methods in the mentioned instruction to install Node.js on an Ubuntu 20.04 server. You may now install Node.js in the manner that best suits your requirements.","prerequisites#Prerequisites":""},"title":"How to Install Node.js and npm on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-node-js-on-centos-8/":{"data":{"":"\nintroduction\nIn this aricle you will learn How To Install Node.js on CentOS 8..\nNode.js is a runtime for JavaScript server-side development. It enables developers to design scalable backend functionality using JavaScript, a language that many web browser developers are already familiar with.\nIn this post, we will demonstrate three distinct methods for installing Node.js on a CentOS 8 server:\nutilising dnf to install the nodejs package from CentOS‚Äôs default AppStream repository utilising nvm, the Node Version Manager, to install and manage different versions of node constructing and installing node from source\nThe majority of users should utilise dnf to install the pre-packaged versions of Node that are pre-installed with their operating system. Use the nvm method if you are a developer or otherwise require the management of many Node installations. Rarely are most users required to build from source.\nTo Install Node.js on CentOS 8 follow the below steps that will help you to increase you knowledge..\nPrerequisites\nYou will require a server running CentOS 8 in order to finish this tutorial. We‚Äôll presume you‚Äôre signed in as a non-root, sudo-capable user on this server.","step-1-node-installation-from-the-centos-appstream-repository#Step .1 Node installation from the CentOS AppStream Repository":"Node.js is accessible via the default AppStream software repository on CentOS 8. Multiple versions are available, and you can choose between them by activating the desired module stream. Using the dnf command, list the available streams for the nodejs module.\n#sudo dnf module list nodejs Output Name Stream Profiles Summary nodejs 10 [d] common [d], development, minimal, s2i Javascript runtime nodejs 12 common, development, minimal, s2i There are two streams available: 10 and 12. The [d] indicates that the default stream is version 10. Switch module streams now if you prefer to install Node.js 12:\n#sudo dnf module enable nodejs:12 You will be asked to confirm your choice. The version 12 stream will then be enabled, and we can proceed with the installation. See the official CentOS AppStream documentation for more information on working with module streams.\nUsing dnf, install the nodejs package:\n#sudo dnf install nodejs dnf will ask you to confirm the actions it will take once more. To do so, press y then ENTER, and the software will be installed.\nCheck that the installation was successful by inquiring about the version number of node:\n#node --version Output v12.13.1 If you installed Node.js 10 instead, the answer you get from ‚Äîversion will be different.\nNoteBoth Node.js versions are long-term support releases, which means they have a longer guaranteed window of maintenance. More lifecycle information can be found on the official Node.js releases page.\nThe npm Node Package Manager tool should be installed as a dependency along with the nodejs package. Check to make sure it was installed correctly as well:\n#npm --version Output 6.12.1 At this point, you have successfully used the CentOS software repositories to install Node.js and npm. In the next section, you‚Äôll learn how to do this with the help of the Node Version Manager.","step-3-putting-node-in-place-from-its-source-code#Step .3 Putting Node in place from its source code":"You can also download the source code and compile it yourself to set up Node.js.\nTo do this, use your web browser to go to the official Node.js download page, right-click on the Source Code link, and choose Copy Link Address or a similar option.\nOnce you‚Äôre back in your SSH session, make sure you‚Äôre in a directory where you can write. We‚Äôll use the home directory of the user who is logged in:\n#cd ~ After that, you should type curl, then paste the link that you copied from the webpage, and after that, you should type | tar xz:\n#curl https://nodejs.org/dist/v12.16.1/node-v12.16.1.tar.gz | tar xz This will download the source using curl, then pipe it directly to the tar utility, which will extract it into the current directory.\nEnter the following code into the newly created source directory:\n#cd node-v* In order to compile the code, we need to get a few packages from the CentOS repositories. Install these now with dnf:\n#sudo dnf install gcc-c++ make python2 You will be asked to confirm that you want to install. To do this, type y and press ENTER. We can now set up and build the software:\n#./configure #make -j4 It will take a long time to put everything together (around 30 minutes on a four-core server). We used the -j4 option to make four compilations happen at the same time. You can leave this option out or change the number to match how many processor cores you have.\nWhen the compilation is done, you can type: to install the software on your system.\n#sudo make install Ask Node to show its version number to see if the installation went well:\n#node --version output v12.16.1 If you see the right version number, that means the installation went well. By default, Node also instals a version of npm that works with it, so that should also be available.\nMust read:- https://utho.com/docs/tutorial/2-methods-for-re-running-last-executed-commands-in-linux/\nYou should now understand How to Install Node.js on CentOS 8‚Ä¶\nThankyou","step2-node-version-manager-installation#Step.2 Node Version Manager installation":"Using nvm, the Node Version Manager, is another flexible method of installing Node.js. Using this piece of software, you can simultaneously install and maintain numerous separate Node.js versions and their corresponding Node packages.\nVisit the project‚Äôs GitHub website to learn how to install NVM on a CentOS 8 system. From the README file that appears on the home page, copy the curl command. You can obtain the installation script‚Äôs most recent version by doing this.\nIt is always a good idea to audit the script to make sure it isn‚Äôt doing anything you don‚Äôt agree with before pipelining the command through to bash. The | bash segment at the end of the curl command can be removed to do that:\n#curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh Check to see whether you like the modifications. When done, run the command with | bash added. The script can be downloaded and performed by typing:\n#curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bash This adds the nvm script to your user account. To use it, first locate your.bash profile file:\n#source ~/.bash_profile You are now able to inquire with NVM regarding the versions of Node that are available:\n#nvm list-remote v12.13.0 (LTS: Erbium) v12.13.1 (LTS: Erbium) v12.14.0 (LTS: Erbium) v12.14.1 (LTS: Erbium) v12.15.0 (LTS: Erbium) v12.16.0 (LTS: Erbium) v12.16.1 (Latest LTS: Erbium) v13.0.0 v13.0.1 v13.1.0 v13.2.0 v13.3.0 v13.4.0 v13.5.0 v13.6.0 It‚Äôs a lengthy list! By typing any of the release versions you see, you can install a particular version of Node. For instance, you can type: to receive version v13.6.0.\n#nvm install v13.6.0 You can see what versions you have by typing:\n#nvm list Output -\u003e v13.6.0 default -\u003e v13.6.0 node -\u003e stable (-\u003e v13.6.0) (default) stable -\u003e 13.6 (-\u003e v13.6.0) (default) The first line shows the currently active version (-\u003e v13.6.0), followed by some named aliases and the versions to which those aliases point.\nNote If you also have a version of Node installed from the CentOS software repositories, you may see a system -\u003e v12.13.1 (or some other version number) line here. You can always use nvm use system to turn on the system version of Node.\nIn addition, you will notice aliases for the various Node long-term support (LTS) releases:\nOutput lts/* -\u003e lts/erbium (-\u003e N/A) lts/argon -\u003e v4.9.1 (-\u003e N/A) lts/boron -\u003e v6.17.1 (-\u003e N/A) lts/carbon -\u003e v8.17.0 (-\u003e N/A) lts/dubnium -\u003e v10.19.0 (-\u003e N/A) lts/erbium -\u003e v12.16.1 (-\u003e N/A) With these aliases, we can also install a release. For example, run the following to install the latest version of erbium that has long-term support:\n#nvm install lts/erbium Output Downloading and installing node v12.16.1... . . . Now using node v12.16.1 (npm v6.13.4) With nvm, you can change between installed versions by using:\nnvm use v13.6.0 [/console]Now using node v13.6.0 (npm v6.13.4)[/console]\nUsing the same method as in the other sections, you can check to see if the install went well by typing:\n#node --version Output v13.6.0 As expected, the correct version of Node is installed on our machine. There is also a compatible version of npm available."},"title":"How To Install Node.js on CentOS 8"},"/utho-docs/docs/linux/how-to-install-node-js-on-ubuntu-20-04/":{"data":{"":"\nIn this article you will see How To Install Node.js on Ubuntu 20.04..\nIntroduction\nNode.js is a runtime for JavaScript server-side development. It enables developers to design scalable backend functionality using JavaScript, a language that many web browser developers are already familiar with.\nThis post will demonstrate three distinct methods for installing Node.js on an Ubuntu 20.04 server:\n_Installing the nodejs package from Ubuntu‚Äôs default software repository using apt\n_Using apt with an alternative PPA software repository to install specific versions of the nodejs package\n*installing nvm, the Node Version Manager, and using it to install and manage different versions of Node.js.\nPrerequisites\nThis guide assumes you are using Ubuntu 20.04. Set up a non-root user account with sudo privileges on your system before you start.","step1-apt-installation-of-nodejs-from-the-default-repositories#Step.1 Apt Installation of Node.js from the Default Repositories":"Node.js is included in Ubuntu 20.04‚Äôs default repositories and can be used to deliver an uniform user experience across many platforms. The version in the repositories as of this writing is 10.19. Although this won‚Äôt be the most recent version, it should be reliable and sufficient for fast language testing.\nwarningNode.js version 10.19, which came with Ubuntu 20.04, is no longer supported or maintained. Use one of the other sections of this guide to install a more recent version of Node instead of using this one in production.\n#sudo apt update install Node.js:\n#sudo apt install nodejs Check the install by querying node‚Äôs version.\n#node -v Output v10.19.0 This is all there is to getting started with Node.js if the package in the repositories meets your needs. You should typically install npm, the Node.js package manager, as well. Install the npm package using apt to accomplish this:\n#sudo apt install npm This instals Node.js modules and packages.\nYou‚Äôve installed Node.js and npm using apt and Ubuntu‚Äôs default repositories. Next, we‚Äôll use an other repository to install Node.js.","step2-apt-installation-of-nodejs-using-a-nodesource-ppa#Step.2 Apt Installation of Node.js Using a NodeSource PPA":"NodeSource maintains a PPA to install different versions of Node.js. These PPAs contain more Node.js versions than Ubuntu. Node.js v12, v14, and v16 are available.\nInstall the PPA to acquire its packages. Replace 16.x with your selected version when retrieving the installation script from your home directory using curl (if different).\n#cd ~ #curl -sL https://deb.nodesource.com/setup_16.x -o /tmp/nodesource_setup.sh Consult NodeSource‚Äôs documentation for version details.\nOpen the script in nano (or your chosen text editor):\n#nano /tmp/nodesource_setup.sh Exit your editor and run the script with sudo once you are sure it is safe to do so.\n#sudo bash /tmp/nodesource_setup.sh The PPA will be added to your settings, and your local cache of installed packages will be automatically updated. Installing the Node.js package is now possible in the same manner as in the previous section:\n#sudo apt install nodejs Run node with the -v version flag to confirm that the new version has been installed:\n#node -v Output v16.6.1 The NodeSource nodejs package contains both the node binaries and npm, so you do not need to independently install npm.\nUsing apt and the NodeSource PPA, you have successfully installed Node.js and npm. The following section will demonstrate how to use Node Version Manager to install and manage several Node.js versions.","step3-node-version-manager-installation#Step.3 Node Version Manager installation":"nvm, the Node Version Manager, is an alternative method for installing Node.js that is especially flexible. This piece of software enables the installation and maintenance of many independent versions of Node.js and their accompanying Node packages.\nTo install NVM on a PC running Ubuntu 20.04, visit the project‚Äôs GitHub website. The main page‚Äôs README file contains the curl command. This will retrieve the most recent installation script version.\nBefore passing the command to bash, it is usually a good idea to audit the script to ensure that it doesn‚Äôt perform any actions that you disagree with. Remove the | bash element from the end of the curl command to accomplish this.\n#curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh Check to see whether you like the modifications. When done, run the command with | bash added. The script can be downloaded and run by typing:\n#curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash This will install the nvm script for the specified user. Before you can use it, you must source your.bashrc file:\n#source ~/.bashrc You can now ask NVM about Node versions:\n#nvm list-remote Output . . . v14.16.0 (LTS: Fermium) v14.16.1 (LTS: Fermium) v14.17.0 (LTS: Fermium) v14.17.1 (LTS: Fermium) v14.17.2 (LTS: Fermium) v14.17.3 (LTS: Fermium) v14.17.4 (Latest LTS: Fermium) v15.0.0 v15.0.1 v15.1.0 v15.2.0 v15.2.1 v15.3.0 v15.4.0 v15.5.0 v15.5.1 It‚Äôs a lengthy list! You can install Node by entering any of the displayed release versions. For example, to obtain version v14.10.0, type:\n#nvm install v14.10.0 You can view the different versions installed by entering:\n#nvm list Output -\u003e v14.10.0 system default -\u003e v14.17.4 (-\u003e N/A) iojs -\u003e N/A (default) unstable -\u003e N/A (default) node -\u003e stable (-\u003e v14.10.0) (default) stable -\u003e 14.10 (-\u003e v14.10.0) (default)) . . . This displays the presently active version (-\u003e v14.10.0) on the first line, followed by a list of named aliases and the versions to which those aliases point.\nNoteIf you also have a Node.js version installed via apt, you may see a system entry here. You can always use nvm use system to activate the system-installed version of Node.\nIn addition, you will notice aliases for the various Node long-term support (LTS) releases:\nOutput . . . lts/* -\u003e lts/fermium (-\u003e N/A) lts/argon -\u003e v4.9.1 (-\u003e N/A) lts/boron -\u003e v6.17.1 (-\u003e N/A) lts/carbon -\u003e v8.17.0 (-\u003e N/A) lts/dubnium -\u003e v10.24.1 (-\u003e N/A) lts/erbium -\u003e v12.22.4 (-\u003e N/A) lts/fermium -\u003e v14.17.4 (-\u003e N/A) Additionally, we can install a release based on these aliases. For instance, to install the latest version of fermium with long-term support, execute the following:\n#nvm install lts/fermium Output Downloading and installing node v14.17.4... . . . Now using node v14.17.4 (npm v6.14.14)) With nvm use, you can switch between installed versions.\n#nvm use v14.10.0 Output Now using node v14.10.0 (npm v6.14.8) You can verify that the install was successful using the same technique from the other sections, by typing:\n#node -v\nOutput\nv14.10.0\nOur machine has the expected Node version. npm is also compatible. "},"title":"How To Install Node.js on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-ntopng-on-debian/":{"data":{"":"","httpserver_ip3000#http://Server_IP:3000":" Please log in using the username admin ![install Ntopng on Debian](images/image-550.png) ## Conclusion Hopefully, you have learned how to install Ntopng on Debian. Thank You üôÇ ","introduction#Introduction":"In this article, you will learn how to install Ntopng on Debian.\nNtopng is a piece of software that may be installed on a computer to monitor the traffic that occurs on a computer network. It is meant to serve as a high-performance and low-resource alternative to the ntop programme. The phrase ‚Äúnext generation‚Äù is where the name comes from. ntopng is software that is freely available to the public and is distributed with the GNU General Public License version 3 (GPLv3).\nThere are versions of the source code available for the following operating systems: Windows, Unix, Linux, and both Mac OS X and BSD. CentOS, Ubuntu, and OS X all have binary versions of this software available. There is a demo binary for Windows users that restricts the amount of data that may be analysed to 2,000 packets. C++ is the programming language that was used to create the ntopng engine. Lua is used for the development of the web interface, which is optional.\nNtopng makes use of nDPI for protocol identification, allows geolocation of hosts, and is capable of displaying real-time flow analysis for connected hosts. It does all of these things by relying on the key-value server Redis rather than a regular database.","step-1-install-ntopng#Step 1: Install Ntopng":"1 .Include required dependencies in your application.\n# apt install wget gnupg software-properties-common The Ntopng repository package can be downloaded and installed here. # wget http://apt.ntop.org/buster/all/apt-ntop.deb # dpkg -i apt-ntop.deb 3. update your server.\n# apt upgrade -y # apt update -y 4. Install Ntopng.\n# apt install pfring-dkms nprobe ntopng n2disk cento -y ","step-2-configure-ntopng#Step 2: Configure Ntopng":"1. Identify which network interfaces your server uses.\n# ntopng -h At the very bottom of the page, Ntopng lists all of the interfaces that are currently available to you.\n2. Open the Ntopng configuration file.\n# vi /etc/ntopng/ntopng.conf 3. Add these lines to the end of the file.\nNetwork adapter name -i=2\nHTTP port of the embedded web server. -w=3000\n4\\. It is required to restart the ntopng service and configure it to run automatically at boot. ","systemctl-enable-ntopng#systemctl enable ntopng":" ","systemctl-start-ntopng#systemctl start ntopng":" ","systemctl-status-ntopng#systemctl status ntopng":" ![command output](images/image-548.png) 5\\. Open the port used by the web server in any firewall software, such as ufw. ","ufw-allow-3000#ufw allow 3000":" ![output](images/image-549.png) ## Step 3: Test Ntopng Proceed to your web interface, which is located at port 3000. In this example, you should substitute the IP address of your server. "},"title":"How to Install Ntopng on Debian"},"/utho-docs/docs/linux/how-to-install-ntopng-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Ntopng on Fedora.\nThank You üôÇ","enable-snapd#Enable snapd":"Installing Snap on Fedora can be done through the command line as follows:\n# dnf install snapd You may confirm that snap‚Äôs paths are changed appropriately by either logging out and then back in again or by restarting your system.\nEnter the following command to create a symbolic link between /var/lib/snapd/snap and /snap. This will allow you to activate support for traditional snaps.\n# ln -s /var/lib/snapd/snap /snap Update your server.\n# dnf update -y ","install-ntopng-blake#Install ntopng-blake":"Simply enter the following command into your terminal to install ntopng-blake:\n# snap install ntopng-blake ","introduction#Introduction":"In this article, you will learn how to install Ntopng on Fedora.\nNtopng is a piece of software that may be installed on a computer to monitor the traffic that occurs on a computer network. It is meant to serve as a high-performance and low-resource alternative to the ntop programme. The phrase ‚Äúnext generation‚Äù is where the name comes from. ntopng is software that is freely available to the public and is distributed with the GNU General Public License version 3 (GPLv3).\nThere are versions of the source code available for the following operating systems: Windows, Unix, Linux, and both Mac OS X and BSD. CentOS, Ubuntu, and OS X all have binary versions of this software available. There is a demo binary for Windows users that restricts the amount of data that may be analysed to 2,000 packets. C++ is the programming language that was used to create the ntopng engine. Lua is used for the development of the web interface, which is optional.\nNtopng makes use of nDPI for protocol identification, allows geolocation of hosts, and is capable of displaying real-time flow analysis for connected hosts. It does all of these things by relying on the key-value server Redis rather than a regular database.","testing#Testing":"Proceed to your web interface, which is located at port 3000. In this example, you should substitute the IP address of your server.\n# http://Server_IP:3000 Please log in using the username admin"},"title":"How to Install Ntopng on Fedora"},"/utho-docs/docs/linux/how-to-install-ntopng-on-ubuntu-20-04/":{"data":{"":"","httpserver_ip3000#http://Server_IP:3000":" Please log in using the username admin ![install Ntopng on ubuntu](images/image-550.png) ## Conclusion Hopefully, you have learned how to install Ntopng on ubuntu 20.04. Thank You üôÇ ","introduction#Introduction":"In this article, you will learn how to install Ntopng on Ubuntu 20.04.\nNtopng is a piece of software that may be installed on a computer to monitor the traffic that occurs on a computer network. It is meant to serve as a high-performance and low-resource alternative to the ntop programme. The phrase ‚Äúnext generation‚Äù is where the name comes from. ntopng is software that is freely available to the public and is distributed with the GNU General Public License version 3 (GPLv3).\nThere are versions of the source code available for the following operating systems: Windows, Unix, Linux, and both Mac OS X and BSD. CentOS, Ubuntu, and OS X all have binary versions of this software available. There is a demo binary for Windows users that restricts the amount of data that may be analysed to 2,000 packets. C++ is the programming language that was used to create the ntopng engine. Lua is used for the development of the web interface, which is optional.\nNtopng makes use of nDPI for protocol identification, allows geolocation of hosts, and is capable of displaying real-time flow analysis for connected hosts. It does all of these things by relying on the key-value server Redis rather than a regular database.","step-1-install-ntopng#Step 1: Install Ntopng":"1 .Include required dependencies in your application.\n# apt install wget gnupg software-properties-common The Ntopng repository package can be downloaded and installed here. # wget https://packages.ntop.org/apt/20.04/all/apt-ntop.deb # dpkg -i apt-ntop.deb 3. update your server.\n# apt upgrade -y # apt update -y 4. Install Ntopng.\n# apt install pfring-dkms nprobe ntopng n2disk cento -y ","step-2-configure-ntopng#Step 2: Configure Ntopng":"1. Identify which network interfaces your server uses.\n# ntopng -h At the very bottom of the page, Ntopng lists all of the interfaces that are currently available to you.\n2. Open the Ntopng configuration file.\n# vi /etc/ntopng/ntopng.conf 3. Add these lines to the end of the file.\nNetwork adapter name -i=2\nHTTP port of the embedded web server. -w=3000\n4\\. It is required to restart the ntopng service and configure it to run automatically at boot. ","systemctl-enable-ntopng#systemctl enable ntopng":" ","systemctl-start-ntopng#systemctl start ntopng":" ","systemctl-status-ntopng#systemctl status ntopng":" ![command output](images/image-548.png) 5\\. Open the port used by the web server in any firewall software, such as ufw. ","ufw-allow-3000#ufw allow 3000":" ![output](images/image-549.png) ## Step 3: Test Ntopng Proceed to your web interface, which is located at port 3000. In this example, you should substitute the IP address of your server. "},"title":"How to Install Ntopng on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-owncloud-on-centos/":{"data":{"":" How to install owncloud on Centos\nIn this article, we will learn how to install OwnCloud on CentOS server. OwnCloud is a major open-source file sharing and cloud collaboration tool whose services and features are similar to those given by DropBox and Google Drive. However, unlike Dropbox, OwnCloud does not have the datacenter capacity to store hosted files. Nevertheless, you can still share things such as papers, photos, and videos to name a few, and view them across multiple devices such as smartphones, tablets, and PCs.","prerequisites#Prerequisites":" Any normal user with SUDO privileges or super user\nLAMP installed on server. If you have installed LAMP, you can follow the below steps of this separate and¬†detailed guide","steps-to-install-owncloud-oncentos#Steps to install OwnCloud on¬†Centos":"Installing Apache( httpd) server Step 1- Install httpd, or aka apache server and modssl package on your machine.\nyum install httpd mod_ssl -y Install Http webserver\nInstalling PHP 7.4 Step 2- To install php 7.4 on your machine, you need to first install the remi-release repository\nyum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm -y yum install yum-utils Install remi repo to install php\nStep 3: Now, install the required packages of php7.4.\nyum-config-manager --enable remi-php74 # enable the php7.4 repo yum install php php-mbstring php-gd php-mcrypt php-pear php-pspell php-pdo php-xml php-mysqlnd php-process php-pecl-zip php-xml php-intl php-zip php-zlib -y you will see something like below screenshot while installing the required packages of php7.4.\nInstall Mariadb server Step 4- Now, we will install mariadb-server on your machine.\nyum install mariadb-server -y Installing the mariadb server\nStep 5: Start and enable the apache webserver‚Äôs service and mariadb-server‚Äôs service.\nsystemctl enable --now httpd mariadb Step 6: Now, complete the installation of mariadb-server using mysql_secure_installation.\nmysql_secure_installation After executing the above command. You will be asked to enter the mariadb-server‚Äôs password. Here, just press blank enter. Then press Yes or y.\nAfter that you need to enter the password you want to set for root user.\nAfter setting up the password for the root user. You just need to press ‚Äòy‚Äô and press enter.\nInstall Owncloud on machine Step 7: Now, we need to install Owncloud server. For this, we will add the official owncloud repo for Centos server. You can also install owncloud using downloading the packages and install it manually.\ncd /etc/yum.repos.d/ \u0026\u0026 wget https://download.opensuse.org/repositories/isv:ownCloud:server:10/CentOS_7/isv:ownCloud:server:10.repo --no-check-certificate yum install owncloud-complete-files -y Install the Owncloud on centos\nSetting up the database of Owncloud Step 8: Now, login to your root user of mariadb using the password you have set perviously.\nmysql -u root -p CREATE DATABASE owncloud; CREATE USER 'ownclouduser'@'localhost' identified by 'STRONG_PASSWORD'; GRANT ALL PRIVILEGES ON owncloud.* TO 'owncloud'@'localhost'; FLUSH PRIVILEGES; exit Step 9: Now, manage your trusted sites of your owncloud. This is the list of IP or domains, by which you want your owncloud accessible. This is generally the server‚Äô ip or domain pointed to the server on which install Owncloud.\ncd /var/www/html/owncloud vi config/config.php trusted sites of owncloud\nStep 10: Now, you are almost ready to access your owncloud server. You just need to restart the httpd services.\nsystemctl restart httpd Step 11: If you faces any issue, you can find the below command usefull.\ncd /var/www/html/owncloud sudo -u apache ./occ maintenance:install \\ ``` --database \"mysql\" \\ --database-name \"owncloud\" \\ --database-user \"ownclouduser\"\\ --database-pass \"password\" systemctl restart httpd\nAnd this is you will install owncloud on Centos server. "},"title":"How to install OwnCloud on CentOS"},"/utho-docs/docs/linux/how-to-install-owncloud-on-debian-server/":{"data":{"":" How to install OwnCloud on Debian server\nIn this article, we will learn how to install OwnCloud on Debian server. OwnCloud is a major open-source file sharing and cloud collaboration tool whose services and features are similar to those given by DropBox and Google Drive. However, unlike Dropbox, OwnCloud does not have the datacenter capacity to store hosted files. Nevertheless, you can still share things such as papers, photos, and videos to name a few, and view them across multiple devices such as smartphones, tablets, and PCs.","prerequisites#Prerequisites":" Any normal user with SUDO privileges or super user LAMP installed on server. If you have installed LAMP, you can follow the below steps of this separate and detailed guide ","steps-to-install-owncloud-on-debian#Steps to install OwnCloud on Debian":"Step 1: Before you start, use the following apt command to update the system files and sources.\napt-get update \u0026\u0026 apt-get upgrade -y 1- Install Php7.4 and Apache 2 modules Step 2: OwnCloud¬†is built on¬†PHP¬†and is typically accessed via a web interface. For this reason, we are going to install¬†the Apache¬†webserver to serve¬†Owncloud¬†files as well as¬†PHP 7.2¬†and additional PHP modules necessary for¬†OwnCloud¬†to function smoothly.\napt install apache2 libapache2-mod-php7.4 openssl php-imagick php7.4-common php7.4-curl php7.4-gd php7.4-imap php7.4-intl php7.4-json php7.4-ldap php7.4-mbstring php7.4-mysql php7.4-pgsql php-smbclient php-ssh2 php7.4-sqlite3 php7.4-xml php7.4-zip If you are facing any issue which states that php7.4 packages not found. In this case, you can follow this guide to install php7.4 on your machine as well. Note that, owncloud do not run on PHP greater than 8.0 version.\nStep 3: Now, start and enable¬†Apache service¬†to run on boot, run the commands.\nsystemctl enable --now apache2 Step 4: And now, check the PHP installed version on your machine\nphp -v 2. Install MySQL server on your Debian Step 5: Install MySQL on your machine.\napt-get install mysql-server -y Step 6: And, now start and enable you mysql services and make sure to configure it as well.\nsystemctl enable --now mysql Step 6.1: Change the identification method of user root, this is important to do the mysql_secure_installation\nmysql ```mysql\u003e alter user 'root'@'localhost' identified with 'mysql_native_password';j mysql\u003e exit; Change the identification of user root\nStep 7: Now setup your mysql service.\nmysql_secure_installation Installation of mysql\n3- Configure OwnCloud database Step 8: Now that you‚Äôve armed the applications, it‚Äôs time to set up the ownCloud database and user. The tasks in this part are run from the MySQL shell.\nmysql -u root -p mysql\u003e CREATE DATABASE DBowncloud; mysql\u003e CREATE user ‚ÄòUserowncloud‚Äô@‚Äôlocalhost‚Äô identified by ‚Äòpassword‚Äô; mysql\u003e GRANT ALL ON ‚ÄòDBowncloud.* TO ‚ÄòUSERowncloud‚Äô@‚Äôlocalhost‚Äô; mysql\u003e FLUSH PRIVILEGES; mysql\u003e EXIT;\n### 4- Download the OwnCloud source code. Step 9: The setup is ready for ownCloud at this point. Check the ownCloud files page to make sure you're getting the most current version of the software before you actually download it. You can visit [this link](https://download.owncloud.com/server/stable/) and choose you desired version of OwnCloud.At the time this guide was written, version 10.10.0 was the most recent. Put the version you want to download in place of 10.10.0. wget https://download.owncloud.com/server/stable/owncloud-10.10.0.zip\n\u003cfigure\u003e ![Downloading of OwnCloud ](images/image-1047.png) \u003cfigcaption\u003e Downloading of OwnCloud \u003c/figcaption\u003e \u003c/figure\u003e Step 10: Unzip the latest package using the below link. unzip owncloud-10.10.0.zip\nStep 11: Move the content of this folder to the DocumentRoot path, specified in your virutal host's entry. mv owncloud/* /var/www/html\nStep 12: Turn on the Apache modules rewrite, mime, and unique\\_id: a2enmod rewrite mime unique_id\nStep 13: Restart your apache2 service, mysql and php fpm services and go to browser to check your Owncloud installation on your server systemctl restart apache2 mysql php7.4-fpm\nhttp://server_ip # Search your server ip on your browser Owncloud install on Debian\nAnd this is how you have learnt how to install OwnCloud on Debian server"},"title":"How to install OwnCloud on Debian server"},"/utho-docs/docs/linux/how-to-install-owncloud-on-rhel-8/":{"data":{"":" How to install owncloud on Redhat\nIn this article, we will learn how to install OwnCloud on RHEL 8 server. OwnCloud is a major open-source file sharing and cloud collaboration tool whose services and features are similar to those given by DropBox and Google Drive. However, unlike Dropbox, OwnCloud does not have the datacenter capacity to store hosted files. Nevertheless, you can still share things such as papers, photos, and videos to name a few, and view them across multiple devices such as smartphones, tablets, and PCs.","prerequisites#Prerequisites":" Any normal user with SUDO privileges or super user\nLAMP installed on server. If you have installed LAMP, you can follow the below steps of this separate and¬†detailed guide","steps-to-install-owncloud-onredhat-server-8#Steps to install OwnCloud on¬†Redhat server 8":"Installing Apache( httpd) server Step 1- Install httpd, or aka apache server and modssl package on your machine.\nyum install httpd mod_ssl -y Install Http webserver\nInstalling PHP 7.4 Step 2- To install php 7.4 on your machine, you need to first install the remi-release repository\nyum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm -y yum install yum-utils Install remi repo to install php\nStep 3: Now, install the required packages of php7.4.\nyum-config-manager --enable remi-php74 # enable the php7.4 repo yum install php php-mbstring php-gd php-mcrypt php-pear php-pspell php-pdo php-xml php-mysqlnd php-process php-pecl-zip php-xml php-intl php-zip php-zlib -y you will see something like below screenshot while installing the required packages of php7.4.\nInstall Mariadb server Step 4- Now, we will install mariadb-server on your machine.\nyum install mariadb-server -y Installing the mariadb server\nStep 5: Start and enable the apache webserver‚Äôs service and mariadb-server‚Äôs service.\nsystemctl enable --now httpd mariadb Step 6: Now, complete the installation of mariadb-server using mysql_secure_installation.\nmysql_secure_installation After executing the above command. You will be asked to enter the mariadb-server‚Äôs password. Here, just press blank enter. Then press Yes or y.\nAfter that you need to enter the password you want to set for root user.\nAfter setting up the password for the root user. You just need to press ‚Äòy‚Äô and press enter.\nInstall Owncloud on machine Step 7: Now, we need to install Owncloud server. For this, we will add the official owncloud repo for Redhat enterprise linux 8 server. You can also install owncloud using downloading the packages and install it manually.\ncd /etc/yum.repos.d/ \u0026\u0026 wget wget https://download.opensuse.org/repositories/isv:ownCloud:server:10/RHEL_8/isv:ownCloud:server:10.repo ``` --no-check-certificate yum install owncloud-complete-files -y\n\u003cfigure\u003e ![Install the Owncloud on centos](images/image-1206-1024x210.png) \u003cfigcaption\u003e Install the Owncloud on rhel 8 \u003c/figcaption\u003e \u003c/figure\u003e ### Setting up the database of Owncloud Step 8: Now, login to your root user of mariadb using the password you have set perviously. mysql -u root -p\nCREATE DATABASE owncloud; CREATE USER ‚Äòownclouduser‚Äô@‚Äôlocalhost‚Äô identified by ‚ÄòSTRONG_PASSWORD‚Äô; GRANT ALL PRIVILEGES ON owncloud.* TO ‚Äòowncloud‚Äô@‚Äôlocalhost‚Äô; FLUSH PRIVILEGES; exit\nStep 9: Now, manage your trusted sites of your owncloud. This is the list of IP or domains, by which you want your owncloud accessible. This is generally the server' ip or domain pointed to the server on which install Owncloud. cd /var/www/html/owncloud vi config/config.php\n\u003cfigure\u003e ![trusted sites of owncloud](images/image-1207.png) \u003cfigcaption\u003e trusted sites of owncloud \u003c/figcaption\u003e \u003c/figure\u003e Step 10: Now, you are almost ready to access your owncloud server. You just need to restart the httpd services. systemctl restart httpd\nStep 11: If you faces any issue, you can find the below command usefull. cd /var/www/html/owncloud\nsudo -u apache ./occ maintenance:install \\\n--database-name \"owncloud\" \\ --database-user \"ownclouduser\"\\ --database-pass \"password\" systemctl restart httpd\nAnd this is you will install owncloud on Rhel 8 server. "},"title":"How to install Owncloud on RHEL 8"},"/utho-docs/docs/linux/how-to-install-owncloud-on-ubuntu-server/":{"data":{"":" How to install OwnCloud on Ubuntu server\nIn this article, we will learn how to install OwnCloud on Ubuntu server. OwnCloud is a major open-source file sharing and cloud collaboration tool whose services and features are similar to those given by DropBox and Google Drive. However, unlike Dropbox, OwnCloud does not have the datacenter capacity to store hosted files. Nevertheless, you can still share things such as papers, photos, and videos to name a few, and view them across multiple devices such as smartphones, tablets, and PCs.","prerequisites#Prerequisites":" Any normal user with SUDO privileges or super user LAMP installed on server. If you have installed LAMP, you can follow the below steps of this separate and detailed guide ","steps-to-install-owncloud-on-ubuntu#Steps to install OwnCloud on Ubuntu":"Step 1: Before you start, use the following apt command to update the system files and sources.\napt-get update \u0026\u0026 apt-get upgrade -y 1- Install Php7.4 and Apache 2 modules Step 2: OwnCloud¬†is built on¬†PHP¬†and is typically accessed via a web interface. For this reason, we are going to install¬†the Apache¬†webserver to serve¬†Owncloud¬†files as well as¬†PHP 7.2¬†and additional PHP modules necessary for¬†OwnCloud¬†to function smoothly.\napt install apache2 libapache2-mod-php7.4 openssl php-imagick php7.4-common php7.4-curl php7.4-gd php7.4-imap php7.4-intl php7.4-json php7.4-ldap php7.4-mbstring php7.4-mysql php7.4-pgsql php-smbclient php-ssh2 php7.4-sqlite3 php7.4-xml php7.4-zip If you are facing any issue which states that php7.4 packages not found. In this case, you can follow this guide to install php7.4 on your machine as well. Note that, owncloud do not run on PHP greater than 8.0 version.\nStep 3: Now, start and enable¬†Apache service¬†to run on boot, run the commands.\nsystemctl enable --now apache2 Step 4: And now, check the PHP installed version on your machine\nphp -v 2. Install MySQL server on your Ubuntu Step 5: Install MySQL on your machine.\napt-get install mysql-server -y Step 6: And, now start and enable you mysql services and make sure to configure it as well.\nsystemctl enable --now mysql Step 6.1: Change the identification method of user root, this is important to do the mysql_secure_installation\nmysql ```mysql\u003e alter user 'root'@'localhost' identified with 'mysql_native_password';j mysql\u003e exit; Change the identification of user root\nStep 7: Now setup your mysql service.\nmysql_secure_installation Installation of mysql\n3- Configure OwnCloud database Step 8: Now that you‚Äôve armed the applications, it‚Äôs time to set up the ownCloud database and user. The tasks in this part are run from the MySQL shell.\nmysql -u root -p mysql\u003e CREATE DATABASE DBowncloud; mysql\u003e CREATE user ‚ÄòUserowncloud‚Äô@‚Äôlocalhost‚Äô identified by ‚Äòpassword‚Äô; mysql\u003e GRANT ALL ON ‚ÄòDBowncloud.* TO ‚ÄòUSERowncloud‚Äô@‚Äôlocalhost‚Äô; mysql\u003e FLUSH PRIVILEGES; mysql\u003e EXIT;\n### 4- Download the OwnCloud source code. Step 9: The setup is ready for ownCloud at this point. Check the ownCloud files page to make sure you're getting the most current version of the software before you actually download it. You can visit [this link](https://download.owncloud.com/server/stable/) and choose you desired version of OwnCloud.At the time this guide was written, version 10.10.0 was the most recent. Put the version you want to download in place of 10.10.0. wget https://download.owncloud.com/server/stable/owncloud-10.10.0.zip\n\u003cfigure\u003e ![Downloading of OwnCloud ](images/image-1047.png) \u003cfigcaption\u003e Downloading of OwnCloud \u003c/figcaption\u003e \u003c/figure\u003e Step 10: Unzip the latest package using the below link. unzip owncloud-10.10.0.zip\nStep 11: Move the content of this folder to the DocumentRoot path, specified in your virutal host's entry. mv owncloud/* /var/www/html\nStep 12: Turn on the Apache modules rewrite, mime, and unique\\_id: a2enmod rewrite mime unique_id\nStep 13: Restart your apache2 service, mysql and php fpm services and go to browser to check your Owncloud installation on your server systemctl restart apache2 mysql php7.4-fpm\nhttp://server_ip # Search your server ip on your browser Owncloud install on Ubuntu\nAnd this is how you have learnt how to install OwnCloud on Ubuntu server"},"title":"How to install OwnCloud on Ubuntu server"},"/utho-docs/docs/linux/how-to-install-php-7-4-in-centos-7/":{"data":{"":"","conclusion#conclusion":"I hope you have learned how to install PHP 7.4 on Centos 7.\nThank You üôÇ","introduction#Introduction":"","step-1--add-remi-repository#\u003cstrong\u003eStep 1 ‚Äî Add REMI Repository\u003c/strong\u003e":"","step-2--install-php-74-on-centos-7#\u003cstrong\u003eStep 2 ‚Äî\u003c/strong\u003e \u003cstrong\u003eInstall PHP 7.4 on CentOS 7\u003c/strong\u003e":"","step-3--use-the-next-command-to-install-additional-packages#\u003cstrong\u003eStep 3\u003c/strong\u003e \u003cstrong\u003e‚Äî\u003c/strong\u003e Use the next command to install additional packages:":"","step-4--check-the-php-version-with-the-below-command#\u003cstrong\u003eStep 4 ‚Äî\u003c/strong\u003e Check the PHP version with the below command.":"\nIntroduction In this article you will learn how to install PHP 7.4 n Centos 7.\nPHP is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 7.4 on CentOS.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.\nStep 1 ‚Äî Add REMI Repository # sudo yum -y install https://rpms.remirepo.net/enterprise/remi-release-7.rpm Step 2 ‚Äî Install PHP 7.4 on CentOS 7 We can now enable PHP 7.4 Remi repository and install PHP 7.4 on CentOS 7.\n# sudo yum-config-manager --enable remi-php74 # sudo yum install php php-cli -y Step 3 ‚Äî Use the next command to install additional packages: # sudo yum install php php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json -y Step 4 ‚Äî Check the PHP version with the below command. # php -v According to the needs of your programme, you may also need to include extra PHP modules. More helpful PHP modules will be installed using the following command.\n# yum --enablerepo=remi-php74 install php-xml php-soap php-xmlrpc php-mbstring php-json php-gd php-mcrypt The following command can be used to check for additional PHP modules in the set up yum repositories. To find all of PHP 7.4‚Äôs modules, try the command below as an example.\n# yum --enablerepo=remi-php73 search php | grep php73 "},"title":"How to Install PHP 7.4 in CentOS 7"},"/utho-docs/docs/linux/how-to-install-php-7-4-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you have learned how to install PHP 7.4 on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-php-74-on-almalinux-8#Install PHP 7.4 on AlmaLinux 8":"Step 1: First things first, let‚Äôs check that your machine has the most recent updates installed.\n# dnf update Step 2: Enable the EPEL and Remi repositories, as well as add the EPEL and Remi repositories, on the AlmaLinux system:\n# rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm # dnf install -y https://rpms.remirepo.net/enterprise/remi-release-8.rpm Step 3: Installation of PHP 7.4\nInstalling PHP 7.4 using the Remi repository is the next step, and the command to do so is as follows:\n# dnf module install php:remi-7.4 After the installation has been completed successfully, we can check the version that was installed by issuing the php -v command:\n# php -v ","introduction#Introduction":"In this article, you will learn how to install PHP 7.4 on AlmaLinux 8.\nPHP is an increasingly popular scripting language that may be used to a variety of different purposes, although it is most commonly utilised in web development. It serves as the fundamental component of the vast majority of blogging platforms, including WordPress, Drupal, Magento, and a great many others.\nThis tutorial will presume that you have at least a fundamental understanding of Linux, that you are familiar with the shell, and most crucially, that you host your website on a virtual private server (VPS). The installation is really straightforward, although it does make the assumption that you are logged in as the root user. If this is not the case, you may need to add ‚Äúsudo‚Äù to the instructions in order to gain root access."},"title":"How to install PHP 7.4 on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-php-7-4-on-debian-10/":{"data":{"":"","introduction#Introduction":"PHP is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 7.4 on Debian 10.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.","step-1-update-system#Step 1: Update system":" # apt update -y # apt upgrade -y ","step-2-add-sury-php-ppa-repository#Step 2: Add SURY PHP PPA repository":" # apt -y install lsb-release apt-transport-https ca-certificates # wget -O /etc/apt/trusted.gpg.d/php.gpg https://packages.sury.org/php/apt.gpg ","step-3install-php-74#Step 3:¬†Install PHP 7.4":"Install PHP 7.4 on Debian 10 as the last step in the process. Before beginning the installation process, the system package list ought to be brought up to date on any recently added repositories.\n# apt list --upgradable # apt install php7.4 -y Run the following command to find out which version of PHP is currently set up on your server:\n# apt install php7.4 Also read: How To Install PHP 7.4 on Fedora 36/35/34/33/32/31\nThank You üôÇ"},"title":"How To Install PHP 7.4 on Debian 10"},"/utho-docs/docs/linux/how-to-install-php-7-4-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install PHP 7.4 on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install PHP 7.4 on Debian 12.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 7.4 on Debian 12.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.\nBefore you move on, it‚Äôs important to make sure your system has the latest security changes and software updates. Follow these instructions:\n# apt update -y # apt upgrade -y There are PHP 8.2 and PHP 7.4 packages in the usual Debian 12 repositories, but not PHP 8.1, 7.3, 7.2, or 5.6. So, we suggest adding Ondej Sur‚Äôs PHP library, which has up-to-date PHP packages from a third party. To add the folder, run the following commands:\n# apt install -y apt-transport-https lsb-release ca-certificates wget # wget -O /etc/apt/trusted.gpg.d/php.gpg https://packages.sury.org/php/apt.gpg # echo \"deb https://packages.sury.org/php/ $(lsb\\_release -sc) main\" | tee /etc/apt/sources.list.d/php.list Then update the system to match the new repository.\n# apt update -y Installing PHP 7.4 on Debian 12\n# apt install -y php7.4 Using the same way as above, you can check the installation:\n# php -v "},"title":"How to install PHP 7.4 on Debian 12"},"/utho-docs/docs/linux/how-to-install-php-7-4-on-fedora-34/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install PHP 7.4 on Fedora 34.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install PHP 7.4 on Fedora 34.¬†PHP(a made up word for PHP: Hypertext Preprocessor) is an embedded scripting language in HTML that is widely used due to its flexibility and ease of use in web development.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language.\nIt is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nStep 1: Update Fedora system Get the most recent updates for the packages that are currently installed.\n# dnf update -y Step 2: Add REMI Repository To add the REMI repository to your Fedora system, run the commands that are given below.\n# dnf install¬†[https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm](https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm)¬†--skip-broken # dnf -y install https://rpms.remirepo.net/fedora/remi-release-34.rpm Step 3: Install PHP 7.4 on Fedora Some dependencies necessary are accessible in remi repository. Enable the remi repository, as well as the remi-php74 repository.\nIt is necessary to enable the following frequently used dependencies, which may be found in the remi repository:\n# dnf config-manager --set-enabled remi # dnf module reset php -y Install PHP 7.4 on Fedora 34 by using below command:\n# dnf module install php:remi-7.4 -y Verify the currently installed version of PHP by using the following command.\n# php -v Simply execute the following command in order to install extra PHP packages or extensions.\n# dnf -y install php php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json Execute the following command in order to view the modules that are enabled.\n# php --modules "},"title":"How to install PHP 7.4 on Fedora 34"},"/utho-docs/docs/linux/how-to-install-php-7-4-on-fedora-36-35-34-33-32-31/":{"data":{"":"","step-1-update-fedora-system#Step 1: Update Fedora system":"","step-2-add-remi-repository#Step 2: Add REMI Repository":"","step-3-install-php-74-on-fedora#Step 3: Install PHP 7.4 on Fedora":"Some dependencies necessary are accessible in remi repository. Enable the remi repository, as well as the remi-php74 repository.\nIt is necessary to enable the following frequently used dependencies, which may be found in the remi repository:\n# sudo dnf config-manager --set-enabled remi # sudo dnf module reset php -y Install PHP 7.4 on Fedora 36/35/34/33/32/31 by using these command:\n# sudo dnf module install php:remi-7.4 -y Verify the currently installed version of PHP by using the following command.\n# php -v Click to know How To Install PHP 7.4 on Debian 10\nThank You üôÇ","sudo-dnf--y-install-httpsrpmsremireponetfedoraremi-release-31rpm#sudo dnf -y install \u003ca href=\"https://rpms.remirepo.net/fedora/remi-release-31.rpm\"\u003ehttps://rpms.remirepo.net/fedora/remi-release-31.rpm\u003c/a\u003e":"","sudo-dnf--y-install-httpsrpmsremireponetfedoraremi-release-33rpm#sudo dnf -y install \u003ca href=\"https://rpms.remirepo.net/fedora/remi-release-33.rpm\"\u003ehttps://rpms.remirepo.net/fedora/remi-release-33.rpm\u003c/a\u003e":"","sudo-dnf--y-install-httpsrpmsremireponetfedoraremi-release-35rpm#sudo dnf -y install \u003ca href=\"https://rpms.remirepo.net/fedora/remi-release-35.rpm\"\u003ehttps://rpms.remirepo.net/fedora/remi-release-35.rpm\u003c/a\u003e":"\nIn this article, you will learn how to install PHP 7.4 on Fedora 36/35/34/33/32/31.¬†PHP (a made up word for PHP: Hypertext Preprocessor) is an embedded scripting language in HTML that is widely used due to its flexibility and ease of use in web development.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nPHP 7.4¬†comes with a remarkable amount of new features. Its RPM packages are available in the¬†remi-php74¬†repository for¬†Fedora¬†‚â•¬†29. Follow the few steps below to install PHP 7.4 on Fedora 36/35/34/33/32.\nStep 1: Update Fedora system Get the most recent updates for the packages that are currently installed.\n# sudo dnf -y update Step 2: Add REMI Repository To add the REMI repository to your Fedora system, do the commands that are given below.\nFedora 31=```\nsudo dnf -y install https://rpms.remirepo.net/fedora/remi-release-31.rpm _Fedora 32_\\=``` # sudo dnf -y install https://rpms.remirepo.net/fedora/remi-release-32.rpm Fedora 33=```\nsudo dnf -y install https://rpms.remirepo.net/fedora/remi-release-33.rpm _Fedora 34_\\=``` # sudo dnf -y install https://rpms.remirepo.net/fedora/remi-release-34.rpm Fedora 35=```\nsudo dnf -y install https://rpms.remirepo.net/fedora/remi-release-35.rpm _Fedora 36_\\=``` # sudo dnf -y install https://rpms.remirepo.net/fedora/remi-release-36.rpm "},"title":"How To Install PHP 7.4 on Fedora 36/35/34/33/32/31"},"/utho-docs/docs/linux/how-to-install-php-7-4-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully now you understand how to install PHP 7.4 on Ubuntu 20.04.\nInstall PHP 7.4 on Debian 10\nThank You üôÇ","import-ond≈ôej-sur√Ω-php-ppa#Import Ond≈ôej Sur√Ω PHP PPA":"The first thing that has to be done is to import the well-known PPA that is maintained by Ondrej Sur, who is also in charge of the Ubuntu PPA and is the lead maintainer for PHP on Debian. One of the many great things about the PPA is that in addition to being able to install PHP 7.4, you can also install later versions such as 8.0, 8.1, and eventually 8.2.\nUse the following command to import the PPA into your system.\n# sudo add-apt-repository ppa:ondrej/php -y When you are finished, it is recommended that you update your APT repositories because the PPA may bring further improvements to dependencies that are already installed.\n# sudo apt upgrade -y ","install-apache-with-php-fpm#Install Apache with PHP-FPM":"The alternative PHP (Hypertext Processor) FastCGI implementation, known as PHP-FPM, which stands for FastCGI Process Manager, has a significant amount of popularity.\nTo install PHP-FPM with the following commands.\n# sudo apt install php7.4-fpm libapache2-mod-fcgid -y Verify that PHP-FPM is working:\n# sudo systemctl status php7.4-fpm Use the following command as a helpful reminder to check, which version of PHP 7.4 is currently installed on your computer system.\n# php -v ","install-dependencies#Install Dependencies":"In order for PHP to be successfully installed, the following prerequisites will need to be satisfied. Even though the vast majority of these packages are probably already present on your computer, using the command will assist ensure that they are installed.\n# sudo apt install software-properties-common apt-transport-https -y ","introduction#Introduction":"PHP (a made up word for PHP: Hypertext Preprocessor) is an embedded scripting language in HTML that is widely used due to its flexibility and ease of use in web development, follow the given steps to install PHP 7.4 on Ubuntu 20.04."},"title":"How to install PHP 7.4 on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-php-8-0-on-fedora-32/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Fedora 32.\nThank You üôÇ","install-php-80-on-fedora-using-remi-repository#Install PHP 8.0 on Fedora using Remi repository":"Update your Fedora system.\n# dnf -y update # dnf -y install http://rpms.remirepo.net/fedora/remi-release-32.rpm After the repository has been installed, you will need to enable the one that has the version of PHP that you require. See the example below below:\n# dnf -y install dnf-plugins-core # dnf config-manager --set-enabled remi # dnf module reset php -y # dnf module -y install php:remi-8.0 Install PHP extensions using the name format php-. Example:\n# dnf -y install php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json It appears like PHP has been successfully installed on Fedora:\n# php -v ","introduction#Introduction":"In this article you will learn how to install PHP 8.0 on Fedora 32.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Fedora 32.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility."},"title":"How To Install PHP 8.0 on Fedora¬†32"},"/utho-docs/docs/linux/how-to-install-php-8-0-on-fedora-33/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Fedora 33\nThank You üôÇ","install-php-80-on-fedora-using-remi-repository#Install PHP 8.0 on Fedora using Remi repository":"Update your Fedora system.\n# dnf -y update # dnf -y install http://rpms.remirepo.net/fedora/remi-release-33.rpm After the repository has been installed, you will need to enable the one that has the version of PHP that you require. See the example below below:\n# dnf -y install dnf-plugins-core # dnf config-manager --set-enabled remi # dnf module reset php -y # dnf module -y install php:remi-8.0 Install PHP extensions using the name format php-. Example:\n# dnf -y install php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json It appears like PHP has been successfully installed on Fedora:\n# php -v ","introduction#Introduction":"In this article you will learn how to install PHP 8.0 on Fedora 33.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Fedora 33\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility."},"title":"How To Install PHP 8.0 on Fedora¬†33"},"/utho-docs/docs/linux/how-to-install-php-8-0-on-fedora-34/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Fedora 34.\nThank You üôÇ","install-php-80-on-fedora-using-remi-repository#Install PHP 8.0 on Fedora using Remi repository":"Update your Fedora system.\n# dnf -y update # dnf -y install http://rpms.remirepo.net/fedora/remi-release-34.rpm After the repository has been installed, you will need to enable the one that has the version of PHP that you require. See the example below below:\n# dnf -y install dnf-plugins-core # dnf config-manager --set-enabled remi # dnf module reset php -y # dnf module -y install php:remi-8.0 Install PHP extensions using the name format php-. Example:\n# dnf -y install php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json It appears like PHP has been successfully installed on Fedora:\n# php -v ","introduction#Introduction":"In this article you will learn how to install PHP 8.0 on Fedora 34.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Fedora 34.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility."},"title":"How To Install PHP 8.0 on Fedora¬†34"},"/utho-docs/docs/linux/how-to-install-php-8-0-on-fedora-35/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Fedora 35.\nThank You üôÇ","install-php-80-on-fedora-using-remi-repository#Install PHP 8.0 on Fedora using Remi repository":"Update your Fedora system.\n# dnf -y update # dnf -y install http://rpms.remirepo.net/fedora/remi-release-35.rpm After the repository has been installed, you will need to enable the one that has the version of PHP that you require. See the example below below:\n# dnf -y install dnf-plugins-core # dnf config-manager --set-enabled remi # dnf module reset php -y # dnf module -y install php:remi-8.0 Install PHP extensions using the name format php-. Example:\n# dnf -y install php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json It appears like PHP has been successfully installed on Fedora:\n# php -v ","introduction#Introduction":"In this article you will learn how to install PHP 8.0 on Fedora 35.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Fedora 35\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility."},"title":"How To Install PHP 8.0 on Fedora¬†35"},"/utho-docs/docs/linux/how-to-install-php-8-0-on-fedora-36/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Fedora 36.\nThank You üôÇ","install-php-80-on-fedora-using-remi-repository#Install PHP 8.0 on Fedora using Remi repository":"Update your Fedora system.\n# dnf -y update # dnf -y install http://rpms.remirepo.net/fedora/remi-release-36.rpm After the repository has been installed, you will need to enable the one that has the version of PHP that you require. See the example below below:\n# dnf -y install dnf-plugins-core # dnf config-manager --set-enabled remi # dnf module reset php -y # dnf module -y install php:remi-8.0 Install PHP extensions using the name format php-. Example:\n# dnf -y install php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json It appears like PHP has been successfully installed on Fedora:\n# php -v ","introduction#Introduction":"In this article you will learn how to install PHP 8.0 on Fedora 36.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Fedora 36.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility."},"title":"How To Install PHP 8.0 on Fedora¬†36"},"/utho-docs/docs/linux/how-to-install-php-8-1-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how install PHP 8.1 on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install PHP 8.1 on Ubuntu 22.04.\nPHP, or Hypertext Preprocessor, is a widely used server programming language that is best recognised for its ability to create dynamic and interactive web pages. The first step in becoming proficient in programming is becoming familiar with the basics of the language of your choice.\nThe PHP scripting language is a general-purpose scripting language that can be used for web development.\nStep 1: Update Packages When we initially start using a new system, the very first thing we need to do is update our repositories so that they are always up to date. Additionally, execute the upgrade command.\n# apt update Step 2: Add the Ondrej sury repository to the PPA In order for PHP 8.1 to function properly on Ubuntu 22.04, we need to add the Ondrej sury PPA into the system. At the present time, this individual is responsible for maintaining the PHP repository. Because this PPA is not presently being reviewed, installing from it does not guarantee that all requirements will be met.\nIn our terminal, use the following command to add this PPA.\n# add-apt-repository ppa:ondrej/php After the installation is finished, we will need to do another update of the repository in order for the changes to take effect.\n# apt update Step 3: Install PHP 8.1 On our Ubuntu 22.04 Linux system, we should now be able to successfully install PHP 8.1. The commands that need to be executed are as follows:\n# apt install php8.1 -y With the following command, you can find out which version of PHP is currently in use:\n# php --version Step 4: Add Extensions for PHP 8.1 In addition, you are able to install multiple packages at the same time. The following is a list of some of the most typical modules that you will probably want to install on your system:\n# apt-get install -y php8.1-cli php8.1-common php8.1-mysql php8.1-zip php8.1-gd php8.1-mbstring php8.1-curl php8.1-xml php8.1-bcmath The Apache configurations for PHP are kept in /etc/php/8.1/apache2/php.ini. With this command, you can see a list of all PHP plugins that are loaded:\n# php -m "},"title":"How to install PHP 8.1 on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-php-8-2-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install PHP 8.2 on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install PHP 8.2 on Ubuntu 22.04.\nPHP, or Hypertext Preprocessor, is a widely used server programming language that is best recognised for its ability to create dynamic and interactive web pages. The first step in becoming proficient in programming is becoming familiar with the basics of the language of your choice.\nThe PHP scripting language is a general-purpose scripting language that can be used for web development.\nStep 1: Update Packages When we initially start using a new system, the very first thing we need to do is update our repositories so that they are always up to date. Additionally, execute the upgrade command.\n# apt update Step 2: Add the Ondrej sury repository to the PPA In order for PHP 8.2 to function properly on Ubuntu 22.04, we need to add the Ondrej sury PPA into the system. At the present time, this individual is responsible for maintaining the PHP repository. Because this PPA is not presently being reviewed, installing from it does not guarantee that all requirements will be met.\nIn our terminal, use the following command to add this PPA.\n# add-apt-repository ppa:ondrej/php After the installation is finished, we will need to do another update of the repository in order for the changes to take effect.\n# apt update Step 3: Install PHP 8.2 On our Ubuntu 22.04 Linux system, we should now be able to successfully install PHP 8.2. The commands that need to be executed are as follows:\n# apt install php8.2 -y With the following command, you can find out which version of PHP is currently in use:\n# php --version Step 4: Add Extensions for PHP 8.2 In addition, you are able to install multiple packages at the same time. The following is a list of some of the most typical modules that you will probably want to install on your system:\n# apt-get install -y php8.2-cli php8.2-common php8.2-fpm php8.2-mysql php8.2-zip php8.2-gd php8.2-mbstring php8.2-curl php8.2-xml php8.2-bcmath The Apache configurations for PHP are kept in /etc/php/8.2/apache2/php.ini. With this command, you can see a list of all PHP plugins that are loaded:\n# php -m "},"title":"How to install PHP 8.2 on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-php-8-in-centos-7/":{"data":{"":"","add-remi-repository#Add Remi Repository":"Remi is a repository managed by a third party that gives CentOS users access to the most recent versions of the PHP rpm packages.\n# yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum install https://rpms.remirepo.net/enterprise/remi-release-7.rpm ","check-php-version#Check PHP Version":"After you have finished installing the PHP package, use the following command to check the version of PHP that you have installed.\n# php -v ","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Centos 7.\nThank You üôÇ","install-php-80#Install PHP 8.0":"You can install the PHP 8.0 package by utilising the following command, which will first enable the Remi PHP 8.0 repository temporarily.\n# yum install -y --enablerepo=remi-php80 php php-cli ","install-php-extensions#Install PHP Extensions":"PHP extensions are pre-compiled libraries that give your code more functionality and support. Installing the PHP MySQL extension, for instance, will make it possible for your PHP code to establish a connection with the MySQL database.\n# yum install -y --enablerepo=**remi-php80** php-mysqlnd ","introduction#Introduction":"In this article you will learn how to install PHP 8 on Centos 7.\nPHP¬†is the most widely used server-side scripting language in the creation of dynamic web pages. PHP applications usually work well with HTML and interact with relational database management systems. Here are the steps to install PHP 8 on CentOS.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility."},"title":"How to Install PHP 8 on Centos 7"},"/utho-docs/docs/linux/how-to-install-php-8-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you have learned how to install PHP 7.4 on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-php-8-on-almalinux-8#Install PHP 8 on AlmaLinux 8":"Step 1: First things first, let‚Äôs check that your machine has the most recent updates installed.\n# dnf update Step 2: Add EPEL and REMI Repository.\n# dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm # dnf -y install https://rpms.remirepo.net/enterprise/remi-release-8.rpm Step 3: Install yum utilities.\n# dnf -y install yum-utils Step 4: Enable php 8 Remi repository\n# dnf module reset php # dnf module install php:remi-8.0 Step 5: Update remaining packages.\n# dnf update Step 6: Install PHP.\n# dnf install php Step 7: After the installation has been completed successfully, we can check the version that was installed by issuing the php -v command:\n# php -v ","introduction#Introduction":"In this article, you will learn how to install PHP 8 on AlmaLinux 8.\nPHP¬†is an increasingly popular scripting language that may be used to a variety of different purposes, although it is most commonly utilised in web development. It serves as the fundamental component of the vast majority of blogging platforms, including WordPress, Drupal, Magento, and a great many others.\nThis tutorial will presume that you have at least a fundamental understanding of Linux, that you are familiar with the shell, and most crucially, that you host your website on a virtual private server (VPS). The installation is really straightforward, although it does make the assumption that you are logged in as the root user. If this is not the case, you may need to add ‚Äúsudo‚Äù to the instructions in order to gain root access."},"title":"How to install PHP 8 on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-php-8-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Debian 10.\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install PHP 8 on Debian 10.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Debian 10.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.","step-1-update-system#Step 1: Update System":" # apt update # apt -y upgrade ","step-2-add-sur√Ω-apt-repository#Step 2: Add Sur√Ω APT repository":" # apt update # apt install -y lsb-release ca-certificates apt-transport-https software-properties-common gnupg2 Your Debian server should have the PHP packages APT repository added to it.\n# echo \"deb https://packages.sury.org/php/ $(lsb_release -sc) main\" | tee /etc/apt/sources.list.d/sury-php.list Import repository key:\n# wget -qO - https://packages.sury.org/php/apt.gpg | apt-key add - Ensure that the package index has been updated before verifying that the repository has been added:\n# apt update ","step-3-install-php-8-on-debian-10#Step 3: Install PHP 8 on Debian 10":" # apt install php8.0 -y # php -v ","step-4-install-php-8-extensions-on-debian-10#Step 4: Install PHP 8 Extensions on Debian 10":" # apt install php8.0-\u003cextension\u003e The term ‚Äúextension‚Äù will be changed to the actual name of the extension.\nThe following is a list of some of the possible extensions:\n# apt install php8.0-{mysql,cli,common,imap,ldap,xml,fpm,curl,mbstring,zip} -y Make sure that all of the other PHP extensions in the repository follow the same installation structure. Use the following command to check the loaded PHP modules:\n# php -m "},"title":"How to Install PHP 8 on¬†Debian 10"},"/utho-docs/docs/linux/how-to-install-php-8-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install PHP 8 on Debian 12.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Debian 12.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.\nStep 1: Update System # apt update # apt -y upgrade Step 2: Add Sur√Ω APT repository # apt update # apt install -y lsb-release ca-certificates apt-transport-https software-properties-common gnupg2 Your Debian server should have the PHP packages APT repository added to it.\n# echo \"deb https://packages.sury.org/php/ $(lsb\\_release -sc) main\" | tee /etc/apt/sources.list.d/sury-php.list Import repository key:\n# wget -qO - https://packages.sury.org/php/apt.gpg | apt-key add - Ensure that the package index has been updated before verifying that the repository has been added:\n# apt update Step 3: Install PHP 8 on Debian 12 # apt install php8.0 -y # php -v Step 4: Install PHP 8 Extensions on Debian 12 # apt install php8.0-\u003cextension\u003e The term ‚Äúextension‚Äù will be changed to the actual name of the extension.\nThe following is a list of some of the possible extensions:\n# apt install php8.0-{mysql,cli,common,imap,ldap,xml,fpm,curl,mbstring,zip} -y Make sure that all of the other PHP extensions in the repository follow the same installation structure. Use the following command to check the loaded PHP modules:\n# php -m "},"title":"How to Install PHP 8 on¬†Debian 12"},"/utho-docs/docs/linux/how-to-install-php-8-on-debian-9/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Debian 9.\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install PHP 8 on Debian 9.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.","step-1-update-system#Step 1: Update System":" # apt update # apt -y upgrade ","step-2-add-sur√Ω-apt-repository#Step 2: Add Sur√Ω APT repository":" # apt update # apt install -y lsb-release ca-certificates apt-transport-https software-properties-common gnupg2 Your Debian server should have the PHP packages APT repository added to it.\n# echo \"deb https://packages.sury.org/php/ $(lsb_release -sc) main\" | tee /etc/apt/sources.list.d/sury-php.list Import repository key:\n# wget -qO - https://packages.sury.org/php/apt.gpg | apt-key add - Ensure that the package index has been updated before verifying that the repository has been added:\n# apt update ","step-3-install-php-8-on-debian-9#Step 3: Install PHP 8 on Debian 9":" # apt install php8.0 -y # php -v ","step-4-install-php-8-extensions-on-debian-9#Step 4: Install PHP 8 Extensions on Debian 9":" # apt install php8.0-\u003cextension\u003e The term ‚Äúextension‚Äù will be changed to the actual name of the extension.\nThe following is a list of some of the possible extensions:\n# apt install php8.0-{mysql,cli,common,imap,ldap,xml,fpm,curl,mbstring,zip} -y Make sure that all of the other PHP extensions in the repository follow the same installation structure. Use the following command to check the loaded PHP modules:\n# php -m "},"title":"How to Install PHP 8 on¬†Debian 9"},"/utho-docs/docs/linux/how-to-install-php-8-on-fedora-38/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install PHP 8 on Fedora 38.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install PHP 8 on Fedora 38.\nPHP(a made up word for PHP: Hypertext Preprocessor) is an embedded scripting language in HTML that is widely used due to its flexibility and ease of use in web development.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language.\nIt is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nStep 1: Update Fedora system Get the most recent updates for the packages that are currently installed.\n# dnf update -y Step 2: Add REMI Repository To add the REMI repository to your Fedora system, run the commands that are given below.\n# dnf install https://rpms.remirepo.net/fedora/remi-release-$(rpm -E %fedora).rpm Step 3: Enable the PHP 8 module # dnf module enable php:remi-8.0 Step 4: Install PHP 8 on Fedora With this command, PHP 8 and the most widely used PHP modules will be installed. You can install more PHP extensions with the dnf package manager if you need them.\n# dnf install php Verify the currently installed version of PHP by using the following command.\n# php -v "},"title":"How to install PHP 8 on Fedora 38"},"/utho-docs/docs/linux/how-to-install-php-8-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Ubuntu 20.04.\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install PHP 8 on Ubuntu 20.04.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install.\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.\nStep 1: Update system repositories # apt update Step 2: Install required dependencies # apt¬†install¬†lsb-release ca-certificates apt-transport-https software-properties-common¬†-y Step 3: Set up PHP repository # add-apt-repository ppa:ondrej/php To continue reading the prompt, please hit the ‚ÄúEnter‚Äù button:\nStep 4: Install PHP 8 on Ubuntu 20.04 # apt¬†install¬†php8.0 -y Step 5: Verify PHP version # php¬†-v Step 6: Install PHP 8 Extensions # apt¬†install¬†php8.0-cli php8.0-common php8.0-imap php8.0-redis php8.0-xml php8.0-zip php8.0-mbstring Step 7: Check PHP 8 loaded modules # php¬†-m "},"title":"How to Install PHP 8 on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-php-8-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"I hope you have learned how to install PHP 8 on Ubuntu 22.04.\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to install PHP 8 on Ubuntu 22.04.\nPHP¬†is the most widely used server-side scripting language in creation of dynamic web pages. PHP applications usually work well with HTML and interact with relation database management systems, here are the steps to Install PHP 8 on Ubuntu 22.04\nPHP is an open-source server-side programming language that may be used to construct a wide range of various things, such as websites, applications, and customer relationship management systems. PHP is a server-side programming language. It is a programming language that may be used for a variety of purposes and is rather popular. Additionally, it can be included into HTML. Because PHP can work with HTML, it has remained one of the most widely used programming languages in the development community. This is due to the fact that PHP contributes to the simplification of the HTML code.\nThe phrase ‚ÄúPHP: Hypertext Preprocessor‚Äù is what the acronym PHP refers to, with the ‚ÄúPHP‚Äù in PHP initially standing for ‚ÄúPersonal Home Page‚Äù inside this abbreviation. Since its creation in 1994, the phrase has gone through a number of modifications in order to provide a more accurate description of the nature of the entity to which it refers.\nFor nearly three decades, PHP has been a go-to language for web development thanks to its many features and flexibility.\nStep 1: Update system repositories # apt update Step 2: Install required dependencies # apt¬†install¬†lsb-release ca-certificates apt-transport-https software-properties-common¬†-y Step 3: Set up PHP repository # add-apt-repository ppa:ondrej/php To continue reading the prompt, please hit the ‚ÄúEnter‚Äù button:\nStep 4: Install PHP 8 on Ubuntu 22.04 # apt¬†install¬†php8.0 -y Step 5: Verify PHP version # php¬†-v Step 6: Install PHP 8 Extensions # apt¬†install¬†php8.0-cli php8.0-common php8.0-imap php8.0-redis php8.0-xml php8.0-zip php8.0-mbstring Step 7: Check PHP 8 loaded modules # php¬†-m "},"title":"How to Install PHP 8 on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-php-in-centos-7/":{"data":{"":"\nPHP is the part of our setup that processes code for dynamic content display. It can run scripts, link to our MySQL databases, and view processed content to our web server.\nStep 1. We can use the following command for the installation of our components. The php-mysql kit will also be included.\n# yum install php php-mysql Step 2. PHP should be built problem-free. To function with PHP we have to restart the Apache web server.\n# systemctl restart httpd Step 3. We can install the php modules by the following command.\n# yum install php-fpm Step 4. We can test the php using the test page. We will edit create a php file at given location.\n# vi /var/www/html/info.php Now copy the content as given below in the file and save the file by :wq\n[filecode file=\"/var/www/html/index.php\" [/filecode]\nStep 5 : We can access the page by following url.\nhttp://your_server_ip/info.php\nThank you!"},"title":"How to Install PHP in CentOS 7"},"/utho-docs/docs/linux/how-to-install-php-on-centos-7/":{"data":{"":"Description\nOn CentOS 7, which will be covered in this tutorial, we will learn how to install PHP on centos 7. PHP is a popular open-source general-purpose scripting language that is well suited for web development and can be integrated into HTML. PHP can also be thought of as a server-side scripting language. PHP stands for PHP: Hypertext Preprocessor, which is a recursive acronym in and of itself. PHP is an abbreviation that stands for PHP.\nFollow the below steps to learn How to install PHP on CentOS 7\nNOTE:- The Apache web server should already be set up.","step1-update-the-linux-system#Step.1 Update the Linux system":"Before proceeding with the installation of the PHP packages, you are required to first bring the operating system up to speed by utilising the yum update command.\nyum update How to install PHP on CentOS 7","step2-install-php#Step.2 Install PHP":"Install PHP by issuing the yum install php instruction at the command prompt.\nyum install php ","step3-check-version#Step.3 Check version":"After the installation has been completed, you should verify the version of PHP that is currently installed by executing the php version command.\nphp --version I truly wish that each and every one of these things was crystal plain to you, and I really do hope that they were. PHP installation instructions for CentOS 7\nMust read:- https://utho.com/docs/tutorial/find-multiple-ways-to-user-account-info-and-login-details-in-linux/"},"title":"How to install PHP on CentOS 7"},"/utho-docs/docs/linux/how-to-install-php-on-ubuntu-18-04/":{"data":{"":"Description\nIn this article we will learn How to install PHP on Ubuntu 18.04. PHP, which stands for ‚ÄúHypertext Preprocessor,‚Äù is a general-purpose programming language that is open-source and extensively used. It is especially well-suited for the building of websites and may be integrated in HTML.\nFollow the below steps to How to install PHP on Ubuntu 18.04.","step-1-update-server#Step 1 Update server":"In the first step, you need to use the sudo apt update command to download and install all the available updates. You can then use the sudo apt upgrade command to update packages to the current version, as shown below.\napt-get update ","step-2-install-php#Step 2 Install PHP¬†":"At this point, you need to install PHP by running the command sudo apt-get install php.\napt-get install php ","step-3-checking-the-php-version#Step 3 checking the php version":"When the installation is finished, use the php ‚Äìversion command to check the version of PHP that was installed. as shown below.","step-4-test-your-first-php-program#Step 4 Test your First PHP Program":"Congratulations PHP has now been set up correctly. Let‚Äôs make our first PHP programme create a file testing.php and enter below lines and run it by typing php testing.php.\n\u003c?php ```phpinfo(); ?\u003e php testing.php I really hope that you have a complete understanding of all the processes to how to install PHP on Ubuntu 18.04.\nMust Read :- https://utho.com/docs/tutorial/how-to-install-ncurses-library-on-ubuntu-20-04/\nThankYou"},"title":"How to install PHP on Ubuntu 18.04"},"/utho-docs/docs/linux/how-to-install-phpmyadmin-on-centos/":{"data":{"":"\nIntroduction In this article we will learn how to install phpMyAdmin on CentOS. The database utility,¬†phpMyAdmin,¬†is used for managing MySQL databases through a graphical web-based interface. It can be configured to manage a local database (on the same system), or a remote database (over a network).","change-alias#Change Alias":"Open phpMyAdmin.conf using a text editor:\nsudo vim /etc/phpMyAdmin/config.inc.php Near the top, you should see two lines:\nAlias /phpMyAdmin /usr/share/phpMyAdmin Alias /phpmyadmin /usr/share/phpMyAdmin ‚ÄúAlias‚Äù is how the internet will see your phpMyAdmin configuration.\nSince the default configuration is an easy target for bots and hackers, consider changing the alias setting.¬†Just put a ‚Äò#‚Äô sign before the existing entries so the program sees them as comments, rather than instructions.¬†Then add your own line:\nAlias /MySecretLogin /usr/share/phpMyAdmin Now, when you go to your login screen, you‚Äôll have to type¬†https://IP_OR_DOMAIN/MySecretLogin¬†(or whatever you choose) to gain access.\nStep 5: Restart Apache\nsudo systemctl restart httpd.service Step 6: Verify phpMyAdmin is Working\nTo check if phpMyAdmin is working correctly, enter your¬†servers IP and /phpmyadmin¬†in a web browser. For example:\n125.0.0.2/phpmyadmin You should see the PhpMyAdmin login screen.\nThank You!","installation-of-phpmyadmin#Installation of phpMyAdmin":"Step 1: Install EPEL Repository\nType the following at the command prompt, then hit enter:\nsudo yum install -y epel-release Once that operation finishes, it‚Äôs a good idea to refresh and update the EPEL repository:\nsudo yum ‚Äìy update Step 2: Install Apache Web Server\nyum install httpd -y Verify the status of Apache by running with the command:\nsystemctl status httpd You can also enter your server‚Äôs IP address in a browser and a testing screen should display:\nStep 3: Installing phpMyAdmin on CentOS 7\nsudo yum -y install phpmyadmin Step 4: Configuring and Securing phpMyAdmin\nYour new software installation includes a default Apache configuration file.¬†You‚Äôll want to make some changes to that configuration to prevent unauthorized access. Here are two common ways of restricting access to unauthorized users.\nRestrict IP Addresses This method can be used to grant remote access to a single workstation.¬†By default, phpMyAdmin is configured so that the server it‚Äôs installed on has access.¬†This change lets you allow or restrict access to specific IP addresses of different or additional computers.\nThe file is located at¬†/etc/httpd/conf.d/phpMyAdmin.conf.¬†Type the following at your command prompt:\nsudo vim /etc/phpMyAdmin/config.inc.php Inside the¬†config¬†file, you should see four (4) lines that refer to ‚ÄúRequire IP‚Äù or ‚ÄúAllow IP.‚Äù¬†By default, they should be set to¬†127.0.0.1, which is the IP address referring to the system you are working on.¬†To allow other systems to access this phpMyAdmin application, add (or change) these numbers to the IP address of the computer you want to grant access to.\nOnce you‚Äôve made the changes, save the file.","prerequisites#Prerequisites":" LAMP (Linux, Apache, MySQL, PHP) stack installed on the server. Install LAMP Stack Lamp stack is very vital for us to install phpmyadmin on CentOS. Access to a¬†sudo user account with root privileges¬†(ability to use the¬†sudo¬†command). The CentOS 7 operating system. The YUM update manager, which is a default component of CentOS 7. A command prompt with root access. To open a command prompt:¬†Menu \u003e Applications \u003e Utilities \u003e Terminal. A text editor. Vim is included with most Linux installations or you can use¬†Nano. "},"title":"How to install phpMyAdmin on CentOS"},"/utho-docs/docs/linux/how-to-install-phpmyadmin-on-linux/":{"data":{"":" How to install phpMyAdmin on Linux","description#Description":"In this tutorial, we will see how to install phpMyAdmin on Linux. Many users need the features of a database management system like MySQL, but they may not feel comfortable interacting with the system only from the MySQL prompt.\nphpMyAdmin was made so that people could use MySQL through a web interface.","installation-of-phpmyadmin#Installation of phpMyAdmin":"Step 1: First copy the download link of the desired phpmyadmin version from the official site of phpmyadmin.\nStep 2: Now download the phpmyadmin on your server.\nwget https://files.phpmyadmin.net/phpMyAdmin/5.2.0/phpMyAdmin-5.2.0-all-languages.tar.gz Step 3: Now extract the content from the newly downloaded phpmyadmin .gz file.\ntar -xjv tar -xzf phpMyAdmin-5.2.0-all-languages.tar.gz Step 4: Now move you phpmyadmin folder to the default document root path of the server( /var/www/html)\nmv phpMyAdmin-5.2.0-all-languages /var/www/html/pma Step 5: Now go to your browser and search - serverip/pva\nThis is it. You have successfully installed the phpmyadmin in just 5 simple steps !!!\nAlso Read: How to check and analyze packets by tcpdump command, How to install Composer on Ubuntu 20.04","prerequisites#Prerequisites":" Super user( root) or any normal user with SUDO privileges. wget command LAMP installed on the Ubuntu machine( Install LAMP on Ubuntu) "},"title":"How to install phpMyAdmin on Linux"},"/utho-docs/docs/linux/how-to-install-podman-on-ubuntu-20-04-lts/":{"data":{"":" How to Install Podman on Ubuntu 20.04 LTSHow to Install Podman on Ubuntu 20.04 LTS\nDescription\nInstalling Podman on Ubuntu 20.04 LTS is the next step, but before we get started, let‚Äôs see what we can learn about this application. According to the official description, Podman, also known as the Pod Manager Tool, is a daemonless open source container engine that allows us to create, manage, and operate OCI containers on our Linux system. There is also the option to use the podman utility that is included in the libpod library. As a result of the recent modification to the Docker licence, podman has the potential to become a useful alternative that is compatible with the vast majority of Linux distributions, including the long-term support version of Ubuntu 20.04 LTS.","add-gpg-key#¬†Add GPG Key":"Then, use the command below to download and add the GPG key. This is needed to make sure the downloaded package is good.","add-repository#Add Repository":"Since podman is not in the default Ubuntu repository, we need to add the Kubic repository to be able to download the podman package. Then it can be installed using one of the package managers in Ubuntu, such as apt or apt-get. To add the repository, use the below command.\necho \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /\" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list ","install-podman#Install Podman":"Now, you can use the apt-get -y install podman command, as shown below, to download and install the podman package from the configured repository. This will install the package and everything it needs to work.\napt-get -y install podman Thankyou","prerequisites#¬†Prerequisites":"a) You should already have an Ubuntu 20.04 LTS server up and running.\nb) You need access to sudo or root to run commands with special permissions.\nb) You need to make sure that the utilities apt-get, curl, and apt-key are installed on your server.","source-os-release#Source OS Release":"First, use the source /etc/os-release command to get the OS release, as shown below.\nsource /etc/os-release ","update-server#Update Server":" apt-get update ","upgrade-server#Upgrade Server":" apt-get -y upgrade "},"title":"How to Install Podman on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-install-postgres-database-in-centos-7/":{"data":{"":"","su--postgres#su ‚Äì postgres**":"\nIntroduction:\nPostgres, is an open-source relational database management system also known as PostgreSQL.It was originally named POSTGRES, referring to its origins as a successor to the Ingres database developed at the University of California, Berkeley\nPerquisite:¬†Server with root privilege\npackages should be updated\nSSH should be working\nInstallation :\nStep 1:¬†We can install postgress sql using below command.\n**# yum install postgresql-server postgresql-contrib** Step 2:¬†One we will done with the installation we will initialize the database using below command.\n**# postgresql-setup initdb** Step 3:¬†We will start and enable the postgress service using below command\n**## systemctl start postgresql** **## systemctl enable postgresql** Step 4: Afterward, we need to setup the database.\nFirstly, we will reset the password of postgres user¬†using the below command\n**## passwd postgres** Step 5:¬†Once the password has been reset, we will login to the postgres user using the below command.\n**```\nsu ‚Äì postgres** **Step 7:**¬†Now, we will initialize the bash shell for database login using below command. ## su ‚Äìshell /bin/bash postgres\n**Step 8**: again we will move to the postgres user using below command **## su ‚Äì postgres\n**step 9:**¬†Now we will login the postgres database ## psql postgres\n**step 9:**¬†We can create a test database using the below command in postgres ## createdb testDB\nThank You :) "},"title":"HOW TO INSTALL POSTGRES DATABASE IN CENTOS 7"},"/utho-docs/docs/linux/how-to-install-postgresql-15-on-ubuntu-22-04/":{"data":{"":"","1-enable-the-postgresql-package-repository#1. Enable the PostgreSQL Package Repository":"","2-install-postgresql-15-database-server-and-client#2. Install PostgreSQL 15 Database Server and Client":"","3-change-the-password-for-the-postgresql-admin-user#3. Change the Password for the PostgreSQL Admin User":"","4-set-up-postgresql-for-remote-access#4. Set up PostgreSQL for Remote Access":"","5-checking-remote-connection#5. Checking Remote Connection":"\nIntroduction In this article, you will learn how to Install PostgreSQL 15 on Ubuntu 22.04.\nPostgreSQL is a powerful object-relational database system that is available as free open source software. It makes use of and extends the SQL language, and it combines this with a number of features that allow it to safely store and grow even the most complex data demands.\nPostgreSQL was initially developed in 1986 as a part of the POSTGRES project at the University of California, Berkeley. The core platform has seen continuous development for more than 35 years. PostgreSQL‚Äôs origins may be traced back to 1986.\n1. Enable the PostgreSQL Package Repository First, get the latest packages. Apt update can do this:\n# apt update Since the PostgreSQL 15 package is not currently accessible in the default package repository, you will need to enable PostgreSQL‚Äôs official package repository using the following commands.\n# sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb\\_release -cs)-pgdg main\" \u003e /etc/apt/sources.list.d/pgdg.list' # wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo tee /etc/apt/trusted.gpg.d/pgdg.asc \u0026\u003e/dev/null 2. Install PostgreSQL 15 Database Server and Client In addition, the postgresql-client package is responsible for installing the client tool, whereas the postgresql package will install the default version of the PostgreSQL database server.\nLet‚Äôs install the client and server for PostgreSQL using the apt command that is listed below:\n# apt install postgresql postgresql-client -y After that, let‚Äôs check to see if the PostgreSQL service is online and functional:\n# systemctl status postgresql Next, using the psql command line programme, check to see what version of PostgreSQL is currently installed:\n# psql --version 3. Change the Password for the PostgreSQL Admin User We don‚Äôt need to enter a password in order to connect to the PostgreSQL server because that setting is default. Now, let‚Äôs put this into action with the help of the psql utility:\n# sudo -u postgres psql In this example, we will be using the postgres user. During the process of installing PostgreSQL, a user with administrative privileges is automatically established.\nIt is not a good idea to give administrative users access to the database without requiring them to provide a password. Hence, let‚Äôs go ahead and change the password for the postgres user:\n# ALTER USER postgres PASSWORD 'demoPassword'; The user‚Äôs password is set to demoPassword by the SQL query above. Please note that because this is a demo environment, we used a very simple password. But it‚Äôs not a good idea to do the same thing in a production environment.\nLet‚Äôs make sure that the password was set up correctly. So, first, use the q command to end the current session with the server.\n# \\\\q The outcome of the commands above.\nNow, let‚Äôs get reconnecting with the database server:\n# psql -h localhost -U postgres Entering the demoPassword string as a password connects us to the database.\n4. Set up PostgreSQL for Remote Access By default, only the localhost can connect to PostgreSQL. We can easily change the configuration, though, so that remote clients can connect.\nPostgreSQL gets its settings from the file postgresql.conf, which is in the directory /etc/postgresql//main/. Here, the word ‚Äúversion‚Äù shows which major version of PostgreSQL is being used.\nIn our case, for example, the file‚Äôs full path is /etc/postgresql/15/main/postgresql.conf.\n# vi /etc/postgresql/15/main/postgresql.conf Now, in a text editor, replace the word ‚Äúlocalhost‚Äù with ‚Äú*‚Äù at the beginning of the line that begins with the listen addresses.\nThis option can be found under the CONNECTIONS AND AUTHORIZATION submenu. The updated file will look like this:\nBy using escape:wq, you can save and close the file.¬†The next step is to open up IPv4 connections from all clients by editing the pg_hba.conf file. In addition to the /etc/postgresql/15/main/ directory, this file can also be found there.\n# vi /etc/postgresql/15/main/pg\\_hba.conf Here‚Äôs how the updated file will look like:\nThe above- mentioned configurations indicate a willingness to take on connections from the 192.168.1.0/24 network.\nIf you‚Äôre using Ubuntu as a firewall, you can open port 5432 for PostgreSQL by typing the following command.\n# ufw allow 5432/tcp 5. Checking Remote Connection Once you‚Äôve finished, restart the service and make sure it‚Äôs running:\n# systemctl restart postgresql # systemctl status postgresql Now that we have that settled, let‚Äôs proceed to access the DB from a remote client.\n# psql -h 192.168.1.192 -U postgres From this point on, we are able to see that the remote client can access the database.","conclusion#Conclusion":"Hopefully, now you have learned how to Install PostgreSQL 15 on Ubuntu 22.04.\nAlso Read: How to Install MariaDB 10.3 on Ubuntu 20.04\nThank You üôÇ","introduction#Introduction":""},"title":"How to Install PostgreSQL 15 on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-postman-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Postman on Centos 7.\nThank You üôÇ","install-postman#Install Postman":" # yum upgrade # yum install snapd -y # snap install postman ","introduction#Introduction":"In this article, you will learn how to install Postman on Centos 7.\nTo put it more plainly, Postman is a computer application that is utilised for API testing. Postman communicates with the web server in order to make an API request and then receives the response, irrespective of what it may be. When sending and receiving requests using Postman, there is no need for any further effort or the setting up of a framework. Utilized to a large extent by both testers and developers for the purpose of improving application testing. It is simple to integrate with both your Continuous Integration (CI) Pipeline and your Continuous Development Pipeline.\nPostman has been utilised by millions of testers due to its extensive feature set as well as its user-friendliness. You may submit requests quickly and easily with its straightforward and uncomplicated graphical user interface; all you need to do is fill in the relevant data, choose the HTTP method, and click the ‚ÄúSend‚Äù button. Automation, which enables users to set up tests and write test suites, is another feature that sees widespread usage.\nPostman provides a paid version in addition to its free version, which allows for multiple user access, control over roles and access, SSO Authentication, and a number of other professional features. Postman is an ideal testing tool because it enables users to set up the required environment, write specifications, and finally monitor every step. This is the only feature that makes Postman ideal.","postman-cli#Postman CLI":"Postman is the company that creates, maintains, and signs the Postman CLI (Command Line Interface), which is the command-line companion. You will also be able to run collections, lint API schemas, perform security and governance checks, as well as log in and out of the system. The app will receive a notification of each and every test result automatically.\n# curl -o- \"https://dl-cli.pstmn.io/install/linux64.sh\" | sh To learn more about Postman, run the following command.¬†# postman "},"title":"How to install Postman on Centos 7"},"/utho-docs/docs/linux/how-to-install-postman-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Postman on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-postman#Install Postman":" # apt upgrade # apt install snapd # snap install postman ","introduction#Introduction":"In this article, you will learn how to install Postman on Debian 12.\nTo put it more plainly, Postman is a computer application that is utilised for API testing. Postman communicates with the web server in order to make an API request and then receives the response, irrespective of what it may be. When sending and receiving requests using Postman, there is no need for any further effort or the setting up of a framework. Utilized to a large extent by both testers and developers for the purpose of improving application testing. It is simple to integrate with both your Continuous Integration (CI) Pipeline and your Continuous Development Pipeline.\nPostman¬†has been utilised by millions of testers due to its extensive feature set as well as its user-friendliness. You may submit requests quickly and easily with its straightforward and uncomplicated graphical user interface; all you need to do is fill in the relevant data, choose the HTTP¬†method, and click the ‚ÄúSend‚Äù button. Automation, which enables users to set up tests and write test suites, is another feature that sees widespread usage.\nPostman provides a paid version in addition to its free version, which allows for multiple user access, control over roles and access, SSO Authentication, and a number of other professional features. Postman is an ideal testing tool because it enables users to set up the required environment, write specifications, and finally monitor every step. This is the only feature that makes Postman ideal.","postman-cli#Postman CLI":"Postman is the company that creates, maintains, and signs the Postman CLI (Command Line Interface), which is the command-line companion. You will also be able to run collections, lint API schemas, perform security and governance checks, as well as log in and out of the system. The app will receive a notification of each and every test result automatically.\n# curl -o- \"https://dl-cli.pstmn.io/install/linux64.sh\" | sh To learn more about Postman, run the following command.¬†# postman "},"title":"How to install Postman on Debian 12"},"/utho-docs/docs/linux/how-to-install-postman-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Postman on Debian.\nThank You üôÇ","install-postman#Install Postman":" # apt upgrade # apt install snapd # snap install postman ","introduction#Introduction":"In this article, you will learn how to install Postman on Debian.\nTo put it more plainly, Postman is a computer application that is utilised for API testing. Postman communicates with the web server in order to make an API request and then receives the response, irrespective of what it may be. When sending and receiving requests using Postman, there is no need for any further effort or the setting up of a framework. Utilized to a large extent by both testers and developers for the purpose of improving application testing. It is simple to integrate with both your Continuous Integration (CI) Pipeline and your Continuous Development Pipeline.\nPostman has been utilised by millions of testers due to its extensive feature set as well as its user-friendliness. You may submit requests quickly and easily with its straightforward and uncomplicated graphical user interface; all you need to do is fill in the relevant data, choose the HTTP method, and click the ‚ÄúSend‚Äù button. Automation, which enables users to set up tests and write test suites, is another feature that sees widespread usage.\nPostman provides a paid version in addition to its free version, which allows for multiple user access, control over roles and access, SSO Authentication, and a number of other professional features. Postman is an ideal testing tool because it enables users to set up the required environment, write specifications, and finally monitor every step. This is the only feature that makes Postman ideal.","postman-cli#Postman CLI":"Postman is the company that creates, maintains, and signs the Postman CLI (Command Line Interface), which is the command-line companion. You will also be able to run collections, lint API schemas, perform security and governance checks, as well as log in and out of the system. The app will receive a notification of each and every test result automatically.\n# curl -o- \"https://dl-cli.pstmn.io/install/linux64.sh\" | sh To learn more about Postman, run the following command.¬†# postman "},"title":"How to install Postman on Debian"},"/utho-docs/docs/linux/how-to-install-postman-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Postman on Fedora.\nThank You üôÇ","install-postman#Install Postman":" # dnf upgrade # dnf install snapd -y # ln -s /var/lib/snapd/snap /snap # snap install postman ","introduction#Introduction":"In this article, you will learn how to install Postman on Fedora.\nTo put it more plainly, Postman is a computer application that is utilised for API testing. Postman communicates with the web server in order to make an API request and then receives the response, irrespective of what it may be. When sending and receiving requests using Postman, there is no need for any further effort or the setting up of a framework. Utilized to a large extent by both testers and developers for the purpose of improving application testing. It is simple to integrate with both your Continuous Integration (CI) Pipeline and your Continuous Development Pipeline.\nPostman has been utilised by millions of testers due to its extensive feature set as well as its user-friendliness. You may submit requests quickly and easily with its straightforward and uncomplicated graphical user interface; all you need to do is fill in the relevant data, choose the HTTP method, and click the ‚ÄúSend‚Äù button. Automation, which enables users to set up tests and write test suites, is another feature that sees widespread usage.\nPostman provides a paid version in addition to its free version, which allows for multiple user access, control over roles and access, SSO Authentication, and a number of other professional features. Postman is an ideal testing tool because it enables users to set up the required environment, write specifications, and finally monitor every step. This is the only feature that makes Postman ideal.","postman-cli#Postman CLI":"Postman is the company that creates, maintains, and signs the Postman CLI (Command Line Interface), which is the command-line companion. You will also be able to run collections, lint API schemas, perform security and governance checks, as well as log in and out of the system. The app will receive a notification of each and every test result automatically.\n# curl -o- \"https://dl-cli.pstmn.io/install/linux64.sh\" | sh To learn more about Postman, run the following command.¬†# postman "},"title":"How to install Postman on Fedora"},"/utho-docs/docs/linux/how-to-install-postman-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Postman on ubuntu 20.04.\nThank You üôÇ","install-postman#Install Postman":" # apt upgrade # apt install snapd # snap install postman ","introduction#Introduction":"In this article, you will learn how to install Postman on ubuntu 20.04.\nTo put it more plainly, Postman is a computer application that is utilised for API testing. Postman communicates with the web server in order to make an API request and then receives the response, irrespective of what it may be. When sending and receiving requests using Postman, there is no need for any further effort or the setting up of a framework. Utilized to a large extent by both testers and developers for the purpose of improving application testing. It is simple to integrate with both your Continuous Integration (CI) Pipeline and your Continuous Development Pipeline.\nPostman has been utilised by millions of testers due to its extensive feature set as well as its user-friendliness. You may submit requests quickly and easily with its straightforward and uncomplicated graphical user interface; all you need to do is fill in the relevant data, choose the HTTP method, and click the ‚ÄúSend‚Äù button. Automation, which enables users to set up tests and write test suites, is another feature that sees widespread usage.\nPostman provides a paid version in addition to its free version, which allows for multiple user access, control over roles and access, SSO Authentication, and a number of other professional features. Postman is an ideal testing tool because it enables users to set up the required environment, write specifications, and finally monitor every step. This is the only feature that makes Postman ideal.","postman-cli#Postman CLI":"Postman is the company that creates, maintains, and signs the Postman CLI (Command Line Interface), which is the command-line companion. You will also be able to run collections, lint API schemas, perform security and governance checks, as well as log in and out of the system. The app will receive a notification of each and every test result automatically.\n# curl -o- \"https://dl-cli.pstmn.io/install/linux64.sh\" | sh To learn more about Postman, run the following command.¬†# postman "},"title":"How to install Postman on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-python-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Python on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"","step-1-add-the-deadsnakes-ppa-to-ubuntus-list-of-repositories#Step 1: Add the deadsnakes PPA to Ubuntu\u0026rsquo;s list of repositories":"\nIntroduction In this article, you will learn how to install Python on Ubuntu 22.04.\nPython is a high-level programming language that can be interpreted and is object-oriented. Python also has dynamic semantics. Its high-level data structures that are built in, in conjunction with dynamic typing and dynamic binding, make it a very attractive choice for Rapid Application Development. Additionally, its use as a scripting language or glue language to connect already existing components makes it a very versatile choice.\nStep 1: Add the deadsnakes PPA to Ubuntu‚Äôs list of repositories # apt install software-properties-common After you have finished installing the programme, you may use the following shell command to add the deadsnakes repository to your system.\n# add-apt-repository ppa:deadsnakes/ppa You are going to be asked to verify that the activity was performed. Click enter to continue.\nIn most cases, an update is triggered when a repository is added. You can perform the task manually if it does not.\n# apt update ","step-2-python-installation-and-setup#Step 2: Python installation and setup":"Installing Python 3.11¬†is a reasonably simple process once you have added the repository.¬†You can install it using apt in the manner described here.\n# apt install python3.11 ","step-3-establishing-python-311-as-the-standard-version#Step 3: Establishing Python 3.11 as the standard version":"Congrats on the successful installation of Python 3.11. But you might want to take note that the default Python version is still the one you had before.\nThe following command will allow you to make the changes necessary.\n# update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 Now that you‚Äôve tested the Python version, you should notice that it‚Äôs 3.11. Run the below command to check the Python version.\n# python --version "},"title":"How to install Python on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-r-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install R on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install R on Ubuntu 22.04.\nR is a popular open-source programming language that is utilized frequently in the processes of data analysis and statistical computing. It is a programming language that is supported by the R Foundation for Statistical Computing and is growing in popularity.\nIt is also extendable and has an active community. R has a large number of user-created packages that cater to specialized fields of research, which broadens the scope of its use.","step-1-installing-r#Step 1: Installing R":"You will begin by adding the external repository that is maintained by CRAN. This is necessary since R is a rapidly developing project, and the most recent stable version isn‚Äôt always available from the repositories provided by Ubuntu.\nYou‚Äôll need to download and install the key.\n# wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter\\_pubkey.asc | gpg --dearmor -o /usr/share/keyrings/r-project.gpg The next step is to add the R source list to the sources.list.d directory, which is the location where APT will look for new sources:\n# echo \"deb \\[signed-by=/usr/share/keyrings/r-project.gpg\\] https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/\" | tee -a /etc/apt/sources.list.d/r-project.list After that, make sure that your package lists are up to date so that APT can read the new R source:\n# apt update You are now prepared to install R using the following command, as it has become available to you at this time.\n# apt install --no-install-recommends r-base If you are asked to confirm the installation, press the word y to proceed. The --no-install-recommends argument prevents any additional software from being installed on the system.\nWhen you start R using the following command, the most recent stable version of R available from CRAN is 4.2.0, which is displayed. This information is accurate as of the time this article was written.\n# sudo -i R This verifies that you were able to install R correctly and enter its interactive shell properly.","step-2-installing-r-packages-from-the-cran#Step 2: Installing R Packages from the CRAN":"R‚Äôs strength is that it has a lot of packages that can be added to it. For demonstration reasons, you will install txtplot, a package that makes ASCII graphs like scatterplot, line plot, density plot, acf, and bar charts:\n\u003e install.packages(‚Äôtxtplot')\nLoad the txtplot library when the installation is done:\n\u003e library(‚Äôtxtplot')\nIf there are no error messages, it means that the library has loaded correctly."},"title":"How to install R on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-red5-server-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Red5 Server on Ubuntu 22.04.\nAlso Read: How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Red5 Server on Ubuntu 22.04.\nRed5 is a media server that runs on open source software and may be used for live streaming solutions of any kind. It is built to be adaptable and has a straightforward plugin architecture, both of which make it possible to personalise practically any video on demand (VOD) or live streaming scenario. Red5 has been utilised by thousands of firms, including Amazon and Facebook, and it continues to be used today.\nRed5 Pro is built on top of the Red5 Media Server as its primary underlying platform. It includes the frameworks for the server-side application and plug-ins, in addition to the fundamental streaming infrastructure that our solution depends on.","step-1-install-java#Step 1: Install Java":"On your Ubuntu machine, you need to have Java version 11 or a later version installed at the very least. In the event that you do not already have Java installed on your system, you can use the following command to install OpenJDK.\n# apt update # apt install default-jdk Additionally, ensure that the JAVA_HOME environment variable is configured correctly on your machine. The JAVA_HOME environment variable can be modified by using the command. Modify the directory path for the version of Java that you have installed.\n# echo 'export JAVA\\_HOME=/usr/lib/jvm/java-11-openjdk-amd64/' \u003e\u003e /etc/bash.bashrc # source /etc/bash.bashrc ","step-2-set-up-the-red5-server-on-ubuntu#Step 2: Set up the Red5 server on Ubuntu":"The Red5 server 1.2.2 can be downloaded by using the following commands.\n# cd /opt/ # wget https://github.com/Red5/red5-server/releases/download/v1.2.2/red5-server-1.2.2.zip # unzip red5-server-1.2.2.zip Now, begin the process of starting the Red5 server by utilising the provided shell script red5.sh, which can be found in the directory.\n# cd /opt/red5-server/ # ./red5.sh \u0026 The Red5 server will start after this.","step-3-open-the-red5-web-interface#Step 3: Open the Red5 Web interface":"On port 5080, the Red5 demo pages and application can be accessible by URLs such as http://server_IP:5080/.\n# http://server\\_IP:5080/ "},"title":"How to install Red5 Server on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-redis-on-centos/":{"data":{"":" How to install Redis on CentOS\nIn this article, you will learn how to install and configure Redis on CentOS 7 or 8. Redis is a data structure that is stored in memory and functions similarly to a store. It can be used as a cache, streaming engine, or database. Redis makes it possible for developers to save data in their programmes using straightforward statements rather than the more complicated query language often required.Redis has support for a wide variety of data structures, including strings, hashes, sets, and many others. In addition to that, it gives you access to the Replication features as well as the Redis cluster, which gives you automatic partitioning.","prerequisites#Prerequisites":" Any normal user with SUDO privileges or super user yum repolist configured centos server ","steps-to-install-redis-cache-on-centos#Steps to install Redis Cache on CentOS":"Step 1: To install the Redis package on CentOS 7, you need to use the YUM package manager to add the EPEL repository to your machine, as shown.\nyum install epel-release Step 2: Once EPEL is set up, you should perform the following to install the Redis package from the source.\nyum install redis -y Step 3: Now, to verify whether the Redis cache server installed on your Centos, use the below command\n#redis-server Successfully installed redis on centos\nHere, in the screenshot, it is highlighted some line. Once you‚Äôve installed the package, you‚Äôll need to set up your server so that Redis works smoothly. As explained, you need to change some settings in the kernel.\nOptimize the Redis server Step 4: Next, set the Linux kernel‚Äôs overcommit memory setting to 1 by adding vm.overcommit_memory = 1 to the /etc/sysctl.conf setup file.\necho \"vm.overcommit_memory = 1\" \u003e\u003e /etc/sysctl.conf Now, navigate to the line which contains the ‚Äúvm.overcommit_memory‚Äù and change the value to 1. If this line is not present in the configuration file, just add it in the end of the file.\noptimize the redis\nStep 5: and then put the change into effect without restarting the system. Run the following command to turn on the setting right away.\nsysctl vm.overcommit_memory=1 changes into effects\nStep 6: Also, use the echo command below to make sure that the visible huge pages kernel feature is turned off. This feature has a bad effect on both memory usage and delay.\necho never \u003e /sys/kernel/mm/transparent_hugepage/enabled Configure the Redis on CentOS Step 7: Then, use any of your favourite text-based tools to open the original Redis setup file and make changes to it as shown.\nvi /etc/redis.conf There are several setting instructions, and the file explains what they mean and how they should be used.\nA common example of setup is letting the Redis server be accessed from outside the network. By default, Redis is set up to only accept calls on the computer where it is running, on the loopback link (127.0.0.1), and it waits on port 6379.\nTo allow remote access, use the ‚Äúbind‚Äù setup command followed by one or more IP addresses to set it to listen to a single interface or multiple chosen interfaces.\nbind 127.0.0.1 bind \u003credis-server-ip\u003e 192.168.2.24 Step 8: Change the number of the port command to connect to a new port.\nport 1234 Step 9: Now, start and enable to Redis service to start using the Redis server.\nsystemctl enable --now redis Step 10: Use the netstat utility to cross check the whether the redis service is listening on the port we have just mentioned the configuration file\nnetstat -tunlp Step 11: Now, to start listening from the redis client, lets start the redis server using the below command and let it run in backgroud using \u0026\nredis-server \u0026 Run the redis server in backgroud\nStep 12: Connect to your redis server.\nredis-cli ## If you want to test it in the redis server machine redis-cli -h \u003credis-serverip\u003e -p \u003credis-port # Connect from remote client Connect to your redis server\nIn the above screenshot, we have also test the working of the redis server-client setup by setting up the key value pair.\nset msg \"hello, Microhost\" # to set the value get msg And this is how you will install the Redis cache on centos 7 and 8 server."},"title":"How to install Redis on CentOS"},"/utho-docs/docs/linux/how-to-install-redis-on-debian/":{"data":{"":"","2-test-the-redis-setup#2. Test the Redis setup":" How to install Redis on Debian\nIn this article you will learn How to install Redis cache on Debian. Redis cache is one of the most widely used caching solutions for accelerating WordPress websites. It is open source like WordPress, making it a wonderful option to use alongside WordPress and not only Wordpress but for your application which uses databases. However, if you are unfamiliar with caching in general, it can be difficult to comprehend what Redis is and how Redis cache operates.\nRedis cache is widely used because it supports nearly all main programming languages. In addition, like WordPress, it is open source, which means it is a well-supported system that is a cost-effective method to speed up your website or application.\nDespite the fact that Redis cache is predominantly used as a caching system to speed up WordPress websites, Redis itself is far more potent. Redis can also be used as a database and a message broker, but the focus of this article will be on its caching capabilities.Redis cache excels when used as an object caching utility for WordPress websites. Since WordPress is such a potent and versatile Content Management System, it contains a large number of frequently accessed data objects.\nPrerequisites A super user or any other user with SUDO privileges Updated Debian 10 or Debian 11 system with apt repositories ready to install packages Steps to install redis on Debian Step 1: If you haven‚Äôt done so in a while, update your local apt package cache:\napt-get update Step 2: Now, install Redis using apt utility\napt-get install redis-server This will get Redis and all of its components and install them. After that, you need to make one important change to the Redis setup file, which was automatically made during startup.\nStep 3: Change the supervised value to systemd\nvi /etc/redis/redis.conf Find the supervised keyword in the file. With this command, you can tell an init system to take care of Redis as a service. This gives you more control over how it works. By default, the supervised command is set to ‚Äúno.‚Äù Since you are using Debian, which uses the systemd init system, change this to systemd\nchange the init system to systemd of Redis\nAt this point, that is the only change you need to make to the Redis configuration file. Save and close the file when you are done. Then, restart the Redis service to make the changes to the settings file take effect:\nStem 4: Now, restart the redis service\nsystemctl restart redis 2. Test the Redis setup Before making any more changes to Redis‚Äôs setup, it‚Äôs a good idea to make sure it works as planned, just like you should do with any new software. In this step, we will talk about a number of ways to make sure Redis is working properly.\nStep 1: First, make sure the Redis service is up and running: use the command-line client to connect to the server:\nredis-cli Step 2: Use the ping command at the next window to see if you can connect.\nping Connectivity to redis server\nStep 3: This result shows that the link to the server is still alive. Next, make sure you can set keys by typing:\nset msg \"Congratulations, It's working !\" Step 4: Retrieve the value by typing:\nget msg able to set key value in redis\nAnd this is how you have learnt How to install Redis on Debian.","prerequisiteshttpswwwdigitaloceancomcommunitytutorialshow-to-install-and-secure-redis-on-ubuntu-18-04prerequisites#\u003ca href=\"https://www.digitalocean.com/community/tutorials/how-to-install-and-secure-redis-on-ubuntu-18-04#prerequisites\"\u003ePrerequisites\u003c/a\u003e":"","steps-to-install-redis-on-debian#Steps to install redis on Debian":""},"title":"How to install Redis on Debian"},"/utho-docs/docs/linux/how-to-install-redis-on-fedora/":{"data":{"":" How to install Redis on Fedora\nIn this article, you will learn how to install and configure Redis on CentOS 7 or 8. Redis is a data structure that is stored in memory and functions similarly to a store. It can be used as a cache, streaming engine, or database. Redis makes it possible for developers to save data in their programmes using straightforward statements rather than the more complicated query language often required.Redis has support for a wide variety of data structures, including strings, hashes, sets, and many others. In addition to that, it gives you access to the Replication features as well as the Redis cluster, which gives you automatic partitioning.","prerequisites#Prerequisites":" Any normal user with SUDO privileges or super user yum server configured Fedora server ","steps-to-install-redis-cache-on-fedora#Steps to install Redis Cache on Fedora":"Step 1: To install the Redis package on Fedora you need to use the YUM package manager to add the EPEL repository to your machine, as shown.\nyum install epel-release Step 2: Once EPEL is set up, you should perform the following to install the Redis package from the source.\nyum install redis -y Step 3: Now, to verify whether the Redis cache server installed on your Fedora, use the below command\n#redis-server Successfully installed redis on Fedora\nHere, in the screenshot, it is highlighted some line. Once you‚Äôve installed the package, you‚Äôll need to set up your server so that Redis works smoothly. As explained, you need to change some settings in the kernel.\nOptimize the Redis server Step 4: Next, set the Linux kernel‚Äôs overcommit memory setting to 1 by adding vm.overcommit_memory = 1 to the /etc/sysctl.conf setup file.\necho \"vm.overcommit_memory = 1\" \u003e\u003e /etc/sysctl.conf Now, navigate to the line which contains the ‚Äúvm.overcommit_memory‚Äù and change the value to 1. If this line is not present in the configuration file, just add it in the end of the file.\noptimize the redis\nStep 5: and then put the change into effect without restarting the system. Run the following command to turn on the setting right away.\nsysctl vm.overcommit_memory=1 changes into effects\nStep 6: Also, use the echo command below to make sure that the visible huge pages kernel feature is turned off. This feature has a bad effect on both memory usage and delay.\necho never \u003e /sys/kernel/mm/transparent_hugepage/enabled Configure the Redis on Fedora Step 7: Then, use any of your favourite text-based tools to open the original Redis setup file and make changes to it as shown.\nvi /etc/redis.conf There are several setting instructions, and the file explains what they mean and how they should be used.\nA common example of setup is letting the Redis server be accessed from outside the network. By default, Redis is set up to only accept calls on the computer where it is running, on the loopback link (127.0.0.1), and it waits on port 6379.\nTo allow remote access, use the ‚Äúbind‚Äù setup command followed by one or more IP addresses to set it to listen to a single interface or multiple chosen interfaces.\nbind 127.0.0.1 bind \u003credis-server-ip\u003e 192.168.2.24 Step 8: Change the number of the port command to connect to a new port.\nport 1234 Step 9: Now, start and enable to Redis service to start using the Redis server.\nsystemctl enable --now redis Step 10: Use the netstat utility to cross check the whether the redis service is listening on the port we have just mentioned the configuration file\nnetstat -tunlp Step 11: Now, to start listening from the redis client, lets start the redis server using the below command and let it run in backgroud using \u0026\nredis-server \u0026 Run the redis server in backgroud\nStep 12: Connect to your redis server.\nredis-cli ## If you want to test it in the redis server machine redis-cli -h \u003credis-serverip\u003e -p \u003credis-port # Connect from remote client Connect to your redis server\nIn the above screenshot, we have also test the working of the redis server-client setup by setting up the key value pair.\nset msg \"hello, Microhost\" # to set the value get msg And this is how you will install the Redis cache on Fedora server."},"title":"How to install Redis on Fedora"},"/utho-docs/docs/linux/how-to-install-redis-on-ubuntu/":{"data":{"":"","2-test-the-redis-setup#2. Test the Redis setup":" How to install Redis on Ubuntu\nIn this article you will learn How to install Redis cache on Ubuntu.\nRedis cache is one of the most widely used caching solutions for accelerating WordPress websites. It is open source like WordPress, making it a wonderful option to use alongside WordPress and not only Wordpress but for your application which uses databases..\nHowever, if you are unfamiliar with caching in general, it can be difficult to comprehend what Redis is and how Redis cache operates.\nRedis cache is widely used because it supports nearly all main programming languages. In addition, like WordPress, it is open source, which means it is a well-supported system that is a cost-effective method to speed up your website or application.\nDespite the fact that Redis cache is predominantly used as a caching system to speed up WordPress websites, Redis itself is far more potent. Redis can also be used as a database and a message broker, but the focus of this article will be on its caching capabilities.Redis cache excels when used as an object caching utility for WordPress websites. Since WordPress is such a potent and versatile Content Management System, it contains a large number of frequently accessed data objects.\nPrerequisites A super user or any other user with SUDO privileges Updated Ubuntu 22.04 or Ubuntu 20.04 system with apt repositories ready to install packages Steps to install redis on Ubuntu Step 1: If you haven‚Äôt done so in a while, update your local apt package cache:\napt-get update Step 2: Now, install Redis using apt utility\napt-get install redis-server This will get Redis and all of its components and install them. After that, you need to make one important change to the Redis setup file, which was automatically made during startup.\nStep 3: Change the supervised value to systemd\nvi /etc/redis/redis.conf Find the supervised keyword in the file. With this command, you can tell an init system to take care of Redis as a service. This gives you more control over how it works. By default, the supervised command is set to ‚Äúno.‚Äù Since you are using Ubuntu, which uses the systemd init system, change this to systemd\nchange the init system to systemd of Redis\nAt this point, that is the only change you need to make to the Redis configuration file. Save and close the file when you are done. Then, restart the Redis service to make the changes to the settings file take effect:\nStem 4: Now, restart the redis service\nsystemctl restart redis 2. Test the Redis setup Before making any more changes to Redis‚Äôs setup, it‚Äôs a good idea to make sure it works as planned, just like you should do with any new software. In this step, we will talk about a number of ways to make sure Redis is working properly.\nStep 1: First, make sure the Redis service is up and running: use the command-line client to connect to the server:\nredis-cli Step 2: Use the ping command at the next window to see if you can connect.\nping Connectivity to redis server\nStep 3: This result shows that the link to the server is still alive. Next, make sure you can set keys by typing:\nset msg \"Congratulations, It's working !\" Step 4: Retrieve the value by typing:\nget msg able to set key value in redis","prerequisiteshttpswwwdigitaloceancomcommunitytutorialshow-to-install-and-secure-redis-on-ubuntu-18-04prerequisites#\u003ca href=\"https://www.digitalocean.com/community/tutorials/how-to-install-and-secure-redis-on-ubuntu-18-04#prerequisites\"\u003ePrerequisites\u003c/a\u003e":"","steps-to-install-redis-on-ubuntu#Steps to install redis on Ubuntu":""},"title":"How to install Redis on Ubuntu"},"/utho-docs/docs/linux/how-to-install-rkhunter-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Rkhunter on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","installation-of-rkhunter#Installation of Rkhunter":"","introduction#Introduction":"","rkhunters-configuration#Rkhunter\u0026rsquo;s configuration":"\nIntroduction In this article, you will learn how to install Rkhunter on Ubuntu 22.04.\nRkhunter, which is also known as Rootkit Hunter, is responsible for searching the system for rootkits. It is an open-source programme that has been developed specifically for Linux and operating systems that are based on Linux.\nYou can use the tool to safeguard your system from being attacked by third parties, maintain your privacy, and enhance its overall security. This programme investigates hidden files, potentially malicious strings, and incorrect permissions in binary files.\nRkhunter does a system scan to determine which rootkits are present by comparing the SHA-1 hashes found in local folders with those found in an online database. Because it is developed in Bash, which offers more portability, this programme is compatible with all UNIX-based operating systems.\nInstallation of Rkhunter The first step is to bring the system up quick with the latest data.\n# apt update Now, type this command to install Rkhunter from Ubuntu‚Äôs official repository:\n# apt¬†install¬†rkhunter The system moves you to the window for configuring packages. Click the ‚ÄúTab‚Äù key. Then, click ‚ÄúOK‚Äù and press ‚ÄúEnter‚Äù to go to the next page.\nHere, choose the type of mail configuration: Select Local only then press ok.\nLast, type the name of the email. The process for downloading is done when you press the ‚ÄúEnter‚Äù button:\nRkhunter‚Äôs configuration After you have installed Rkhunter, it is time to set it up. To open the config file, type the following command:\n# vi /etc/rkhunter.conf Scroll down to the part named UPDATE_MIRRORS and change the value from 0 to 1. Change the MIRRORS_MODE to 0 as well:\nScroll down until you see WEB_CMD after making the changes above. To make it Null, remove away /bin/false:\nSave the file to make sure the changes are done correctly.\nYou can also use the following tools to check for any unexpected configurations:\n# rkhunter -C If the command‚Äôs result is 1, it means that there is an issue with how the operating system is set up.\nIn the end, run the following command to check for rootkits and other threats by updating the Rkhunter database:\n# rkhunter --update After setting up Rkhunter, you can now run the following command to start the Rkhunter scan:\n# rkhunter --check If Rkhunter finds issues with security, it will tell you how to fix them."},"title":"How to install Rkhunter on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-shellinabox-on-debian-server/":{"data":{"":" How to install Shellinabox on Debian server\nIn this tutorial, we will learn how to install Shellinabox on Debian server. Before installing Shellinabox on Debian server to use the browser to access the Debian terminal, we need to figure out how to do that. Markus Gutschke made Shell in a Box, and its name is pronounced ‚Äúshellin‚Äô box.‚Äù It is a web-based programme that acts like a terminal. It gives you a web terminal emulator that lets you access and control your Linux Server SSH Shell remotely from any browser that supports AJAX/JavaScript and CSS, without the need for additional browser plugins like FireSSH. It has a web server built in that works as a web-based SSH client on a certain port. This web server also works as an SSH client that runs on the web.\nIn this article, we‚Äôll talk about how to install Shellinabox and connect to a remote SSH terminal using any modern web browser. Any system can use Shellinabox. When you are behind a firewall and can only let HTTP(s) traffic through, Web-based SSH is a very useful tool.","1--installation#1- Installation":"To install the Shellinabox on your Ubuntu server, first update the apt repository on it. To do so, use the below command.\napt update And now you can install the shellinabox on your machine.\napt install shellinabox -y systemctl start shellinabox systemctl enable shellinabox ","2--configure-shellinabox#2- Configure Shellinabox":"You will find the configuration file has a different name and directory in which it is present as you go from one Linux flavor to another. In Debian the configuration file can be found as- /etc/default/shellinabox\nvi /etc/default/shellinaboxd Content of /etc/default/shellinabox: Should shellinaboxd start automatically\nTCP port that shellinboxd‚Äôs webserver listens on\nParameters that are managed by the system and usually should not need changing: SHELLINABOX_DATADIR=/var/lib/shellinabox SHELLINABOX_USER=shellinabox SHELLINABOX_GROUP=shellinabox\nAny optional arguments (e.g. extra service definitions). Make sure that that argument is quoted.\nBeeps are disabled because of reports of the VLC plugin crashing Firefox on Linux/x86_64.","3--points-to-remember#3- Points to remember:":" For security reasons, we should change the default port number of shellinabox service. Make sure to enable the shellinabox port in your firewall( if you are using one) ","4--verify-the-port-and-running-status-of-shellinabox#4- Verify the port and running status of Shellinabox":"[Using the¬†netstat command](http://microhost.com/docs/tutorial/how-to-real-time-monitor-tcp-and-udp-ports/(opens in a new tab)), you can see the all the listening tcp and udp ports of your server.\nnetstat -tunlp | grep shellinaboxd listening port of shellinabox\nNow, open your web browser and go to https://Your-IP-Address:4200. You should be able to see an SSH terminal that runs on the web. When you log in with your username and password, you should see your shell prompt.\nAfter you hit the browser with your serverip and port number, you will see a warning page on which it is showing a security risk. This warning is showing because of lack of ssl on your server for this site. To ignore this, press ‚ÄòAdvanced‚Äô and then ‚ÄòAccept and Continue‚Äô\nsecurity page of browser\nAfter this, you will see a blank white screen prompt which will ask you the username similar to which you see on your ssh prompt. Here, enter the username, from which you want to login. After username it will ask you the password.\nFor entering the password, you can enter it either by typing manually or just right click on your browser screen and select ‚ÄòPaste from browser‚Äô and paste the password on the next windows.\nTo paste password on your browser\nAfter entering the password on the below like screen, either press enter or click on ‚Äòok‚Äô\nclick ‚Äòok‚Äô to paste the content\nAfter successfully loging in to your server, you can use your browser same as you use your ssh login.\nlogin prompt of the server\nIn this tutorial, you have learnt how to install Shellinabox on Debian server.\nAlso Read: Install SSL on Ubuntu server using Nginx, How to install SSL on CentOS-7.3 with httpd server","overview#Overview:":"Shellinabox is included by default on many Linux distributions, such as Debian, Ubuntu, and Linux Mint, through their default repositories.","prerequisites#Prerequisites:":" Super user( root) or any normal user with SUDO privileges. apt repositories enabled and available to install packages. "},"title":"How to install Shellinabox on Debian server"},"/utho-docs/docs/linux/how-to-install-shellinabox-on-fedora/":{"data":{"":"\nBefore installing Shellinabox on Fedora server to use the browser to access the Fedora terminal, we need to figure out how to do that. Markus Gutschke made Shell in a Box, and its name is pronounced ‚Äúshellin‚Äô box.‚Äù It is a web-based programme that acts like a terminal. It gives you a web terminal emulator that lets you access and control your Linux Server SSH Shell remotely from any browser that supports AJAX/JavaScript and CSS, without the need for additional browser plugins like FireSSH. It has a web server built in that works as a web-based SSH client on a certain port. This web server also works as an SSH client that runs on the web.\nIn this article, we‚Äôll talk about how to install Shellinabox and connect to a remote SSH terminal using any modern web browser. Any system can use Shellinabox. When you are behind a firewall and can only let HTTP(s) traffic through, Web-based SSH is a very useful tool.","1--installation#1- Installation":"To install the Shellinabox on your Fedora server, first install the epel-repolistory on it. To do so, use the below command.\ndnf install epel-release -y And now you can install the shellinabox on your machine.\ndnf install shellinabox -y systemctl start shellinaboxd systemctl enable shellinaboxd ","configure-shellinabox#Configure Shellinabox":"ou will find the configuration file has a different name and directory in which it is present as you go from one Linux flavor to another. In Fedora the configuration file can be found as- /etc/sysconfig/shellinaboxd\nvi /etc/sysconfig/shellinaboxd content of /etc/sysconfig/shellinaboxd\nMake sure to have the SSH option in the OPTS entry in the configuration file.","overview#Overview:":"Shellinabox is included by default on many Linux distributions, such as Debian, Ubuntu, and Linux Mint, through their default repositories. But in Fedora we need to install extra repository for enterprises Linux.","points-to-remember#Points to remember:":" For security reasons, we should change the default port number of shellinabox service. Make sure to enable the shellinabox port in your firewall( if you are using one) ","prerequisites#Prerequisites:":" Super user( root) or any normal user with SUDO privileges. DNF or package repositories enabled and available to install packages. ","verify-the-port-and-running-status-of-shellinabox#Verify the port and running status of Shellinabox":" netstat -tunlp | grep shellinaboxd tcp 0 0 0.0.0.0:4200 0.0.0.0:* LISTEN 1908/shellinaboxd Now, open your web browser and go to https://Your-IP-Address:4200. You should be able to see an SSH terminal that runs on the web. When you log in with your username and password, you should see your shell prompt.\nBrowser‚Äôs first page\nNow, when you do just as mentioned in above step, you will be asked to go advanced and proceed to your serverip. After proceeding further, you will see a blank screen asking for the login password.\nEnter here, your login username and password to continue using your terminal in browser.\nsecurity page of browser\nAfter this, you will see a blank white screen prompt which will ask you the username similar to which you see on your ssh prompt. Here, enter the username, from which you want to login. After username it will ask you the password.\nFor entering the password, you can enter it either by typing manually or just right click on your browser screen and select ‚ÄòPaste from browser‚Äô and paste the password on the next windows.\nTo paste password on your browser\nAfter entering the password on the below like screen, either press enter or click on ‚Äòok‚Äô\nclick ‚Äòok‚Äô to paste the content\nAfter successfully loging in to your server, you can use your browser same as you use your ssh login.\nlogin prompt of the server\nAnd that is it, you have installed the Shellinabox on your Fedora server"},"title":"How to install Shellinabox on Fedora"},"/utho-docs/docs/linux/how-to-install-snap-on-almalinux/":{"data":{"":" How to install snap on AlmaLinux\nSnaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\ndnf repositories configured with CentOS server.","steps-to-install-snap-on-centos-server#Steps to install Snap on CentOS server":"Step 1: Before installing the Snap on your server, you need to install the extra packages for enterprises linux( EPEL) repsitories\ndnf install epel-release Installing EPEL repo\nStep 2: Install the Snap by executing the below command\ndnf install snapd Install the Snap on Centos\nStep 3: Start and enable the snapd socket to start working with snapd\nsystemctl enable --now snapd.socket Output- Created symlink from /etc/systemd/system/sockets.target.wants/snapd.socket to /usr/lib/systemd/system/snapd.socket. Step 4: Now, you must enter the following to establish a symbolic link between /var/lib/snapd/snap and /snap in order to enable support for traditional snaps: ln -s /var/lib/snapd/snap /snap Step 5: Now, either set the PATH varialbe using the below command or restart another terminal\necho export PATH=$PATH:/snap/bin \u003e\u003e ~/.bashrc export .bashrc Step 6: Check the snapd version.\nsnap version Version of Snap\nAnd this is how you will install the Snap on AlmaLinux server"},"title":"How to install Snap on AlmaLinux"},"/utho-docs/docs/linux/how-to-install-snap-on-debian-10/":{"data":{"":" How to install SNAP on Debian 10\nIn this tutorial, we will learn how to install Snap on Debian 10 server. Snaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nInternet enabled server with updated security patches. You can have you fully featured Debian server on Utho Cloud.","steps-to-install-snap-on-debian#Steps to install Snap on Debian":"Step 1: Update the APT repository to install the latest packages.\napt update Step 2: Install the snap package using the below command.\napt install snapd Step 3: Now, verify the installation of your snap package.\nsnap version Snap version\nAnd, this is how you have learnt how to install snap on Debian server"},"title":"How to install SNAP on Debian 10"},"/utho-docs/docs/linux/how-to-install-snap-on-fedora/":{"data":{"":" How to install Snap on Fedora\nSnaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\ndnf repositories configured with Fedora server.","steps-to-install-snap-on-fedora-server#Steps to install Snap on Fedora server":"Step 1: Install the Snap by executing the below command\ndnf install snapd Install the Snap on Fedora\nStep 2: Start and enable the snapd socket to start working with snapd\nsystemctl enable --now snapd.socket Output- Created symlink from /etc/systemd/system/sockets.target.wants/snapd.socket to /usr/lib/systemd/system/snapd.socket. Step 3: Now, you must enter the following to establish a symbolic link between /var/lib/snapd/snap and /snap in order to enable support for traditional snaps: ln -s /var/lib/snapd/snap /snap Step 4: Now, either set the PATH varialbe using the below command or restart another terminal\necho export PATH=$PATH:/snap/bin \u003e\u003e ~/.bashrc export .bashrc Step 5: Check the snapd version.\nsnap version Version of Snap\nAnd this is how you will install the Snap on Fedora server"},"title":"How to install Snap on Fedora"},"/utho-docs/docs/linux/how-to-install-snap-on-rockylinux/":{"data":{"":" How to install Snap on RockyLinux\nSnaps are programmes that have all of their dependencies bundled and ready to run on all widely used Linux distributions from a single build. They automatically update and gracefully roll back.\nThe Snap Store, an app store with millions of users, allows users to find and download Snaps.\nSnaps are safe because they are contained and sandboxed to prevent system compromise. They operate at various confinement levels, which refer to how isolated they are from one another and the base system. More significantly, each snap has a carefully chosen interface that the designer carefully considered depending on the requirements of the snap in order to enable access to particular system resources outside of its confinement, such as network access, desktop access, and more.","prerequesites#Prerequesites":" Any normal user with SUDO privileges or Super user\nyum repositories configured with RockyLinux server.","steps-to-install-snap-on-rockylinux-server#Steps to install Snap on RockyLinux server":"Step 1: Before installing the Snap on your server, you need to install the extra packages for enterprises linux( EPEL) repsitories\ndnf install epel-release dnf upgrade Installing EPEL repo\nStep 2: Install the Snap by executing the below command\nyum install snapd Install the Snap on RockyLinux\nStep 3: Start and enable the snapd socket to start working with snapd\nsystemctl enable --now snapd.socket Output- Created symlink from /etc/systemd/system/sockets.target.wants/snapd.socket to /usr/lib/systemd/system/snapd.socket. Step 4: Now, you must enter the following to establish a symbolic link between /var/lib/snapd/snap and /snap in order to enable support for traditional snaps: ln -s /var/lib/snapd/snap /snap Step 5: Now, either set the PATH varialbe using the below command or restart another terminal\necho export PATH=$PATH:/snap/bin \u003e\u003e ~/.bashrc export .bashrc Step 6: Check the snapd version.\nsnap version Version of Snap\nAnd this is how you will install the Snap on RockyLinux server"},"title":"How to install Snap on RockyLinux"},"/utho-docs/docs/linux/how-to-install-spack-on-ubuntu-20-04/":{"data":{"":"","1-install-dependencies#1. Install Dependencies":"","2-clone-spack-repository#2. Clone Spack Repository":"","3-add-shell-support#3. Add Shell Support":"","4-clear-environment#4. Clear Environment":"","5-test-spack#5. Test Spack":"\nIntroduction In this article, you will learn How to Install Spack on Ubuntu 20.04.\nSpack is a management tool for packages that was developed to accommodate different versions and configurations of software across a broad number of operating systems, hardware platforms, and other types of environments. It was developed for huge supercomputing facilities, the kind of places where a large number of users and application teams share common installs of software on clusters with unusual architectures and make use of libraries that do not have a standard application binary interface (ABI). Because installing a new version of Spack does not disrupt previously established installations, many configurations are able to live on the same computer without causing any problems.\nFirst and foremost, Spack is easy to understand. It provides a straightforward syntax that makes it possible for users to concisely declare version numbers and configuration parameters. Spack is also easy to use for package writers; package files are written in pure Python, and specs enable package authors to maintain a single file for multiple distinct versions of the same product. Spack‚Äôs other benefit is that it is efficient.\n1. Install Dependencies # apt update # apt install build-essential -y 2. Clone Spack Repository Make a copy of the Spack repository in /.spack/Spack (or another location of your choosing).\n# git clone https://github.com/spack/spack ~/.spack/Spack 3. Add Shell Support Spack can be used anywhere by just adding it to the PATH environment variable:\n# . ~/.spack/Spack/share/spack/setup-env.sh To use the spack command every login, append that command to ~/.bash_profile:\n# echo '. ~/.spack/Spack/share/spack/setup-env.sh' \u003e\u003e ~/.bash_profile Then run¬†source ~/.bash_profile¬†or log out to make changes take effect.\n4. Clear Environment For Spack to function properly, a pristine setting is necessary, as it compiles and instals packages from their source code. Make sure you have nothing unnecessary in your PATH.\n# echo $PATH /home/cus/.spack/Spack/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 5. Test Spack Verify the Spack installation.\n# spack -V ","conclusion#Conclusion":"Hopefully, you have learned how to Install Spack on Ubuntu 20.04.","introduction#Introduction":"","next-steps#Next Steps":"Spack allows you to easily install numerous robust scientific programmes, such as pngwriter.\n# spack install pngwriter # spack load pngwriter "},"title":"How to Install Spack on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-squid-proxy-on-ubuntu-server/":{"data":{"":"","configuring-the-squid-proxy-server-to-allow-to-connect-any-device#\u003cstrong\u003eConfiguring the Squid Proxy Server\u003c/strong\u003e \u003cstrong\u003eto allow to connect any device\u003c/strong\u003e":" How to install squid proxy on Ubuntu server\nDescription In this tutorial, we will learn how to install Squid proxy on Ubuntu server. A proxy server is a computer system or router that lets users connect to the internet. So, it makes it harder for hackers to get into a private network. It‚Äôs a server.\nEvery computer that is connected to the network has an IP (Internet Protocol) address that identifies it as a unique device. In the same way, the proxy server is a network computer with its own IP address. But sometimes we want to get into restricted websites or servers, but we don‚Äôt want to show who we are (IP address). When this happens, the proxy server comes into play. The same thing can be done with the proxy server. It has different levels of functionality, security, and privacy depending on how it is used, what the company needs, or what its policies are. In this section, we‚Äôll talk about what a proxy server is, its types, benefits, why you need one, and how it works.\nYou can make web requests from a different IP address than your own by using a proxy server. You can also use a proxy server to find out how the web is served in different places or to avoid being watched or having your web traffic slowed down.\nSquid is a well-known, stable, and open-source HTTP proxy. In this tutorial, you‚Äôll learn how to install and set up Squid on an Ubuntu 20.04 server so that it can act as an HTTP proxy.\nPrerequisites: Super User( root user) or any normal user with SUDO priviliges. apt-get package installer A text editor, such as vi/ vim or nano Install squid proxy Step 1: First let‚Äôs update the package list of apt\n# apt update Step 2: Install the squid daemon\n# apt install squid Installation of squid\nConfiguring the Squid Proxy Server to allow to connect any device By default, Squid doesn‚Äôt let any clients outside of this server connect to it. You‚Äôll need to make some changes to its configuration file, which is located at /etc/squid/squid.conf, in order to make that work.\n# vi /etc/squid/squid.conf length of config file\nSquid configuration file may have more than 9000 numbers of lines and also contains large amount of options. But most of them are already commented and therefore not in use.\nTo find the port on which squid works, find the keyword- http_port in configuration file. In vi or vim, you can search the file by pressing forward slash( / ) and then the keyword.\nhttp_port keyword\nNow search for the ‚Äòhttp_access deny all‚Äô keyword to allow or check which IP pool or Ips are allowed to connect to the squid.\ncheck allowed ip\nNow slightly above the keyword- http_access deny all, you will find another keyword- ‚Äòhttp_access allow localhost‚Äô. This keyword is used only to connect the squid server by localhost.\nkeywork to connect only by localhost\nHere you can specify either allow to all or to any particular ip or the range of ip pool.\nallow a specific ip\nYou can also create the rules just we do in making rules of access control list. You just need to create a rule and place them accordingly.\nRule for ACL\nNow after making all the changes, please do not forget to restart the squid server‚Äôs service.\n# systemctl restart squid Congratulations!!! Your Squid Proxy Server successfully installed.\nThank You","description#Description":"","install-squid-proxy#Install squid proxy":"","prerequisites#Prerequisites:":""},"title":"How to install squid proxy on Ubuntu server"},"/utho-docs/docs/linux/how-to-install-squid-proxy-server-on-centos/":{"data":{"":"","#":"\nPrerequisites: CentOS operating system Access to a terminal window/command-line (Ctrl-Alt-T) A CentOS user with root or sudo priviledges The¬†yum package installer, included by default A text editor, such as¬†vim Let‚Äôs begin the installation.\nLogin to your server via Putty.\nStep 1: Refresh CentOS Software Repositories\nsudo yum -y update Step 2: Install Squid Package on CentOS\nyum -y install squid Now start Squid by entering the following command:\nsystemctl start squid To set up an automatic start at boot:\nsystemctl enable squid Review the status of the service, use:\nsystemctl status squid In the example below, we see that the state is ‚ÄòActive.‚Äô\nConfiguring the Squid Proxy Server The Squid configuration file is found at¬†/etc/squid/squid.conf.\n1. Open the file in your preferred text editor (vim¬†was used in this example}:\nsudo vi /etc/squid/squid.conf 2. Navigate to find the¬†http_port option. Typically, this is set to listen on¬†Port 3218. This port usually carries TCP traffic. If your system is configured for traffic on another port, change it here:\nYou may also set the proxy mode to¬†transparent¬†if you‚Äôd like to prevent Squid from modifying your requests and responses.\nChange it as follows:\nhttp_port 1234 transparent 3. Navigate to the¬†http_acacess deny all¬†option.\nIt is currently configured to block all HTTP traffic, and no web traffic is allowed as shown below.\nChange this to the following:\nhttp_access allow all 4. Restart the Squid service by entering:\nsudo systemctl restart squid Squid Proxy Server successfully installed.\nThank You!"},"title":"How to Install Squid Proxy Server on CentOS"},"/utho-docs/docs/linux/how-to-install-ssl-through-cpanel/":{"data":{"":"\nSSL (Secured Sockets Layer) is used to establish¬†authenticated and encrypted links between network Computers\nNow for¬†Installing SSL is Server through Cpanel First of allLogin¬†in cpanel by¬†SERVER_IP:2083¬†Now login in cpanel and under the Security option select¬†SSL/TLS option\nIn the right side of window click on CSR option\nAfter that fill required details and click on Generate option\nNow share this file with SSL team they will share one file.\nOpen the file manager option in cpanel , Now Paste that file under this directory\npublc_html/.well-known/pki-validation/\nNote:- If hidden files are Not showing Please follw this step\nclick on settings option then click on Show hidden files\nNow Again Open SSL/TLS option under security and select CERTIFICATES(CRT) option\nNow Click on choose file option then upload the Certificate and then click on save Certificate.\nSSL is installed on server , Changes will reflect on server with In 2 hrs\nThank you :)"},"title":"How to install SSL through Cpanel ."},"/utho-docs/docs/linux/how-to-install-streamlit-on-ubuntu-20-04/":{"data":{"":"","1-keep-the-server-updated#1. Keep the server updated":"","2-install-python-pip#2. Install python pip":"","3-install-streamlit#3. Install Streamlit":"","4-deploy-streamlit-app#4. Deploy Streamlit app":"","5-use-screen#5. Use Screen":"\nIntroduction If you need to create and distribute a data app quickly, Streamlit is your best bet. With Streamlit, you can quickly and easily create shared web apps from data scripts. Everything is written in Python. No charge at all. You don‚Äôt need to have experience in the front end to use this tool. It‚Äôs a free, public-domain app framework for Machine Learning and Data Science groups. Build stunning data applications in hours, not weeks. Follow these 5 steps to Install Streamlit on Ubuntu 20.04.\n1. Keep the server updated Using the following command, please update the server:\n# apt upgrade -y 2. Install python pip The following command will established the Python package manager, pip:\n# apt install python3-pip -y 3. Install Streamlit Set up Streamlit with the python pip package manager.\n# pip install streamlit 4. Deploy Streamlit app One python file must be created using the following command in order to deploy the Streamlit application:\n# vi app.py To the app.py file, paste the following:\nimport streamlit as st st.text_input(\"Your name\", key=\"name\") You can access the value at any point with:\nUse the escape:wq key combination to save your work and close the file.\nYou can now deploy the application on port 80 by typing the following command:\n# streamlit run app.py --server.port 80 The Streamlit app can be accessed via a web browser by entering the server‚Äôs IP address.\n5. Use Screen When you log out of an SSH session, Streamlit will automatically end the session. We need to utilise a screen tool to make it permanent. You can disconnect from the server using a different terminal after you‚Äôve established a connection in screen. Specifically, a persistence session will be formed.\n# screen -S streamlit Deploy Streamlit app:\n# streamlit run app.py --server.port 80 Now disconnect from the server and open a new terminal; from there, use the following command to display information about your current screen experience.\n# screen -list We finished the setup procedure and deployed our first app without any problems. I hope you understand now how to install Streamlit on Ubuntu 20.04.\nSee how How to Setup Flatpak on Ubuntu 20.04.\nThank You üôÇ","introduction#Introduction":""},"title":"How to Install Streamlit on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-streamlit-on-ubuntu-22-04/":{"data":{"":"","introduction#Introduction":"In this article you will learn how to install Streamlit on Ubuntu 22.04.\nIf you need to create and distribute a data app quickly, Streamlit is your best bet. With¬†Streamlit, you can quickly and easily create shared web apps from data scripts. Everything is written in Python. No charge at all. You don‚Äôt need to have experience in the front end to use this tool. It‚Äôs a free, public-domain app framework for Machine Learning and Data Science groups. Build stunning data applications in hours, not weeks. Follow these 5 steps to Install Streamlit on Ubuntu 20.04.\nStep 1. Keep the server updated Using the following command, please update the server:\n# apt upgrade -y Step 2. Install python pip The following command will established the Python package manager, pip:\n# apt install python3-pip -y Step 3. Install Streamlit Set up Streamlit with the python pip package manager.\n# pip install streamlit Step 4. Deploy Streamlit app One python file must be created using the following command in order to deploy the Streamlit application:\n# vi app.py To the app.py file, paste the following:\nimport streamlit as st st.text_input(\"Your name\", key=\"name\") You can access the value at any point with:\nUse the escape:wq key combination to save your work and close the file.\nYou can now deploy the application on port 80 by typing the following command:\n# streamlit run app.py --server.port 80 The Streamlit app can be accessed via a web browser by entering the server‚Äôs IP address.\nStep 5. Use Screen When you log out of an SSH session, Streamlit will automatically end the session. We need to utilise a screen tool to make it permanent. You can disconnect from the server using a different terminal after you‚Äôve established a connection in screen. Specifically, a persistence session will be formed.\n# streamlit run app.py --server.port 80 Deploy Streamlit app:\n# streamlit run app.py --server.port 80 Now disconnect from the server and open a new terminal; from there, use the following command to display information about your current screen experience.\n# screen -list We finished the setup procedure and deployed our first app without any problems. I hope you understand now how to install Streamlit on Ubuntu 22.04.\nRead Also:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ"},"title":"How to install Streamlit on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-tcpping-on-almalinux/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install tcpping on AlmaLinux.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install tcpping on AlmaLinux.\nThe TCPPING protocol starts with a group of known members and then¬†pings¬†them one by one in order to discover new ones.\nThere are situations that may arise in which an external server has blocked ICMP traffic headed in its direction. Because of this, you are unable to ping the host in order to verify that it is online. You can check the presence of the host in this kind of scenario by either using telnet to connect to a port that is already known to you or by attempting to make a TCP connection to the host.\nWhen you attempt to set up a TCP connection to the remote host, the remote host will either accept the connection or refuse it by sending a RST package. This happens whenever you try to create a connection. Even with only this piece of evidence, it is possible to make a positive identification of the host.\nStep 1: Update your machine.\n# dnf update -y Step 2: Run the following command to install tcptraceroute:\n# dnf install tcptraceroute Step 3: After that, get tcpping from the internet and install it.\n# cd /usr/bin # wget http://www.vdberg.org/~richard/tcpping # chmod 755 tcpping Step 4: Executing tcping in the manner described below is all that is required to test the latency of a network.\n# tcpping google.com # tcpping other\\_server\\_ip "},"title":"How to install tcpping on AlmaLinux"},"/utho-docs/docs/linux/how-to-install-tcpping-on-centos/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install tcpping on CentOS.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install tcpping on CentOS.\nThe TCPPING protocol starts with a group of known members and then¬†pings¬†them one by one in order to discover new ones.\nThere are situations that may arise in which an external server has blocked ICMP traffic headed in its direction. Because of this, you are unable to ping the host in order to verify that it is online. You can check the presence of the host in this kind of scenario by either using telnet to connect to a port that is already known to you or by attempting to make a TCP connection to the host.\nWhen you attempt to set up a TCP connection to the remote host, the remote host will either accept the connection or refuse it by sending a RST package. This happens whenever you try to create a connection. Even with only this piece of evidence, it is possible to make a positive identification of the host.\nStep 1: Update your machine.\n# yum update -y Step 2: Run the following command to install tcptraceroute:\n# yum install tcptraceroute Step 3: After that, get tcpping from the internet and install it.\n# cd /usr/bin # wget http://www.vdberg.org/~richard/tcpping # chmod 755 tcpping Step 4: Executing tcping in the manner described below is all that is required to test the latency of a network.\n# tcpping google.com # tcpping other\\_server\\_ip "},"title":"How to install tcpping on CentOS"},"/utho-docs/docs/linux/how-to-install-tcpping-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install tcpping on Debian.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install tcpping on Debian.\nThe TCPPING protocol starts with a group of known members and then¬†pings¬†them one by one in order to discover new ones.\nThere are situations that may arise in which an external server has blocked ICMP traffic headed in its direction. Because of this, you are unable to ping the host in order to verify that it is online. You can check the presence of the host in this kind of scenario by either using telnet to connect to a port that is already known to you or by attempting to make a TCP connection to the host.\nWhen you attempt to set up a TCP connection to the remote host, the remote host will either accept the connection or refuse it by sending a RST package. This happens whenever you try to create a connection. Even with only this piece of evidence, it is possible to make a positive identification of the host.\nStep 1: Update your machine.\n# apt update -y Step 2: Run the following command to install tcptraceroute:\n# apt-get install tcptraceroute Step 3: After that, get tcpping from the internet and install it.\n# cd /usr/bin # wget http://www.vdberg.org/~richard/tcpping # chmod 755 tcpping Step 4: Executing tcping in the manner described below is all that is required to test the latency of a network.\n# tcpping google.com # tcpping other\\_server\\_ip "},"title":"How to install tcpping on Debian"},"/utho-docs/docs/linux/how-to-install-tcpping-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install tcpping on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install tcpping on Fedora.\nThe TCPPING protocol starts with a group of known members and then¬†pings¬†them one by one in order to discover new ones.\nThere are situations that may arise in which an external server has blocked ICMP traffic headed in its direction. Because of this, you are unable to ping the host in order to verify that it is online. You can check the presence of the host in this kind of scenario by either using telnet to connect to a port that is already known to you or by attempting to make a TCP connection to the host.\nWhen you attempt to set up a TCP connection to the remote host, the remote host will either accept the connection or refuse it by sending a RST package. This happens whenever you try to create a connection. Even with only this piece of evidence, it is possible to make a positive identification of the host.\nStep 1: Update your machine.\n# dnf update -y Step 2: Run the following command to install tcptraceroute:\n# dnf install tcptraceroute Step 3: After that, get tcpping from the internet and install it.\n# cd /usr/bin # wget http://www.vdberg.org/~richard/tcpping # chmod 755 tcpping Step 4: Executing tcping in the manner described below is all that is required to test the latency of a network.\n# tcpping google.com # tcpping other\\_server\\_ip "},"title":"How to install tcpping on Fedora"},"/utho-docs/docs/linux/how-to-install-tcpping-on-ubuntu/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install tcpping on Ubuntu.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install tcpping on Ubuntu.\nThe TCPPING protocol starts with a group of known members and then pings them one by one in order to discover new ones.\nThere are situations that may arise in which an external server has blocked ICMP traffic headed in its direction. Because of this, you are unable to ping the host in order to verify that it is online. You can check the presence of the host in this kind of scenario by either using telnet to connect to a port that is already known to you or by attempting to make a TCP connection to the host.\nWhen you attempt to set up a TCP connection to the remote host, the remote host will either accept the connection or refuse it by sending a RST package. This happens whenever you try to create a connection. Even with only this piece of evidence, it is possible to make a positive identification of the host.\nStep 1: Update your machine.\n# apt update -y Step 2: Run the following command to install tcptraceroute:\n# apt-get install tcptraceroute Step 3: After that, get tcpping from the internet and install it.\n# cd /usr/bin # wget http://www.vdberg.org/~richard/tcpping # chmod 755 tcpping Step 4: Executing tcping in the manner described below is all that is required to test the latency of a network.\n# tcpping google.com # tcpping other\\_server\\_ip "},"title":"How to install tcpping on Ubuntu"},"/utho-docs/docs/linux/how-to-install-the-latest-mysql-on-debian-10/":{"data":{"":"","step-1-including-the-mysql-software-repository-in-the-compilation#**Step-1 Including the MySQL Software Repository in the compilation":"","step-2--installing-mysql#\u003cstrong\u003eStep 2 ‚Äî Installing MySQL\u003c/strong\u003e":"","step-3--securing-mysql#\u003cstrong\u003eStep 3 ‚Äî Securing MySQL\u003c/strong\u003e":"","step-4--testing-mysql#\u003cstrong\u003eStep 4 ‚Äì Testing MySQL\u003c/strong\u003e":"\nIntroduction\nMySQL is a popular open-source database management system used for many applications. MySQL is part of the LAMP stack, which comprises Linux, Apache, and PHP.\nMariaDB is the default MySQL variant in Debian 10. If you need functionality only available in Oracle‚Äôs MySQL, you can install packages from a MySQL developer repository.\nIn this lesson, you‚Äôll add the MySQL repository, install MySQL, secure the install, and test that MySQL is running and responding to commands.\nPrerequisites\nYou will need one Debian 10 server with a non-root user with sudo privileges and a firewall configured before beginning this tutorial.\n**Step-1 Including the MySQL Software Repository in the compilation **\nMySQL developers provide a.deb package for configuring and installing software repositories. Once the repositories are set up, you can install software using apt.\nInstall GnuPG, an open-source OpenPGP implementation, first.\nUpdate the local package index with upstream modifications.\n#sudo apt update Installing the gnupg package comes next:\n#sudo apt install gnupg APT will install gnupg and its dependencies after you confirm the installation.\nThe MySQL.deb package will then be downloaded with wget and installed with the dpkg command.\nOpen your web browser and navigate to the MySQL download page. Locate the Download button in the lower-right corner and proceed to the next page. This page will ask you to log in or create an Oracle web account. You can skip that and go straight to the No thanks, just start my download link. Copy the link address by right-clicking it (this option may be worded differently, depending on your browser).\nYou will now download the file. On your server, navigate to a writeable directory, such as the temporary /tmp directory used in this example:\n#cd /tmp Then, use the wget programme to download the file, being sure to replace the highlighted area with the location you copied:\n#wget https://dev.mysql.com/get/mysql-apt-config_0.8.22-1_all.deb The file should now be in your current directory. List the files you want to confirm:\n#ls The file you just downloaded will be included in the output list of files, as seen in the example below, which is highlighted:\nOutput . . . mysql-apt-config_0.8.22-1_all.deb . . . You are now ready to begin installation. Run the dpkg command to install, uninstall, and inspect.deb software packages. The -i option specifies that you want to install from the specified file:\n#sudo dpkg -i mysql-apt-config* You‚Äôll be given a configuration screen during the installation where you can choose the version of MySQL you‚Äôd like to use and choose to install repositories for additional MySQL-related utilities. The defaults will only add the repository details for the most recent stable version of MySQL. Use the down arrow to get to the Ok menu choice and press ENTER to select this for our purposes.\nThe repository will now be fully added by the package. To make the new software packages available, you must update your apt package cache:\n#sudo apt update You‚Äôre ready to install the MySQL server software now that you‚Äôve added the MySQL repositories. If you need to update the configuration of these repositories, run sudo dpkg-reconfigure mysql-apt-config, then sudo apt-get update to refresh your package cache.\nStep 2 ‚Äî Installing MySQL You may now use apt to install the most recent MySQL server package after adding the repository and after your package cache has been recently updated:\n#sudo apt install mysql-server apt will scan all available mysql-server packages and determine that the package provided by MySQL is the most recent and best candidate. It will then calculate package dependencies and request your approval before proceeding with the installation. Enter y followed by ENTER. The software will be downloaded.\nDuring the configuration phase of the installation, you will be prompted to enter a root password. To proceed, enter and confirm a secure password. Following that, you will be prompted to select a default authentication plugin. To understand the options, read the display. If you are unsure, select Use Strong Password Encryption.\nMySQL should now be installed and operational. Check with systemctl:\n#sudo systemctl status mysql Output ‚óè mysql.service - MySQL Community Server Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: en Active: active (running) since Thu 2022-02-24 18:59:22 UTC; 23min ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 3722 (mysqld) Status: \"Server is operational\" Tasks: 38 (limit: 4915) Memory: 371.7M CGroup: /system.slice/mysql.service ‚îî‚îÄ3722 /usr/sbin/mysqld Feb 24 18:59:21 sql-debian systemd[1]: Starting MySQL Community Server... Feb 24 18:59:22 sql-debian systemd[1]: Started MySQL Community Server. MySQL is installed and operating as indicated by the Active: active (running) line. You‚Äôll add a little additional security to the installation in the following step.\nStep 3 ‚Äî Securing MySQL For some security-related updates on your fresh install, MySQL includes a command. Run it right away:\n#mysql_secure_installation This will ask for your MySQL root password. Enter it. Now answer yes/no questions. Recap:\nFirst, you‚Äôre asked about the validate password plugin, which enforces password restrictions for MySQL users. Enable this based on your security needs. Enter y to enable or skip it. If enabled, you‚Äôll be asked to choose a password validation level from 0‚Äì2. Press ENTER to continue.\nYou‚Äôll be asked to change the root password. Since you just installed MySQL, you can skip this. Continue without changing the password.\nThe rest are true. You must remove anonymous MySQL users, disable remote root logins, delete the test database, and reload privilege tables to verify the changes take effect. Good concepts. Enter each y.\nAll prompts will end the script. Now MySQL is secure. Next, launch a client that connects to the server and returns data.\nStep 4 ‚Äì Testing MySQL The MySQL administrative client for the command line is called mysqladmin. It will allow you to connect to the server and output details about the version and status. Mysqladmin is told to log in as the MySQL root user by the -u root, -p informs the client to request a password, and version is the actual command you want to execute:\n#mysqladmin -u root -p version The output will show the MySQL server‚Äôs version, uptime, and other status information as in the examples below:\nOutput mysqladmin Ver 8.0.28 for Linux on x86_64 (MySQL Community Server - GPL) Copyright (c) 2000, 2022, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Server version 8.0.28 Protocol version 10 Connection Localhost via UNIX socket UNIX socket /var/run/mysqld/mysqld.sock Uptime: 25 min 31 sec Threads: 2 Questions: 20 Slow queries: 0 Opens: 143 Flush tables: 3 Open tables: 62 Queries per second avg: 0.013 This output confirms that you installed and secured the most recent MySQL server successfully."},"title":"How To Install the Latest MySQL on Debian 10"},"/utho-docs/docs/linux/how-to-install-the-linux-apache-mariadb-php-lamp-stack-on-centos-7/":{"data":{"":"\nLAMP, which runs on Linux as the operating system, is an open-source web-development platform using Apache as the web-based server and PHP as an object-oriented scripting language. (Instead of PHP, Perl or Python are sometimes used.)\nPrerequisites\nBefore you begin with this guide, you should have root user account privileges set up on your server.\nLog in to the server with (SSH).\nUpdate the server packages using the command yum update -y .\nSTEP 1-Installation of Apache Server\nThe Apache web server is the most popular web server in the world, making hosting websites a great default option.\nWe can easily install Apache with the package manager of CentOS, yum. Use the below command to install the Apache server .\n# sudo yum install httpd When prompted, enter Y to confirm the Apache installation. Once the installation is complete, start your Apache server with this command:\n# sudo systemctl start httpd You can test if your server is running by entering your public IP address or your domain name in your web browser.\nYou can enable Apache to start on boot with:\n# sudo systemctl enable httpd.service Step 2 ‚Äî Installing MySQL (MariaDB)\nWith your web server up and running, you can install MariaDB. It will organise and provide access to databases where your site can store information.\nTo install the MariaDB software package, run:\n# sudo yum install mariadb-server When the installation is completed, start MariaDB:\n# sudo systemctl start mariadb You can enable MariaDB to start on boot with this command:\n# sudo systemctl enable mariadb.service To improve the security of your database server, it‚Äôs recommended that you run a security script that comes pre-installed with MariaDB. This script will remove some insecure default settings and lock down access to your database system.\nStart the interactive script by running:\n# sudo mysql_secure_installation This script will take you through a series of prompts where you can make some changes to your MariaDB setup. The first prompt will ask you to enter the current database root password. This is not to be confused with the system root user. The database root user is an administrative user with full privileges over the database system. Because you just installed MariaDB and haven‚Äôt made any configuration changes, this password will be blank. Press ENTER at the prompt\nThe next prompt asks you whether you‚Äôd like to set up a database root password. Type N and then press ENTER.\nFrom there, you can press Y, and then ENTER, to accept the defaults for all the subsequent questions. This will remove anonymous users and the test database, disable remote root login, and load these new rules so that the server immediately respects the changes you have made.\nWhen you‚Äôre finished, log in to the MariaDB console by entering:\n# sudo mysql -u root -p This connects you to the MariaDB server as the administrative database user root:\nFor increased security, it‚Äôs best to have dedicated user accounts with less expansive privileges set up for every database. This is especially important if you plan on having multiple databases hosted on your server.\nTo demonstrate such a setup, create a database named example_database and a user named example_user. You can replace these names with different values.\nRun the following command from your MariaDB console to create a new database:\n# MariaDB [(none)]\u003e CREATE DATABASE example_database; You can create a new user and grant them full privileges on the custom database you‚Äôve just created. The following command defines this user‚Äôs password as ‚Äúpassword,‚Äù but you should replace this value with a secure password:\n# MariaDB [(none)]\u003eGRANT ALL ON example_database.* TO 'example_user'@'localhost' IDENTIFIED BY 'password' WITH GRANT OPTION; This command gives the example_user user full privileges over the example_database database, while preventing this user from creating or modifying other databases on your server.\nUse the FLUSH statement to reload and save the privileges you just granted to example_user:\n# MariaDB [(none)]\u003e FLUSH PRIVILEGES; Exit the MariaDB shell:\n# MariaDB [(none)]\u003e exit You can test if the new user has the proper permissions by logging in to the MariaDB console again, but this time using the example_user credentials you created above:\n# mysql -u example_user -p Note the -p flag in this command, which will prompt you for the password you chose when creating the example_user user. After logging in to the MariaDB console, confirm that you have access to the example_database database with this statement:\n# MariaDB [(none)]\u003e SHOW DATABASES; Your example_database should be listed in the output:\nTo exit the MariaDB shell, type:\n# MariaDB [(none)]\u003e exit Your database system is set up and you can move on to installing PHP.\nStep 3 ‚Äî Installing PHP\n3.1 Add EPEL and REMI Repository\n# sudo yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # sudo yum -y install https://rpms.remirepo.net/enterprise/remi-release-7.rpm 3.2 Install PHP 7.4 on CentOS 7\nWe can now enable PHP 7.4 Remi repository and install PHP 7.4 on CentOS 7.\n# sudo yum -y install yum-utils # sudo yum-config-manager --enable remi-php74 Install PHP 7.4 on CentOS 7.\n# sudo yum update # sudo yum install php php-cli Accept installation prompt.\nUse the next command to install additional packages:\n# sudo yum install php php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json The current PHP version should be 7.4.![]\nThank You üôÇ"},"title":"New"},"/utho-docs/docs/linux/how-to-install-the-opengl-library-on-ubuntu-20-04/":{"data":{"":"\nDescription\nIn this article, we are going to study How to Install the OpenGL Library on Ubuntu 20.04 OpenGL is a free, cross-platform API tool for making 2D and 3D graphics that is open source. This API library is made to talk to the graphics processing unit (GPU) so that drawing can be done faster by the hardware. There are many OpenGL tools that can be used for different kinds of graphics. Here are the steps for installing the freeglut OpenGL library on computers that use Ubuntu 20.04 LTS. The OpenGL Utility Toolkit (GLUT) library can also be used with the Freeglut library. Find out more on the page.Please proceed by following the steps below How to Install the OpenGL Library on Ubuntu 20.04.","step-1-update-server#Step 1: Update Server":"In the first step, use the sudo apt update command to load all the updates from the default Ubuntu repo. In the second step, use the sudo apt upgrade command to update the packages to the latest version, as shown below.\nsudo apt update \u0026\u0026 sudo apt upgrade ","step-2-install-opengl-library#Step 2: Install OpenGL Library":"In the subsequent step, you will use the command sudo apt install freeglut3-dev to install OpenGL Library from the default repository that comes with Ubuntu. This will be demonstrated further down. The program, as well as all of its dependencies, will be downloaded and installed as a result of this action.\nsudo apt install freeglut3-dev ","step-3-verify-installation#Step 3: Verify Installation":"After the installation has been completed successfully, you can check the installed file path by using the dpkg -L freeglut3-dev command, as is demonstrated below.\ndpkg -L freeglut3-dev ","step-4-uninstall-opengl-library#Step 4: Uninstall OpenGL Library":"After you have finished making use of the freeglut library, you have the option of removing it from your computer by using the command sudo apt delete freeglut3-dev, which is displayed below.\nImportant:- Caution is advised while performing the following command, as the removal of this package may, on occasion, have an effect on other applications that are now active.\nsudo apt remove freeglut3-dev I really hope that you have taken everything into careful consideration to How to Install the OpenGL Library on Ubuntu 20.04\nMust Read :- https://utho.com/docs/tutorial/how-to-install-wine-on-ubuntu-20-04/"},"title":"How to Install the OpenGL Library on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-tinycp-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install TinyCP on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-tinycp#Install TinyCp":"On your Ubuntu virtual machine, you will need to download and install the ‚Äúgnupg‚Äù programme as well as the ‚Äúca-certificates‚Äù package. To accomplish that, use these commands.\n# apt install apt-transport-https dirmngr gnupg ca-certificates The next step is to include the keys from TinyCP.\n# apt-key adv --fetch-keys http://repos.tinycp.com/debian/conf/gpg.key # echo \"deb http://repos.tinycp.com/debian all main\" | tee /etc/apt/sources.list.d/tinycp.list You will now be updating your repositories in the following step.\n# apt-get update Now, install TinyCP.\n# apt-get install tinycp You should be able to see this on your screen once the installation process is finished. You will be able to access the TinyCP webpage by using the URL that has been provided for you[http://Server_IP:65118]. Your login credentials are going to be the username and the password.","introduction#Introduction":"In this article, you will learn how to install TinyCP on Debian 12.\nIt‚Äôs likely that you‚Äôre already familiar with several control panel¬†programmes if you use Linux. These kind of programmes give you a front end through which you may control and administer your system. You may control a variety of settings and configurations by using these applications, just like you do with any other control panel that is integrated right in.\nTinyCP¬†is one of the applications that can be used as control panels that are available to you. It is a simple control panel that operates in a web-based environment and may be used to carry out a variety of administrative tasks. You may manage the software packages installed on your system with TinyCP, operate a variety of online apps, set up servers for file sharing, handle emails and databases, and do a lot more besides.\nTinyCP is a lightweight control panel that offers a wide range of capabilities on Linux-based systems.TinyCP provides users with access to a wide variety of features. Nevertheless, not all of them will be installed when you initially use TinyCP. It is up to you to decide whatever programmes you wish to put on your machine and use. These features include the following:\nDomain Management\nMailboxes\nDatabases\nFTP\nSamba\nFirewall\nVPN\nGIT\nSVN","testing#Testing":"In order to access TinyCP, as was previously said, input the URL into the browser of your choice. You will be taken to a page like that in the future.\nAfter you have entered your username and password, you will be brought to the screen that may be found below. This is the TinyCP app."},"title":"How to Install TinyCP on Debian 12"},"/utho-docs/docs/linux/how-to-install-tinycp-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install TinyCP on Debian.\nThank You üôÇ","install-tinycp#Install TinyCp":"On your Ubuntu virtual machine, you will need to download and install the ‚Äúgnupg‚Äù programme as well as the ‚Äúca-certificates‚Äù package. To accomplish that, use these commands.\n# apt install apt-transport-https dirmngr gnupg ca-certificates The next step is to include the keys from TinyCP.\n# apt-key adv --fetch-keys http://repos.tinycp.com/debian/conf/gpg.key # echo \"deb http://repos.tinycp.com/debian all main\" | tee /etc/apt/sources.list.d/tinycp.list You will now be updating your repositories in the following step.\n# apt-get update Now, install TinyCP.\n# apt-get install tinycp You should be able to see this on your screen once the installation process is finished. You will be able to access the TinyCP webpage by using the URL that has been provided for you[http://Server_IP:65118]. Your login credentials are going to be the username and the password.","introduction#Introduction":"In this article, you will learn how to install TinyCP on Debian.\nIt‚Äôs likely that you‚Äôre already familiar with several control panel programmes if you use Linux. These kind of programmes give you a front end through which you may control and administer your system. You may control a variety of settings and configurations by using these applications, just like you do with any other control panel that is integrated right in.\nTinyCP is one of the applications that can be used as control panels that are available to you. It is a simple control panel that operates in a web-based environment and may be used to carry out a variety of administrative tasks. You may manage the software packages installed on your system with TinyCP, operate a variety of online apps, set up servers for file sharing, handle emails and databases, and do a lot more besides.\nTinyCP is a lightweight control panel that offers a wide range of capabilities on Linux-based systems.TinyCP provides users with access to a wide variety of features. Nevertheless, not all of them will be installed when you initially use TinyCP. It is up to you to decide whatever programmes you wish to put on your machine and use. These features include the following:\nDomain Management Mailboxes Databases FTP Samba Firewall VPN GIT SVN ","testing#Testing":"In order to access TinyCP, as was previously said, input the URL into the browser of your choice. You will be taken to a page like that in the future.\nAfter you have entered your username and password, you will be brought to the screen that may be found below. This is the TinyCP app."},"title":"How to Install TinyCP on Debian"},"/utho-docs/docs/linux/how-to-install-tinycp-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install TinyCP on Ubuntu 20.04.\nThank You üôÇ","install-tinycp#Install TinyCp":"On your Ubuntu virtual machine, you will need to download and install the ‚Äúgnupg‚Äù programme as well as the ‚Äúca-certificates‚Äù package. To accomplish that, use these commands.\n# apt install gnupg ca-certificates The next step is to include the keys from TinyCP.\n# apt-key adv --fetch-keys http://repos.tinycp.com/ubuntu/conf/gpg.key # echo \"deb http://repos.tinycp.com/ubuntu all main\" | sudo tee /etc/apt/sources.list.d/tinycp.list You will now be updating your repositories in the following step.\n# apt-get update Now, install TinyCP.\n# apt-get install tinycp You should be able to see this on your screen once the installation process is finished. You will be able to access the TinyCP webpage by using the URL that has been provided for you[http://Server_IP:65118]. Your login credentials are going to be the username and the password.","introduction#Introduction":"In this article, you will learn how to install TinyCP on Ubuntu 20.04.\nIt‚Äôs likely that you‚Äôre already familiar with several control panel programmes if you use Linux. These kind of programmes give you a front end through which you may control and administer your system. You may control a variety of settings and configurations by using these applications, just like you do with any other control panel that is integrated right in.\nTinyCP is one of the applications that can be used as control panels that are available to you. It is a simple control panel that operates in a web-based environment and may be used to carry out a variety of administrative tasks. You may manage the software packages installed on your system with TinyCP, operate a variety of online apps, set up servers for file sharing, handle emails and databases, and do a lot more besides.\nTinyCP is a lightweight control panel that offers a wide range of capabilities on Linux-based systems.TinyCP provides users with access to a wide variety of features. Nevertheless, not all of them will be installed when you initially use TinyCP. It is up to you to decide whatever programmes you wish to put on your machine and use. These features include the following:\nDomain Management Mailboxes Databases FTP Samba Firewall VPN GIT SVN ","testing#Testing":"In order to access TinyCP, as was previously said, input the URL into the browser of your choice. You will be taken to a page like that in the future.\nAfter you have entered your username and password, you will be brought to the screen that may be found below. This is the TinyCP app."},"title":"How to Install TinyCP on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-tinycp-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install TinyCP on Ubuntu 22.04.\nThank You üôÇ","install-tinycp#Install TinyCp":"On your Ubuntu virtual machine, you will need to download and install the ‚Äúgnupg‚Äù programme as well as the ‚Äúca-certificates‚Äù package. To accomplish that, use these commands.\n# apt install gnupg ca-certificates The next step is to include the keys from TinyCP.\n# apt-key adv --fetch-keys http://repos.tinycp.com/ubuntu/conf/gpg.key # echo \"deb http://repos.tinycp.com/ubuntu all main\" | sudo tee /etc/apt/sources.list.d/tinycp.list You will now be updating your repositories in the following step.\n# apt-get update Now, install TinyCP.\n# apt-get install tinycp You should be able to see this on your screen once the installation process is finished. You will be able to access the TinyCP webpage by using the URL that has been provided for you[http://Server_IP:65118]. Your login credentials are going to be the username and the password.","introduction#Introduction":"In this article, you will learn how to install TinyCP on Ubuntu 22.04.\nIt‚Äôs likely that you‚Äôre already familiar with several control panel programmes if you use Linux. These kind of programmes give you a front end through which you may control and administer your system. You may control a variety of settings and configurations by using these applications, just like you do with any other control panel that is integrated right in.\nTinyCP is one of the applications that can be used as control panels that are available to you. It is a simple control panel that operates in a web-based environment and may be used to carry out a variety of administrative tasks. You may manage the software packages installed on your system with TinyCP, operate a variety of online apps, set up servers for file sharing, handle emails and databases, and do a lot more besides.\nTinyCP is a lightweight control panel that offers a wide range of capabilities on Linux-based systems.TinyCP provides users with access to a wide variety of features. Nevertheless, not all of them will be installed when you initially use TinyCP. It is up to you to decide whatever programmes you wish to put on your machine and use. These features include the following:\nDomain Management Mailboxes Databases FTP Samba Firewall VPN GIT SVN ","testing#Testing":"In order to access TinyCP, as was previously said, input the URL into the browser of your choice. You will be taken to a page like that in the future.\nAfter you have entered your username and password, you will be brought to the screen that may be found below. This is the TinyCP app."},"title":"How to Install TinyCP on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-tomcat-on-ubuntu/":{"data":{"":"","2--configuring-admin-users#2 ‚Äî Configuring Admin Users":"","3--creating-asystemdservice#3 ‚Äî Creating a¬†systemd¬†service":"","4--accessing-the-web-interface#4 ‚Äî Accessing the Web Interface":"Now that the Tomcat service is up and running, you can set up the firewall to let Tomcat accept connections. After that, you‚Äôll be able to use its web interface.\nTomcat uses port 8080 to accept HTTP requests. Run the following command to make that port available for traffic:\n# ufw allow 8080 You can now get to Tomcat by going to your server‚Äôs IP address in your browser:\nhttp://serverip:8080 Tomcat default page after installing on Ubuntu server\nAnd that is how you will install Apache Tomcat 10 on Ubuntu server.","apt-install-default-jdk#apt install default-jdk":"","chmod--r-ux-opttomcatbin#chmod -R u+x /opt/tomcat/bin":"","introduction#Introduction":"","prerequisites#Prerequisites":"","systemctl-enable-tomcat#systemctl enable tomcat":" How to install Tomcat 10 on Ubuntu server\nIntroduction This guide will demonstrate how to install Apache Tomcat 10 on Ubuntu server. Install Tomcat 10, configure users and roles, and navigate the administrative user interface.. Java applications are served via Apache Tomcat, a web server and servlet container. It is an open source implementation of the Jakarta Servlet, Jakarta Server Pages, and other Jakarta EE platform technologies.\nThis guide will demonstrate how to install Apache Tomcat 10 on Ubuntu server. Install Tomcat 10, configure users and roles, and navigate the administrative user interface.\nPrerequisites One Ubuntu 20.04 server A super user ( root ) or any normal user with SUDO privileges. 1 ‚Äî Installing Tomcat To start, you‚Äôll need to get the most recent version and set up a separate user with the right permissions. The Java Development Kit will also be put in (JDK).\nFor security, Tomcat should run as a separate user who doesn‚Äôt have any special rights. Run the command below to make a user named tomcat:\n# useradd -m -d /opt/tomcat -U -s /bin/false tomcat Option -s is used to set the by default shell¬†/bin/false¬†This will ensure that it‚Äôs not possible to log in as¬†tomcat.\nNow install the JDK. To do this first update the package repository.\n# apt update apt install default-jdk Output after installing the default-JDK\nYou can check the java version\n# java -version Output\nopenjdk version ‚Äú11.0.16‚Äù 2022-07-19\nOpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu120.04)\nOpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu120.04, mixed mode, sharing)\nTo install Tomcat, you need the latest Core Linux build for Tomcat 10, which you can get from the downloads page. Choose the most recent version of Core Linux that ends in.tar.gz. At the time of writing, version 10.0.20 was the most recent.\nDownload page of Tomcat\nAnd now navigate to the¬†/tmp¬†directory:\n# cd /tmp Download the tomcat version using wget and then extract the new downloaded file\n# wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.0.23/bin/apache-tomcat-10.0.23.tar.gz Now create a new directory in /opt directory with name tomcat and then extract the tomcat files here.\n# tar xzvf apache-tomcat-10*tar.gz -C /opt/tomcat --strip-components=1 Since you have already made a user, you can now give tomcat ownership of the extracted installation by running:\n# chown -R tomcat:tomcat /opt/tomcat/ chmod -R u+x /opt/tomcat/bin 2 ‚Äî Configuring Admin Users You will set up privileged users in Tomcat‚Äôs configuration so that they can see the Manager and Host Manager pages. You‚Äôll need to get rid of the IP address restrictions, which stop any IP address from the outside from getting to those pages.\nThe file /opt/tomcat/conf/tomcat-users.xml is where Tomcat users are set up. Use the following command to open the file so it can be changed and append the below content.\n# vi /opt/tomcat/conf/tomcat-users.xml \u003crole rolename=\"manager-gui\" /\u003e \u003cuser username=\"manager\" password=\"manager_password\" roles=\"manager-gui\" /\u003e \u003crole rolename=\"admin-gui\" /\u003e \u003cuser username=\"admin\" password=\"admin_password\" roles=\"manager-gui,admin-gui\" /\u003e Replace the passwords that are underlined with your own. Save and close the file when you‚Äôre done.\nHere, you set up two user roles, manager-gui and admin-gui, that give each user access to the Manager and Host Manager pages. You also set up roles for two users named manager and admin.\nHere, you set up two user roles, manager-gui and admin-gui, that give each user access to the Manager and Host Manager pages. You also set up roles for two users named manager and admin.\nTomcat is set up so that access to the admin pages is limited unless the connection comes from the server itself. You will need to change the config files for those pages so that the users you just made can get to them.\nTo remove the restriction from the Manager page, you need to edit its config file Comment out the¬†_**Valve**_¬†definition, as shown::\n# /opt/tomcat/webapps/manager/META-INF/context.xml \u003cContext antiResourceLocking=\"false\" privileged=\"true\" \u003e \u003cCookieProcessor className=\"org.apache.tomcat.util.http.Rfc6265CookieProcessor\" sameSiteCookies=\"strict\" /\u003e \u003c!-- \u003cValve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" /\u003e --\u003e \u003cManager sessionAttributeValueClassNameFilter=\"java\\.lang\\.(?:Boolean|Integer|Long|Number|String)|org\\.apache\\.catalina\\.filters\\.CsrfPreventionFilter\\$LruCache(?:\\$1)?|java\\.util\\.(?:Linked)?HashMap\"/\u003e \u003c/Context\u003e And now repeat for¬†Host Manager:\n# vi /opt/tomcat/webapps/host-manager/META-INF/context.xml Now that you‚Äôve set up two users, manager and admin, you can use them to get into parts of the management interface that are normally locked. You will now make a service for Tomcat in systemd.\n3 ‚Äî Creating a¬†systemd¬†service You will now make a systemd service that will keep Tomcat running in the background. In case of an error or failure, the systemd service will also automatically restart Tomcat.\nTomcat is a Java application, so it needs the Java runtime, which you installed in step 1 with the JDK. You need to know where Java is before you can run the service. Using the following command, you can find out:\n# update-java-alternatives -l Output:\njava-1.11.0-openjdk-amd64 1111 /usr/lib/jvm/java-1.11.0-openjdk-amd64\nIn the last column, you can see the path to where Java is. In a moment, you‚Äôll need the path to define the service.\nUnder /etc/systemd/system, you will save the tomcat service in a file called tomcat.service. Make the file editable by doing.\n# vi /etc/systemd/system/tomcat.service [Unit]\nDescription=Apache Tomcat Web Application Container\nAfter=network.target\n[Service]\nType=forking\nUser=tomcat\nGroup=tomcat\nEnvironment=‚ÄúJAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64‚Äù\nEnvironment=‚ÄúJAVA_OPTS=-Djava.security.egd=file:///dev/urandom -Djava.awt.headless=true‚Äù\nEnvironment=‚ÄúCATALINA_BASE=/opt/tomcat‚Äù\nEnvironment=‚ÄúCATALINA_HOME=/opt/tomcat‚Äù\nEnvironment=‚ÄúCATALINA_PID=/opt/tomcat/temp/tomcat.pid‚Äù\nEnvironment=‚ÄúCATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC‚Äù\nExecStart=/opt/tomcat/bin/startup.sh\nExecStop=/opt/tomcat/bin/shutdown.sh\n[Install]\nWantedBy=multi-user.target\nChange the highlighted JAVA HOME value if it is different from the one you wrote down earlier.\nHere, you set up a service that will run Tomcat by running the scripts it provides for starting up and shutting down. You also set a few environment variables to set its home directory (which is still /opt/tomcat) and limit the amount of memory the Java VM can use (in CATALINA OPTS). If the Tomcat service fails, it will automatically start up again.\nSave and close the file when you‚Äôre done.\nReload the systemd daemon so that it knows about the new service:\n# systemctl daemon-reload You can then start and enable the Tomcat service by typing:\n# systemctl start tomcat systemctl enable tomcat "},"title":"How to install Tomcat 10 on Ubuntu server"},"/utho-docs/docs/linux/how-to-install-vagrant-on-ubuntu-20-04-lts/":{"data":{"":"\nDescription\nInstall Vagrant on Ubuntu 20.04 LTS. Ruby is the programming language used in the development of the open-source software known as Vagrant. Because of how easily it integrates with a variety of virtual environment providers including VirtualBox, VMware, and Docker, it has gained a significant amount of popularity as a method for rapidly constructing virtual environments. It helps to streamline the process and lessen the amount of labour required to run and operate virtual machines (VMs) on your systems. Vagrant makes it possible to do difficult tasks by providing a straightforward set of commands.","check-version#Check Version":" vagrant --version ","configure-vm#Configure VM":"Once Vagrantfile is made, you can run the vagrant up command to use Vagrantfile to create and set up a VM. This is the most important command because it is the only one that makes Vagrant virtual machines (VMs). Read the documentation to learn more.\nvagrant up ","connect-vm#Connect VM":"You need to use the vagrant ssh command, as shown below, to connect to the default VM. If there are multiple nodes running, you can connect to a specific one with the vagrant ssh node name\u003e command. Vagrant ssh documentation: more info\nvagrant ssh Vagrant on Ubuntu 20.04 LTS. Vagrant, a piece of open-source software, was made using the programming language Ruby. Because it works well with VirtualBox, VMware, and Docker, among others, it has become very popular as a way to quickly build virtual environments. It makes the process of running and operating virtual machines (VMs) on your systems easier and less time-consuming. Vagrant makes it easy to do hard things by giving you a simple set of commands.\nMust Read: For Application Data Storage on Fedora 14, Use MongoDB\nThankyou","download-vagrant#Download Vagrant":"You can use any file transfer tool, like wget or curl, to get the latest version of Vagrant from the official website. Here, we‚Äôre using the wget command to download the latest Vagrant Debian package, version 2.2.19. It will download the package to the working directory.\nwget https://releases.hashicorp.com/vagrant/2.2.19/vagrant_2.2.19_x86_64.deb ","install-provider#Install Provider":"To make VMs, Vagrant needs a provider. Vagrant‚Äôs default provider is VirtualBox, so to install it, run the command apt install virtualbox, as shown below.\napt install virtualbox ","install-vagrant#Install Vagrant":"You can now install this package using the apt package manager. To start the installation, you must run the command apt install./vagrant 2.2.19 x86 64.deb.\napt install ./vagrant_2.2.19_x86_64.deb ","setup-the-vagrantfile#Setup the Vagrantfile":"Vagrant can be used to make a VM. Before you can do that, you need to set up the Vagrantfile with the settings for the VM you want to start. For example, here we‚Äôre going to make an Ubuntu VM, so we need to first set up the environment by running the vagrant init ubuntu/focal64 command.\nvagrant init ubuntu/focal64 ","update-server#Update Server":"It is strongly advised that before going through the steps to install Vagrant, you check for any updates from all of the configured repositories by using the apt update or apt-get update command, as shown below. This is done before going through the steps to install Vagrant. This will also update the package cache with any information on packages that is currently available in any of the preset repositories.\napt update ","what-exactly-is-a-provider#What exactly is a Provider":"In the Vagrant Ecosystem, a provider is a software virtualization solution such as VirtualBox, VMware Fusion, or VMware Workstation.","what-is-a-vagrantfile#What is a Vagrantfile":"A Vagrantfile is a configuration file that is written in Ruby syntax. Vagrant uses this file to figure out what to do when the user sends a command from the terminal."},"title":"How to Install Vagrant on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-install-vagrant-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Vagrant on Ubuntu 22.04.\nAlso Read:¬†How to install PHP 8.2 on Ubuntu 22.04\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Vagrant on Ubuntu 22.04.\nVagrant provides DevOps with a variety of different development environments, which enables developers to work across a number of different operating systems. The fact that this tool can be installed on the vast majority of Linux distributions is one of the most appealing aspects of Vagrant. It is really necessary to be aware that Vagrant is a tool that is generated by a command line.\nIn addition, you need to install a virtualization engine on your machine, such as Hyper-V, VirtualBox, or Docker, to use the Vagrant tools. KVM and VMware are supported by the Vagrant plugin system as well.\nStep 1: Update packages First, make sure that the server repo has the most recent versions.\n# apt update Step 2: Upgrade packages After you have executed the update command, you will be informed of the total number of packages that need to be upgraded once the update command has been completed. Execute the following command in order to upgrade them.\n# apt upgrade Step 3: Installation of VirtualBox Following that, we will run the following command in order to install VirtualBox on our machine:\n# apt install virtualbox Step 4: Installation of Vagrant After you have finished installing VirtualBox, you should now download the Vagrant deb package by typing the following command into the terminal of your machine:\n# wget https://releases.hashicorp.com/vagrant/2.2.19/vagrant\\_2.2.19\\_x86\\_64.deb After that, use the subsequent command to install the deb package from the apt installer:\n# apt install ./vagrant\\_2.2.19\\_x86\\_64.deb Step 5: Verify the installation Here, we‚Äôll use the check version command to see if we installed Vagrant on our Ubuntu machine correctly.\n# vagrant --version "},"title":"How to install Vagrant on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-visual-studio-code-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Visual Studio Code on Debian 10.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Visual Studio Code on Debian 10.\nVisual Studio Code is a free code editor that was developed by Microsoft. It is compatible with several operating systems, including Linux, Windows, and Mac OS, and it is available for download. It is an effective tool that facilitates the debugging of code, the running of tasks, and the activation of version control.\nIt includes several features that set it apart from other code editors, such as automated code completion, snippets, refactoring, and syntax highlighting, amongst many more.","step-1-system-update#Step 1: System update":"It is generally recommended that you update your operating system before installing any new software; as a result, the following command can be used by you to carry out this task.\n# apt update¬†\u0026\u0026¬†apt upgrade¬†-y ","step-2-install-packages#Step 2: Install packages":"Now that the system has been updated, there are certain packages that need to be installed on your system before you can install the editor. You can only do this once the system has been updated.\n# apt¬†install¬†software-properties-common apt-transport-https¬†wget¬†-y ","step-3-import-repository#Step 3: Import repository":"After the packages have been installed, the next step is to include the repository for Visual Studio Code; however, before that step can be completed, the Microsoft GPG key must be imported in order to authenticate the packages that have been installed.\n# wget¬†-O-¬†https://packages.microsoft.com/keys/microsoft.asc¬†|¬†gpg¬†--dearmor¬†|¬†tee¬†/usr/share/keyrings/vscode.gpg This will ensure that the packages that have been installed are genuine. Let‚Äôs move on now and work toward merging the Microsoft Visual Source repository.\n# echo¬†deb¬†\\[arch=amd64 signed-by=/usr/share/keyrings/vscode.gpg\\]¬†https://packages.microsoft.com/repos/vscode stable main¬†|¬†tee¬†/etc/apt/sources.list.d/vscode.list ","step-4-update-the-system-again#Step 4: Update the system again":"After you have imported the repository and installed the packages, it is highly advised that you then update your operating system.\n# apt update -y ","step-5-install-the-editor#Step 5: Install the editor":"To install Visual Studio Code Editor, all you need to do now is run the command that is provided below on the terminal.\n# apt¬†install¬†code Run the following command to check the version:\n# code --version --user-data-dir To launch the application, run the following command.\n# code¬†\u0026 "},"title":"How to install Visual Studio Code on Debian 10"},"/utho-docs/docs/linux/how-to-install-visual-studio-code-on-ubuntu-22-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Visual Studio Code on Ubuntu 22.04.\nAlso Read:¬†How to Install NGINX Web Server on Ubuntu 22.04 LTS\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Visual Studio Code on Ubuntu 22.04.\nVisual Studio Code is a free code editor that was developed by Microsoft. It is compatible with several operating systems, including Linux, Windows, and Mac OS, and it is available for download. It is an effective tool that facilitates the debugging of code, the running of tasks, and the activation of version control.\nIt includes several features that set it apart from other code editors, such as automated code completion, snippets, refactoring, and syntax highlighting, amongst many more.","step-1-system-update#Step 1: System update":"It is generally recommended that you update your operating system before installing any new software; as a result, the following command can be used by you to carry out this task.\n# apt update¬†\u0026\u0026¬†apt upgrade¬†-y ","step-2-install-packages#Step 2: Install packages":"Now that the system has been updated, there are certain packages that need to be installed on your system before you can install the editor. You can only do this once the system has been updated.\n# apt¬†install¬†software-properties-common apt-transport-https¬†wget¬†-y ","step-3-import-repository#Step 3: Import repository":"After the packages have been installed, the next step is to include the repository for Visual Studio Code; however, before that step can be completed, the Microsoft GPG key must be imported in order to authenticate the packages that have been installed.\n# wget¬†-O-¬†https://packages.microsoft.com/keys/microsoft.asc¬†|¬†gpg¬†--dearmor¬†|¬†tee¬†/usr/share/keyrings/vscode.gpg This will ensure that the packages that have been installed are genuine. Let‚Äôs move on now and work toward merging the Microsoft Visual Source repository.\n# echo¬†deb¬†\\[arch=amd64 signed-by=/usr/share/keyrings/vscode.gpg\\]¬†https://packages.microsoft.com/repos/vscode stable main¬†|¬†tee¬†/etc/apt/sources.list.d/vscode.list ","step-4-update-the-system-again#Step 4: Update the system again":"After you have imported the repository and installed the packages, it is highly advised that you then update your operating system.\n# apt update -y ","step-5-install-the-editor#Step 5: Install the editor":"To install Visual Studio Code Editor, all you need to do now is run the command that is provided below on the terminal.\n# apt¬†install¬†code Run the following command to check the version:\n# code --version --user-data-dir To launch the application, run the following command.\n# code¬†\u0026 "},"title":"How to install Visual Studio Code on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-webmin-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Webmin on AlmaLinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Webmin on AlmaLinux 8.\nWebmin is a tool that we may use to control our command line AlmaLinux 8 server that is running without any kind of graphical interface. It is an open-source utility that only requires a few instructions in order to be quickly installed. It provides remote management of the server network as well as the hardware and backup, among other features.\nUsers have the ability to install a variety of modules to extend the functionality of the system. These modules include the ability to add support for LAMP, a Heartbeat Monitor, a Squid Proxy Server, a DHCP server, and many more.\nIn this tutorial, we will go over the steps required to install Webmin on AlmaLinux 8 by making use of the command line and the official repository.\nStep 1: Add Webmin Repository Let‚Äôs go ahead and add the Webmin yum repository to Almalinux 8 so that we can access the Webmin packages and install them with only one command.\n# vi /etc/yum.repos.d/webmin.repo Copy and paste the following content in your file and save it by pressing escape: wq\n[Webmin] name=Webmin Distribution Neutral ### not using ### baseurl=https://download.webmin.com/download/yum mirrorlist=https://download.webmin.com/download/yum/mirrorlist enabled=1 gpgkey=https://download.webmin.com/jcameron-key.asc gpgcheck=1 Step 2: Update the system Execute the system update command, which makes the repository cache to be refreshed and will help the system to recognise the most recent Webmin repository addition along with all of the packages that are available under it.\n# dnf update -y Step 3: Install Webmin At long last, everything we require is in its proper location. On our Almalinux Linux operating system that is based on RPM, let‚Äôs now execute a single command in order to download and install Webmin.\n# dnf install webmin Step 4: Allow Port in Firewall To access the web interface of this web-based system configuration tool from any system browser that can reach the Server‚Äôs IP address or domain where we put it, we have to open a Webmin port number of 10000 in the Firewall.\n# firewall-cmd --add-port=10000/tcp --permanent # firewall-cmd --reload Step 5: Access Webmin If you use Webmin on a machine with a graphical user interface, you can use the local address in the URL, like http://localhost:10000.\n# https://server-ip-address:10000 After logging into the webmin, you can manage your server through GUI mode."},"title":"How to install Webmin on AlmaLinux 8"},"/utho-docs/docs/linux/how-to-install-webmin-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Webmin on Centos 7.\nThank You üôÇ","installing-webmin#Installing Webmin":"To begin, if you haven‚Äôt done so in a while, you should refresh the package index on your server:\n# yum update -y After that, we need to add the Webmin repository in order for our package management to be able to be used to install and update Webmin. In order to accomplish this, we must first add the repository to the/etc/yum.repos.d/webmin.repo.\n# vi /etc/yum.repos.d/webmin.repo After that, add the following lines to the file so that the new repository may be defined:\n[Webmin] name=Webmin Distribution Neutral #baseurl=http://download.webmin.com/download/yum mirrorlist=http://download.webmin.com/download/yum/mirrorlist enabled=1 Save the file and exit the editor. If you used¬†vi, do so by pressing¬†escape colon wq.\nAfter that, you will need to add the Webmin PGP key so that your operating system will trust the newly created repository. However, in order to achieve that, you will need to install the gnupg1 package. This is the GNU programme for secure communication as well as the storing of sensitive data.\nAfter that, you will need to use wget to download the Webmin PGP key, and then you will need to add it to the list of keys on your system:\n# wget http://www.webmin.com/jcameron-key.asc # rpm --import jcameron-key.asc Next, make another round of updates to the list of packages in order to incorporate the Webmin repository, which should now be trusted:\n# yum update -y Install Webmin afterwards:\n# yum install webmin -y The installation of Webmin is finished. You are now able to log in to https://your server:10000 either as root with your root password or as any other user who is able to utilise sudo.","introduction#Introduction":"In this article, you will learn how to install Webmin on Centos 7.\nWebmin is a powerful and flexible control panel for the operation of Unix-like servers that is accessed through the web. The user is able to change and control open-source applications such as BIND, Apache HTTP Server, PHP, or MySQL through the usage of Webmin. Webmin also enables the user to customise the internals of the operating system, such as users, disc quotas, services, or configuration files.\nWebmin is mostly constructed on the Perl scripting language and operates as its own process in addition to a web server. It communicates using the TCP port number 10000 by default and may be set up to use SSL if OpenSSL and the extra needed Perl Modules are installed on the system. Webmin is designed around modules, each of which has an interface to the configuration files and the Webmin server. This makes it quite easy to add new functionality to Webmin. Because of the modular architecture of Webmin, it is feasible for anyone who has an interest to build plugins for the setting of desktop environments. Controlling several machines over a single interface is possible with Webmin, as is logging in without interruption to other Webmin servers located on the same local area network (LAN) or subnet.\nIn the event that a plugin is unavailable for a particular job, it is possible to open a terminal and carry out a variety of operations using a command line interface (CLI). This is especially helpful in situations where it is not possible to utilise SSH or a comparable protocol."},"title":"How to install Webmin on Centos 7"},"/utho-docs/docs/linux/how-to-install-webmin-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Webmin on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","installing-webmin#Installing Webmin":"To begin, if you haven‚Äôt done so in a while, you should refresh the package index on your server:\n# apt update -y After that, we need to add the Webmin repository in order for our package management to be able to be used to install and update Webmin. In order to accomplish this, we must first add the repository to the /etc/apt/sources.list file.\n# vi /etc/apt/sources.list After that, add the following line to the very end of the file in order to incorporate the new repository:\ndeb http://download.webmin.com/download/repository sarge contrib Save the file and exit the editor. If you used¬†vi, do so by pressing¬†escape colon wq.\nAfter that, you will need to add the Webmin PGP key so that your operating system will trust the newly created repository. However, in order to achieve that, you will need to install the gnupg1 package. This is the GNU programme for secure communication as well as the storing of sensitive data.\nAfter that, you will need to use wget to download the Webmin PGP key, and then you will need to add it to the list of keys on your system:\n# wget -q -O- http://www.webmin.com/jcameron-key.asc | sudo apt-key add Next, make another round of updates to the list of packages in order to incorporate the Webmin repository, which should now be trusted:\n# apt update Install Webmin afterwards:\n# apt install webmin -y Note: If you followed the prerequisite step and installed and enabled ufw, you will need to execute the following command in order to allow Webmin to pass through the firewall:\n# ufw allow 10000 The installation of Webmin is finished. You are now able to log in to https://your server:10000 either as root with your root password or as any other user who is able to utilise sudo.","introduction#Introduction":"In this article, you will learn how to install Webmin on Debian 12.\nWebmin is a powerful and flexible control panel for the operation of Unix-like servers that is accessed through the web. The user is able to change and control open-source applications such as BIND, Apache HTTP Server,¬†PHP, or MySQL through the usage of Webmin. Webmin also enables the user to customise the internals of the operating system, such as users, disc quotas, services, or configuration files.\nWebmin is mostly constructed on the Perl scripting language and operates as its own process in addition to a web server. It communicates using the¬†TCP¬†port number 10000 by default and may be set up to use SSL if OpenSSL and the extra needed Perl Modules are installed on the system. Webmin is designed around modules, each of which has an interface to the configuration files and the Webmin server. This makes it quite easy to add new functionality to Webmin. Because of the modular architecture of Webmin, it is feasible for anyone who has an interest to build plugins for the setting of desktop environments. Controlling several machines over a single interface is possible with Webmin, as is logging in without interruption to other Webmin servers located on the same local area network (LAN) or subnet.\nIn the event that a plugin is unavailable for a particular job, it is possible to open a terminal and carry out a variety of operations using a command line interface (CLI). This is especially helpful in situations where it is not possible to utilise SSH or a comparable protocol."},"title":"How to install Webmin on Debian 12"},"/utho-docs/docs/linux/how-to-install-webmin-on-debian/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Webmin on Debian.\nThank You üôÇ","installing-webmin#Installing Webmin":"To begin, if you haven‚Äôt done so in a while, you should refresh the package index on your server:\n# apt update -y After that, we need to add the Webmin repository in order for our package management to be able to be used to install and update Webmin. In order to accomplish this, we must first add the repository to the /etc/apt/sources.list file.\n# vi /etc/apt/sources.list After that, add the following line to the very end of the file in order to incorporate the new repository:\ndeb http://download.webmin.com/download/repository sarge contrib Save the file and exit the editor. If you used¬†vi, do so by pressing¬†escape colon wq.\nAfter that, you will need to add the Webmin PGP key so that your operating system will trust the newly created repository. However, in order to achieve that, you will need to install the gnupg1 package. This is the GNU programme for secure communication as well as the storing of sensitive data.\nAfter that, you will need to use wget to download the Webmin PGP key, and then you will need to add it to the list of keys on your system:\n# wget -q -O- http://www.webmin.com/jcameron-key.asc | sudo apt-key add Next, make another round of updates to the list of packages in order to incorporate the Webmin repository, which should now be trusted:\n# apt update Install Webmin afterwards:\n# apt install webmin -y Note: If you followed the prerequisite step and installed and enabled ufw, you will need to execute the following command in order to allow Webmin to pass through the firewall:\n# ufw allow 10000 The installation of Webmin is finished. You are now able to log in to https://your server:10000 either as root with your root password or as any other user who is able to utilise sudo.","introduction#Introduction":"In this article, you will learn how to install Webmin on Debian.\nWebmin is a powerful and flexible control panel for the operation of Unix-like servers that is accessed through the web. The user is able to change and control open-source applications such as BIND, Apache HTTP Server, PHP, or MySQL through the usage of Webmin. Webmin also enables the user to customise the internals of the operating system, such as users, disc quotas, services, or configuration files.\nWebmin is mostly constructed on the Perl scripting language and operates as its own process in addition to a web server. It communicates using the TCP port number 10000 by default and may be set up to use SSL if OpenSSL and the extra needed Perl Modules are installed on the system. Webmin is designed around modules, each of which has an interface to the configuration files and the Webmin server. This makes it quite easy to add new functionality to Webmin. Because of the modular architecture of Webmin, it is feasible for anyone who has an interest to build plugins for the setting of desktop environments. Controlling several machines over a single interface is possible with Webmin, as is logging in without interruption to other Webmin servers located on the same local area network (LAN) or subnet.\nIn the event that a plugin is unavailable for a particular job, it is possible to open a terminal and carry out a variety of operations using a command line interface (CLI). This is especially helpful in situations where it is not possible to utilise SSH or a comparable protocol."},"title":"How to install Webmin on Debian"},"/utho-docs/docs/linux/how-to-install-webmin-on-fedora-2/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Webmin on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Webmin on Fedora.\nWebmin¬†is a tool that we may use to control our command line Fedora server that is running without any kind of graphical interface. It is an open-source utility that only requires a few instructions in order to be quickly installed. It provides remote management of the server network as well as the hardware and backup, among other features.\nUsers have the ability to install a variety of modules to extend the functionality of the system. These modules include the ability to add support for LAMP, a Heartbeat Monitor, a Squid Proxy Server, a DHCP server, and many more.\nIn this tutorial, we will go over the steps required to install Webmin on Fedora by making use of the command line and the official repository.\nStep 1: Add Webmin Repository Let‚Äôs go ahead and add the Webmin yum repository to Fedora so that we can access the Webmin packages and install them with only one command.\n# vi /etc/yum.repos.d/webmin.repo Copy and paste the following content in your file and save it by pressing escape: wq\n[Webmin] name=Webmin Distribution Neutral ### not using ### baseurl=https://download.webmin.com/download/yum mirrorlist=https://download.webmin.com/download/yum/mirrorlist enabled=1 gpgkey=https://download.webmin.com/jcameron-key.asc gpgcheck=1 Step 2: Update the system Execute the system update command, which makes the repository cache to be refreshed and will help the system to recognise the most recent Webmin repository addition along with all of the packages that are available under it.\n# dnf update -y Step 3: Install Webmin At long last, everything we require is in its proper location. On our Fedora Linux operating system that is based on RPM, let‚Äôs now execute a single command in order to download and install Webmin.\n# dnf install webmin Step 4: Access Webmin If you use Webmin on a machine with a graphical user interface, you can use the local address in the URL, like http://localhost:10000.\n# https://server-ip-address:10000 After logging into the webmin, you can manage your server through GUI mode."},"title":"How to install Webmin on Fedora"},"/utho-docs/docs/linux/how-to-install-webmin-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Webmin on Fedora.\nThank You üôÇ","installing-webmin#Installing Webmin":"To begin, if you haven‚Äôt done so in a while, you should refresh the package index on your server:\n# dnf update -y After that, we need to add the Webmin repository in order for our package management to be able to be used to install and update Webmin. In order to accomplish this, we must first add the repository to the/etc/yum.repos.d/webmin.repo.\n# vi /etc/yum.repos.d/webmin.repo After that, add the following lines to the file so that the new repository may be defined:\n[Webmin] name=Webmin Distribution Neutral #baseurl=http://download.webmin.com/download/yum mirrorlist=http://download.webmin.com/download/yum/mirrorlist enabled=1 Save the file and exit the editor. If you used¬†vi, do so by pressing¬†escape colon wq.\nAfter that, you will need to add the Webmin PGP key so that your operating system will trust the newly created repository. However, in order to achieve that, you will need to install the gnupg1 package. This is the GNU programme for secure communication as well as the storing of sensitive data.\nAfter that, you will need to use wget to download the Webmin PGP key, and then you will need to add it to the list of keys on your system:\n# wget http://www.webmin.com/jcameron-key.asc # rpm --import jcameron-key.asc Next, make another round of updates to the list of packages in order to incorporate the Webmin repository, which should now be trusted:\n# dnf update -y Install Webmin afterwards:\n# dnf install webmin -y The installation of Webmin is finished. You are now able to log in to https://your server:10000 either as root with your root password or as any other user who is able to utilise sudo.","introduction#Introduction":"In this article, you will learn how to install Webmin on Fedora.\nWebmin is a powerful and flexible control panel for the operation of Unix-like servers that is accessed through the web. The user is able to change and control open-source applications such as BIND, Apache HTTP Server, PHP, or MySQL through the usage of Webmin. Webmin also enables the user to customise the internals of the operating system, such as users, disc quotas, services, or configuration files.\nWebmin is mostly constructed on the Perl scripting language and operates as its own process in addition to a web server. It communicates using the TCP port number 10000 by default and may be set up to use SSL if OpenSSL and the extra needed Perl Modules are installed on the system. Webmin is designed around modules, each of which has an interface to the configuration files and the Webmin server. This makes it quite easy to add new functionality to Webmin. Because of the modular architecture of Webmin, it is feasible for anyone who has an interest to build plugins for the setting of desktop environments. Controlling several machines over a single interface is possible with Webmin, as is logging in without interruption to other Webmin servers located on the same local area network (LAN) or subnet.\nIn the event that a plugin is unavailable for a particular job, it is possible to open a terminal and carry out a variety of operations using a command line interface (CLI). This is especially helpful in situations where it is not possible to utilise SSH or a comparable protocol."},"title":"How to install Webmin on Fedora"},"/utho-docs/docs/linux/how-to-install-webmin-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to install Webmin on Ubuntu 20.04.\nThank You üôÇ","installing-webmin#Installing Webmin":"To begin, if you haven‚Äôt done so in a while, you should refresh the package index on your server:\n# apt update After that, we need to add the Webmin repository in order for our package management to be able to be used to install and update Webmin. In order to accomplish this, we must first add the repository to the /etc/apt/sources.list file.\n# vi /etc/apt/sources.list After that, add the following line to the very end of the file in order to incorporate the new repository:\ndeb http://download.webmin.com/download/repository sarge contrib Save the file and exit the editor. If you used¬†vi, do so by pressing¬†escape colon wq.\nAfter that, you will need to add the Webmin PGP key so that your operating system will trust the newly created repository. However, in order to achieve that, you will need to install the gnupg1 package. This is the GNU programme for secure communication as well as the storing of sensitive data.\nAfter that, you will need to use wget to download the Webmin PGP key, and then you will need to add it to the list of keys on your system:\n# wget -q -O- http://www.webmin.com/jcameron-key.asc | sudo apt-key add Next, make another round of updates to the list of packages in order to incorporate the Webmin repository, which should now be trusted:\n# apt update Install Webmin afterwards:\n# apt install webmin -y Note: If you followed the prerequisite step and installed and enabled ufw, you will need to execute the following command in order to allow Webmin to pass through the firewall:\n# ufw allow 10000 The installation of Webmin is finished. You are now able to log in to https://your server:10000 either as root with your root password or as any other user who is able to utilise sudo.","introduction#Introduction":"In this article, you will learn how to install Webmin on ubuntu 20.04.\nWebmin is a powerful and flexible control panel for the operation of Unix-like servers that is accessed through the web. The user is able to change and control open-source applications such as BIND, Apache HTTP Server, PHP, or MySQL through the usage of Webmin. Webmin also enables the user to customise the internals of the operating system, such as users, disc quotas, services, or configuration files.\nWebmin is mostly constructed on the Perl scripting language and operates as its own process in addition to a web server. It communicates using the TCP port number 10000 by default and may be set up to use SSL if OpenSSL and the extra needed Perl Modules are installed on the system. Webmin is designed around modules, each of which has an interface to the configuration files and the Webmin server. This makes it quite easy to add new functionality to Webmin. Because of the modular architecture of Webmin, it is feasible for anyone who has an interest to build plugins for the setting of desktop environments. Controlling several machines over a single interface is possible with Webmin, as is logging in without interruption to other Webmin servers located on the same local area network (LAN) or subnet.\nIn the event that a plugin is unavailable for a particular job, it is possible to open a terminal and carry out a variety of operations using a command line interface (CLI). This is especially helpful in situations where it is not possible to utilise SSH or a comparable protocol."},"title":"How to install Webmin on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-webuzo-on-debian/":{"data":{"":"","conclusion#Conclusion":"Congratulations, Webuzo has been successfully installed on a server running Debian . Build shared hosting packages on a single server with the help of this control panel, and set up web applications that can be accessed by several users at the same time.\nRead How to Install Webuzo on Fedora\nThank You üôÇ","introduction#Introduction":"In this article we will discuss about How to Install Webuzo on Debian, Webuzo is a control panel for multi-user shared hosting that enables users to deploy various apps and share server resources with other users simultaneously. In addition, the control panel provides an interactive management interface that makes it possible to install applications, manage domains, schedule jobs, build databases, and manage users without having any prior experience or knowledge of technical administration.\nThis tutorial will describe how to install Webuzo on a server that has been freshly installed with Debian","step-1-install-webuzo#Step 1: Install Webuzo":"Webuzo provides a straightforward installation script that we can use to set up the control panel and choose which system services we would also like to set up along with it.\n1. To install Webuzo, download the installation script.\n# wget http://files.webuzo.com/install.sh 2. Make the script executable by modifying its permissions so that it can be run.\n# chmod 700 install.sh 3. Start the installation of Webuzo by running the script in the directory.\n# ./install.sh --v3 ","step-2-configure-uncomplicated-firewall-ufw#Step 2: Configure Uncomplicated Firewall (ufw)":"Allowing access to only Webuzo ports and important service ports like HTTP, HTTPS, and MySQL, which are utilised by hosted apps the majority of the time, can help you secure your server. Do not open port 21, for instance, if you do not intend to send data via the File Transfer Protocol (FTP).\n1. Allow the essential ports.\n# ufw allow 80,443,3306,21,22,53,25,143/tcp 2. Open the Webuzo ports.\n# ufw allow 2003:2004/tcp 3. Due to the fact that ports 2002 and 2005 are alternatives, this article restricts Webuzo to only ports 2003 and 2004.\n4. Confirm the new rules that have been added to your firewall table.\n# ufw status We may proceed with the configuration of the web control panel now that Webuzo has been installed correctly and the server‚Äôs panel ports have been opened.\nIn a web browser, navigate to the location of your server on port 2004.\nhttp://SERVER-IP:2004 "},"title":"How to Install Webuzo on Debian"},"/utho-docs/docs/linux/how-to-install-webuzo-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Congratulations, Webuzo has been successfully installed on a server running Fedora . Build shared hosting packages on a single server with the help of this control panel, and set up web applications that can be accessed by several users at the same time.\nHopefully, you have learned how to install Webuzo on Fedora.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Webuzo on Fedora.\nWebuzo is a control panel for multi-user shared hosting that enables users to simultaneously deploy a variety of apps and share server resources with other users. This is made possible by Webuzo‚Äôs multi-user shared hosting model. You will gain the knowledge necessary to install Webuzo on Fedora by reading this post. Webuzo is a control panel for shared hosting that supports many users. Even if the user has no prior experience with or knowledge of technical administration, they are still able to install applications, manage domains, schedule jobs, build databases, and manage users thanks to the control panel‚Äôs interactive management interface. This makes it possible for users to install applications, manage domains, schedule jobs, build databases, and manage users. This is due to the fact that the control panel is used to carry out activities such as the development of databases, the scheduling of jobs, the management of domains, and the installation of applications. This is made possible by the fact that it is able to execute all of these things, which in turn makes it possible to complete all of these things through the control panel.\nWithin the context of this tutorial, we will discuss how to install Webuzo on a server that has recently been set up using Fedora.\nStep 1. Update the Server. # yum update -y Step 2: To download the Webuzo installer, use the following command. # wget -N [http://files.webuzo.com/install.sh](http://files.webuzo.com/install.sh) Step 3: Make sure that the installer.sh file has the Execute permission. # chmod 700 install.sh Step 4: Begin the process of installing Webuzo by executing the script that is located within the directory. # ./install.sh Step 5: Navigate to the location of your server that is listening on port 2004 using a web browser. http://SERVER-IP:2004 "},"title":"How to Install Webuzo on Fedora"},"/utho-docs/docs/linux/how-to-install-webuzo-v3-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Congratulations, Webuzo has been successfully installed on a server running Ubuntu 20.04 . Build shared hosting packages on a single server with the help of this control panel, and set up web applications that can be accessed by several users at the same time.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Webuzo on ubuntu 20.04.\nThis article will guide you through the process of installing Webuzo v3 on Ubuntu 20.04, Webuzo is a control panel for multi-user shared hosting that enables users to deploy various apps and share server resources with other users simultaneously. In addition, the control panel provides an interactive management interface that makes it possible to install applications, manage domains, schedule jobs, build databases, and manage users without having any prior experience or knowledge of technical administration.\nThis tutorial will describe how to install Webuzo V3 on a server that has been freshly installed with Ubuntu 20.04.","step-1-install-webuzo#Step 1: Install Webuzo":"Webuzo provides a straightforward installation script that we can use to set up the control panel and choose which system services we would also like to set up along with it.\n1. To install Webuzo, download the installation script.\n# wget http://files.webuzo.com/install.sh 2. Make the script executable by modifying its permissions so that it can be run.\n# chmod 700 install.sh 3. Start the installation of Webuzo by running the script in the directory.\n# ./install.sh --v3 ","step-2-configure-uncomplicated-firewall-ufw#Step 2: Configure Uncomplicated Firewall (ufw)":"Allowing access to only Webuzo ports and important service ports like HTTP, HTTPS, and MySQL, which are utilised by hosted apps the majority of the time, can help you secure your server. Do not open port 21, for instance, if you do not intend to send data via the File Transfer Protocol (FTP).\n1. Allow the essential ports.\n# ufw allow 80,443,3306,21,22,53,25,143/tcp 2. Open the Webuzo ports.\n# ufw allow 2003:2004/tcp 3. Due to the fact that ports 2002 and 2005 are alternatives, this article restricts Webuzo to only ports 2003 and 2004.\n4. Confirm the new rules that have been added to your firewall table.\n# ufw status We may proceed with the configuration of the web control panel now that Webuzo has been installed correctly and the server‚Äôs panel ports have been opened.\nIn a web browser, navigate to the location of your server on port 2004.\nhttp://SERVER-IP:2004 Get a Webuzo License Key\nWebuzo offers a premium package with a higher number of available applications and extra tools like a backup utility and Spam Assassin, among others that are not available with the free trial plan.\nTo buy a license key, visit the Webuzo website, select a package and click¬†Buy Now. You will be redirected to the Softaculous website, sign up for an account, verify your email, then buy a Webuzo license to receive your activation key via email."},"title":"How to Install Webuzo v3 on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-webuzo/":{"data":{"":"\nWebuzo is multi user Control panel built for Hosting Providers resellers and website owners. Its powerful and easy to use, so we are going to install webuzo on centos7. Webuzo is perfect for use in enterprises, small to medium business situations and just about 240 web applications are supported by Webuzo.For using Webuzo there is no system admin skills are required.\nStep 1. Update the Server.\n#yum update -y Step 2: Use this command to get Webuzo installer.\n# wget -N [http://files.webuzo.com/install.sh](http://files.webuzo.com/install.sh) Step 3: Provide Execute permission on installer.sh file.\n#chmod +x install.sh Step 4. Now run the installer.sh.\n# ./install.sh Installation Done on server.\nStep 5: Now Access Webuzo in browser.\nWith SSL :https://Server_IP:2003\nWithout SSL : http://Server_Ip:2002\nStep 6. Now Complete installation by Providing all details.\nThank you!!"},"title":"How to Install Webuzo"},"/utho-docs/docs/linux/how-to-install-wekan-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Congratulations, your server now has Wekan installed. The control panel is now available, and you can start making and managing cards right away.\nI hope you have learned how to install wekan on Debian 10.\nThank You üôÇ","introduction#Introduction":"","step-1-install-and-configure-wekan#Step 1. Install and Configure Wekan":"\nIntroduction In this article, you will learn how to install wekan on Debian 10.\nWekan is a free and open-source Kanban Board software that is built on the Meteor JavaScript framework. Its primary purpose is to facilitate the management of projects and tasks through the use of cards. Using Wekan allows you to quickly and simply invite team members to the board, define tasks, and assign them to your member within a certain time frame. Because of all of these features, using Wekan is an excellent solution for enabling project cooperation among engineers or other teams. When a task is divided among members of a team, more productivity is achieved in a shorter amount of time as a direct result of this increased efficiency. It is a web programme that is similar to Trello that allows you to accomplish a wide variety of activities while having a small learning curve and a simplified user interface.\nWhen compared to other tools in its class, such as Trello, the most apparent advantage of utilising Wekan is that it enables anyone to easily contribute to and alter the content of the platform. You will have full control over it on your own, eliminating the need to rely on a service provided by a third party. You will also have the ability to host it on your own server, giving you full control over the information you store as well as the flexibility to use it anyway you see appropriate.\nYou will learn how to install Wekan on a server running Debian 10 by reading this tutorial.\nStep 1. Install and Configure Wekan Update system packages.\n# apt update Install snap. Snap is a software distribution and package management system that was developed by Canonical.\n# apt install snapd -y Install Wekan.\n# snap install wekan Establish a web URL root for use with Wekan.\n# snap set wekan root-url=\"http://your_server_ip\" Set Wekan http port.\n# snap set wekan port='80' Reboot the snap server‚Äôs MongoDB service.\n# systemctl restart snap.wekan.mongodb Restart Wekan service on snap.\n# systemctl restart snap.wekan.wekan Check status.\n# ss -tunelp | grep 80 Make the Wekan service auto-start at system boot.\n# snap enable wekan Install MongoDB tools.\n# apt install mongo-tools -y Restart Wekan.\n# systemctl restart snap.wekan.wekan Create a schedule for Snap Auto-updates to run between 2:00 AM and 4:00 AM, at which point updates will be installed automatically.\n# snap set core refresh.schedule=02:00-04:00 Reload snap.\n# snap refresh ","step-2-access-wekan-web-interface#Step 2. Access Wekan Web Interface":"Before you can access the dashboard, you‚Äôll need an administrator account. Using a web browser, go to http://ServerIP/sign-up to visit the sign-up page and create an account. If you choose to create a Wekan account, you will be taken directly to the dashboard after account activation. For instance:\n# http://ServerIP/sign-up "},"title":"How to Install Wekan on Debian 10"},"/utho-docs/docs/linux/how-to-install-wekan-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Congratulations, your server now has Wekan installed. The control panel is now available, and you can start making and managing cards right away.\nI hope you have learned how to install wekan on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install wekan on Debian 12.\nWekan is a free and open-source¬†Kanban Board¬†software that is built on the Meteor JavaScript framework. Its primary purpose is to facilitate the management of projects and tasks through the use of cards. Using Wekan allows you to quickly and simply invite team members to the board, define tasks, and assign them to your member within a certain time frame. Because of all of these features, using Wekan is an excellent solution for enabling project cooperation among engineers or other teams. When a task is divided among members of a team, more productivity is achieved in a shorter amount of time as a direct result of this increased efficiency. It is a web programme that is similar to Trello that allows you to accomplish a wide variety of activities while having a small learning curve and a simplified user interface.\nWhen compared to other tools¬†in its class, such as Trello, the most apparent advantage of utilising Wekan is that it enables anyone to easily contribute to and alter the content of the platform. You will have full control over it on your own, eliminating the need to rely on a service provided by a third party. You will also have the ability to host it on your own server, giving you full control over the information you store as well as the flexibility to use it anyway you see appropriate.\nYou will learn how to install Wekan on a server running Debian 10 by reading this tutorial.\nStep¬†1. Install and Configure Wekan Update system packages.\n# apt update Install snap. Snap is a software distribution and package management system that was developed by Canonical.\n# apt install snapd -y Install Wekan.\n# snap install wekan Establish a web URL root for use with Wekan.\n# nap set wekan root-url=\"http://your\\_server\\_ip\" Set Wekan http port.\n# snap set wekan port='80' Reboot the snap server‚Äôs MongoDB service.\n# systemctl restart snap.wekan.mongodb Restart Wekan service on snap.\n# systemctl restart snap.wekan.wekan Check status.\n# ss -tunelp | grep 80 Make the Wekan service auto-start at system boot.\n# snap enable wekan Restart Wekan.\n# systemctl restart snap.wekan.wekan Create a schedule for Snap Auto-updates to run between 2:00 AM and 4:00 AM, at which point updates will be installed automatically.\n# snap set core refresh.schedule=02:00-04:00 Reload snap.\n# snap refresh Step 2. Access Wekan Web Interface Before you can access the dashboard, you‚Äôll need an administrator account. Using a web browser, go to http://ServerIP/sign-up to visit the sign-up page and create an account. If you choose to create a Wekan account, you will be taken directly to the dashboard after account activation. For instance:\n# http://ServerIP/sign-up "},"title":"How to Install Wekan on Debian 12"},"/utho-docs/docs/linux/how-to-install-wekan-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install wekan on ubuntu 20.04.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install wekan on ubuntu 20.04.\nWekan is a free and open-source Kanban Board software that is built on the Meteor JavaScript framework. Its primary purpose is to facilitate the management of projects and tasks through the use of cards. Using Wekan allows you to quickly and simply invite team members to the board, define tasks, and assign them to your member within a certain time frame. Because of all of these features, using Wekan is an excellent solution for enabling project cooperation among engineers or other teams. When a task is divided among members of a team, more productivity is achieved in a shorter amount of time as a direct result of this increased efficiency. It is a web programme that is similar to Trello that allows you to accomplish a wide variety of activities while having a small learning curve and a simplified user interface.\nWhen compared to other tools in its class, such as Trello, the most apparent advantage of utilising Wekan is that it enables anyone to easily contribute to and alter the content of the platform. You will have full control over it on your own, eliminating the need to rely on a service provided by a third party. You will also have the ability to host it on your own server, giving you full control over the information you store as well as the flexibility to use it anyway you see appropriate.\nYou will learn how to install Wekan on a server running Ubuntu 20.04 by reading this tutorial.","step-1-install-and-configure-wekan#Step 1. Install and Configure Wekan":"Update system packages.\n# apt update Install snap. Snap is a software distribution and package management system that was developed by Canonical.\n# apt install snapd -y Install Wekan.\n# snap install wekan Establish a web URL root for use with Wekan.\n# snap set wekan root-url=\"http://your_server_ip\" Set Wekan http port.\n# snap set wekan port='80' Reboot the snap server‚Äôs MongoDB service.\n# systemctl restart snap.wekan.mongodb Restart Wekan service on snap.\n# systemctl restart snap.wekan.wekan Check status.\n# ss -tunelp | grep 80 Make the Wekan service auto-start at system boot.\n# snap enable wekan Install MongoDB tools.\n# apt install mongodb-clients -y Restart Wekan.\n# systemctl restart snap.wekan.wekan Create a schedule for Snap Auto-updates to run between 2:00 AM and 4:00 AM, at which point updates will be installed automatically.\n# snap set core refresh.schedule=02:00-04:00 Reload snap.\n# snap refresh ","step-2-access-wekan-web-interface#Step 2. Access Wekan Web Interface":"Before you can access the dashboard, you‚Äôll need an administrator account. Using a web browser, go to http://ServerIP/sign-up to visit the sign-up page and create an account. If you choose to create a Wekan account, you will be taken directly to the dashboard after account activation. For instance:\n# http://ServerIP/sign-up "},"title":"How to Install Wekan on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-wine-on-alma-linux/":{"data":{"":" How to install Wine on Alma Linux\nIn this article we will learn how to install¬†wine¬†on Alma Linux 7 and 8. Starting with the very first step. Wine is a compatibility layer that was developed for multiple POSIX-based operating systems, such as Linux, Macintosh, and BSD, to enable these systems to run Windows-based software. Wine is available for free and is open source. Wine is a programme that, in its most basic form, automatically translates Windows API calls into POSIX calls. This eliminates the speed and memory penalties that are associated with using other methods and enables you to integrate Windows applications onto your desktop in an uncluttered manner. It is simple to understand and not too difficult to put into practise. Also, the installation process is relatively straightforward in virtually all of the well-known Linux variants.\nStep 1: Update your Server\nyum update ","step-2-install-epel-release-repository#Step 2: Install EPEL Release Repository":"You will only be able to obtain the wine package through the EPEL repository. In order to install and activate this repository, you will need to use the yum install epel-release command, as demonstrated further below.\nyum install epel-release ","step-3-install-wine-package#Step 3: Install Wine package":"With the yum install wine command, which will be demonstrated further down, you will be able to install the wine package from the EPEL repository. The package, as well as all of its dependencies, will be downloaded and installed as a result of this action.\nyum install wine ","step-4-verify-installation-of-package#Step 4: Verify Installation of package":"After the installation has been completed successfully, you will be able to validate all of the wine-related packages that have been installed by querying the rpm database using the rpm -qa | grep -i wine command, as will be demonstrated below.\nrpm -qa | grep -i wine ","step-5-check-version-of-package#Step 5: Check Version of package":"Using the wine ‚Äîversion command, as seen below, is another option for determining the currently installed version of wine.\nwine --version I really hope that you‚Äôve got all of those steps down for how to install¬†wine¬†on¬†Alma Linux"},"title":"How to install Wine on Alma Linux"},"/utho-docs/docs/linux/how-to-install-wine-on-rockylinux-8/":{"data":{"":"","description#Description":"In this article, we will learn how to install¬†wine¬†on RockyLinux8. Starting with the very first step. Wine is a compatibility layer that was developed for multiple POSIX-based operating systems, such as Linux, Macintosh, and BSD, to enable these systems to run Windows-based software. Wine is available for free and is open source. Wine is a programme that, in its most basic form, automatically translates Windows API calls into POSIX calls. This eliminates the speed and memory penalties that are associated with using other methods and enables you to integrate Windows applications onto your desktop in an uncluttered manner. It is simple to understand and not too difficult to put into practise. Also, the installation process is relatively straightforward on virtually all of the well-known Linux variants. In this section, we will go over the steps necessary to install Wine on a server.","step-1-update-server#Step 1: Update Server":" yum update ","step-2-install-epel-release-repository#Step 2: Install EPEL Release Repository":"You will only be able to obtain the wine package through the EPEL repository. In order to install and activate this repository, you will need to use the yum install epel-release command, as demonstrated further below.\nyum install epel-release ","step-3-install-wine-package#Step 3: Install Wine package":"With the yum install wine command, which will be demonstrated further down, you will be able to install the wine package from the EPEL repository. The package, as well as all of its dependencies, will be downloaded and installed as a result of this action.\nyum install wine ","step-4-verify-installation-of-package#Step 4: Verify Installation of package":"After the installation has been completed successfully, you will be able to validate all of the wine-related packages that have been installed by querying the rpm database using the rpm -qa | grep -i wine command, as will be demonstrated below.\nrpm -qa | grep -i wine ","step-5-check-version-of-package#Step 5: Check Version of package":"Using the wine ‚Äîversion command, as seen below, is another option for determining the currently installed version of wine.\nwine --version I really hope that you‚Äôve got all of those steps down for how to install¬†wine¬†on RockyLinux8"},"title":"How to install Wine on RockyLinux 8"},"/utho-docs/docs/linux/how-to-install-wine-on-ubuntu-20-04/":{"data":{"":"\nDescription\nIn this article we will learn How to Install Wine on Ubuntu 20.04 Users have frequently had the requirement to execute Windows applications on POSIX-based operating systems such as Linux, MacOS, and BSD. In order to fulfil this requirement, an interoperability layer was developed, which enables Windows programmes to execute on various flavours of Linux. This compatibility layer is referred to as Wine in the industry. A significant number of people from all around the world use it. Wine is a POSIX-compatible software package that is both free and open-source, and it can be rapidly installed on any computer running a POSIX-based operating system. The steps necessary to install wine on a computer are as follows: running Ubuntu 20.04 LTS.\nPlease proceed by following the steps below. How to Install Wine on Ubuntu 20.04.","step-1-update-server#Step 1: Update Server":" sudo apt update Execute the sudo apt upgrade command in order to check if any of the tools that have been installed require an update.","step-2-install-wine#Step 2: Install Wine":"You can choose to install wine using any of the following methods, depending on the architecture of your computer.\nsudo apt install wine You have the option of installing wine on a 64-bit architecture from the standard Ubuntu repository by using the command sudo apt install wine64, which is displayed down below..\nsudo apt install wine64 ","step-3-verify-version#Step 3: Verify Version":"Use the wine ‚Äìversion command, as demonstrated further down, to determine which version of Wine is installed on a 32-bit operating system.\nwine --version ","step-4-uninstall-wine#Step 4: Uninstall Wine":"When you are finished utilising Wine, you can remove it from your system using any of the following methods; which method you use will depend on the architecture of your machine and how you installed Wine in the first place.\nsudo apt remove wine I really hope that you have a good grasp on everything that has been discussed in this essay that how to Install Wine on Ubuntu 20.04.\nMust Read :- https://utho.com/docs/tutorial/how-to-install-jshon-on-ubuntu-20-04/"},"title":"How to Install Wine on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-wmclock-on-ubuntu-20-04/":{"data":{"":"\nDescription\nIn this article, we will acquire new knowledge how to Install wmclock on Ubuntu 20.04. The free and open-source wmclock is a dockable clock for the Window Maker window manager. It is essentially an applet created by Alfredo Kojima to display the date and time in a dockable component similar to the NextStep(tm) operating system‚Äôs clock. It is also quite simple to install on the majority of popular Linux distributions. Here are the instructions for installing wmclock on Ubuntu 20.04.\nPlease proceed by following the steps below. How to Install wmclock on Ubuntu 20.04..","step-1-update-your-server#Step 1: Update Your Server":"First, using the sudo apt update and sudo apt upgrade commands, bring all of the installed packages up to speed with the most recent version available in the Ubuntu repository. This will get you started.\napt update \u0026\u0026 sudo apt upgrade How to Install wmclock on Ubuntu 20.04","step-2-install-wmclock-package#Step 2: Install wmclock package":"Using the command apt install wmclock, which is given below, you may download and install wmclock from the default repository that comes with Ubuntu. The program, as well as all of its dependencies, will be downloaded and installed as a result of this action.\napt install wmclock ","step-3-verify-installed-package#Step 3: Verify Installed package":"After a successful installation, use the dpkg -L wmclock command to verify the installed files path, as shown below.\ndpkg -L wmclock ","step-4-show-version#Step 4: Show Version":"Use the wmclock ‚Äìversion command, as shown below, to determine the current installed version of wmclock.\nwmclock --version ","step-5-uninstall-wmclock#Step 5: Uninstall wmclock":"When you‚Äôre finished with wmclock, you can remove it from your system by running the sudo apt remove wmclock command, as shown below.\napt remove wmclock I really hope that you have a complete understanding of all the processes to How to Install wmclock on Ubuntu 20.04\nMust Read :- https://utho.com/docs/tutorial/how-to-install-ncurses-library-on-ubuntu-20-04/\nThank You"},"title":"How to Install wmclock on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-wordpress-with-lemp-on-centos-server/":{"data":{"":"","1-setup-the-mysql-database-for-wordpress#1. Setup the MySQL database for wordpress":"","2-download-and-install-the-wordpress#2. Download and install the Wordpress":"","3-configure-the-wordpress#3. Configure the Wordpress.":"","4-complete-the-installation-of-wordpress#4. Complete the installation of Wordpress":"\nWordPress is one of the most popular content management systems (CMS) on the internet right now. It lets users set up flexible blogs and websites using a MySQL backend and PHP processing. At a very high rate, WordPress is used by both new and experienced engineers. It is a great choice for quickly getting a website up and running. After setting up WordPress for the first time, almost all website management can be done through its graphical interface. This and other features make WordPress a great choice for websites that are meant to grow.\nLEMP stack is a set of free software that helps web servers start up and run. The letters stand for the words Linux, Nginx, MySQL, and PHP. Arch Linux is already running on the server, so that part is taken care of.\nPrerequisites Super user or any normal user with SUDO privileges. The yum repository configured to install the services. Already LEMP installed services. Please follow¬†this link¬†to install LEMP on CentOS server. 1. Setup the MySQL database for wordpress Information for the website and its users is managed by WordPress using a relational database. This feature may be provided by MariaDB, a derivative of MySQL, which is already installed. However, we must create a database and a user for WordPress to utilise.\n# mysql -u root -p \u003e CREATE DATABASE wordpressdb;\n\u003e CREATE USER wordpressuser@localhost IDENTIFIED BY ‚Äòpassword‚Äô;\n\u003e GRANT ALL PRIVILEGES ON wordpressdb.* TO wordpressuser@localhost IDENTIFIED BY ‚Äòpassword‚Äô;\nHere, in above MySQL commands, we have created a new database named as wordpressdb, a user named as wordpressuser which will be identified by the password you will provide while executing the command. Please note that all these keywords are just couple of variables, therefore you can choose them as you please.\nIn last command we have granted the rights to the user ‚Äòwordpressuser‚Äô on ‚Äòwordpressdb‚Äô database, which is identified by password you will provide later in the same command.\nNow save the changes and exit from mysql\n\u003e FLUST PRIVILEGES\n\u003e EXIT\n2. Download and install the Wordpress One PHP module has to be installed before we download WordPress in order for it to function correctly. WordPress won‚Äôt be able to resize photos to make thumbnails without this module. Using yum, we can get the package straight from the CentOS default repositories:\n# yum install php-gd Now we need to restart nginx service so that it recognizes the new module:\n# systemctl restart nginx Now download the latest version of the wordpress by wget command and extract the newly downloaded wordpress by tar command.\n# wget http://wordpress.org/latest.tar.gz tar -zxvf latest.tar.gz After extracting the latest.tar.gz file, you must see a new directory named as wordpress. Move the content of this directory to your website directory or you can move the content to the default page directory on your server.\n# mv wordpress/* /usr/share/nginx/html/ Please note that, in some case, nginx uses the same root directory of your website same as apache/ httpd which /var/www/html. You can ensure this by looking at the keyword ‚Äòroot‚Äô in /etc/nginx/nginx.conf.\n3. Configure the Wordpress. Later, a web interface will be used to finish the majority of WordPress settings. To make sure that WordPress can connect to the MySQL database that we set up for it, we must do several tasks from the command line.\nWordPress‚Äôs primary configuration file is known as wp-config.php. The default installation comes with an example configuration file that mostly matches the values we need. To enable WordPress to identify and use the file, all we need to do is copy it to the place designated by default for configuration files:\n# cd /usr/share/nginx/html cp wp-config-sample.php wp-config.php The parameters that store the data from our database are the only parts of this file that need to be changed. The DB NAME, DB USER, and DB PASSWORD variables must be changed in the MySQL settings section for WordPress to successfully connect and authenticate to the database that we built.\nWith the data from the database you established, fill in the values of these parameters. It should seem as follows:\nChanges must be made in wp-config.php file\n4. Complete the installation of Wordpress Now goto your favourite browser and hit your server ip.\nhttp://serverip_or_domain-name Browser page after hitting serverip\nHere, just click on Let‚Äôs go button.\nDatabase Setup on wordpress\nHere just fill out your database details, which you have created in step 1 for your wordpress and click on submit button.\njust click on ‚Äòinstalling now‚Äô link\nHere, on the this screen just click on installing now.\nEnter the required configuration details here\nNow quickly fill up the required details . Do not forget to copy the password after showing it. It will be used to login in next step. After clicking on ‚ÄúInstall WordPress‚Äù you will see the below login page.\nLogin page of Wordpress\nNow login using the credentials which you have copied in previous step.\nCongratulation!!! Your wordpress is setup\nAnd that is it!! You have successfully installed the wordpress on your CentOS server using LEMP.\nFollow the Microhost documents to understand and learn cool and new features like this.","prerequisites#Prerequisites":"","tar--zxvf-latesttargz#tar -zxvf latest.tar.gz":""},"title":"How to install Wordpress with LEMP on CentOS server"},"/utho-docs/docs/linux/how-to-install-wordpress-with-lemp-on-ubuntu/":{"data":{"":"","1--configure-mysql-user-for-wordpress#1 ‚Äî Configure MySQL User for WordPress":"","2--install-the-additional-php-extensions#2 ‚Äî Install the Additional PHP Extensions.":"","3--configure-nginx#3 ‚Äî Configure Nginx":"","4--download-and-configure-the-wordpress#4- Download and configure the Wordpress":"","apt-install-php-curl-php-gd-php-intl-php-mbstring-php-soap-php-xml-php-xmlrpc-php-zip#apt install php-curl php-gd php-intl php-mbstring php-soap php-xml php-xmlrpc php-zip":"","apt-install-wget--y#apt install wget -y":"","chmod--r-755-varwwwhtml#chmod -R 755 /var/www/html/":"","cp-wp-config-samplephp-wp-configphp#cp wp-config-sample.php wp-config.php":"\nWordPress is one of the most popular content management systems (CMS) on the internet right now. It lets users set up flexible blogs and websites using a MySQL backend and PHP processing. At a very high rate, WordPress is used by both new and experienced engineers. It is a great choice for quickly getting a website up and running. After setting up WordPress for the first time, almost all website management can be done through its graphical interface. This and other features make WordPress a great choice for websites that are meant to grow. In this article I mentioned 4 Easy-to-follow steps for installing WordPress with LEMP on Ubuntu‚Ä¶\nLEMP stack is a set of free software that helps web servers start up and run. The letters stand for the words Linux, Nginx, MySQL, and PHP. Arch Linux is already running on the server, so that part is taken care of.\nPrerequisites Super user or any normal user with SUDO privileges. apt repository configured to install the services. Already LEMP installed services. Please follow this link to install LEMP on Ubuntu 1 ‚Äî Configure MySQL User for WordPress WordPress uses MySQL to manage and store information about the site and its users. Even though MySQL is already set up, let‚Äôs make a database and a user for WordPress to use.\nIf MySQL is set up to use the auth socket authentication plugin, which is the default, you can log in to the MySQL administrative account using below command.\n# mysql -u root -p \u003e CREATE DATABASE wordpressdb DEFAULT CHARACTER SET utf8 COLLATE utf8_unicode_ci; \u003e CREATE USER 'wordpressuser'@'localhost' IDENTIFIED BY 'password'; \u003e GRANT ALL ON wordpressdb.* TO 'wordpressuser'@'localhost'; First, we‚Äôve made a separate database that can be run by WordPress. You can call this whatever you want, but to keep things simple, we‚Äôll use ‚ÄúWordPressdb‚Äù in this guide. Next, we have created a new MySQL user account that will only be used to work with our new database. From a management and security point of view, it‚Äôs a good idea to make databases and accounts that only serve one purpose. We will call ourselves wordpressuser. Please ensure that you have used an alphanumeric password for this user. Next,¬†grant access wordpress user to the database you created.¬†Now you can exit from mysql.\n\u003e exit 2 ‚Äî Install the Additional PHP Extensions. When setting up the LEMP stack, PHP and MySQL could only talk to each other with a very small number of extensions. Many of WordPress‚Äôs plugins use extra PHP extensions, and this tutorial will teach you how to use a few more.\nLet‚Äôs use WordPress to download and install some of the most popular PHP extensions by typing:\n# apt update apt install php-curl php-gd php-intl php-mbstring php-soap php-xml php-xmlrpc php-zip Now after successfully installing the php extension, just restart php-fpm services.\n# systemctl restart php7.4-fpm 3 ‚Äî Configure Nginx First of all, We need to put index.php as the first value of our index directive so that when a directory is requested, files named index.php are served if they are available.\n# vi /etc/nginx/sites-enabled/default Make the index.php as first option\nNow, let‚Äôs check our configuration for syntax errors by typing:\n# nginx -t Now restart the nginx service.\n# systemctl restart nginx 4- Download and configure the Wordpress Now that your server software is set up, let‚Äôs get WordPress downloaded and set up. For security reasons, you should always get the latest version of WordPress directly from the project‚Äôs website.\n# wget wget https://wordpress.org/latest.zip If you have no binary of wget( wget command not found ), then you can install by using\napt install wget -y And now extract the newly downloaded file by using tar command, most probably, it should be available on your server. Therefore to extract the file, use the below command. unzip latest \u003e If you have no binary of unzip( unzip command not found ), then you can install by using \u003e \u003e ``` # apt install unzip -y output after unziping the latest downloaded wordpress\nlatest created wordpress directory\nAfter extracting the file, move this file‚Äôs content to /var/www/html directory\n# mv wordpress/* /var/www/html/ Next, give the webserver ownership of the WordPress files by giving the website directory the right permissions.\n# chown -R www-data:www-data /var/www/html/ chmod -R 755 /var/www/html/ Now, go to the /var/www/html/ directory and copy wp-config-sample.php to wp-config.php. Also, make sure to get rid of the nginx index page that comes by default.\n# cd /var/www/html cp wp-config-sample.php wp-config.php Then, update the highlighted details with your database info in the MySQL settings section\n# vi wp-config.php updating wp-config.php\nNow, restart the nginx and mariadb services using the commands below:\n# systemctl restart nginx mariadb Now go to you browser and enter your serverip\nhttp://server-ip Select the language in this page\nWith this article, I hope you now have a better understanding of how to install WordPress with LEMP on Ubuntu‚Ä¶","prerequisites#Prerequisites":"","unzip-latest#unzip latest":""},"title":"How to Install WordPress with LEMP on Ubuntu"},"/utho-docs/docs/linux/how-to-install-xrdp-server-on-ubuntu-22-04/":{"data":{"":" How to Install Xrdp Server on Ubuntu 22.04\nIn this tutorial, we will learn how to install Xrdp server on Ubuntu 22.04 and configure it. Xrdp is an open-source implementation of the Microsoft¬†Remote Desktop Protocol¬†(RDP) that allows you to control a remote system graphically .\nInstalling Desktop Environment\nUbuntu servers are operated via the command line and do not have pre-installed desktop environment.\nThere are multiple desktop environments available in Ubuntu repositories that you can select. Here we are going to install Gnome, which is the default desktop environment in¬†Ubuntu 22.04.\nInstall Gnome\nTo install the Ubuntu desktop environment, run the command:\n$ sudo apt update $ sudo apt install ubuntu-desktop Installing Xrdp\nXrdp is incuded in the default Ubuntu repositories. To install it, run:\n$ sudo apt install xrdp Once the installation is complete, the Xrdp service will automatically start. You can verify it by typing:\n$ sudo systemctl status xrdp Output will look like this ‚Äì\nRunning service of XRDP\nThe output shows that, the xrdp daemon is active and running.\nThe Xrdp daemon listens on port 3389 on all interfaces. If you are using firewall on ubuntu server then you need to open Xrdp port.\n$ sudo ufw allow 3389 Thereafter, reload the firewall and confirm if the port has been opened.\n$ sudo ufw reload Connecting to the Xrdp Server\nNow that you have set up your Xrdp server, its time to open your Xrdp client and connect to the server.\nIf you have a Windows PC, you can use the default RDP client. This will open up the RDP client. In the ‚ÄúComputer‚Äù field, enter the remote server IP address and click ‚ÄúConnect‚Äù\nAccess your server\nOn the login screen, enter your username and password and click ‚ÄúOK‚Äù.\nYou can see the default Gnome desktop after logging in. This is what it should look like:\nIf you are running macOS, you can install the Microsoft Remote Desktop application from the Mac App Store. Linux users can use an RDP client such as Remmina\nConfiguring a remote desktop allows you to manage your Ubuntu 22.04 server from your local machine through an easy to use graphic interface."},"title":"How to Install Xrdp Server on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-install-xrdp-server-remote-desktop-on-ubuntu-20-04/":{"data":{"":"Xrdp is an open-source implementation of the Microsoft Remote Desktop Protocol (RDP) that allows you to control a remote system graphically .\nIn this tutorial, we will learn how to install and configure Xrdp server on Ubuntu 20.04.\nInstalling Desktop Environment\nUbuntu servers are operated via the command line and do not have pre-installed desktop environment.\nThere are multiple desktop environments available in Ubuntu repositories that you can select. Here we are going to install Gnome, which is the default desktop environment in Ubuntu 20.04.\nInstall Gnome\nTo install the Ubuntu desktop environment, run the command:\n$ sudo apt update $ sudo apt install ubuntu-desktop Installing Xrdp\nXrdp is incuded in the default Ubuntu repositories. To install it, run:\n$ sudo apt install xrdp Once the installation is complete, the Xrdp service will automatically start. You can verify it by typing:\n$ sudo systemctl status xrdp Output will look like this ‚Äì\nThe output shows that, the xrdp daemon is active and running.\nThe Xrdp daemon listens on port 3389 on all interfaces. If you are using firewall on ubuntu server then you need to open Xrdp port.\n$ sudo ufw allow 3389 Connecting to the Xrdp Server\nNow that you have set up your Xrdp server, its time to open your Xrdp client and connect to the server.\nIf you have a Windows PC, you can use the default RDP client. This will open up the RDP client. In the ‚ÄúComputer‚Äù field, enter the remote server IP address and click ‚ÄúConnect‚Äù\nOn the login screen, enter your username and password and click ‚ÄúOK‚Äù.\nYou can see the default Gnome desktop after logging in. This is what it should look like:\nIf you are running macOS, you can install the Microsoft Remote Desktop application from the Mac App Store. Linux users can use an RDP client such as Remmina\nConfiguring a remote desktop allows you to manage your Ubuntu 20.04 server from your local machine through an easy to use graphic interface."},"title":"How to Install Xrdp Server (Remote Desktop) on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-install-zabbix-4-4-in-centos-7/":{"data":{"mysql--u-root--p#mysql -u root -p":"","rpm---importhttprepozabbixcomrpm-gpg-key-zabbixhttprepozabbixcomrpm-gpg-key-zabbix##¬†rpm \u0026ndash;import¬†\u003ca href=\"http://repo.zabbix.com/RPM-GPG-KEY-ZABBIX\"\u003ehttp://repo.zabbix.com/RPM-GPG-KEY-ZABBIX\u003c/a\u003e":"","rpm--uvhhttpsrepozabbixcomzabbix44rhel7x86_64zabbix-release-44-1el7noarchrpmhttpsrepozabbixcomzabbix44rhel7x86_64zabbix-release-44-1el7noarchrpm#rpm -Uvh¬†\u003ca href=\"https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm\"\u003ehttps://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm\u003c/a\u003e":"","systemctl-restart-httpd#systemctl restart httpd":"","vi-etchttpdconfdzabbixconf#vi /etc/httpd/conf.d/zabbix.conf":"","vi-etchttpdconfdzabbixconf-1#vi /etc/httpd/conf.d/zabbix.conf":"","vi-etczabbixzabbix_serverconf#vi /etc/zabbix/zabbix_server.conf":"\n1.¬†Login to the server via Putty (SSH port 22)\n2.¬†Set the SELinux in disabled mode, use the following command and reboot your server.\n# # sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config 3. Restart the server by running\n``` # reboot\n4**. Install apache and mariadb** yum update yum install httpd mariadb-server -y **5\\. Start Apache and mariaDB Services.** #¬†systemctl enable httpd \u0026\u0026 systemctl start httpd\n#¬†systemctl enable mariadb \u0026\u0026 systemctl start mariadb\n**6.¬†Set Mariadb root Password:** a.¬†Run:¬†#¬†[mysql\\_secure\\_installation](https://manastri.blogspot.com/2019/09/securing-mysql-mariadb-with.html) b.¬†It'll ask for setting the root password. Press Y to do so. ![](images/Screenshot_7-9.png) c.¬†disallow remote¬†root login d.¬†Remove anonymous user e.¬†it'll drop test databases ![](images/Screenshot_8-11.png) **7.¬†Install Zabbix Server with MySQL** #¬†rpm ‚Äìimport¬†http://repo.zabbix.com/RPM-GPG-KEY-ZABBIX rpm -Uvh¬†https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm 8**.¬†Now use the below command to install Zabbix and necessary packages** #¬†yum install zabbix-server-mysql zabbix-web-mysql zabbix-agent zabbix-get zabbix-sender zabbix-java-gateway -y\n**9\\. Edit PHP timezon**e vi /etc/httpd/conf.d/zabbix.conf **9\\. Edit PHP timezone** vi /etc/httpd/conf.d/zabbix.conf \u003cfigure\u003e ![](images/Screenshot_12-1-1.png) \u003cfigcaption\u003e Save the file and exit. \u003c/figcaption\u003e \u003c/figure\u003e 1**0.¬†Restart httpd service using the below command**: systemctl restart httpd **11.¬†Edit create and import initial zabbix database and user:** mysql -u root -p ![](images/Screenshot_14-1-1.png) \\[filecode file\\] Enter password:Welcome to the MariaDB monitor.¬†Commands end with ; or \\\\g.Your MariaDB connection id is 10Server version: 5.5.60-MariaDB MariaDB ServerCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\\\h' for help. Type '\\\\c' to clear the current input statement. **MariaDB \\[(none)\\]\u003eCREATE DATABASE zabbixdb CHARACTER SET utf8 COLLATE utf8\\_bin;** Query OK, 1 row affected (0.00 sec) **MariaDB \\[(none)\\]\u003eGRANT ALL PRIVILEGES ON zabbixdb.\\* TO zabbixuser@localhost IDENTIFIED BY \"**a\\_strong\\_password**\";** Query OK, 0 rows affected (0.00 sec) **MariaDB \\[(none)\\]\u003eFLUSH PRIVILEGES;** Query OK, 0 rows affected (0.00 sec) **MariaDB \\[(none)\\]\u003eexit** Bye \\[/filecode\\] **12.**¬†**After creating the Zabbix database and user we need to import the zabbix initial database using the below commands:** #¬†zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -u zabbixuser -p zabbixdb\n**13.**¬†**Now we need to edit database configuration in the Zabbix server configuration file zabbix\\_server.conf:** vi /etc/zabbix/zabbix_server.conf Specify the database name for zabbix , database user name and the password DBHost=localhost DBUser=zabbixuser DBUser=zabbixuser DBPassword=YOURPASSWORD **14.**¬†**Now enable and start zabbix service:** #¬†systemctl enable zabbix-server\n``` # systemctl start zabbix-server\u0026\u0026systemctl enable zabbix-agent #¬†systemctl start zabbix-agent 15.¬†Setup Zabbix Web Frontend\nNavigate to¬†http://ip_address/zabbix¬†or¬†http://host_name/zabbix\nclick Next\n16.¬†Please enter DataBase details:\n18.¬†Summary:\n19.¬†Finish the installation:\n20.¬†Login Prompt:\nDefault username and password is ‚ÄúAdmin‚Äù \u0026 ‚Äúzabbix‚Äù\nThank You :)","yum-install-httpd-mariadb-server--y#yum install httpd mariadb-server -y":"","yum-update#yum update":""},"title":"How to install Zabbix 4.4 in CentOS 7"},"/utho-docs/docs/linux/how-to-install-zabbix-agent-on-centos-7/":{"data":{"":"\nDescription\nWhen a native Zabbix agent is built in the C programming language, it has the capability of running on a variety of supported platforms, such as Linux, UNIX, and Windows, and collecting data from a device about its CPU consumption, memory usage, disc usage, and network interface usage.\nThe Zabbix Agent Installation Guide for CentOS 7. This is the Zabbix agent, and it collects all data by using the agent‚Äôs configuration file. So let‚Äôs get started with this instruction for step by step, shall we?","add-an-rpm-repository-for-the-zabbix-agent-installation#Add an RPM repository for the Zabbix agent installation.":" # rpm -Uvh¬†[https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm](https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm) ","after-that-run-the-install-command#After that, run the install command.":" # yum install zabbix-agent -y ","check-status-using-below-command#Check status using below command.":" # systemctl status zabbix-agent Conclusion\nZabbix is a piece of open source software that may be used as a monitoring tool for a variety of IT components, such as networks, servers, virtual machines (VMs), and cloud services. Monitoring indicators like as network use, CPU load, and disc space consumption are made available via Zabbix.When a native Zabbix agent is built in the C programming language, it has the capability of running on a variety of supported platforms, such as Linux, UNIX, and Windows, and collecting data from a device about its CPU consumption, memory usage, disc usage, and network interface usage.\nWhat are the features of Zabbix?\nZabbix is compatible with a wide variety of protocols, making it suitable for the remote monitoring of services.\nWeb monitoring. Scriptable synthetic monitoring. SNMP (v1/2c/3) polling and trapping. Java application monitoring. IPMI. SSH/Telnet checks. Thank You","now-insert-the-below-content-into-the-zabbix-agentdconf-file-using-vi#now insert the below content into the zabbix agentd.conf file using vi.":" PidFile=/var/run/zabbix/zabbix_agentd.pid LogFile=/var/log/zabbix/zabbix_agentd.log LogFileSize=1024 Server=103.127.29.54, 89.47.59.30 ServerActive=127.0.0.1\nHostname=ssd14blrxc Include=/etc/zabbix/zabbix_agentd.d/*.conf Then, hit Esc to save, followed by typing :wq and pressing Enter to finish.\nStart the zabix agent service by entering the following command.\n# systemctl enable zabbix-agent ","open-the-above-mentioned-file-using-vi-command#open the above mentioned file using vi command.":" # vi zabbix_agentd.conf ","then-open-the-configuration-file-that-is-located-below#Then open the configuration file that is located below.":" # cd /etc/zabbix/ "},"title":"How To Install Zabbix Agent On Centos 7"},"/utho-docs/docs/linux/how-to-install-zimbra-on-ubuntu-20-04-lts/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install Zimbra on Ubuntu 20.04 LTS.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to install Zimbra on Ubuntu 20.04 LTS.\nThe Zimbra Collaboration Server is a component of open-source software that facilitates collaboration and includes both a mail server and a web client. It enables the use of e-mail, calendaring, the creation of user lists and distribution lists, the sharing of files, chatting, as well as the management of various mail server activities. Zimbra Collaboration Server is compatible with many different operating systems and platforms, such as Ubuntu 12.04, Ubuntu 14.04, Ubuntu 18.04, and Ubuntu 20.04, as well as Redhat Enterprise Linux 6, Redhat Enterprise Linux 7, CentOS 6, and CentOS 7. In addition to this, it is supported on cloud platforms such as VMware vCloud Director and VMware vCloud Air, as well as virtualization platforms such as VMware vSphere, XenServer 6, and KVM.","step-1-update-and-upgrade-existing-packages#Step 1: Update and upgrade existing packages":"To begin, you will need to update and upgrade the packages that are already installed. In order to accomplish this, enter the following commands into Terminal:\n# apt update # apt upgrade ","step-2-change-hostname#Step 2: Change hostname":" # hostnamectl set-hostname your_domain_name Use the Terminal command shown below to edit the file right now:\n# vi /etc/hosts After entering the following information, save and exit the hosts file.\n# 192.168.72.167 mail.your_domain_name mail Where mail.your_domain_name is the fully qualified domain name and 192.168.72.167 is the IP address of your Zimbra server.\n127.0.0.1 localhost 127.0.1.1 ubuntu The following lines are desirable for IPv6 capable hosts\nff02::1 ip6-allnodes ff02::2 ip6-allrouters 192.168.72.167 your_domain_name 192.168.72.167 mail 192.168.72.167 mail.your_domain_name 192.168.72.167 webmail.your_domain_name sane and exit the file by press escape :wq","step-3-install-dns-server#Step 3: Install DNS server":"We will set up dnsmasq as our DNS server in this phase. However, because systemd-resolve uses port 53, we must first disable it before installing dnsmasq. On port 53, the DNS server dnsmasq also operates, which may result in a port conflict.\n# systemctl disable systemd-resolved # systemctl stop systemd-resolved Now delete the symlink file resolv.conf:\n# rm¬†/etc/resolv.conf then use the command below to build a fresh resolv.conf file:\n# sh¬†-c¬†'echo nameserver 8.8.8.8 \u003e\u003e /etc/resolv.conf' To install dsmasq, use the command following in Terminal:\n# apt¬†install¬†dnsmasq After dnsmasq installation is complete, use the command below to modify the dnsmasq configuration file:\n# vi /etc/dnsmasq.conf Add the lines below at the end of the configuration file:\nserver=192.168.72.167 domain=your_domain_name mx-host= your_domain_name, mail.your_domain_name, 5 mx-host=mail.your_domain_name, mail.your_domain_name, 5 listen-address=127.0.0.1 sane and exit the file by press escape :wq\nThen restart the dnsmasq service in Terminal using the following command:\n# systemctl restart dnsmasq ","step-4-downloading-and-installing-zimbra-collaboration-tool#Step 4: Downloading and installing Zimbra Collaboration Tool":"Similarly, run the following wget command to download Zimbra:\n# wget -c [https://files.zimbra.com/downloads/8.8.15_GA/zcs-8.8.15_GA_4179.UBUNTU20_64.20211118033954.tgz](https://files.zimbra.com/downloads/8.8.15_GA/zcs-8.8.15_GA_4179.UBUNTU20_64.20211118033954.tgz) The format of the downloaded file will be.tgz. Enter the following command in Terminal to extract the file:\n# tar¬†-xvzf [zcs-8.8.15_GA_4179.UBUNTU20_64.20211118033954.tgz](https://files.zimbra.com/downloads/8.8.15_GA/zcs-8.8.15_GA_4179.UBUNTU20_64.20211118033954.tgz) Follow the steps below in order to access the extracted folder using the cd command:\n# cd [zcs-8.8.15_GA_4179.UBUNTU20_64.20211118033954](https://files.zimbra.com/downloads/8.8.15_GA/zcs-8.8.15_GA_4179.UBUNTU20_64.20211118033954.tgz) Then, execute the installation as follows:\n# ./install.sh To accept the accompanying Software License agreement, press y. Then press y again to install using Zimbra‚Äôs package repository.\nExcept for zimbra-imapd, which is beta-only and not installed by default, type y to install all of the packages.\nAfter being informed that changes will be made to the system, choose y to proceed with the installation.\nWhen setup is complete, you‚Äôll see this screen and be prompted to set up any remaining objects.\nHere, we shall set the unconfigured administrator password for Zimbra. The Admin Password is likewise shown as * in the zimbra-store section. To access the zimbra-store area, press 7 on your keyboard.\nNow configure the Admin Password by pressing 4. You will be requested to set the admin password. Enter any passphrase (with a minimum of 6 characters).\nNow, press a to apply the settings, then press y again to save the settings. When prompted that the system will be updated, press y.\nNow that the setting is complete, you will get the following page; press Enter to go.\nNow, the Zimbra mail server installation is complete.\nUsing the below command will help you start all the services in Zimbra.\n# systemctl start imap.service Now hit the below link in your browser and login to your Zimbra account.\n[Console]# https://Your_server_IP:7071 [/console]"},"title":"How to install Zimbra on Ubuntu 20.04 LTS"},"/utho-docs/docs/linux/how-to-locate-files-that-have-suid-and-sgid-permissions/":{"data":{"":"","#":"\nDescription In this section, we will walk you through the process of locating files that have SUID (Setuid) and SGID (Setgid) configured, in addition to explaining auxiliary file permissions, which are referred to as ‚Äúspecial permissions‚Äù in Linux. To Find SUID and SGID Permissions, first know What is SUID and SGID ?","how-to-find-files-with-suid-set#How to Find Files with SUID Set":"Using the -perm (print files only with permissions set to 4000) option, the following command is an example that will locate all files in the current directory that have the SUID setting.\n# find . -perm /4000 When you use the ls command with the -l option (which stands for long listing), you will be able to see the permissions that are set on the files that are listed.","how-to-find-sgid-set-files#How to Find SGID Set Files":"Enter the following command to search for files that have the SGID parameter set.\n# find . -perm /2000 Run the following command to search for files that contain a setting for both SUID and SGID.\n# find . -perm /6000 You were shown how to locate files in Linux that have the SUID (Setuid) and SGID (Setgid) settings in place. If you have any questions\nThank You ","what-is-suid-and-sgid#What is SUID and SGID?":"SUID is a special file permission that can only be applied to executable files. It gives other users the ability to run the file with the permissions that would normally be granted to the file‚Äôs owner. You will observe a s (which stands for SUID), which denotes a particular authorization for the user to execute, rather than the usual x, which represents execute permissions.\nOther users are granted the ability to inherit the effective GID of the file group owner through the use of a unique file permission known as SGID. This permission can be applied to executable files as well. Also, instead of the typical x, which denotes execute permissions, you will find an s, which stands for the SGID notation for special permission for group user.\nUsing the search command, let‚Äôs investigate how to locate files that have the SUID and SGID parameters defined.\nThe syntax consists of the following:\n# find directory -perm /permissions NOTE: If you are administering your system as a regular user, you will need to use the sudo command in order to get root rights so that you may access and list certain directories and files. These directories and files include /etc, /bin, and /sbin, amongst others."},"title":"How to Locate Files That Have SUID and SGID Permissions"},"/utho-docs/docs/linux/how-to-make-a-large-file-in-linux/":{"data":{"":"\nDescription\nIn this article we will guide How to Make a Large File in Linux.\nIn this article you will know to Make a Large File in Linux. There are many reasons why you might want to make a big file in Linux. One scenario could be testing the tools for keeping track of disc space to see if they work right. If your system doesn‚Äôt have a lot of data, it will be hard to test the disc monitoring features. So, when this happens, you can quickly make a big fake file to test your disc monitoring software.\nIn the same way, you might need to make a big file on your Linux system for some other reason. So, we‚Äôll look at some of the easy-to-find tools that can be used to do this job quickly and easily.\nFollow the below steps to learn How to Make a Large File in Linux","seq-command#seq command":"This is likely one of the slower ways to do something. Using this method, you can add long sequences to the file, which will make it much bigger in the end. You can use the sequence in a way that makes the size you want. The bigger the sequence, the bigger the size. For example, here we‚Äôre using the seq 1000000000 ¬ª large file command to write sequence 1000000000 to a Microhost. This will make a file about 15GB in size, but the process will be much slower than the others. Check out the MAN page for more options.\nseq 1000000000 \u003e\u003e Microhost du -sh Microhost I hope you really understand all the steps How to Make a Large File in Linux.\nMust Read:- https://utho.com/docs/wp-admin/post.php?post=8387\u0026action=edit\nThankyou","truncate-command#truncate command":"The truncate command is another very useful tool that you might want to use. The truncate -s size\u003e file name\u003e syntax is used to make a file. With the truncate -s 10G¬†Mcrohosti¬†command, we are making a file called Microhost that is 10G¬†in size. The du -sh ‚Äîapparent-size Microhost command can be used to find out how big the file is.\nIt‚Äôs important to know that truncate always makes files with an apparent size. This means that it doesn‚Äôt actually store any data, but makes the OS think it does, so the file size grows.\ntruncate -s 10G Microhost du -sh --apparent-size Microhost ","using-dd-command#Using dd command":"This is a well-known method in Linux that has likely been used for a few years. The dd command writes the blocks from the input file to the output file. You might have used this command to make a copy of the data on your hard drive. Here, we‚Äôre increasing the size of the output file called ‚ÄúMicrohost‚Äù by writing the blocks from the input ‚Äú/dev/zero‚Äù file to a count of 8G and using a 4K block size. But it is important to note that this command is slower than fallocate and truncate because I/O operations take a long time.\ndd if=/dev/zero of=Microhost bs=4k iflag=fullblock,count_bytes count=5G You can determine the file‚Äôs size using the du -sh Microhost command, as seen below.\ndu -sh Microhost ","using-the-fallocate-command#Using the fallocate command":"Most likely, this is the fastest way to make a big file. You can make a big file with the syntax fallocate -l ‚Äúsize‚Äù ‚Äúfile name.‚Äù Here, we use the fallocate -l 1G Microhost file command to make an 1G file called Microhost. Use the du -sh Microhost command, as shown below, to find out how big the file is. Check out the manual page for more options.\nfallocate -l 1G Microhost "},"title":"How to Make a Large File in Linux"},"/utho-docs/docs/linux/how-to-make-a-linux-user-change-their-password-upon-login/":{"data":{"":"\nDescription\nChanging the user password expiry information in Linux was the topic of our most recent post, in which we examined many examples of the chage command. If you missed that piece, you can find it here. In this post, we will go over the steps that need to be taken in order to coerce a Linux user into changing their password the next time they log in to their account.\nYou should be aware that you may also use this approach to compel a user to change their password at their initial login if you have recently created a user account for that user and have set the default password to something else.\nThere are two different approaches that can be taken to accomplish this, each of which is broken down into more specific steps below.","using-chage-command#Using chage Command":"You also have the option of using the chage command with the -d or ‚Äîlastday option, which allows you to choose the number of days that have passed since march 1, 1980, which is the day on which the password was most last altered.\nNow, to set the password expiry of the user, run the following command by specifying the day to zero (0). This indicates that the password has not been changed since the above date (i.e., march 1st, 1980), which means that the password has literally become invalid and needs to be changed immediately before the user can access the system again.\n# chage --lastday 0 microhost or\n# chage --lastday 1980-03-01 microhost Next, examine the details regarding the user ravi‚Äôs password expiration and ageing with the chage command by using the -l option as shown in the example.\n# chage -l microhost Conclusion\nIt is usually suggested that you remind users to update their account passwords on a frequent basis for reasons pertaining to security. This article explains two different approaches that can be taken to require users to alter their passwords at their subsequent login.\nMust Read : Getting Started SELinux\nThank You","using-passwd-command#Using passwd Command":"You can use the passwd command, which is used to change a user‚Äôs password, to cause a user‚Äôs password to expire. To do this, specify the -e or ‚Äîexpire switch along with the username as shown in the example above. This will cause the user‚Äôs password to become invalid, which will then require the user to change his or her password.\n# passwd --expire microhost Next, use the chage command to check that the information regarding the user microhost‚Äôs password expiration and ageing has not changed.\n# chage -l microhost You may tell that the user‚Äôs password needs to be updated by looking at the output of the chage command after you have already executed the passwd command. Before the user microhost¬†is allowed to access a shell, he will be prompted to change his password the next time he tries to log in."},"title":"How to Make a Linux User Change Their Password Upon Login?"},"/utho-docs/docs/linux/how-to-migrate-a-mysql-database-between-two-servers/":{"data":{"":"","step-1-the-first-thing-you-need-to-do-is-run-a-mysql-dump#\u003cstrong\u003eStep-1 The first thing you need to do is run a MySQL dump.\u003c/strong\u003e":"","step-2-the-second-step-is-to-copy-the-database#\u003cstrong\u003eStep-2 The second step is to copy the database.\u003c/strong\u003e":"","step3-import-the-database-which-is-the-third-step#\u003cstrong\u003eStep.3 Import the Database, which is the third step.\u003c/strong\u003e":"\nSCP (Secure Copy), a method of transferring files derived from the SSH Shell, can be used to transfer databases across virtual private servers. Remember that both of the virtual servers‚Äô passwords must be known.\nThere are two steps in the database migration process:\nStep-1 The first thing you need to do is run a MySQL dump. We need to use the mysqldump command to create a backup copy of the database file on the primary virtual server before we can move it to the new virtual private server (VPS).\n#mysqldump -u root -p --opt [database name] \u003e [database name].sql You can now transfer the database after running the dump.\nStep-2 The second step is to copy the database. The database can be copied with the assistance of SCP. If you used the command that came before it, you exported your database to the folder in your home directory.\nThe following syntax is utilised by the SCP command:\n#scp [database name].sql [username]@[servername]:path/to/database/ A sample transfer might look like this:\nscp newdatabase.sql user@example.com:~/ The database will be moved to the new virtual private server after you login.\nStep.3 Import the Database, which is the third step. After the information has been moved to the new server, you will be able to import the database into MySQL as follows:\n#mysql -u root -p newdatabase \u003c /path/to/newdatabase.sql After this, the transfer that you were working on using SCP will be finished."},"title":"How To Migrate a MySQL Database Between Two Servers"},"/utho-docs/docs/linux/how-to-migrate-from-centos-8-to-arch-linux-8-7/":{"data":{"":" How to migrate from CentOS 8 to Arch Linux 8.7\nIn this article, you will see how to migrate from CentOS 8 to Arch Linux 8.7. But before that let‚Äôs see what is Arch Linux. AlmaLinux (opens new window) is an enterprise Linux distribution that is Open Source and will always be free. It is run by the community and focuses on long-term stability and a strong production-grade platform. AlmaLinux OS is exactly the same as RHEL¬Æ in every way. The AlmaLinux OS Foundation was set up as a 501(c)(6) non-profit to take care of the project‚Äôs ownership and management.\nCentOS and RedHat announced the end of life (EOL) for CentOS Linux on December 8, 2020. They also told others to shift their attention to the CentOS Stream project.\nCentOS and Red Hat said that CentOS 8 would be supported until 2029 after it came out.\nBut because of a recent announcement, we know that CentOS Linux 7, which is based on RHEL 7, will continue to be supported, and its lifecycle will end in June 2024, just like RHEL 7.\nBut updates and support for CentOS Linux 8 will go on until December 31, 2021, and there won‚Äôt be a CentOS Linux 9.","caution-#Caution !":"Despite the fact that our migration proceeded smoothly and without a problem, we still strongly advise that you back up all of your files just in case. As they say, it‚Äôs better to be secure than sorry, and you want to be protected in case something goes wrong.","prerequisites#Prerequisites":" Super User or any normal user with SUDO privileges. Internet enabled CentOS 8 server ","steps-to-migrate-centos-8-to-arch-linux#Steps to Migrate CentOS 8 to Arch Linux":"Step 1: AlmaLinux conversion requires CentOS 8.4 or 8.5. Before switching to AlmaLinux, it is advised to upgrade to 8.5, though it is not necessary if you are running CentOS 8.4 or higher. If your system recently got new updates, rebooting is advised.\nyum clean all \u0026\u0026 yum upgrade -y init 6 Step 2: Ensure your Operating System.\ncat /etc/os-release or\ncat /etc/redhat-release Before migrating OS\nStep 3: The CentOS 8 mirrorlists are no longer available as of January 31, 2022. Your dnf configuration files need to be updated to refer to a legitimate mirror in order to complete dnf update -y successfully. You can update dnf to version 8.5 and then install AlmaLinux by using the accompanying sed commands, which are provided for your convenience.\nsed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[baseos\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/BaseOS/$basearch/os' /etc/yum.repos.d/CentOS-Linux-BaseOS.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[appstream\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/AppStream/$basearch/os' /etc/yum.repos.d/CentOS-Linux-AppStream.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[cr\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/ContinuousRelease/$basearch/os' /etc/yum.repos.d/CentOS-Linux-ContinuousRelease.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[devel\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/Devel/$basearch/os' /etc/yum.repos.d/CentOS-Linux-Devel.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[extras\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/extras/$basearch/os' /etc/yum.repos.d/CentOS-Linux-Extras.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[fasttrack\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/fasttrack/$basearch/os' /etc/yum.repos.d/CentOS-Linux-FastTrack.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[ha\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/HighAvailability/$basearch/os' /etc/yum.repos.d/CentOS-Linux-HighAvailability.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[plus\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/centosplus/$basearch/os' /etc/yum.repos.d/CentOS-Linux-Plus.repo sed -i -e '/mirrorlist=http:\\/\\/mirrorlist.centos.org\\/?release=$releasever\u0026arch=$basearch\u0026repo=/ s/^#*/#/' -e '/baseurl=http:\\/\\/mirror.centos.org\\/$contentdir\\/$releasever\\// s/^#*/#/' -e '/^\\[powertools\\]/a baseurl=https://mirror.rackspace.com/centos-vault/8.5.2111/PowerTools/$basearch/os' /etc/yum.repos.d/CentOS-Linux-PowerTools.repo Step 4: Download the almalinux-deploy.sh script:\ncurl -O https://raw.githubusercontent.com/AlmaLinux/almalinux-deploy/master/almalinux-deploy.sh Step 6: Change the file permission to make the newly downloaded script executable.\nchmod +x almalinux-deploy.sh Step 5: Execute the script and review the results for mistakes:\n./almalinux-deploy.sh Running Alma Linux script\nAfter successfully completion of the script, you will recieve the below message as a confirmation!\nMIgration done from Centos 8 to Alma Linux\nStep 6: Rebooting is advised if using the AlmaLinux kernel to boot:\nreboot Step 7: Now, confirm the OS release by the command we have executed in step 2\ncat /etc/os-release OS version after migrating"},"title":"How to migrate from CentOS 8 to Arch Linux 8.7"},"/utho-docs/docs/linux/how-to-move-a-postgresql-data-directory-to-a-new-location-on-ubuntu-22-04/":{"data":{"":"\nHow To Move a PostgreSQL Data Directory to a New Location on Ubuntu 22.04\nIntroduction\nDatabases expand over time, sometimes outgrowing the space available on their original file system. When they‚Äôre on the same partition as the rest of the operating system, this can cause I/O contention.\nRAID, network block storage, and other devices can provide redundancy and scalability, among other benefits. This tutorial will walk you through relocating PostgreSQL‚Äôs data directory, whether you‚Äôre looking to add more space, optimise performance, or take advantage of other storage features.\nPrerequisites\nYou will need the following items to finish this guide:\nA non-root user with sudo privileges on an Ubuntu 22.04 server. More information on creating a user with these privileges can be found in our Initial Server Setup with Ubuntu 22.04 guide.\nPostgreSQL is already installed on your server. If you haven‚Äôt already done so, the guide How to Install and Use PostgreSQL on Ubuntu 22.04 can help.\nThroughout this tutorial, the data will be moved to a block storage device mounted at /mnt/volume nyc1 01. Before continuing with this tutorial, if you‚Äôre using Block Storage on DigitalOcean, read our documentation on How to Create and Set Up Volumes for Use with Droplets for help mounting your volume.\nRegardless of the underlying storage, the steps below can assist you in moving the data directory to a new location.\nStep 1- Transferring the PostgreSQL Data Directory to a New Location\nBefore we begin moving PostgreSQL‚Äôs data directory, let‚Äôs check the current location with an interactive PostgreSQL session. psql is the command used to enter the interactive monitor, and -u postgres instructs sudo to run psql as the system‚Äôs postgres user:\n#sudo -u postgres psql To display the current data directory, open the PostgreSQL prompt and type the following command:\npostgres=#SHOW data_directory; Output data_directory ----------------------------- /var/lib/postgresql/14/main (1 row) This output confirms that PostgreSQL is configured to use the default data directory, /var/lib/postgresql/14/main, so that‚Äôs the directory you need to move. Once you‚Äôve confirmed the directory on your system, use the q meta-command to exit the psql prompt:\npostgres=#q Stop PostgreSQL before making changes to the data directory to ensure the integrity of the data:\n#sudo systemctl stop postgresql The output of all service management commands is not displayed by systemctl. Use the following command to confirm that you have successfully stopped the service:\n#sudo systemctl status postgresql The output should indicate that PostgreSQL is inactive (dead), indicating that it has been terminated:\nOutput ‚óã postgresql.service - PostgreSQL RDBMS Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor\u003e Active: inactive (dead) since Thu 2022-06-30 18:46:35 UTC; 27s ago Process: 4588 ExecStart=/bin/true (code=exited, status=0/SUCCESS) Main PID: 4588 (code=exited, status=0/SUCCESS) CPU: 1ms Now that the PostgreSQL server is no longer running, use rsync to move the existing database directory to the new location. The -a flag preserves permissions and other directory properties, while -v produces verbose output to assist you in following the progress. You‚Äôll run rsync from the postgresql directory to replicate the original directory structure in the new location. You can avoid permissions issues for future upgrades by creating that postgresql directory within the mount-point directory and retaining ownership by the PostgreSQL user.\nNote: Be sure there is no trailing slash on the directory, which may be added if you use TAB completion. If you do include a trailing slash, rsync will dump the contents of the directory into the mount point instead of copying over the directory itself.\nFollowing the project convention won‚Äôt harm, especially if PostgreSQL needs to run different versions in the future. The version directory, 14, isn‚Äôt technically necessary since you‚Äôve specified the location explicitly in the postgresql.conf file:\n#sudo rsync -av /var/lib/postgresql /mnt/volume_nyc1_01 Once the copy is finished, give the current folder a.bak extension and leave it there until you‚Äôre sure the transfer worked. Having directories with similar names in both the new and old locations will help to prevent confusion:\n#sudo mv /var/lib/postgresql/14/main /var/lib/postgresql/14/main.bak You may now prepare PostgreSQL to access the data directory in its new location by configuring it to use the new path.\n**Step 2 -Using the New Data Location as the Target\n**\nThe default setting for the data directory configuration directive in the /etc/postgresql/14/main/postgresql.conf file is /var/lib/postgresql/main. This file should be modified to reflect the new data directory:\n#sudo nano /etc/postgresql/14/main/postgresql.conf /etc/postgresql/14/main/postgresql.conf . . . data_directory = '/mnt/volume_nyc1_01/postgresql/14/main' . . . By hitting CTRL + X, Y, then ENTER, you can save and close the document. To set up PostgreSQL to use the new data directory location, all you have to do is this. The PostgreSQL service must now be restarted, and it must be verified that it is pointing to the appropriate data directory.\nStep 3 ‚Äî Restarting PostgreSQL\nStart the PostgreSQL server using systemctl after updating the data-directory directive in the postgresql.conf file:\n#sudo systemctl start postgresql Check the status of the PostgreSQL server once again using systemctl to ensure that it began successfully:\n#sudo systemctl status postgresql If the service started up right, the Active line of the command‚Äôs output will say ‚Äúactive (exited)‚Äù\nOutput ‚óè postgresql.service - PostgreSQL RDBMS Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor\u003e Active: active (exited) since Thu 2022-06-30 18:50:18 UTC; 3s ago Process: 4852 ExecStart=/bin/true (code=exited, status=0/SUCCESS) Main PID: 4852 (code=exited, status=0/SUCCESS) CPU: 1ms Lastly, enter the PostgreSQL command prompt to confirm that the new data directory is truly active:\n#sudo -u postgres psql Again, make sure the value for the data directory is correct:\npostgres=#SHOW data_directory; Output data_directory ---------------------------------------- /mnt/volume_nyc1_01/postgresql/14/main (1 row) This demonstrates that PostgreSQL is utilising the new location for the data directory. After that, take a moment to make sure you can access your database and interact with the data inside. You can delete the backup data directory once you‚Äôve ensured the accuracy of any current data:\n#sudo rm -Rf /var/lib/postgresql/14/main.bak With that, you‚Äôve successfully changed the location of your PostgreSQL data directory."},"title":"How To Move a PostgreSQL Data Directory to a New Location on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-partition-and-format-storage-devices-in-linux/":{"data":{"":"\nIntroduction\nLinux disc preparation is simple. Specialized demands may require different tools, filesystem formats, or partitioning strategies, but the basics remain the same.\nThis guide explains:\nDetecting the new disc.\nCreating a single drive-wide partition (most operating systems expect a partition layout, even if only one filesystem is present)\nExt4 filesystem formatting (the default in most modern Linux distributions)\nInstallation Boot filesystem auto-mounting\nTo Partition and Format Storage Devices in Linux read below steps..","step-1-install-parted#Step 1 ‚Äî¬†Install Parted":"The parted utility partitions drives. Linux includes most low-level filesystem operations by default. The exception is parted, which generates partitions.\nIf you don‚Äôt have parted on Ubuntu or Debian, type:\n# sudo apt update # sudo apt install parted On a server running RHEL, Rocky Linux, or Fedora, you can install it by typing:\n# sudo dnf install parted Every other command used in this guide should be preloaded, so you can go to the next step.","step-5--mount-the-new-filesystem#Step 5 ‚Äî Mount the New Filesystem":"Mount the filesystem.\nFor temporarily mounted filesystems, the Filesystem Hierarchy Standard suggests /mnt or a subdirectory (like removable drives). It doesn‚Äôt suggest where to put permanent storage, so you can choose. For this instruction, mount /mnt/data.\nmkdir this directory:\n# sudo mkdir -p /mnt/data Temporary Filesystem Mounting\n# sudo mount -o defaults /dev/sda1 /mnt/data Automatically Mounting the Filesystem at Boot\n# sudo nano /etc/fstab put the entry in fstab file you can copy from /etc/mtab file\n# sudo mount -a Testing the Mount point\nCheck the filesystem after mounting the volume.\ndf output shows if the disc is available. You can exclude superfluous information about tmpfs by appending -x tmpfs to df output.\n# df -h -x tmpfs Output Filesystem Size Used Avail Use% Mounted on /dev/vda1 20G 1.3G 18G 7% / /dev/sda1 99G 60M 94G 1% /mnt/data You can test the disk‚Äôs read/write capability by writing to a test file.\n# echo \"success\" | sudo tee /mnt/data/test_file Check the file‚Äôs writing by reading it back.\n# cat /mnt/data/test_file Output success After verifying the new filesystem‚Äôs functionality, you can delete the file.\n# sudo rm /mnt/data/test_file Must Read:-¬†https://utho.com/docs/tutorial/how-to-make-a-large-file-in-linux/\nHopefully you have understand the above steps How To Partition and Format Storage Devices in Linux..\nThankyou","step2-find-the-new-disc#Step.2 Find the new disc":"Before configuring the drive, it must be properly identified on the server.\nIf this is a brand-new drive, you can identify it on your server by observing the lack of a partitioning scheme. If you instruct parted to list the partition layout of your drives, it will generate an error for any discs that lack a proper partition scheme. This can assist in identifying the new disc:\n# sudo parted -l | grep Error The fresh, unpartitioned drive should display an unknown disc label error:\nOutput Error: /dev/sda: unrecognized disk label Additionally, you can use the lsblk command to locate a disc of the correct size that has no linked partitions:\n# lsblk Output NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 100G 0 disk vda 253:0 0 20G 0 disk ‚îî‚îÄvda1 253:1 0 20G 0 part / Once you have determined the name that the kernel has assigned to your disc, you can partition it.","step3-new-drive-partition#Step.3 New Drive Partition":"As noted in the introduction, in this guide you will build a single partition that spans the entire drive.\n**Select a standard partition\n**\nTo accomplish this, you must first select the partitioning standard to employ. GPT and MBR are the options available. GPT is a more recent standard, although MBR is supported by a greater number of older operating systems. For a typical cloud server, GPT is the superior choice.\nTo select the GPT standard, submit the discovered disc to parted with mklabel gpt:\n# sudo parted /dev/sda mklabel gpt Use mklabel msdos to utilise the MBR format.\n# sudo parted /dev/sda mklabel msdos Create the New Partition\n# sudo parted -a opt /dev/sda mkpart primary ext4 0% 100% This command can be broken down as follows:\n/dev/sda is the disc you are partitioning, and parted -a opt sets the optimal alignment type by default.\nmkpart main ext4 creates an independent (bootable, not extended from another) partition utilising the ext4 filesystem.\n0% 100% indicates that this partition should span the entirety of the disc.\n# lsblk ","step4-implement-a-filesystem-on-the-new-partition#Step.4 Implement a Filesystem on the New Partition":"You can initialise an Ext4 partition once you have one. Ext4 isn‚Äôt the only filesystem option, but it‚Äôs the easiest for a single Linux drive. Windows utilises NTFS and exFAT, although they have limited support on other platforms (they‚Äôre read-only in some circumstances and can‚Äôt be used as a boot drive). macOS uses HFS+ and APFS, with the same limitations. ZFS and BTRFS, newer Linux filesystems than Ext4, have different needs and are suitable for multi-disk arrays.\nmkfs.ext4 initialises Ext4 filesystems. The -L flag labels partitions. Choose a name for this drive:\n# sudo mkfs.ext4 -L datapartition /dev/sda1 If you wish to modify the partition label in the future, you can execute the e2label command:\n# sudo e2label /dev/sda1 newlabel lsblk shows how to identify partitions. Find partition‚Äôs name, label, and UUID.\n‚Äîfs tells lsblk to print all of this information.\n# sudo lsblk --fs You can also use lsblk -o followed by the options.\n# sudo lsblk -o NAME,FSTYPE,LABEL,UUID,MOUNTPOINT "},"title":"How To Partition and Format Storage Devices in Linux"},"/utho-docs/docs/linux/how-to-prevent-a-user-from-login-in-linux/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to prevent a user from login in Linux.\nAlso read: How to Install Varnish Cache with Apache on CentOS 7\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to prevent a user from login in Linux.\nThe rights that are associated with an account (also known as permissions, user groups, bits, or flags) determine the level of access that a user has to a resource. Access levelling can be either automatic or requested, depending on the situation.\nFirst, using the useradd command, we will create four users and set their passwords using the passwd command.\n# useradd microhost1 passwd microhost1 Then we do the same things for the next three users, that is, microhost2, microhost3, and microhost4.\nWe see the users‚Äô list from the below command.\n# cat /etc/passwd If you want to prevent a user from logging in, then you should change the path to /bin/bash to /sbin/nologin\nTo change the path, use the below command.\n# vi /etc/passwd To save the file,press escape colon wq.\nThen try to login as a microhost1 user and you will see ‚ÄúThis account is currently not available‚Äù.\nIf you want to login as a microhost1 user, then you would change the path /sbin/nolgin to /bin/bash.\n# vi /etc/passwd Then try to login as a microhost1 user.\n# su - microhost1 "},"title":"How to prevent a user from login in Linux"},"/utho-docs/docs/linux/how-to-prevent-file-and-directories-from-being-deleted-even-by-root/":{"data":{"":"","description#\u003cstrong\u003eDescription\u003c/strong\u003e":"\nDescription On operating systems that are similar to Unix, such as Linux, the account or user name known as root has the ability to change any and all files and folders on the system by default. In this post, we will demonstrate how to prevent even the root user from deleting directories or files in Linux by using the chmod command.\nIt is necessary to make a file unmodifiable with the chattr command in order to prevent any user on the system, even root, from being able to delete the file. On a Linux file system, the characteristics of files may be modified with this command.\nMake a File Undeletable and See How to Do It The following command renders the /var/www/microhost/ file unchangeable (or undeletable). Because of this, it is impossible to make any changes to the file; for example, it cannot be renamed or deleted. You won‚Äôt even be able to establish a link to it, and you won‚Äôt be able to add any data to the file either.\nNote that in order to change or delete this property using the sudo command, you will need to have the superuser privilege:\n#sudo chattr +i /var/www/microhost Or\n#sudo chattr +i -V /var/www/microhost Use the lsattr command as shown here in order to inspect the attributes of a file.\nNow, attempt to delete the immutable file using both the standard user account and the root account.\n#rmdir /var/www/microhost #sudo rmdir /var/www/microhost How to Make a Directory Unable to Be Deleted Recursively You may make the following changes to the characteristics of directories and their contents in a recursive manner by using the -R switch.\nUse the -i sign to delete the attribute mentioned above in order to restore mutability to a file. The steps for doing so are as follows.\n#sudo chattr -i /var/www/microhost/ In this post, we demonstrated how to prevent even the root user on a Linux system from deleting certain files.\nThank You "},"title":"How to Prevent File and Directories from Being Deleted, Even by Root"},"/utho-docs/docs/linux/how-to-protect-your-web-sites-by-using-username-and-password-in-apache-on-centos/":{"data":{"":"\nThis article is about to Protect your Web Sites by using Username and password in Apache on CentOS. When you‚Äôre in charge of online projects, you often have to limit who can see them to keep them safe from the outside world. There could be a number of reasons for this, such as the fact that you don‚Äôt want search engine crawlers to see your site while it‚Äôs still being built.\nIn this tutorial, I‚Äôll show you how to make Apache web server protect different web site directories with passwords. There are many ways to do this, but we will only talk about the two most common ones.","configuration-to-protect-default-site#Configuration to protect default site.":"To¬†protect the main web root directory¬†/var/www/html, open your Apache‚Äôs configuration file and change to the highlighted content from the following code.","for-on-apache-22-version#For On Apache 2.2 Version:":" \u003cDirectory /var/www/html\u003e Options Indexes Includes FollowSymLinks MultiViews AllowOverride All Order allow,deny Allow from all \u003c/Directory\u003e ","on-apache-24-version#On Apache 2.4 Version:":" # vi /etc/httpd/conf/httpd.conf \u003cDirectory /var/www/html\u003e Options Indexes Includes FollowSymLinks MultiViews AllowOverride All Require all granted \u003c/Directory\u003e And now after saving the file, restart the httpd service.\n# systemctl restart httpd Now, we‚Äôll use the htpasswd command to make a username and password for our protected directory. This command is used to handle basic authentication user files.\nExample and syntax of the htpasswd command is as follow.\n# htpasswd -c /path/filename username Here, -c is used to create a file in which the username and password will be saved. Note that, the password will be hashed format.\nNow we will create the credential file in /etc/httpd directory for the user- useradmin\n# htpasswd -c /etc/httpd/credfile useradmin # vi /var/www/html/.htaccess Now in this file add the following content.\nAuthType Basic AuthName \"Restricted Access\" AuthUserFile /etc/httpd/credfile require user useradmin Now you can save the file and test your setup. Open your web browser and type in your IP address or domain name, such as:\nhttp://serverip Prompt you will get after browsing server-ip\nEnter the credentials created by htpasswd command\nThe default page of the site","requirements#Requirements":"To protect in Apache on CentOS you need,\nyum repository configured Centos server A super user( root) or any normal user with SUDO privileges. "},"title":"How to Protect your Web Sites by using Username and password in Apache on CentOS."},"/utho-docs/docs/linux/how-to-protect-your-web-sites-by-using-username-and-password-in-apache-on-ubuntu/":{"data":{"":"\nThis article is about to Protect your Web Sites by using Username and password in Apache on¬†Ubuntu server. When you‚Äôre in charge of online projects, you often have to limit who can see them to keep them safe from the outside world. There could be a number of reasons for this, such as the fact that you don‚Äôt want search engine crawlers to see your site while it‚Äôs still being built.\nIn this tutorial, I‚Äôll show you how to make Apache web server protect different web¬†site directories¬†with passwords.","configuration-to-protect-default-site#Configuration to protect default site.":"To¬†protect the main web root directory¬†/var/www/html, open your Apache‚Äôs configuration file and change to the highlighted content from the following code.\nvi /etc/apache2/apache2.conf ","for-on-apache-22-version#For On Apache 2.2 Version:":" \u003cDirectory /var/www/html\u003e Options Indexes Includes FollowSymLinks MultiViews AllowOverride All Order allow,deny Allow from all \u003c/Directory\u003e ","on-apache-24-version#On Apache 2.4 Version:":" \u003cDirectory /var/www/html\u003e Options Indexes Includes FollowSymLinks MultiViews AllowOverride All Require all granted \u003c/Directory\u003e And now after saving the file, restart the apache2 service.\nsystemctl restart httpd Now, we‚Äôll use the htpasswd command to make a username and password for our protected directory. This command is used to handle basic authentication user files.\nExample and syntax of the htpasswd command is as follow.\nhtpasswd -c /path/filename username Here, -c is used to create a file in which the username and password will be saved. Note that, the password will be hashed format.\nNow we will create the credential file in /etc/apache2 directory for the user- cred-user\nhtpasswd -c /etc/apache2/cred_file cred-user vi /var/www/html/.htaccess Now in this file add the following content.\nAuthType Basic AuthName \"Restricted Access\" AuthUserFile /etc/apache2/cred_file require user cred-user Now you can save the file and test your setup. Open your web browser and type in your IP address or domain name, such as:\nhttp://server-ip Prompt you will get after browsing server-ip\nEnter the credentials created by htpasswd command","requirements#Requirements:":" yum repository configured Centos server A super user( root) or any normal user with SUDO privileges. "},"title":"How to Protect your Web Sites by using Username and password in Apache on Ubuntu."},"/utho-docs/docs/linux/how-to-real-time-monitor-tcp-and-udp-ports/":{"data":{"":"","#":"","list-all-open-ports-in-linux#\u003cstrong\u003eList All Open Ports in Linux\u003c/strong\u003e":"","real-time-monitoring-of-tcp-and-udp-open-ports#\u003cstrong\u003eReal-time monitoring of TCP and UDP open ports\u003c/strong\u003e":"\nDescription This article will show you How to Watch TCP and UDP Ports in Real-time, Each network service that is running on a Linux system uses a specific protocol (the most common ones being the TCP (Transmission Control Protocol) and UDP (User Datagram Protocol)) and a port number in order to communicate with other processes or services. A port is a logical construct that identifies a particular process/application or a type of network service. In terms of software, especially at the level of the operating system, a port is a logical construct that identifies a\nOn a Linux machine, we will demonstrate how to use a socket summary to list and monitor or observe operating TCP and UDP ports in real-time on a Linux machine.\nList All Open Ports in Linux To generate a list of all open ports on a Linux machine, you may use the netstat command or the ss programme in the following manner.\nIt is also very important to point out that the netstat command has been deprecated, and that the ss command has been implemented to provide more extensive network data in its place.\n# sudo netstat -tulpn Or\n# sudo ss -tulpn The State column in the output of the preceding command indicates whether a port is in a listening state (LISTEN) or not\n*In the command that was just shown, the flag:\n-t ‚Äì enables listing of TCP ports.\n-l ‚Äì prints only listening sockets.\n-n ‚Äì shows the port number.\n-p ‚Äì show process/program name.\nReal-time monitoring of TCP and UDP open ports To monitor TCP and UDP ports in real time, use the netstat or ss tools in conjunction with the watch application, as illustrated.\n# sudo watch netstat -tulpn Or\n# sudo watch ss -tulpn To exit, use Ctrl+C.\nHopefully now you have a better understanding of Real-Time Monitor TCP and UDP Ports.\nThank You "},"title":"How to Real-Time Monitor TCP and UDP Ports"},"/utho-docs/docs/linux/how-to-recognize-active-directories-using-shell-variables-and-characters/":{"data":{"":"","#":"\nDescription The user‚Äôs home directory, the current working directory, and the previous working directory are just some of the special directories that a Linux user is bound to work with a significant number of times when using the shell command line.\nTherefore, having the ability to quickly access or zero in on these directories by utilising specific approaches that are unique to Linux can be a useful bonus skill for any user, regardless of experience level.\nA user can identify his or her home directory, current working directory, and previous working directory from the shell by using special shell characters and environment variables. In this beginner‚Äôs guide, we will look at how a user can do this.","using-environmental-variables#Using Environmental Variables":"In addition to the characters that were just discussed, there are also some environmental variables that are designed to cooperate with the directories that we are concentrating on. In the following part of this tutorial, we are going to discuss some of the most significant environmental variables that can be used to recognise directories via the command line.\nYou can verify that the value of $HOME is the same as the value of the tilde character (), which is the home directory of the currently logged in user, by using the echo command in the following manner:\n#echo $HOME $PWD is an abbreviation that stands for ‚ÄúPrint Working Directory,‚Äù and as its name suggests, it displays the complete directory path of the current working directory in the shell command line, as shown in the following example:\n#echo $PWD #echo $PWD ","using-only-the-most-essential-cd-commands#Using Only the Most Essential cd Commands":"In addition, you may access both your home directory and the directory in which you were working before by using a few simple commands. On the command line, for instance, if you are in any area of your file system, entering cd and then pressing Enter will bring you to your home directory:\n#echo $PWD #cd #echo $PWD Using the cd - command, as shown below, is another option for moving back to the previous working directory:\n#echo $PWD #echo $OLDPWD #cd - #echo $PWD In this article, we went over some basic but helpful command line tips for new Linux users to recognise various special folders from inside the shell command line. These recommendations were provided for new Linux users.\nThank You ","using-particular-characters-from-the-shell#Using Particular Characters from the Shell":"When we work with directories using the command line, there are particular characters that the shell recognises and interprets as meaning something specific. The tilde, or, will be our first topic of discussion because it is the symbol that allows access to the home directory of the currently logged-in user:\n#echo ~ On the command line, the second character is the dot (.) character, which represents the directory that the user is currently working in. You can see that the output of the commands ls and ls. produce the same result, which is a listing of the contents of the directory that is currently being used as the working directory.\n#ls #ls . The third set of special characters is comprised of two dots separated by a semicolon (..), which denotes the directory that is located directly above the user‚Äôs current working directory.\nThe directory that can be seen above /var in the image below is the root directory (/). Because of this, when we use the ls command in the manner described below, the contents of the root directory are listed:\n#ls .. "},"title":"How to Recognize Active Directories Using Shell Variables and Characters"},"/utho-docs/docs/linux/how-to-remove-ftp-account-in-plesk/":{"data":{"":"","-successfully-deleted-ftp-account#* Successfully deleted FTP account":"\nIntroduction\nThe abbreviation ‚ÄúFTP‚Äù stands for ‚ÄúFile Transfer Protocol,‚Äù which is the name given to a set of guidelines that determine how files are sent between computer systems using the internet. File Transfer Protocol (FTP) is used by businesses to transfer data between computers, and websites utilise FTP to upload and receive files from the servers that host their websites.\n1.Go to Websites \u0026 Domains And To delete an FTP account for a certain domain, click that domain.\n2.click¬†FTP Access.\n3.Click remove an FTP account\n4.Select the FTP account you wish to remove by clicking on it\n*Please follow the procedures below to delete your FTP account.\n1.Go to Websites \u0026 Domains\n*And To delete an FTP account for a certain domain, click that domain 2.click¬†FTP Access. *Next, choose the FTP account that you wish to delete by clicking on it, then pressing the button, and finally clicking the ‚Äúok‚Äù button. * Successfully deleted FTP account Click to know How to add FTP account in plesk\nThank You","2clickftp-access#2.click¬†\u003cstrong\u003eFTP Access\u003c/strong\u003e.":"","and-to-delete-an-ftp-account-for-a-certain-domain-click-that-domain#*And To delete an FTP account for a certain domain, click that domain":"","next-choose-the-ftp-account-that-you-wish-to-delete-by-clicking-on-it-then-pressing-the-button-and-finally-clicking-the-ok-button#*Next, choose the FTP account that you wish to delete by clicking on it, then pressing the button, and finally clicking the \u0026ldquo;ok\u0026rdquo; button.":""},"title":"How to remove FTP account in plesk"},"/utho-docs/docs/linux/how-to-reset-centos-7-8-root-password/":{"data":{"":"\nFirst, reboot or power on your¬†CentOS 8¬†system. Select the kernel that you want to boot into. Next, press¬†‚Äòe‚Äô¬†to interrupt the boot process and make changes.\nReplace the kernel parameter¬†ro¬†with¬†rw¬†and append an extra kernel parameter¬†init=/sysroot/bin/sh.\nPress Ctrl+x¬†combination on the keyboard to enter single-user mode.\nNext, run the command below to mount the root file system in read and write mode.\nchroot /sysroot You can now change the root password by executing the command:\npasswd To apply the changes, exit and reboot the CentOS 8 system.\nThankyou."},"title":"How to reset forgotten root password in centos 7/8"},"/utho-docs/docs/linux/how-to-reset-debian-root-password/":{"data":{"":"\nIn this tutorial, we will learn how to reset a forgotten root password in a¬†Debian system.¬†So, first power on or reboot Debian system. It should be presented with a¬†GRUB¬†menu as shown below. On the first option, proceed and press the¬†‚Äòe‚Äô¬†key on the keyboard before the system starts booting .\nScroll down and locate the line that begins with¬†‚Äòlinux‚Äô¬†that precedes the¬†/boot/vmlinuz-*¬†section that also specifies the¬†UUID.\nMove the cursor to the end of this line, just after¬†‚Äòro net.ifnames=0 biosdevname=0 quiet‚Äô¬†and append the parameter¬†init=/bin/bash.\nNext hit¬†ctrl + x¬†to enable it to boot in single-user mode with the root filesystem mounted with read-only¬†(ro)¬†access rights.\nTo reset the password, we need to change the access right from¬†read-only¬†to¬†read-write. Therefore, run the command below to remount the root filesystem with¬†rw¬†attributes.\nmount -n -o remount,rw / Next, reset the root password by executing the good old¬†passwd¬†command as shown.\npasswd With the root password successfully changed,¬†reboot¬†your¬†Debian system by Ctrl + Alt + Del .\nAnd that‚Äôs how reset a forgotten root password on¬†Debian.\nThankyou."},"title":"How to reset forgotten root password in Debian"},"/utho-docs/docs/linux/how-to-reset-forgotten-root-password-in-fedora-34internal/":{"data":{"":"\nResetting Password in Fedora 34\nFor resetting password Reboot the server and interrupt¬†booting process by pressing up and down arrow key and press e button from keyboard\nAfter that find the line starting with linux and append that line with ‚Äúrw init=/bin/bash‚Äù And then press Ctrl+x\nNow enter passwd command for resetting password\n#passwd root\nNow reboot the system by using /sbin/reboot -f command\n#sbin/reboot -f\nNow After reboot you can access your server with new password\nThankyou :)"},"title":"How to reset forgotten root Password in Fedora 34."},"/utho-docs/docs/linux/how-to-reset-the-mysql-root-password-in-centos-7/":{"data":{"":"\nMySQL is an open-source relational database management system . Its name consists a combination of ‚ÄòMy‚Äô and ‚ÄòSQL‚Äô as the name for the Structured Query Language of the co-founder Michael Widenius‚Äôs daughter.\nStep 1: Login into the server using root credentials on putty.\nStep 2. Stop the mysql service using the below command.\n# service mysqld stop Step 3. Set the mySQL environment option by using the below command.\n# systemctl set-environment MYSQLD_OPTS=\"--skip-grant-tables Step 4. Start the mysql service .\n# systemctl start mysqld Step 5. Login to mysql using root user\n# mysql -u root Step 6. Update the root user password with these mysql commands\nmysql\u003e UPDATE mysql.user SET authentication_string = PASSWORD('MyNewPassword') -\u003e WHERE User = 'root' AND Host = 'localhost' mysql\u003e FLUSH PRIVILEGES; mysql\u003e quit Step 7. Stop mysqld service.\n# systemctl stop mysqld Step 8. Unset the mysql environment option so it starts normally next time.\n# systemctl unset-environment MYSQLD_OPTS Step 9. Start mysql normally.\n# systemctl start mysqld Step 10. Now, login to mysql with the new password as shown in the below screenshot.\nThank you!!"},"title":"How to reset the MySQL root password in CentOS 7"},"/utho-docs/docs/linux/how-to-reset-ubuntu-16-18-root-password/":{"data":{"":"\nFirstly, you need to power on¬†or¬†reboot¬†your¬†Ubuntu¬†system. You should get a grub menu as shown below.\nNext, press the¬†'e'¬†key to edit the grub parameters. This should display a screen as shown below.\nScroll down until you get to the line that begins with¬†'linux /boot/vmlinuz'¬†the entire line is highlighted below. Narrow down to a section that reads¬†\"ro quiet splash $vt_handoff\".\nReplace¬†‚Äúro ifnames=0 biosdevname=0‚Äù with¬†‚Äúrw init=/bin/bash‚Äù¬†as shown. The purpose is to set the root file system with¬†read¬†and¬†write¬†commands denoted by the¬†rw¬†prefix.\nThereafter, press¬†ctrl + x¬†or¬†F10¬†to¬†reboot¬†your system. Your system will boot into a root shell screen as shown below. By running the command you can confirm that the root filesystem had access rights to read and write.\nmount | grep -w / The output in the screenshot below confirms¬†read¬†and¬†write¬†access rights denoted by¬†rw.\nTo reset the root password execute the command.\npasswd Provide a new password and confirm it. Thereafter, you will get a ‚Äòpassword updated successfully‚Äô notification.\nWith the root password successfully changed,¬†reboot¬†into your¬†Ubuntu¬†system by running the command.\nexec /sbin/init Thankyou."},"title":"How to reset forgotten root password in ubuntu 16/18"},"/utho-docs/docs/linux/how-to-reset-your-mysql-or-mariadb-root-password-on-ubuntu-18-04/":{"data":{"":"\nIntroduction\nPassword forgetting happens to even the best of us. If you have access to the server and a user account with sudo rights, you can still acquire access and reset the root password for your MySQL or MariaDB database.\nnote:\nAs long as you connect from the system‚Äôs root account, the default MySQL or MariaDB configuration on fresh Ubuntu 18.04 installations typically lets you access the database (with full administrator capabilities) without requiring a password. It might not be essential to reset the password in this case. Try using the sudo mysql command to access the database before changing the database root password. Only if the default setting for authentication was adjusted, and this results in an access denied issue, follow the steps in this tutorial.\nThis guide shows how to reset MySQL and MariaDB‚Äôs root passwords on Ubuntu 18.04. Changing the root password depends on whether MySQL or MariaDB is installed and the distribution‚Äôs default systemd settings or third-party packages. These instructions may work with different system or database server versions, but they‚Äôve been tested with Ubuntu 18.04 and distribution-supplied packages.\nPrerequisites\nYou‚Äôll need the following to recover your MySQL or MariaDB root password:\nUsing a sudo user or another method of gaining access to the server with root capabilities, you can connect to the Ubuntu 18.04 server that is running MySQL or MariaDB. Create a test server with an ordinary, non-root user who has sudo capabilities using the initial server setup lesson if you want to test out the recovery techniques in this guide without having them impact your production server. Install MySQL after that the best way to set up MySQL on Ubuntu 18.04.","step-1--identifying-the-database-version-and-stopping-the-server#Step 1 ‚Äî Identifying the Database Version and Stopping the Server":"MySQL or MariaDB, a well-liked drop-in replacement that is 100% compatible with MySQL, are both supported by Ubuntu 18.04. Depending on which of these you have installed, you‚Äôll need to use various commands to recover the root password. To find out which database server you‚Äôre running, follow the instructions in this section.\nThe command below will verify your version:\n#mysql --version The result will display ‚ÄúMariaDB‚Äù followed by the version number if MariaDB is being used:\nMariaDB output mysql Ver 15.1 Distrib 10.1.47-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2 If you are using MySQL, you will see output similar to this:\nMySQL output mysql Ver 14.14 Distrib 5.7.32, for Linux (x86_64) using EditLine wrapper Note the database you are using because it may affect the commands you should use in the remaining sections of this guide.\nYou must terminate the database server in order to modify the root password. You can use the next command to do so if you are using MariaDB:\n#sudo systemctl stop mariadb To turn off the database server for MySQL, use the following command:\n#sudo systemctl stop mysql You can reset the root password by restarting the database in safe mode after the database has been terminated.","step-3--changing-the-root-password#Step 3 ‚Äî Changing the Root Password":"The grant tables are not loaded, and networking support is not enabled, thus the database server is now operating in a restricted state. This enables password-free access to the server but prevents you from running commands that change data. Now that you have access to the server, you must load the grant tables in order to reset the root password.\nGive the FLUSH PRIVILEGES command to the database server to instruct it to reload the grant tables:\nmysql\u003e #FLUSH PRIVILEGES; Now you can modify the root password. Whether you‚Äôre using MariaDB or MySQL will affect the approach you use.\nPassword-changing MariaDB\nExecute the following command to change the root account‚Äôs password if you‚Äôre using MariaDB, making sure to replace new password with a secure new one that you‚Äôll remember:\nmariadb\u003e #UPDATE mysql.user SET password = PASSWORD('new_password') WHERE user = 'root'; This output will appear to show that the password has changed:\nOutput Query OK, 1 row affected (0.00 sec) Execute the following two commands to ensure that MariaDB will use its default authentication mechanism for the new password you supplied to the root account. MariaDB permits the use of custom authentication techniques.\nmariadb\u003e #UPDATE mysql.user SET authentication_string = '' WHERE user = 'root'; mariadb\u003e #UPDATE mysql.user SET plugin = '' WHERE user = 'root'; Whenever you run a statement, you‚Äôll see the following results:\nOutput Query OK, 0 rows affected (0.01 sec) The password has now been updated. In order to restart the database server in regular mode, type quit to close the MariaDB console and move on to Step 4.\nChanging the MySQL Password\nExecute the following command for MySQL to update the root user‚Äôs password, substituting new password with a reliable password you can remember:\nmysql\u003e #UPDATE mysql.user SET authentication_string = PASSWORD('new_password') WHERE user = 'root'; The following output will show that the password change was successful:\nOutput Query OK, 1 row affected (0.00 sec) Execute the following command to instruct MySQL to utilise its default authentication mechanism in order to authenticate the root user with the new password:\nmysql\u003e #UPDATE mysql.user SET plugin = 'mysql_native_password' WHERE user = 'root'; You should see output that is comparable to the earlier command:\nOutput Query OK, 1 row affected (0.00 sec) The password has been updated. Enter the word exit to close the MySQL console.\nRestarting the database in default operating mode will be helpful.\nBringing Back Your Database Server‚Äôs Settings to Their Defaults\nReverting the changes you made will enable networking and load the grant tables, allowing you to restart the database server in normal mode. Again, the approach you choose will depend on whether you utilised MySQL or MariaDB.\nUnset the MYSQLD OPTS environment variable for MariaDB that you previously set:\n#sudo systemctl unset-environment MYSQLD_OPTS restart the service using systemctl:\n#sudo systemctl restart mariadb Remove MySQL‚Äôs modified systemd configuration:\n#sudo systemctl revert mysql You can expect to see results along these lines:\nOutput Removed /etc/systemd/system/mysql.service.d/override.conf. Removed /etc/systemd/system/mysql.service.d. To effect the changes, reload the systemd configuration after that:\n#sudo systemctl daemon-reload restart the service:\n#sudo systemctl restart mysql Now that the database has been restarted, everything is as it should be. By using a password to log in as the root user, you may verify that the new password is functional.\n#mysql -u root -p A password prompt will appear. You will successfully access the database prompt after entering your new password.","step2-database-server-restarting-without-permission-checks#Step.2 Database Server Restarting Without Permission Checks":"Running MySQL and MariaDB without permission checks gives root access without a password. Stop the database from loading grant tables, which store user privileges. You may want to disable networking to prevent other clients from connecting to the temporarily vulnerable server.\nStarting the server without loading the grant tables varies every database server.\nConfiguring MariaDB to Start Without Grant Tables\nWe‚Äôll utilise the systemd unit file to define additional options for the MariaDB server daemon so that it can be started without the grant tables.\nRun the command below to set the MYSQLD OPTS environment variable, which MariaDB uses when it starts up. When starting MariaDB, the ‚Äîskip-grant-tables and ‚Äîskip-networking options instruct it to do so without loading the grant tables or networking features:\n#sudo systemctl set-environment MYSQLD_OPTS=\"--skip-grant-tables --skip-networking\" start the MariaDB server:\n#sudo systemctl start mariadb Although this command doesn‚Äôt output anything, it will restart the database server using the updated environment variable settings.\nWith sudo systemctl status mariadb, you may confirm that it began.\nNow you shouldn‚Äôt need to enter a password to access to the database as the MariaDB root user:\n#sudo mysql -u root MariaDB prompt Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. MariaDB [(none)]\u003e You can modify the root password as indicated in Step 3 now that you have access to the database server.\nSetting Up MySQL to Launch Without Grant Tables\nYou must modify the systemd settings of MySQL to allow additional command-line parameters to be passed to the server during starting if you want to start the server without its grant tables.\nRun the following command to accomplish this:\n#sudo systemctl edit mysql This command opens MySQL‚Äôs service overrides in the VI editor. These modify MySQL service defaults. Add the following to this empty file:\n[Service] ExecStart= ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid --skip-grant-tables --skip-networking The first ExecStart statement removes the default setting, while the second one gives systemd a new startup command with parameters to stop loading networking and grant tables.\nCTRL-x will close the file, Y will save your changes, and ENTER will confirm the file name.\nTo apply these modifications, reload the systemd configuration:\n#sudo systemctl daemon-reload start the MySQL server:\n#sudo systemctl start mysql The database server will launch even though the programme doesn‚Äôt produce any output. The grant tables and networking will not be enabled.\nAs the root user, connect to the database:\n#sudo mysql -u root see a database shell prompt:\nMySQL prompt Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. mysql\u003e You can modify the root password now that you have access to the server."},"title":"How To Reset Your MySQL or MariaDB Root Password on Ubuntu 18.04"},"/utho-docs/docs/linux/how-to-save-a-command-output-to-a-file-in-linux/":{"data":{"":"","#":"\nDescription In Linux, you can do a lot with the results of a command. You can put the results of a command into a variable, send them through a pipe to another command or programme to be processed, or send them to a file for further study.\nUnder this brief post, I will demonstrate a simple command-line technique that may be very helpful. Specifically, I will show you how to observe the result of a command on the screen while concurrently writing to a file in Linux.\nIn addition to seeing output on the screen, you may also write to a file.\ndf is a command that can be used on a Linux operating system to detect the kind of file system that is present on a partition in addition to providing a comprehensive overview of the amount of disc space that is both available and being utilised by a file system.\n#df When you use the -h option, the statistics about the file system disc space will be shown in a manner that is ‚Äúhuman readable‚Äù (displays statistics details in bytes, mega bytes and gigabyte).\n#df -h Execute the command that is provided below in order to show the aforementioned information on the screen, as well as save it to a file, for example so that it may be analysed at a later time or sent to a system administrator.\n#df -h | tee df.txt #cat df.txt The tee command does the magic in this case; it reads from standard input and writes to files as well as standard output.\nUsing the -a or ‚Äîappend option, you may add a file that already exists.\n#df -h | tee -a df.txt *If you want additional knowledge, you can get it by reading the df and tee man pages. #man df #man tee You should now be able to observe the results of a command on the screen as well as write to a file under Linux as a result of reading this brief tutorial.\nThank You "},"title":"How to Save a Command Output to a File in Linux"},"/utho-docs/docs/linux/how-to-schedule-an-activity-at-a-specific-time-or-at-a-time-in-the-future-using-the-at-command/":{"data":{"":"","#":"\nDescription The at command can be used as an alternative to the cron job scheduler since it enables you to schedule a command to run once at a specified time without requiring you to change a configuration file.\nThe only prerequisite is to download and install this tool, then start it and make sure its execution is enabled.","installing-package#Installing Package:":" # yum install -y at Next, run the at service at boot time and make sure it‚Äôs enabled.","set-at-jobs#Set at Jobs:":"When atd is up and running, you will have the ability to schedule any command or job using the instructions below. When the next minute begins (so if the current time is 22:32:13, the command will be executed at 22:33:00), we want to send four ping probes to www.google.com and then report the results to the user who invoked the command by sending them an email (-m, which requires Postfix or an equivalent programme):\n# echo \"ping -c 4 www.google.com\" | at -m now + 1 minute In the event that you do not use the -m option, the command will still be carried out, but nothing will be written to the standard output. On the other hand, you have the option of redirecting the output to a file instead.\nIf you do not use the -m option, the command will still be performed, but nothing will be displayed to the standard output. However, another option available to you is to save the result to a file rather than displaying it directly.\nAs an example,\nFollow these steps to run updatedb tonight at 10 o‚Äôclock (or tomorrow if the current date is later than 10 o‚Äôclock):\n# echo \"updatedb\" | at -m 22 In order to shutdown¬†the machine at 22:44 tonight (these instructions follow the same format as the one given in the previous example):\nUsing the plus sign and the time specification that you want to postpone the execution by, you may also delay it by minutes, hours, days, weeks, months, or years, just as in the first example.\nThe POSIX standard governs time requirements and standards.\nIf you wish to run a command or carry out a certain activity at a specific time just once, you should use the at command rather than the cron job scheduler. This is a good rule of thumb to follow. Utilize cron for all other circumstances. Know, How to remove FTP account in plesk\nThank You ","start-service#Start service:":" # systemctl start atd # systemctl enable atd "},"title":"How to Schedule an Activity at a Specific Time or at a Time in the Future Using the 'at' Command"},"/utho-docs/docs/linux/how-to-schedule-your-task-using-crontab/":{"data":{"":"","crontab-options#\u003cstrong\u003eCrontab Options\u003c/strong\u003e":" How to schedule your task using crontab\nDescription: In this article, we will learn how to schedule your task using crontab. The crontab programme is used to add, remove, or list the tables that tell the cron daemon what to do.\nA crontab file tells the cron daemon to do something, like ‚Äúrun this command at this time on this date.‚Äù Each user can have their own crontab. These files are in the /var directory, but you shouldn‚Äôt edit them directly.\nA cron job is any task that you schedule using crons. Cron jobs help us automate tasks that we do every hour, every day, every month, or every year.\nImportant Files: To allow user to use crontab, make entry in- /etc/cron.allow To deny user to use crontab, make entry in- /etc/cron.deny Add username in /etc/cron.deny\nBy default, every user is allowed to use crontab to schedule their tasks. But if we want to deny any particular user, make an entry in above file number 2 using vim or any other way you like.\nHow to use crontab Before using crontab, we need to understand the syntax to schedule any task using crontab.\nBelow is the syntax summary to use crontab\n* * * * * Command_to_execute Argument_for_command | | | | | | | | | Day of the Week ( 0 - 6 ) ( Sunday = 0 ) | | | | | | | Month ( 1 - 12 ) | | | | | Day of Month ( 1 - 31 ) | | | Hour ( 0 - 23 ) | Min ( 0 - 59 ) In the above summary, every asterisk( * ) represent a different time period of different values.\nAsterisk name( from left to right)ValueDescriptionMinute of the hour0-59Command would be executed at the specific minute.Hours of the day0-23Command would be executed at the specific hour.Day of the Month1-31Commands would be executed in these days of the months.Month of the year1-12The month in which tasks need to be executed.Weekdays0-6Days of the week where commands would run. Here, 0 is Sunday. Crontab Options To add/ delete/ edit a cron job crontab -e To list the cronjobs of the current user crontab -l To remove/ deinstall the cronjobs crontab -r To list another user‚Äôs cronjobs crontab -u username -l To edit another user‚Äôs cronjobs. crontab -u username -e ","description#Description:":"","examples#Examples:":"To test the crontab, first create a script called¬†test.sh¬†which prints the system date and time and appends it to a file.\n# vim test.sh #!/bin/sh date \u003e\u003e testfile To make sure that this script executes, we need to give it executable permission\n# chmod +x test.sh To run /usr/bin/test.sh at every minute\n* * * * * /usr/bin/sh test.sh To run test.sh everyday at 10pm (22:00)\n0 22 * * * /usr/bin/sh test.sh To restart the httpd service every Saturday at 1am (01:00)\n0 1 * * 7 /usr/bin/systemctl restart httpd To run test.sh at 07:30, 09:30 13:30 and 15:30\n30 7,9,13,15 * * * /usr/bin/sh test.sh Thanks you!!!","how-to-use-crontab#How to use crontab":""},"title":"How to schedule your task using crontab"},"/utho-docs/docs/linux/how-to-send-email-from-centos-7/":{"data":{"":"\nPostfix is a flexible mail server that is available on most Linux distribution. Though a full feature mail server, Postfix can also be used as a simple relay host to another mail server, or smart host.\nStep 1. Login to your server via SSH Putty.\nStep 2. Install Postfix, the SASL authentication framework, and mailx.\n#¬†yum -y install postfix cyrus-sasl-plain mailx Step 3. Restart Postfix to detect the SASL framework.\n#¬†systemctl restart postfix Step 4.¬†Start Postfix on boot\n#¬†systemctl enable postfix Step 5. Open the¬†/etc/postfix/main.cf file.\n# vi /etc/postfix/main.cf Paste the following into the file:\nrelayhost = [[smtp.gmail.com](http://smtp.gmail.com/)]: smtp_use_tls = yes smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt smtp_sasl_security_options = noanonymous smtp_sasl_tls_security_options = noanonymous Save and Exit.\n:wq Step 6.¬†Configure Postfix SASL Credentials\nAdd the Gmail credentials for authentication.¬†Create a ‚Äú/etc/postfix/sasl_passwd‚Äù file\n# touch¬†/etc/postfix/sasl_passwd Add the following line to the file:\n[[smtp.gmail.com](http://smtp.gmail.com/)]:username:password Save and exit.\n:wq NOTE: Replace username with your¬†Gmail ID¬†and password with your¬†Gmail Password\nStep 7.¬†Create a Postfix lookup table from the sasl_passwd text file by running the following command:\n#¬†postmap /etc/postfix/sasl_passwd Step 8.¬†Sending mail\nRun the following command to send mail:\n#¬†echo \"This is test mail.\" | mail -s \"message\" xyz[@yahoo.com](mailto:xxxxx@yyy.com) NOTE:¬†‚Äòxyz@yahoo.com‚Äô can be any Mail ID that the user wants to send email to.\n‚ÄòThis is test mail‚Äô¬†is the Body Message of the Email. Customizable according to the user.\n‚Äòmessage‚Äô¬†is the Subject of the Email.¬†Customizable according to the user.\nStep 9. Mail received.\nThank You!"},"title":"How to send an E-mail from CentOS 7"},"/utho-docs/docs/linux/how-to-set-manual-or-static-ip-address-on-centos/":{"data":{"ifup-eth0#ifup eth0":"\nIn this article we will discuss How to Set Manual or static IP Address on CentOS,\nThere will come a moment in your career as a Linux system administrator when you will be tasked with the responsibility of configuring networking on your machine. On desktop computers, you are able to utilise dynamic IP addresses; but, in order to set up a server architecture, you will need to configure a static IP address (at least in most cases).\nThe following information on Internet Protocol version 4 (IPv4) will be used so that we may accomplish the objectives of this tutorial.\nIP address: 192.168.10.10 Netmask: 255.255.255.0 Hostname: microhost.example.com Domain name: example.com Gateway: 192.168.10.1 DNS Server 1: 8.8.8.8 DNS Server 2:¬†4.4.4.4 To configure static IP address, open the /etc/sysconfig/network-scripts/ifcfg-eth0\nNote: The filename ifcfg-eth may varies from CentOS to Redhat linux. In CentOS it is available named as ifcfg-eth0, whereas it will be available only as eth0.\nAlso, the file name will be depend on your device name available on your Linux machines such as ens0 or so on.\n# vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=‚Äúeth0‚Äù\nHWADDR=‚Äú00:08:a2:0a:ba:b8‚Äù\nBOOTPROTO=dhcp\nONBOOT=yes\nUUID=‚Äú41171a6f-bce1-44de-8a6e-cf5e782f8bd6‚Äù\nIPV6INIT=yes\nTYPE=Ethernet\nNAME=‚Äúeth0‚Äù\nNow make changes like shown below, please do not forget to make changes according to your need\nDEVICE=eth0\nHWADDR=00:16:3e:65:de:88\nBOOTPROTO=static\nONBOOT=yes\nNM_CONTROLLED=no\nIPADDR=192.168.1.10\nNETMASK=255.255.255.0\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\nDNS2=4.4.44\nPlease note that, You only need to change the following points.\nIPADDR GATEWAY DNS1 DNS2 The rest of the entries should be left unchanged.\nNext edit¬†resolve.conf to change or update the DNS nameserver\n# vi /etc/resolv.conf nameserver 8.8.8.8\nnameserver 8.8.4.4\nOnce you have made your changes restart the networking manager service with:\n# systemctl restart NetworkManager or you can just down and up the network interface using the below command\n# ifdown eth0 ifup eth0 Now to check the configured ip on your machine, run the below command\n# ifconfig output of the command"},"title":"How to Set Manual or static IP Address on CentOS"},"/utho-docs/docs/linux/how-to-set-manual-or-static-ip-address-on-debian-server/":{"data":{"":"\nIn this article we will discuss about how to set Manual or static IP Address on Debian server, there will come a moment in your career as a¬†Linux¬†system administrator when you will be tasked with the responsibility of configuring networking on your machine. On desktop computers, you are able to utilise dynamic IP addresses; but, in order to set up a server architecture, you will need to configure a static IP address (at least in most cases).\nThe following information on¬†Internet Protocol version 4¬†(IPv4) will be used so that we may accomplish the objectives of this tutorial.\nIP address: 192.168.10.10 Netmask: 255.255.255.0 Hostname: microhost.example.com Domain name: example.com Gateway: 192.168.10.1 DNS Server 1: 8.8.8.8 DNS Server 2:¬†4.4.4.4 Before doing anything, first check the network device name of your system, in which you want to set static ip.\nifconfig output of ifconfig file\nTo configure static IP address in¬†Debian/¬†Ubuntu, open the following file:\nvi /etc/network/interfaces You will see the below lines in the above opened file.\nauto eth0 iface eth0 inet dhcp Change the line to the below code.\nauto eth0 iface eth0 inet static address 192.168.0.100 netmask 255.255.255.0 gateway 192.168.0.1 dns-nameservers 4.4.4.4 dns-nameservers 8.8.8.8 Save the file and exit the file.\nNow make the required entry in /etc/resolv.conf file as follow\n[/console]# vi /etc/resolv.conf [/console]\nnameserver 8.8.8.8 nameserver 4.4.4.4 Now just down your network device and then up the same to made the changes reflected.\nifdown eth0# ifup eth0 Now confirm whether your changes just reflected yet or not.\nifconfig output of ifconfig file"},"title":"How to Set Manual or static IP Address on Debian server"},"/utho-docs/docs/linux/how-to-set-manual-or-static-ip-address-on-fedora/":{"data":{"":"\nIn this article we will discuss How to Set Manual or static IP Address on Fedora,\nThere will come a moment in your career as a¬†Linux¬†system administrator when you will be tasked with the responsibility of configuring networking on your machine. On desktop computers, you are able to utilise dynamic IP addresses; but, in order to set up a server architecture, you will need to configure a static IP address (at least in most cases).\nThe following information on¬†Internet Protocol version 4¬†(IPv4) will be used so that we may accomplish the objectives of this tutorial.\nIP address: 192.168.10.10 Netmask: 255.255.255.0 Hostname: microhost.example.com Domain name: example.com Gateway: 192.168.10.1 DNS Server 1: 8.8.8.8 DNS Server 2:¬†4.4.4.4 To configure static IP address, open the /etc/sysconfig/network-scripts/ifcfg-eth0\nNote: The filename ifcfg-eth may varies from CentOS to Redhat linux. In CentOS it is available named as ifcfg-eth0, whereas it will be available only as eth0.\nAlso, the file name will be depend on your device name available on your Linux machines such as ens0 or so on.\nvim /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=‚Äúeth0‚Äù\nHWADDR=‚Äú00:08:a2:0a:ba:b8‚Äù\nBOOTPROTO=dhcp\nONBOOT=yes\nUUID=‚Äú41171a6f-bce1-44de-8a6e-cf5e782f8bd6‚Äù\nIPV6INIT=yes\nTYPE=Ethernet\nNAME=‚Äúeth0‚Äù\nNow make changes like shown below, please do not forget to make changes according to your need\nDEVICE=eth0\nHWADDR=00:16:3e:65:de:88\nBOOTPROTO=static\nONBOOT=yes\nNM_CONTROLLED=no\nIPADDR=192.168.1.10\nNETMASK=255.255.255.0\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\nDNS2=4.4.44\nPlease note that, You only need to change the following points.\nIPADDR GATEWAY DNS1 DNS2 The rest of the entries should be left unchanged.\nNext edit¬†resolve.conf¬†to change or update the DNS nameserver\nvi /etc/resolv.conf nameserver 8.8.8.8\nnameserver 8.8.4.4\nOnce you have made your changes restart the networking manager service with:\nsystemctl restart NetworkManager or you can just down and up the network interface using the below command\nifdown eth0 # ifup eth0 Now to check the configured ip on your machine, run the below command\nifconfig output of the command\nI hope you are now fully able to set a manual or static IP address on Fedora"},"title":"How to Set Manual or static IP Address on Fedora"},"/utho-docs/docs/linux/how-to-set-manual-or-static-ip-address-on-ubuntu-server/":{"data":{"ifup-eth0#ifup eth0":" How to Set Manual or static IP Address on Ubuntu server\nIn this article we will discuss about how to set Manual or static IP Address on Ubuntu server, there will come a moment in your career as a Linux system administrator when you will be tasked with the responsibility of configuring networking on your machine. On desktop computers, you are able to utilise dynamic IP addresses; but, in order to set up a server architecture, you will need to configure a static IP address (at least in most cases).\nThe following information on Internet Protocol version 4 (IPv4) will be used so that we may accomplish the objectives of this tutorial.\nIP address: 192.168.10.10 Netmask: 255.255.255.0 Hostname: microhost.example.com Domain name: example.com Gateway: 192.168.10.1 DNS Server 1: 8.8.8.8 DNS Server 2:¬†4.4.4.4 Before doing anything, first check the network device name of your system, in which you want to set static ip.\n# ifconfig output of ifconfig file\nTo configure static IP address in¬†Debian/¬†Ubuntu, open the following file:\n# vi /etc/network/interfaces You will see the below lines in the above opened file.\nauto eth0 iface eth0 inet dhcp Change the line to the below code.\nauto eth0 iface eth0 inet static address 192.168.0.100 netmask 255.255.255.0 gateway 192.168.0.1 dns-nameservers 4.4.4.4 dns-nameservers 8.8.8.8 Save the file and exit the file.\nNow make the required entry in /etc/resolv.conf file as follow\n[/console]# vi /etc/resolv.conf [/console]\nnameserver 8.8.8.8 nameserver 4.4.4.4 Now just down your network device and then up the same to made the changes reflected.\n# ifdown eth0 ifup eth0 Now confirm whether your changes just reflected yet or not.\n# ifconfig output of ifconfig file"},"title":"How to Set Manual or static IP Address on Ubuntu server"},"/utho-docs/docs/linux/how-to-set-or-change-timezone-on-ubuntu-20-04/":{"data":{"":"","changing-the-time-zone-using-thetimedatectlcommand#Changing the time zone using the¬†timedatectl¬†command":"You‚Äôll need to know the long name of the time zone you want to use before you can change it. ‚ÄúRegion/City‚Äù is the format for the time zones.\nYou can either list the files in the /usr/share/zoneinfo directory or use the timedatectl command with the list-timezones option\n# timedatectl list-timezones Output:\n.\n.\n.\nAfrica/Windhoek\nAmerica/Adak\nAmerica/Anchorage\nAmerica/Araguaina\nAmerica/Argentina/Buenos_Aires\nAmerica/Argentina/Catamarca\nAmerica/Argentina/Cordoba\nAmerica/Argentina/Jujuy\nAmerica/Argentina/La_Rioja\nAmerica/Argentina/Mendoza\nAmerica/Argentina/Rio_Gallegos\nAmerica/Argentina/Salta\nAmerica/Argentina/San_Juan\nAmerica/Argentina/San_Luis\nAmerica/Argentina/Tucuman\nAmerica/Argentina/Ushuaia\nAmerica/Asuncion\nAmerica/Atikokan\nAmerica/Bahia\nAmerica/Bahia_Banderas\nAmerica/Barbados\nlines 1-40\nOnce you know the correct time zone for where you are, run the following command as the\n# timedatectl set-timezone Asia/Kolkata Now again run the timedatectl command to ensure the changes.\n# timedatectl Hopefully now you can Set or Change Timezone on Ubuntu 20.04\nMust Read : How to Install Hastebin on Ubuntu 20.04","introduction#Introduction":"In this article you will know How to To Set or Change Timezone on Ubuntu 20.04. For many system-related tasks and processes, it is important to use the right time zone. For example, the cron daemon uses the timezone of the system to run cron jobs, and the timestamps in the log files are also based on the timezone of the system.\nOn Ubuntu, the time zone is set when the system is installed, but it is easy to change it later.","prerequisites#Prerequisites":" A ubuntu server A Super user ( root ) or any normal user with SUDO privileges. ","what-is-current-timezone-#What is current timezone ?":"The command-line programme timedatectl lets you see and change the system‚Äôs time and date. It can be used on all modern Linux systems that use systemd, such as Ubuntu 20.04.\nRun timedatectl with no arguments to see what time zone the system is in:\n# timedatectl Output of timedatectl command\n/etc/localtime is linked to a binary timezone identifier in the /usr/share/zoneinfo directory, which sets the system‚Äôs timezone.\nYou can also find the file that the symlink points to to see the current system‚Äôs time zone:\n# ls -l /etc/localtime Output:\nlrwxrwxrwx 1 root root 33 May 10 2020 /etc/localtime -\u003e /usr/share/zoneinfo/Europe/Berlin\nIn the /etc/timezone file, the timezone of the system is also written:\n# cat /etc/timezone Output:\nEurope/Berlin"},"title":"How To Set or Change Timezone on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-set-up-a-node-js-application-with-apache-on-centos-7/":{"data":{"":"\nNode.js is an open source Javascript programming environment to make server-side and networking applications simple to create.","installing-and-using-pm2#Installing and Using PM2":"We will install PM2, which is a process manager for Node.js applications. PM2¬†offers¬†a¬†simple¬†way¬†to¬†manage¬†programs¬†and¬†daemonize¬†them¬†(run¬†them¬†as¬†a¬†service). It will start the nodejs application on boot time.\nTo¬†install¬†PM2¬†use¬†this¬†command:\nnpm install pm2@latest ‚Äìg We will use the¬†pm2 start¬†command to run the application,¬†myapps.js, in the background:\npm2 start myapps.js sudo pm2 startup systemd To ensure PM2 knows which applications to start on boot, we need to save the current process list.\npm2 save pm2 list We have completed the installation and configuration of node js with apache web server.\nThank You","installing-nodejs#Installing Node.js":"First of all, You need to enable node.js yum repository in your system provided by the Node.js official website. You also need development tools to build native add-ons to be installed on your system.\nyum install -y gcc-c++ make curl -sL https://rpm.nodesource.com/setup_14.x | sudo -E bash - After adding a yum repository in your system lets install Node.js package. NPM will also be installed with node.js. This command will also install many other dependent packages on your system.\nsudo yum install nodejs After installing node.js verify and check the installed version. You can find more details about current version on node.js¬†official website.\nnode -v Also, check the version of npm.\nnpm -v Afterward , we need to test the nodejs functionality by putting a test application.\nWe will create a test file myapp.js in current working directory and the content of file would be same as given below. You can use IP address or domain name as per your requirement.\nvi myapps.js var http = require('http'); http.createServer(function (req, res) { res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello World'); }).listen(3000, \"APP_PRIVATE_IP_ADDRESS\"); console.log('Server running at http://APP_PRIVATE_IP_ADDRESS:3000/'); Now we can test our application whether it is running or not.\nnode myapps.js You will see the output when you execute the URL in browser.\nhttp://APP_PRIVATE_IP_ADDRESS:3000","setting-up-an-apache-reverse-proxy-server#Setting Up an Apache Reverse Proxy Server":"Now that your application is running, and listening to a private IP address, you need to set up a way to access it for your users. For this reason, we must set up an apache web server as a reverse proxy. When you already have an apache server installed, you can easily copy the location block to the server block of your choosing (ensure that the address does not interfere with any current information on your web server).\nNow we have to define proxy and virtual-host in apache:\nPlease add these lines at the end of the file /etc/httpd/conf/httpd.conf\nvi /etc/httpd/conf/httpd.conf These below two lines for the proxy-\nLoadModule proxy_module modules/mod_proxy.so LoadModule proxy_http_module modules/mod_proxy_http.so These below given lines for the virtual host-\n\u003cVirtualHost *:80\u003e ServerName APP_PRIVATE_IP_ADDRESS/domain name ProxyRequests On ProxyPass / http://APP_PRIVATE_IP_ADDRESS:3000 ProxyPassReverse / http://APP_PRIVATE_IP_ADDRESS :3000 \u003c/VirtualHost\u003e "},"title":"How-to-set-up-a-node-js-application with apache on-centos-7"},"/utho-docs/docs/linux/how-to-set-up-ssh-keys-on-ubuntu-20-04/":{"data":{"":"\nIntroduction\nIn this article we will learn How to Set Up SSH Keys on Ubuntu 20.04‚Ä¶.\nSSH is an encrypted server administration and communication protocol. Most of your time with an Ubuntu server will be spent in an SSH terminal session.\nIn this guide, we‚Äôll set up Ubuntu 20.04 SSH keys. All users should use SSH keys to log into their server securely.\nTo Set Up SSH Keys on Ubuntu 20.04 follow the below steps that will help you to increase your knowledge..","step-1-putting-together-the-key-pair#Step-1. Putting together the Key Pair":"First, create a key pair on the client (your computer):\n#ssh-keygen Recent versions of ssh-keygen create a 3072-bit RSA key pair by default, which is secure for most use cases (you may optionally pass in the -b 4096 flag to create a larger 4096-bit key).\nYou should see this after entering the command:\nOutput Generating public/private rsa key pair. Enter file in which to save the key (/your_home/.ssh/id_rsa): Enter to save the key pair in your home directory‚Äôs.ssh/ subdirectory, or specify another path.\nIf you have an SSH key pair, you may see this prompt:\nOutput /home/your_home/.ssh/id_rsa already exists. Overwrite (y/n)? If you overwrite the disc key, you can‚Äôt use the old key to authenticate. This is a destructive process that can‚Äôt be undone.\nYou‚Äôll see a prompt:\nOutput Enter passphrase (empty for no passphrase): Optionally, enter a secure passphrase here. A passphrase prevents unauthorised logins. How to Configure SSH Key-Based Authentication on a Linux Server for more security information.\nOutput should look like this:\nOutput Your identification has been saved in /your_home/.ssh/id_rsa Your public key has been saved in /your_home/.ssh/id_rsa.pub The key fingerprint is: SHA256:/hk7MJ5n5aiqdfTVUZr+2Qt+qCiS7BIm5Iv0dxrc3ks user@host The key's randomart image is: +---[RSA 3072]----+ | .| | + | | + | | . o . | |o S . o | | + o. .oo. .. .o| |o = oooooEo+ ‚Ä¶o| |.. o _o+=._+o‚Ä¶.| | =+=ooB=o‚Ä¶. | +----[SHA256]-----+ Now you have public and private keys to authenticate. Next, place the public key on your server so you can log in with SSH keys.","step-2--copies-of-the-public-key-being-saved-to-your-ubuntu-server#Step-2 . Copies of the Public Key Being Saved to Your Ubuntu Server":"To copy your public key to Ubuntu, use ssh-copy-id. Due to its simplicity, this method is recommended. If you don‚Äôt have ssh-copy-id on your client machine, use one of the two methods below (copying via password-based SSH, or manually copying the key).\nCopying the Public Key Using¬†ssh-copy-id\nssh-copy-id is included by default in many operating systems, so it may be on your system. You must have password-protected SSH access to your server for this to work.\nTo use the tool, enter the remote host and SSH user account password. Your public SSH key will be copied to this account.\nSyntax\n#ssh-copy-id username@remote_host It is possible that you will see the following message:\nOutput The authenticity of host '203.0.113.1 (203.0.113.1)' can't be established. ECDSA key fingerprint is fd:fd:d4:f9:77:fe:73:84:e1:55:00:ad:d6:6d:22:fe. Are you sure you want to continue connecting (yes/no)? yes Your computer doesn‚Äôt recognise the remote host. First time connecting to a new host. Enter ‚Äúyes‚Äù to proceed.\nNext, the utility searches for the id rsa.pub key we created. When it finds the key, it requests the remote user‚Äôs password.\nOutput /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys username@203.0.113.1's password: Press ENTER after entering the password (your typing is hidden for security). The utility will use your password to connect to the remote host. It copies your /.ssh/id rsa.pub key to a file called authorized keys in the remote account‚Äôs /.ssh directory.\nThis is the output:\nOutput Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'username@203.0.113.1'\" and check to make sure that only the key(s) you wanted were added. ","step-4--ubuntu-ssh-key-authentication#Step-4 . Ubuntu SSH Key Authentication":"After completing one of the above procedures, you should be able to log into the remote host without the password.\nIt‚Äôs the same process.\n#ssh username@remote_host Output The authenticity of host '203.0.113.1 (203.0.113.1)' can't be established. ECDSA key fingerprint is fd:fd:d4:f9:77:fe:73:84:e1:55:00:ad:d6:6d:22:fe. Are you sure you want to continue connecting (yes/no)? yes Using password authentication will further secure your system if key-based authentication was successful.","step-5--server-password-authentication#Step-5 . Server Password Authentication":"If you can log into SSH without a password, you‚Äôve successfully set up SSH-key-based authentication. However, password-based authentication is still active, leaving your server vulnerable to brute-force attacks.\nBefore completing this section, make sure SSH-key-based authentication is enabled for the root account or a non-root account with sudo privileges. This step locks password-based logins, so ensuring administrative access is key.\nAfter confirming that your remote account has administrative privileges, log into your remote server with SSH keys as root or with sudo privileges. Then, open SSH‚Äôs configuration file.\n#sudo nano /etc/ssh/sshd_config Search the file for PasswordAuthentication. This line can be #-commented out. Remove the # and set no on the line. This disables SSH password login.\n/etc/ssh/sshd_config . . . PasswordAuthentication no . . . When finished, press CTRL+X, Y to confirm saving, and ENTER to exit nano. To activate the changes, restart sshd.\n#sudo systemctl restart ssh Open a new terminal window to test SSH before closing your current session.\n#ssh username@remote_host After verifying SSH‚Äôs functionality, you can close all server sessions.\nUbuntu‚Äôs SSH daemon only accepts SSH-key authentication. Disabled password logins.\nI hope you have understood how to set up SSH keys on Ubuntu 20.04.\nMust read :-\nThankyou","step3-copying-the-public-key-using-ssh#Step.3 Copying the Public Key Using SSH":"If you don‚Äôt have ssh-copy-id but have password-based SSH access to a server account, you can upload keys using conventional SSH.\nWe can do this by using the cat command to read our local public SSH key and piping it to the remote server.\nOn the other hand, we can make sure our /.ssh directory exists and has the right permissions.\nWe can then output the piped content into a file called authorized keys. We‚Äôll use ¬ª to append content instead of overwrite. This lets us add keys without deleting them.\nCommand is:\n#cat ~/.ssh/id_rsa.pub | ssh username@remote_host \"mkdir -p ~/.ssh \u0026\u0026 touch ~/.ssh/authorized_keys \u0026\u0026 chmod -R go= ~/.ssh \u0026\u0026 cat \u003e\u003e ~/.ssh/authorized_keys\" Output The authenticity of host '203.0.113.1 (203.0.113.1)' can't be established. ECDSA key fingerprint is fd:fd:d4:f9:77:fe:73:84:e1:55:00:ad:d6:6d:22:fe. Are you sure you want to continue connecting (yes/no)? yes This means your computer can‚Äôt see the remote host. When connecting to a new host, this happens. Press ENTER to continue.\nAfter that, you‚Äôll be asked for the remote user password.\nOutput username@203.0.113.1's password: ","step4-making-a-manual-copy-of-the-public-key#Step.4 Making a Manual Copy of the Public Key":"If you don‚Äôt have password-based SSH access to your server, you must do the above manually.\nWe will manually append your id rsa.pub file to your remote machine‚Äôs /.ssh/authorized keys file.\nType this into your computer to view id rsa.pub:\n#cat ~/.ssh/id_rsa.pub Output ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCqql6MzstZYh1TmWWv11q5O3pISj2ZFl9HgH1JLknLLx44+tXfJ7mIrKNxOOwxIxvcBF8PXSYvobFYEZjGIVCEAjrUzLiIxbyCoxVyle7Q+bqgZ8SeeM8wzytsY+dVGcBxF6N4JS+zVk5eMcV385gG3Y6ON3EG112n6d+SMXY0OEBIcO6x+PnUSGHrSgpBgX7Ks1r7xqFa7heJLLt2wWwkARptX7udSq05paBhcpB0pHtA1Rfz3K2B+ZVIpSDfki9UVKzT8JUmwW6NNzSgxUfQHGwnW7kj4jp4AT0VZk3ADw497M2G/12N0PPB5CnhHf7ovgy6nL1ikrygTKRFmNZISvAcywB9GVqNAVE+ZHDSCuURNsAInVzgYo9xgJDW8wUw2o8U77+xiFxgI5QSZX3Iq7YLMgeksaO4rBJEa54k8m5wEiEE1nUhLuJ0X/vh2xPff6SQ1BL/zkOhvJCACK6Vb15mDOeCSq54Cr7kvS46itMosi/uS66+PujOO+xt/2FWYepz6ZlN70bRly57Q06J+ZJoc9FfBCbCyYH7U/ASsmY095ywPsBo1XQ9PqhnN1/YOorJ068foQDNVpm146mUpILVxmq41Cj55YKHEazXGsdBIbXWhcrRf4G2fJLRcGUr9q8/lERo9oxRm5JFX6TCmj6kmiFqv+Ow9gI0x8GvaQ== demo@test Once you have remote server access, make sure /.ssh exists. If the directory doesn‚Äôt exist, this command creates it.\nmkdir -p ~/.ssh Now you can create or edit the authorized keys file. Using this command, you can add your id rsa.pub file to the authorized keys file.\necho public_key_string \u003e\u003e ~/.ssh/authorized_keys Substitute the cat /.ssh/id rsa.pub output in the above command. It‚Äôs ssh-rsa. AAAA‚Ä¶.\nFinally, we‚Äôll set the permissions for /.ssh and authorized keys:\nchmod -R go= ~/.ssh This removes ‚Äúgroup‚Äù and ‚Äúother‚Äù permissions for /.ssh/.\nIf you‚Äôre using root to set up user keys, make sure the /.ssh directory belongs to the user and not root:\n#chown -R microhost:microhost ~/.ssh In this tutorial, our user is sammy; substitute your own username.\nOur Ubuntu server now supports passwordless login."},"title":"How to Set Up SSH Keys on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-setup-flatpak-on-ubuntu-20-04/":{"data":{"":"","1-keep-the-server-updated#1. Keep the server updated":"","2-install-flatpak#2. Install Flatpak":"","3-add-the-flathub-repository#3. Add the Flathub repository":"","4-install-apps#4. Install apps":"\nIntroduction In this article you will learn to Setup Flatpak on Ubuntu 20.04, When it comes to creating and distributing Linux desktop applications, Flatpak is the latest technology. Follow the steps in this article to set up Flatpak packages on Ubuntu 20.04 and manage them.\nA application for managing software deployment and packages on Linux, originally known as xdg-app, Flatpak is now known as just Flatpak. It is promoted as having a sandbox environment, in which users are able to operate application software while remaining completely separate from the rest of the system.\nIn order for applications that use Flatpak to gain access to resources such as Bluetooth, sound (using PulseAudio), the network, and files, the appropriate permissions must be granted. These rights are established by the person who is in charge of maintaining the Flatpak package, but users on their own systems have the ability to add or delete them.\nAnother important benefit of using Flatpak is that it enables application developers to directly offer updates to end users, bypassing distributions in the process. This eliminates the need for the developers to package and test the programme in a manner specific to each distribution.\n1. Keep the server updated # apt upgrade -y 2. Install Flatpak # apt install flatpak -y 3. Add the Flathub repository The best site to find and download Flatpak applications is Flathub. If you want to make use of it, type:\n# flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo 4. Install apps # flatpak install flathub com.nordpass.NordPass -y To run the app, use the following command:\n# flatpak run com.nordpass.NordPass To update the app, use following command:\n# flatpak update com.nordpass.NordPass To list all installed app, use following command:\n# flatpak list ","conclusion#Conclusion":"Hopefully, you now understand how to setup Flatpak on Ubuntu 20.04. The setup process was successful and is now complete.\nThank You üôÇ","introduction#Introduction":""},"title":"How to Setup Flatpak on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-setup-load-balancer-for-applications-running-on-custom-port/":{"data":{"":"\nIntroduction: In this article, we are going to setup a testing environment to test the load distribution using loadbalancer among applications running on custom ports rather then port 80. In this testing , we are using 3 different servers installed and configured apache on 3 different ports.\nStep 1: Firstly, we need to make 3 different servers ready with apache running on respective ports (8080, 8081, 8082) .¬†You can refer ‚Äúapache port change‚Äù article for more details on apache installation on custom port.¬†Step 2: We need to create a load balancer to check the traffic distribution among these above three servers. Please have a look on the below screenshot for your reference.\nStep 3: Once the loadbalacer would be successfully completed, we need to add our backend servers in it to distribute the traffic of all three servers. For managing the loadbalancer we need to click on action button and then click on manage load¬†balancer option as per the below screenshot.\nStep 4¬†: Now we have to add the servers in the loadbalancer.\nStep 5¬†: Once the servers will be added then we need to modify the loadbalancer configuration file, as it takes by default traffic distribution on port 80. Hence we need to change the port to our respective ports that are 8080, 8081,8082. The path of configuration file is /etc/haproxy/haproxy.cfg. Please have a look on the highlighted part of screenshot for more details.\nStep 6¬†: Once the modification will be done in the haproxy¬†conf file then you need to save the file and restart the haproxy service using below command\n# **systemctl restart haproxy** Step 7: Now we can verify the testing while accessing the lodbalancer ip in browser.\nThank you :)"},"title":"HOW TO SETUP LOAD BALANCER FOR APPLICATIONS RUNNING ON CUSTOM PORT"},"/utho-docs/docs/linux/how-to-setup-rsyslog-server-on-ubuntu-22-04/":{"data":{"":" How to configure rsyslog server on ubuntu 20","introduction#Introduction:":"In this tutorial, you will learn how to setup the Rsyslog server on Ubuntu 22.04. Logs are an important part of the core of any network. They keep a lot of diagnostic information, like how the kernel, applications, daemons, services, network behaviour, user actions, and so on, work.\nThey make sure that server events are clear so that problems with the Linux system can be fixed. The best way to handle and look at log data is to put all of the logs in one place. Centralising logs protects against accidental data loss and makes sure that they can be accessed even if the server is down.\nRsyslog is the most famous tool for putting all of a Linux system‚Äôs logs in one place. In this tutorial, we‚Äôll learn how to centralise the Ubuntu 22.04 Rsyslog logging system.","prerequisites#Prerequisites":" Super user or any normal user with SUDO privileges.\napt repository configured. You can deploy a new, speedy server within 30 seconds by signing up here.","setup-rsyslog-client#Setup Rsyslog client":"Step 7: Once you have finished setting up the rsyslog server, go to your rsyslog client computers and set them up such that they submit logs to a distant rsyslog server.\nvi /etc/rsyslog.conf Step 8: Allow preservation of FQDN in your configuration file:\n$PreserveFQDN on Step 9: Add remote rsyslog server information at the end:\n*.* @@rsysog-server-ip:514 ## for using IP address instead of FQDN#OR*.* rsysog-server-fqdn:514 ## for using FQDN Step 9: Do more required changes in the client configuration file.\n$ActionQueueFileName queue$ActionQueueMaxDiskSpace 1g$ActionQueueSaveOnShutdown on$ActionQueueType LinkedList$ActionResumeRetryCount -1 Client rsyslog configuration\nStep 10: Restart the rsyslog services on client side.\nsystemctl restart rsyslog And this is what you have learned about how to set up an rsyslog server on Ubuntu 22.04","setup-rsyslog-server-on-ubuntu-2204#Setup Rsyslog (server) on Ubuntu 22.04":"Step 1: Most versions of Linux come with the rsyslog package and all of its dependencies already loaded. Run the following command to make sure it was installed correctly:\nrpm -qa rsyslog Step 2: If it is not installed, you can install it using the below command.\napt-get update \u0026\u0026 apt-get install rsyslog -y Step 3: Now set up the rsyslog service to run in server mode:\nvi /etc/rsyslog.conf Step 4: Uncomment the lines for linking the udp and tcp ports:\nmodule(load=\"imudp\")input(type=\"imudp\" port=\"514\")# provides TCP syslog receptionmodule(load=\"imtcp\")input(type=\"imtcp\" port=\"514\") Step 5: Let‚Äôs make a template that tells rsyslog server how to store syslog messages as they come in. Add the template just before the end of the file.\n$template remote-incoming-logs,\"/var/log/%HOSTNAME%/%PROGRAMNAME%.log\" *.* ?remote-incoming-logs\u0026 ~ After entering these detials, just same and exit it.\nStep 6: Restart your rsyslog service to reflect the changes.\nsystemctl restart rsyslog ","what-is-rsyslog#What is Rsyslog:":"Rsyslog is a powerful, lightweight, open-source log processing daemon that can read messages from many different systems and send them out in different forms. It is an improved version of Syslog server, and it can be set up in the same ways.\nBut you can add more modules to it to handle log messages and send them to different log files and devices. This makes it an enterprise-level log management system.\nThe client-server approach of Rsyslog lets it be set up as either a client or a centralised logging system, so it can do both jobs at the same time.\nIt can run as a server that other network devices send logs to. Or, as a client, by sending log messages about events happening on a local machine to a remote syslog server."},"title":"How to setup Rsyslog server on Ubuntu 22.04"},"/utho-docs/docs/linux/how-to-setup-sftp-only-user-accounts-on-centos-7/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how To Setup SFTP-only User Account on CentOS 7\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Setup SFTP-only user account on CentOS 7.\nBy reading the information contained in this article. It is possible that a Systems Administrator will occasionally need to create a new user account for a user who will have no access to the system other than the ability to manage their own files via SFTP. This post will provide you with a solution to the problem that you are having and show you how to fix it.\nIn addition, if you are not currently logged in as root, you will require access to the sudo command in order to carry out the steps outlined in this article.","step-1-create-a-dedicated-sftp-group-and-a-dedicated-sftp-user#Step 1: Create a dedicated sFTP group and a dedicated sFTP user":" # groupadd sftpusers # useradd -g sftpusers -s /sbin/nologin microhost # passwd microhost ","step-2-modify-the-configuration-of-the-sshd-service#Step 2: Modify the configuration of the sshd service":"Open the sshd service‚Äôs configuration file:\n# vi /etc/ssh/sshd_config Find the line:\nSubsystem sftp /usr/libexec/openssh/sftp-server Replace it with:\nSubsystem sftp internal-sftp Please add the following lines to the end of the file. The sftpusers group name must match the one you provided in the previous step.\nMatch Group sftpusers X11Forwarding no AllowTcpForwarding no ChrootDirectory %h ForceCommand internal-sftp Save and quit by escape :wq\nAfter making modifications, the sshd service must be restarted for them to take effect.\n# systemctl restart sshd.service ","step-3-create-a-dedicated-directory-for-the-sftp-only-user#Step 3: Create a dedicated directory for the sFTP-only user":"For the sFTP-only user, you must choose a folder and limiting their access to that folder‚Äôs contents:\n# chown -R root /home/microhost # chmod -R 755 /home/microhost # mkdir /home/microhost/files # chown microhost. /home/microost/files Now, the user microhost can only upload or download files in the directory /home/microhost/files. He or she can never touch other users‚Äô files.\nKnow How to Install Varnish Cache with Apache on CentOS 7"},"title":"How to Setup SFTP-only User Account on CentOS 7"},"/utho-docs/docs/linux/how-to-setup-sftp-user-account-on-fedora/":{"data":{"":"","conclusion#Conclusion":"In this guide, you successfully setup SFTP user Account on Fedora server, then tested connectivity through a terminal session and FileZilla. You can create multiple users with different directories to securely upload and download files on your server.\nKnow, How to Install MongoDB on¬†Fedora 36/35/34\nThank You üôÇ","configure-sftp#Configure SFTP":"After creating the user accounts and the sftp group, you will need to enable SFTP in the primary SSH configuration file.\n# vi /etc/ssh/sshd_config # line 123: comment out and add a line\n#Subsystem sftp /usr/libexec/openssh/sftp-server Subsystem sftp internal-sftp Add the following lines to the end of the file. Replace sftpcorner with your actual sftp group.\nMatch Group sftpcornerChrootDirectory %hPasswordAuthentication yesAllowTcpForwarding noX11Forwarding noForceCommand internal-sftp Save and close the file by escape :wq\nRestart the SSH server for changes to take effect.\n# systemctl restart sshd ","introduction#Introduction":"In this article you will know about how to Setup SFTP User Account on Fedora.\nA secure shell (SSH) session is required for the use of the Secure File Transfer Protocol (SFTP), which is a secure method for exchanging data between a local and distant computer. It is an upgraded version of the standard file transfer protocol (FTP), which adds an additional layer of protection during the process of transferring files and establishing a connection.\nIn this tutorial, you will learn how to set up SFTP User account on Fedora and enable the user to only access files that are located within the user‚Äôs home directory.","login-to-sftp#Login to SFTP":" # sftp exampleuser@Server_IP List files within the directory. Your output should be similar to the one below:\nexampleuser@Server_IP‚Äôs password:\nConnected to server_IP\nFileZilla and Cyberduck are the most popular SFTP clients available for Windows, Mac, and Linux desktop to test connectivity using a desktop client.","make-a-new-group-for-sftp-users-and-join-it-you-should-provide-the-name-of-the-group-you-want-instead-of-sftpcorner#Make a new group for SFTP users and join it. You should provide the name of the group you want instead of sftpcorner.":" # groupadd sftpcorner Make a brand-new account for the user. Simply replace ‚Äúexampleuser‚Äù with the user name you want to use.\n# adduser exampleuser Please create a password for exampleuser.\n# passwd example After that, the user should be added to the SFTP group.\n# usermod -G sftpcorner exampleuser Put access restrictions on the user so they can‚Äôt access files outside of their home directory.\n# chown root:root /home/exampleuser Now, within the user‚Äôs home directory, create new subdirectories. These are used in the process of file transfer.\n# mkdir /home/exampleuser/uploads Give the user ownership of the subdirectories and files in the directory.\n# chown -R exampleuser:exampleuser /home/exampleuser/uploads The next step is to give the user permission to read and write all of the files contained within their home directory.\n# chmod -R 755 /home/exampleuser/ "},"title":"How to Setup SFTP User Account on Fedora"},"/utho-docs/docs/linux/how-to-setup-sftp-user-accounts-on-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"In this guide, you successfully setup SFTP user account on Ubuntu 20.04 server, then tested connectivity through a terminal session and FileZilla. You can create multiple users with different directories to securely upload and download files on your server.\nThank You üôÇ","configure-sftp#Configure SFTP":"After creating the user accounts and the sftp group, you will need to enable SFTP in the primary SSH configuration file.\nUsing an editor of your choice, open the file¬†/etc/ssh/sshd_config.\n# vi /etc/ssh/sshd_config Add the following lines to the end of the file. Replace sftpcorner with your actual sftp group.\nMatch Group sftpcorner ChrootDirectory %h PasswordAuthentication yes AllowTcpForwarding no X11Forwarding no ForceCommand internal-sftp Save and close the file by escape :wq\nBelow are the functions for each of the above configuration lines:\n1. Match Group sftpcorner:¬†Match the user group¬†sftpcorner.\n2. ChrootDirectory %h:¬†Restrict access to directories within the user‚Äôs home directory.\n3. PasswordAuthentication yes:¬†Enable password authentication.\n4. AllowTcpForwarding no:¬†Disable TCP forwarding.\n5. X11Forwarding no:¬†Don‚Äôt permit Graphical displays.\n6. ForceCommand internal-sftp:¬†Enable SFTP only with no shell access.\nAlso, confirm if SFTP is enabled (it is by default). The line below should be uncommented in¬†/etc/ssh/sshd_config:\noverride default of no subsystems ```Subsystem sftp /usr/lib/openssh/sftp-server Restart the SSH server for changes to take effect.\n# systemctl restart sshd ","introduction#Introduction":"In this article you will know about to Setup SFTP User Account on Ubuntu 20.04.\nA secure shell (SSH) session is required for the use of the Secure File Transfer Protocol (SFTP), which is a secure method for exchanging data between a local and distant computer. It is an upgraded version of the standard file transfer protocol (FTP), which adds an additional layer of protection during the process of transferring files and establishing a connection.\nIn this tutorial, you will learn how to set up SFTP User accounts on Ubuntu 20.04 and enable the user to only access files that are located within the user‚Äôs home directory.","login-to-sftp#Login to SFTP":"Open a new terminal window and log in with¬†sftp¬†using a valid user account and password.\n# sftp exampleuser@Server_IP List files within the directory. Your output should be similar to the one below:\nexampleuser@Server_IP‚Äôs password:\nConnected to server_IP\n# ls Also, try creating a new directory within the subdirectory to test user permissions.\n# cd uploads # mkdir files Confirm creation of the new directory:\n# ls FileZilla and Cyberduck are the most popular SFTP clients available for Windows, Mac, and Linux desktop to test connectivity using a desktop client.","setup-sftp#Setup SFTP":"Make a new group for SFTP users and join it. You should provide the name of the group you want instead of sftpcorner.\n# addgroup sftpcorner Make a brand-new account for the user. Simply replace ‚Äúexampleuser‚Äù with the user name you want to use.\n# adduser exampleuser To continue, kindly enter the full name of the user and their password.\nAfter that, the user should be added to the SFTP group.\n# usermod -G sftpcorner exampleuser Put access restrictions on the user so they can‚Äôt access files outside of their home directory.\n# chown root:root /home/exampleuser Now, within the user‚Äôs home directory, create new subdirectories. These are used in the process of file transfer.\n# mkdir /home/exampleuser/uploads Give the user ownership of the subdirectories and files in the directory.\n# chown -R exampleuser:exampleuser /home/exampleuser/uploads The next step is to give the user permission to read and write all of the files contained within their home directory.\n# chmod -R 755 /home/exampleuser/ "},"title":"How to Setup SFTP User Account on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-solve-cannot-connect-to-cwp-admin-panel/":{"data":{"":"\nHostname SSL cert can be reason for not starting cwp when all ports are allowed in OS and Cloud Firewall.\nStep 1. Login to your linux server\nRun The following command:\nsh /usr/local/cwpsrv/htdocs/resources/scripts/generate_hostname_ssl Step 2. Now run the follwoing command\nsh /usr/local/cwpsrv/htdocs/resources/scripts/restart_cwpsrv Now connecting your CWP through browser by hitting http://server_ip:2030\nYour CWP should now have no difficulty in connecting.\nThank you!"},"title":"How to solve \"Cannot connect to CWP Admin Panel\""},"/utho-docs/docs/linux/how-to-solve-zimbra-error-message-does-not-meet-ipv6-sending-guidelines-regarding-ptr/":{"data":{"":"\nZimbra Webpage ‚Äî zimbra\nAs we are using MTA hence i have logged in to MTA server and checked postfix inet protocol and found it was enabled for all\nLog into your server SSH and run the following commands:\nroot@mta:/etc/postfix# postconf inet_protocols inet_protocols = all root@mta:/etc/postfix# postconf inet_protocols=ipv4 root@mta:/etc/postfix# postconf inet_protocols inet_protocols = ipv4 root@mta:/etc/postfix# service postfix restart root@mta:/etc/postfix# Thank you!"},"title":"How to solve Zimbra error \"message does not meet IPv6 sending guidelines regarding PTR\""},"/utho-docs/docs/linux/how-to-start-stop-and-restart-mysql-server-on-centos-7/":{"data":{"":"","introduction#Introduction":"MySQL is¬†a database management system. This article will show you how to start, stop, and restart MySQL Server on CentOS 7, It may be anything from a simple shopping list to a picture gallery or the vast amounts of information in a corporate network. To add, access, and process data stored in a computer database, you need a database management system such as MySQL Server.\nOne of the most well-known and widely used open-source relational databases is called MariaDB Server. It is made by the people who created MySQL in the first place, so you know it will always be open source. It is included in the majority of cloud providers and comes pre-configured in the majority of Linux versions.\nIt is founded on the values of performance, openness, and stability, and the MariaDB Foundation promises that contributions will be accepted based on the strength of their technical content. Recent additions to the capabilities include compatibility features with Oracle Database, enhanced clustering with Galera Cluster 4, and Temporal Data Tables, which make it possible to query the data as it existed at any point in time in the past.\nYou have to Install MySQL by using the below article.\nWe will stop the service of MySQL with the following command.\n# systemctl stop mysqld We will start the service of MySQL with the following command.\n# systemctl start mysqld We will restart the service of MySQL with the following command.\n# systemctl restart mysqld We can check the status of MySQL with the following command.\n# systemctl status mysqld Hopefully, this article have guide you how to start, stop, and restart MySQL Server on CentOS 7.\nThank You üôÇ"},"title":"How to Start, Stop, and Restart MySQL Server on centos 7"},"/utho-docs/docs/linux/how-to-switch-su-to-a-different-user-account-without-a-password/":{"data":{"":"","1pam-authentication-module#\u003cstrong\u003e1.PAM Authentication Module\u003c/strong\u003e":"","2use-of-the-sudoers-file#\u003cstrong\u003e2.Use of the Sudoers File\u003c/strong\u003e":"","description#\u003cstrong\u003eDescription\u003c/strong\u003e":"","thank-you#\u003cstrong\u003eThank You\u003c/strong\u003e":"\nDescription Without requiring a password, we will demonstrate how to switch to a different user account or log in to a certain account. For instance, we have a user account that is called postgres, which is the default name for the PostgreSQL superuser system account. We want every user in the group called postgres to be able to switch to the postgres account using the su command without having to enter a password.\nOnly the root user has the ability to switch to another user account without having to input their password when doing so. Any other user who attempts to switch user accounts will be prompted to enter the password of the user account they are switching to (or, if they are using the sudo command, they will be prompted to enter their own password). If the user does not provide the correct password, they will receive a ‚Äúauthentication failed‚Äù error message, which is displayed in the following screenshot.\nTo resolve the problem described above, you can select either of the two options that are offered below. 1.PAM Authentication Module PAM, which stands for pluggable authentication modules, is the foundation of user authentication on contemporary Linux operating systems. We may change the default PAM settings for the su command by editing the /etc/pam.d/su file. This will allow users who belong to a certain group to switch to another user account without needing to provide a password.\n# vim /etc/pam.d/su As shown in the screenshot, add the following configurations after ‚Äúauth adequate pam rootok.so.‚Äù\nauth [success=ignore default=1] pam_succeed_if.so user = postgres auth sufficient pam_succeed_if.so use_uid user ingroup postgres The first line in the preceding setup checks if the target user is postgres; if so, the service checks the current user; otherwise, the default=1 line is bypassed and the standard authentication processes are performed. auth [success=ignore default=1] pam_succeed_if.so user = postgres The line that follows checks to see if the current user is a member of the postgres group; if so, the authentication process is considered successful and returns adequate as a consequence. Otherwise, the standard authentication procedures are followed. auth sufficient pam_succeed_if.so use_uid user ingroup postgres Make sure the file is saved, and then close it. Next, use the usermod command to add the user (like cloud) that you want to be able to su to the account postgres without a password.\nIf you now try to log in to the postgres account using the user cloud, you shouldn‚Äôt be asked for a password because it is displayed in the following screenshot that you are already logged in.\n# su - postgres 2.Use of the Sudoers File By adding a few modifications to the sudoers file, you will be able to su to another account without being required to enter a password. To be able to execute the sudo command in this scenario, the user (for example, aaronk) who will switch to another user account (for example, postgres) needs to be included in the sudoers file or in the sudo group.\n# sudo visudo Then, as seen in the screenshot, add the following configuration below the line ‚Äú%wheel ALL=(ALL:ALL) ALL.‚Äù\nSave and exit the file:\nNow try to log in to the postgres account using the cloud user; the shell should not request you to enter a password at this point.\n# sudo su - postgres That‚Äôs all I have to say for now! Please refer to the PAM manual entry page (man pam.conf) and also the sudo command‚Äôs website for any more information (man sudo).\nman pam.conf man sudo Thank You "},"title":"How to Switch (su) to a Different User Account Without a Password"},"/utho-docs/docs/linux/how-to-test-internet-connection-speed-in-ubuntu-20-04/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you know how to test internet speed on Ubuntu.\nThank You üôÇ","install-speedtest#\u003cstrong\u003eInstall Speedtest\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to test internet speed on ubuntu 20.04.\nIt is not unusual for users to inquire about the bandwidth of their Internet connection. When using the desktop version of the operating systems, all you have to do to check the speed is type the relevant query into a search engine and observe how long it takes for the results to load on any of the websites that come up in the search. This is the only thing you need to do in order to check the speed. In contrast to this, the process will be different while utilising the server-based alternative. You will learn how to check the speed of your connection using Ubuntu 20.04 by following the steps in this guide. When typing commands, it is imperative that one always do it as the root user.\nISPs have reported seeing traffic loads that are greater than they have ever been due to an increase in the number of individuals staying at home and spending more time on the internet. If you have observed that your network speed has slowed down at certain periods, the cause is likely a worldwide overflow.\nInstall Speedtest You must add the Speedtest repository in order to install it. Put in place the necessary packages first.\n# apt install gnupg1 apt-transport-https dirmngr -y Step 1. Add a key for the repository\n# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 379CE192D401AB61 Step 2. Add the repository itself\n# curl -s https://install.speedtest.net/app/cli/install.deb.sh | sudo bash Step 3. Update the packages list\n# apt update Step 4. The final step is to download an application that will estimate how quickly your Internet performs\n# apt install speedtest-cli Step 5. Internet connection speed test\n# speedtest-cli ","introduction#Introduction":""},"title":"How to Test Internet Speed on Ubuntu 20.04"},"/utho-docs/docs/linux/how-to-test-internet-speed-on-almalinux-8/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you have learned how to test internet speed on Almalinux 8.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to test internet speed on Almalinux 8.\nWhen you notice that the speed at which you can access the internet on your server is slow, the first thing you need to do in order to remedy the problems caused by the slow connectivity is to check the connection speed.\nThe SpeedTest CLI makes it possible to monitor the current internet speed of your AlmaLinux server from the command line. Additionally, it delivers dependable technology and a worldwide service network to the command line, which is the driving force behind SpeedTest.\nStep 1: Python can be installed on your machine by executing the command that is provided in the following section.\n# dnf install python2 -y Step 2: Using the command that is given below, download the file called speedtest_cli.py.\n# wget -O speedtest-cli https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py Step 3: To make the script file executable, use the command that is listed below.\n# chmod +x speedtest-cli Step 4: You are now able to test the speed of your system‚Äôs internet connection by using the speedtest-cli command.\n# python2 speedtest-cli Step 5: Execute the following command to determine how fast your internet connection is in bytes.\n# python2 speedtest-cli --bytes "},"title":"How to Test Internet Speed on Almalinux 8"},"/utho-docs/docs/linux/how-to-test-internet-speed-on-debian-10/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you know how to test internet speed on Debian 10.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-speedtest#\u003cstrong\u003eInstall Speedtest\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to test internet speed on Debian 10.\nIt is not unusual for users to inquire about the bandwidth of their Internet connection. When using the desktop version of the¬†operating systems,¬†all you have to do to check the speed is type the relevant query into a search engine and observe how long it takes for the results to load on any of the websites that come up in the search. This is the only thing you need to do in order to check the speed. In contrast to this, the process will be different while utilising the server-based alternative. You will learn how to check the speed of your connection using¬†Debian 10¬†by following the steps in this guide. When typing commands, it is imperative that one always do it as the root user.\nISPs have reported seeing traffic loads that are greater than they have ever been due to an increase in the number of individuals staying at home and spending more time on the internet. If you have observed that your network speed has slowed down at certain periods, the cause is likely a worldwide overflow.\nInstall Speedtest You must add the Speedtest repository in order to install it. Put in place the necessary packages first.\n# apt install gnupg1 apt-transport-https dirmngr -y Step 1.¬†Add a key for the repository\n# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 379CE192D401AB61 Step 2.¬†Add the repository itself\n# curl -s https://install.speedtest.net/app/cli/install.deb.sh | sudo bash Step 3.¬†Update the packages list\n# apt update Step 4.¬†The final step is to download an application that will estimate how quickly your Internet performs\n# apt install speedtest-cli Step 5.¬†Internet connection speed test\n# speedtest-cli ","introduction#Introduction":""},"title":"How to test internet speed on Debian 10"},"/utho-docs/docs/linux/how-to-test-internet-speed-on-debian-12/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you know how to test internet speed on Debian 12.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","install-speedtest#\u003cstrong\u003eInstall Speedtest\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to test internet speed on Debian 12.\nIt is not unusual for users to inquire about the bandwidth of their¬†Internet connection. When using the desktop version of the¬†operating systems,¬†all you have to do to check the speed is type the relevant query into a search engine and observe how long it takes for the results to load on any of the websites that come up in the search. This is the only thing you need to do in order to check the speed. In contrast to this, the process will be different while utilising the server-based alternative. You will learn how to check the speed of your connection using¬†Debian 12¬†by following the steps in this guide. When typing commands, it is imperative that one always do it as the root user.\nISPs have reported seeing traffic loads that are greater than they have ever been due to an increase in the number of individuals staying at home and spending more time on the internet. If you have observed that your network speed has slowed down at certain periods, the cause is likely a worldwide overflow.\nInstall Speedtest You must add the Speedtest repository in order to install it. Put in place the necessary packages first.\n# apt install gnupg1 apt-transport-https dirmngr -y Step 1.¬†Add a key for the repository\n# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 379CE192D401AB61 Step 2.¬†Add the repository itself\n# curl -s https://install.speedtest.net/app/cli/install.deb.sh | sudo bash Step 3.¬†Update the packages list\n# apt update Step 4.¬†The final step is to download an application that will estimate how quickly your Internet performs\n# apt install speedtest-cli Step 5.¬†Internet connection speed test\n# speedtest-cli ","introduction#Introduction":""},"title":"How to test internet speed on Debian 12"},"/utho-docs/docs/linux/how-to-test-internet-speed-on-fedora/":{"data":{"":"","conclusion#Conclusion":"Hopefully, Now you have learned how to test internet speed on Fedora.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn how to test internet speed on Fedora.\nWhen you notice that the speed at which you can access the internet on your server is slow, the first thing you need to do in order to remedy the problems caused by the slow connectivity is to check the¬†connection speed.\nThe SpeedTest CLI makes it possible to monitor the current internet speed of your Fedora server from the command line. Additionally, it delivers dependable technology and a worldwide service network to the command line, which is the driving force behind SpeedTest.\nStep 1: Python can be installed on your machine by executing the command that is provided in the following section.\n# dnf install python2 -y Step 2: Using the command that is given below, download the file called speedtest_cli.py.\n# wget -O speedtest-cli https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py Step 3: To make the script file executable, use the command that is listed below.\n# chmod +x speedtest-cli Step 4: You are now able to test the speed of your system‚Äôs internet connection by using the speedtest-cli command.\n# python2 speedtest-cli Step 5: Execute the following command to determine how fast your internet connection is in bytes.\n# python2 speedtest-cli --bytes "},"title":"How to Test Internet Speed on Fedora"},"/utho-docs/docs/linux/how-to-troubleshoot-with-nmap-in-centos/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have tested how to Troubleshoot with nmap in centos.\nAlso read: How To Install MariaDB 10.7 on CentOS 7\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to Troubleshoot with nmap in centos.\nNmap, which stands for ‚ÄúNetwork Mapper,‚Äù is a network scanner that was developed by Gordon Lyon (also known by his pseudonym Fyodor Vaskovich). Sending out packets and evaluating the responses received from those packets is how Nmap finds hosts and services on a computer network.\nNmap is a tool that may be used to investigate computer networks and offers a variety of functions, such as host discovery and the detection of services and operating systems. Scripts can be used to extend the capabilities of these features so that they can perform more sophisticated vulnerability detection, service discovery, and other detections. During a scan, Nmap is able to adjust itself to different network conditions, such as latency and congestion.\nIf you read the article that has been provided for you here, you will gain more knowledge about the topics that have been discussed. How to Determine the Specifics of the Existing Issues Employing the Nmap Tool when Running on CentOS It is highly recommended that nmap be used whenever there is a requirement to determine whether or not a server is online and able to take connections. This is because nmap is an excellent tool. This decision needs to be made anytime there is a need for it. The nmap tool should be used whenever there is a need to discover whether or not a server is online and able to accept connections. This can be done when there is a need to determine whether or not a server is online. This approach to diagnosing issues with port connectivity involves use of the nmap software, which can be obtained by downloading it from the following location: Nmap is a powerful port scanning application that, once it has scanned the ports on a server, gives you information on the state of those ports as well as the server that they are located on.\nnmap your server‚Äôs IP address from another machine. These control toggles can in helpful:\n-Pn¬†: Treat all hosts as online and skip host discovery.\n-p¬†: List of ports to scan.\n--reason¬†: Display the reason a port is in a particular state.\nExample: To scan ports 22, 53, 80, and 443 on your server IP\n# nmap -Pn -p 22,53,80,443 --reason server_IP Finished scanning 1 IP address on 1 active host in 0.24 seconds with nmap."},"title":"How to Troubleshoot with nmap in centos"},"/utho-docs/docs/linux/how-to-update-or-upgrade-centos-7-1-7-2-7-3-7-4-7-5-or-7-6-to-centos-7-7/":{"data":{"":" How to Update or Upgrade CentOS 7.1, 7.2, 7.3, 7.4, 7.5, or 7.6 to CentOS 7.7\nDescription\nArticle for How to Update or Upgrade CentOS 7.1, 7.2, 7.3, 7.4, 7.5, or 7.6 to CentOS 7.7\nCentOS is a Linux distribution that offers users a free and community-supported computing platform that is functionally compatible with Red Hat Enterprise Linux, the distribution that servers as its upstream source.\nFollow the below steps to How to Update or Upgrade CentOS 7.1, 7.2, 7.3, 7.4, 7.5, or 7.6 to CentOS 7.7","check-current-os-version#Check Current OS Version":" cat /etc/redhat-release ```or cat /etc/os-release\n![cat /etc/release](images/image-649.png) NOTE: Ensure that a backup is created for each of the applications and services. If this is a production server and you are running your applications and services on it, then please make sure to take a complete backup at least once before beginning the update. This way, in the event that your applications are unable to support the updated packages, you will have a backup to fall back on. ## Check for Updates sudo yum check-update\n## Clean up yum package manager The local listings and metadata of previously installed packages are stored in the yum packager manager database. It is necessary to run the clean up process at least once in order to clear out all of the previously cached data. sudo yum clean all\n## After Clean Up, reboot¬†the server. A single restart of the server is required in order to clear out the local cache and repository. The process will move along more quickly as a result, and any unnecessary complications will be avoided. reboot\n## Upgrade CentOS to the most recent version available. sudo yum upgrade\nThen you must perform a single reboot. sudo reboot\n## Check to See What Version CentOS Is Running cat /etc/redhat-release\n![cat /etc/redhat-release](images/image-650.png) CentOS is a Linux distribution that provides users with a free and community-supported computing platform that functions in the same manner as its upstream source, Red Hat Enterprise Linux. I really hope that you have a complete understanding of all that has been explained. How to Update or Upgrade CentOS 7.1, 7.2, 7.3, 7.4, 7.5, or 7.6 to CentOS 7.7 Must Read : [https://utho.com/docs/tutorial/how-to-install-gawk-on-ubuntu-20-04/](https://utho.com/docs/tutorial/how-to-install-gawk-on-ubuntu-20-04/) Thankyou. "},"title":"How to Update or Upgrade CentOS 7.1, 7.2, 7.3, 7.4, 7.5, or 7.6 to CentOS 7.7"},"/utho-docs/docs/linux/how-to-upgrade-mysql-5-7-to-8-0-in-ubuntu-16-04/":{"data":{"":"","#":"\nWe will complete this task in two phases. upgrade mysql 5.7 to 8.0 in Ubuntu 16.04\nPHASE 1 - Uninstalling mysql 5.7 PHASE 2 - Installing mysql 8.0 -‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nFirstly, backup your databases from phpmyadmin if you have phpmyadmin installed on your server. OR, take backup of the databases using mysqldump.\nImport and Export Databases in MySQL using mysqldump\n-‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nLet‚Äôs begin with phase 1 - uninstalling mysql 5.7\nStep 1. Log into your linux server via ssh and stop mysql services\nsystemctl stop mysql Step 2. Now run the following command to uninstall MySQL package\nupgrade mysql 5.7 to 8.0 in Ubuntu 16.04\nsudo apt-get remove dbconfig-mysql Step 3. Now remove the all files related to MySQL\nsudo apt-get remove --purge mysql* Step 4. It asks to remove data directories, if you want to remove , Enter on¬†Yes¬†button.\nStep 5. Next it asks to remove all MySQL databases, if you want to remove, Enter on¬†Yes¬†button.\nStep 6. To remove undependent packages run the following command\nsudo apt-get autoremove Step 7. To clean local apt¬†pacakges run the following command\nsudo apt-get autoclean We have now installed mysql 5.7 from ubuntu 16.04.\n-‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nNow let‚Äôs start with phase 2 - installing mysql 8.0\nStep 1. Install the¬†MySQL/Oracle Apt¬†repository first\nwget https://dev.mysql.com/get/mysql-apt-config_0.8.14-1_all.deb dpkg -i mysql-apt-config_0.8.10-1_all.deb The MySQL APT repository installation package allows you to pick what MySQL version you want to install, as well as if you want access to Preview Versions.\nStep 2. Update repository configuration and install MySQL Server\napt-get update apt-get install mysql-server The installation process asks you to set a¬†password for the root user:\nStep 3. Next, the installation script asks you whether to use Strong Password Encryption or Legacy Authentication:\nWhile using strong password, it is recommend for security purposes, not all applications and drivers support this new authentication method. Going with Legacy Authentication is a¬†safer choice\nAll Done upgrade mysql 5.7 to 8.0 in Ubuntu 16.04\nStep 4. You should have MySQL 8.0 Server running. You can test it by connecting to it with a command line client:\nMySQL upgraded on Ubuntu 16.04 LTS\nThank You!"},"title":"How to upgrade mysql 5.7 to 8.0 in Ubuntu 16.04"},"/utho-docs/docs/linux/how-to-use-at-command-to-schedule-a-task-in-linux/":{"data":{"":"","installation-and-uses-of-at-command#Installation and uses of \u0026lsquo;at\u0026rsquo; command":"","prerequesties#Prerequesties:":"","sudo-apt-get-install-atd-on-debian-and-derivatives#sudo apt-get install atd [on Debian and derivatives]":"","systemctl-enable-at#systemctl enable at":"\nThe at command is a substitute for the cron job scheduler that enables you to schedule a task to execute only once at a certain time without changing any configuration files.\nPrerequesties: yum or apt repository configured on your server so that your can install the utility Super user such as root or any other normal user with SUDO privileges. Installation and uses of ‚Äòat‚Äô command For this tutorial, We are using CentOS 7.7 cloud from MIcrohost Cloud. But we will show you how to install the ‚Äòat‚Äô utility on both debian and fedora flavor Linux machines.\n# yum install atd [on CentOS based systems] sudo apt-get install atd [on Debian and derivatives] Next, start and enable the at service at the boot time.\n# systemctl start at systemctl enable at You may schedule any command or job as follows after atd is operational. To ping microhost.com domain four times at the very next minute. you will use the bellow command.\n# echo \"ping microhost.com\" | at now + 1 minute Nothing will be written on standard output when this command is run. However, you might decide to send the results to a file instead by redirecting the output.\n# echo \"ping microhost.com\" | at now + 1 minute \u003e\u003e pingfile Please also be aware that ‚Äòat‚Äô supports custom 2-digit (indicating hours) and 4-digit times in addition to the following preset times: now, noon, and midnight (hours and minutes).\nFor Example:\nIf the current date is later than today at 11 p.m., execute updatedb tomorrow:\n# echo \"updatedb\" | at 23:00 To restart the server at¬†23:45¬†today (same criteria as in the previous example applies):\n# echo \"restart -r now\" | at 23:45 The + sign and the required time specification, like in the first example, may also be used to postpone the execution by a specified number of minutes, hours, days, weeks, months, or years.\nFor example: To start the httpd services at after 10 minutes\n# echo \"systemctl restart httpd\" | at now + 10minute \u003e restartHTTPDreport The above command will restart the httpd service after 10 minute of executing the command and then will save the output in restartHTTPDreport file."},"title":"How to Use ‚Äòat‚Äô Command to Schedule a Task in Linux"},"/utho-docs/docs/linux/how-to-use-iperf-to-test-network-performance/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to use iperf to test network performance.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to use iperf to test network performance.\niperf3 is a free, open-source, command-line based tool that works across multiple platforms and is capable of providing real-time network throughput measurements. It is one of the effective instruments for measuring the maximum bandwidth that can be supported by IP networks (supports IPv4 and IPv6).\nYou will need to improve both the throughput and the latency of your network‚Äôs receiving and sending capabilities in order to achieve maximum, or at least improved, network performance. This can be accomplished by increasing the throughput of your network‚Äôs receiving capabilities. However, before you can begin the real tuning process, you need to run several tests to collect statistics on the overall performance of the network. These statistics will serve as a roadmap for the tuning procedure.\nIts results include the time interval in seconds, the amount of data transferred, the bandwidth (transfer rate), and loss, in addition to other helpful network performance indicators. This is the primary purpose of the tool, which is to assist in adjusting TCP connections across a certain path, and it is on this topic that we will concentrate our attention in this article.","requirements#Requirements:":"Two networked PCs with the iperf3 application installed.\n# apt install iperf3 -y (Debian/Ubuntu) # yum install iperf3 -y (RHEL/CentOS) # dnf install iperf3 -y (Fedora 22+) In order to assess the throughput of a network between two computers, iperf makes use of the concepts of a ‚Äúclient‚Äù and a ‚Äúhost.‚Äù\nHost: On the host system, the iperf application is told to wait for a connection from a client:\n# iperf3 -s Here,\n-s Allows you to listen in the server role\nClient: Establish a connection to the first on your client. Change the value of (server ip) to the host‚Äôs IP address.\n# iperf3 -c (server ip) Here,\n-c: Connect to a server that is listening at‚Ä¶","test-results#Test Results":"After the test has been completed, both the client and the host will submit their outcomes:\nHost As you can see in the above screenshot, within a time interval of one second 437 KBytes of data is being transferred at a bandwidth of 3.58 Mbits/sec and so on.\nAs a result, total of 81.7 MBytes of data is being transferred at a bandwidth of 137 Mbits/sec within a time interval of five seconds.\nClient In iperf3, the column Retr, which stands for ‚ÄúRetransmitted TCP packets,‚Äù shows how many TCP packets had to be sent again. The value in Retr should be as low as possible. The best value would be 0, which means that no matter how many TCP packets have been sent, not a single one needs to be sent again.\nOne of the things that determines how many bytes can be sent out at any given time is the congestion window (CWND). The sender controls the congestion window, which keeps the link between the sender and the receiver from getting too busy with too much traffic."},"title":"How to Use Iperf to Test Network Performance"},"/utho-docs/docs/linux/how-to-use-iptables-firewall-in-linux/":{"data":{"":" How to use IPTABLES firewall in Linux","deleting-rules-by-chain-and-number#Deleting Rules by Chain and Number":"Iptables rules can also be removed by their chain and line number. If you list the rules in a table and specify the ‚Äîline-numbers option, you will be able to see the line numbers for each rule.\niptables -L --line-numbers You should take note of the rule‚Äôs chain and line number once you‚Äôve decided which rule to remove. Next, enter iptables -D followed by the chain number and rule number.\nFor example, if we want to delete the input rule that drops invalid packets, we can see that it is rule 3 of the INPUT chain. So this command should be run:\niptables -D INPUT 3 Now that you know how to delete individual firewall rules, let‚Äôs go over how you can¬†flush¬†chains of rules.\nIn this tutorial, you have learned How to use IPTABLES firewall in Linux.\nAlso Read: How to install Apache on CentOS 7, Change SSH Default Port 22 to Custom Port","deleting-rules-by-specification#Deleting Rules by Specification":"In iptables, rule deletion can be accomplished in a few different ways. You can do this by issuing the iptables command with the -D option and the rule specification. The output of iptables -S, the rules list, can be helpful if you want to eliminate rules using this approach.\nUse this command, for instance, to remove the rule that discards invalid incoming packets (-A INPUT -m conntrack ‚Äîctstate INVALID -j DROP):\nsudo iptables -D INPUT -m conntrack --ctstate INVALID -j DROP ","description#Description":"In this tutorial, you will learn how to use the IPTABLES firewall in Linux. The iptables command is a powerful way to control your Linux firewall on your own computer. It gives you thousands of options for managing network traffic through a simple syntax.\nHere, we need to say that iptables rules take effect as soon as they are entered. There is no configuration to reload or daemon to restart. Because of this, you must be very careful, or you could lock yourself out of the system you are using. Rules that let you into the system should always come before those that don‚Äôt.\nNote: When working with firewalls, be careful not to block SSH traffic and lock yourself out of your own server (port :22, by default). If your firewall settings cause you to lose access, you may need to connect to it through an out-of-band console to get access back.","listing-rules-as-tables#Listing Rules as Tables":"When listing the iptables rules, a table view can be helpful for contrasting the various options. Run the iptables command with the -L argument to see a table of all currently active rules.\niptables -L All of the current rules will be displayed here, organised by chain.\nDirectly following the -L option, you can enter the name of the desired chain (INPUT, OUTPUT, TCP, etc.) to restrict the output to just that channel.\nHave a look at this sample INPUT sequence:\niptables -L INPUT Output begins with the chain‚Äôs name (INPUT) and the default policy it uses (DROP). Each column‚Äôs heading appears on the next line, and then the rules for the chain itself are presented. The following is a breakdown of what each heading means:\ntarget: The target tells the rule what to do if a packet matches the rule. A packet may be granted passage, rejected, recorded, or forwarded to another chain in order to be evaluated against additional rules. Prot: The protocol which could be TCP, UDP, ICMP, or ‚Äúall.‚Äù opt: This column, which shows IP choices, is rarely utilised. source: Indicative of the traffic‚Äôs origin IP address or subnet, or any other destination: The packets‚Äô final Internet Protocol (IP) address or subnet, or any other The rule‚Äôs alternatives are given in the unlabeled final column. What doesn‚Äôt fit in the other categories of the regulation. This information may include the packet‚Äôs source and destination ports as well as its connection status.","listing-rules-by-specification#Listing Rules by Specification":"Let‚Äôs start with a primer on enumerating regulations. Your current iptables rules can be viewed in a table or as a list of rule parameters. Similar results can be obtained using either approach.\nUsing the iptables command with the -S option will display a complete specification list of all currently active rules.\niptables -S output of above command\nWithout the iptables command in front of them, the output looks identical to the original commands used to construct them. If you‚Äôve ever worked with iptables-persistent or iptables save, you‚Äôll recognise this as being quite similar to the iptables rules configuration files.\nListing a Specific Chain Following the -S option, you can enter the name of the chain you want to restrict the output to (INPUT, OUTPUT, TCP, etc.). This command would display all rule specifications in the TCP chain.\niptables -S TCP output of the above command\nThis section will examine the alternative method of viewing the currently-running iptables rules, which is in the form of a rules table.","prerequisites#Prerequisites":" Super user( root) or any normal user with SUDO privileges. Binary iptables installed Linux server "},"title":"How to use IPTABLES firewall in Linux"},"/utho-docs/docs/linux/how-to-use-lsyncd-to-sync-directories-on-centos/":{"data":{"":" How to use lsyncd to sync directory on centos","important-points#Important points":" Two server with password-less authentication enabled on both server for each other.\nNeed either a normal user with sudo privileges or sudo user.\nServer 1- 103.127.28.167 and Server 2- 103.209.147.242\nDirectory 1- /var/www/ to be sync with /var/www/ directory of other server\nDirectory 2- /var/lib/mysql to be sync with /var/lib/mysql directory of other server","steps-to-enable-mirror-synchronisation#Steps to enable mirror synchronisation":"Setup Password-less authentication Step 1: Now, generate the SSH key to be used for authentication. Execute the below command on both servers.\nssh-keygen After executing the above command, you just need to press enter for rest of the prompts you will get.\nStep 2: Copy the public key on other servers to complete the password-less authentication.\nssh-copy-id root@server-ip . Please change ‚Äúserver-ip‚Äù to the IP address of a computer other than the one you are using to run this command. Here, you will be asked to enter the ‚Äúserver-ip‚Äù password.\nInstall \u0026 configure lsyncd binary Step 3: Now, install lsyncd on your both machines. To install lsyncd, you need to install epel-release first.\nyum install -y epel-release yum -y install lua lua-devel pkgconfig gcc asciidoc lsyncd\nStep 4: Now, open the lsyncd configuration file and comment out the line you see using two dashs(--). Then, copy the text below and paste it into your file. vi /etc/lsyncd.conf\n![](images/image-1208-1024x724.png) substitute the target ip to the IP address of the remote computer you want to sync your directory with. Also, in source, you need to define the directory you want to sync with remote server's directory. \u003e Content of /etc/lsyncd configuration file. \u003e \u003e ``` \u003e settings { \u003e logfile = \"/var/log/lsyncd/webserver.log\", \u003e maxDelays = 1 \u003e } \u003e sync { \u003e default.rsync, \u003e source=\"/var/www\", \u003e target=\"103.127.28.212:/var/www\", \u003e rsync = { \u003e compress = true, \u003e acls = true, \u003e verbose = true, \u003e owner = true, \u003e group = true, \u003e perms = true, \u003e rsh = \"/usr/bin/ssh -p 22 -o StrictHostKeyChecking=no\" \u003e } \u003e } \u003e ``` Step 5: Now, restart your server lsyncd services on both of your server. systemctl restart lsyncd\nAnd this is how you will mirror sync your directory with the directory of remote server or you can say you have learnt how to use lsyncd to sync directories on Centos 7 ","what-is-lsync#What is lsync?":"Although rsync is a superb and flexible backup solution, it does have one drawback: you must manually launch it whenever you want to back up your data. Yes, you can use cron to set up scheduled backups, but even this approach is unable to offer live synchronisation that is smooth. The lsyncd tool, a command-line application that uses rsync to synchronise (or rather mirror) local folders with a remote machine in real time, is what you need if this is what you want.\nLsyncd keeps an eye on the inotify or fsevents local directory trees event monitor interface. For a brief period of time, it gathers and integrates events before spawning one (or more) processes to synchronise the adjustments. By default, rsync is used. Lsyncd is a lightweight live mirror solution that is relatively simple to install, doesn‚Äôt require new filesystems or block devices, and doesn‚Äôt degrade the speed of the local filesystem.\nInstead of sending the move destination over the wire again, Rsync+ssh is an advanced action configuration that leverages SSH to act on file and directory moves directly on the target."},"title":"How to use lsyncd to sync directories on Centos"},"/utho-docs/docs/linux/how-to-use-mtr-command-in-linux/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to use MTR command in Linux.\nAlso Read: How to Use Iperf to Test Network Performance\nThank You üôÇ","install-mtr-on-linux#Install MTR on Linux":"With the following command, you will install MTR on different operating systems.\nMTR on Debian/Ubuntu # apt-get install mtr MTR on Fedora/CentOS # yum install mtr MTR on Arch/Manjaro # pacman -S mtr MTR on BSD # pkg install mtr ","introduction#Introduction":"In this article, you will learn how to use MTR command in Linux.\nMy Traceroute, also known as MTR, is a tool that evaluates the state of a network connection through the use of the traceroute and ping commands.\nMy traceroute is a computer programme that was initially known as Matt‚Äôs traceroute. It is a network diagnostic utility that combines the capabilities of the traceroute and ping computer programmes into a single application.\nMTR examines routers along the route path by putting a cap on the number of hops that individual packets are allowed to take and then listening for responses about when those packets expire.","use-of-mtr#Use of MTR":"Installing MTR on your web server and running it against your local machine will help you gain a deeper comprehension of the performance problems that are affecting your network. MTR can be used to perform real-time scans as well as scans in a summary format.\nExecute mtr with a domain, server IP, or server hostname, and you will be able to view real-time performance statistics.\n# mtr domain.com # mtr Server\\_IP During the scan, you will have the following options that are available to you:\nHelp ‚Äì Display open command options for the current MTR testing Display mode ‚Äì Change how data on packets and pings are shown Restart statistics ‚Äì Start the test over again Order of fields ‚Äì Change the order of the columns that are open (press Enter to leave) quit ‚Äì Stop MTR You can always copy the results. We suggest that you wait until you have sent at least 50 packages to get an accurate estimate.\n# mtr -rw yourdomain.com -c 100 -c¬†or¬†--report-cycles¬†‚Äì Set how many pings are sent to each hope to test its stability. (each cycle lasts one second) -r¬†or¬†--report¬†‚Äì Run the test in the background, and when it‚Äôs done, print out the results. -w¬†or¬†--report-wide¬†‚Äì Make sure that the results list full hostnames instead of??? If you do not specify a number with the -c option, MTR will stop functioning after it has sent 10 packets.\nTo put MTR outcomes into a CSV file, do the following:\n# mtr -rwC yourdomain.com -c 100 \u003e mtr-results.csv Output options:\n-l¬†or¬†--raw -C¬†or¬†--csv¬†(separator is a semi-colon ‚Äú;‚Äù) -j¬†or¬†--json -x¬†or¬†--xml "},"title":"How to use MTR command in Linux"},"/utho-docs/docs/linux/how-to-use-nmap-to-scan-for-open-ports/":{"data":{"":"\nMany new system administrators find networking to be a big and confusing topic. To understand them, you have to learn about the different layers, protocols, interfaces, and tools and utilities.\nPorts are the ends of logical communications in TCP/IP and UDP networking. A web server, an application server, and a file server can all run from the same IP address. In order for these services to talk to each other, they each listen and talk on a different port. When you connect to a server, you use the server‚Äôs IP address and a port.\nMost of the time, the software you use will tell you what port to use. For example, when you connect to https://microhost.com, you‚Äôre connecting to the digitalocean.com server on port 443, which is the default port for secure web traffic. Since it is the default, your browser will automatically add the port.\nIn this tutorial, you‚Äôll learn more about ports. You‚Äôll use the netstat programme to find open ports, and then you‚Äôll use the nmap programme to find out how a machine‚Äôs network ports are set up. When you‚Äôre done, you‚Äôll be able to find common ports and look for open ports on your systems.","checking-open-ports#Checking Open Ports":"You can scan for open ports with a number of tools. Most Linux distributions have netstat installed by default.\nBy running the command with the following parameters, you can find out quickly which services you are running:\nnetstat -tunlp This shows the service‚Äôs port and listening socket, as well as the UDP and TCP protocols.\nThe nmap tool is another way to find out what ports are open.","scanning-ports-with-nmap#Scanning Ports with nmap":"With Nmap, you can find out a lot about a host. It can also make the people in charge of the target system think that someone is trying to do harm. Because of this, you should only test it on servers you own or where the owners have been told.\nThe people who made nmap set up a test server at scanme.nmap.org.\nYou can practise nmap on this or one of your own servers.\nHere are some of the most common things you can do with nmap. We will run them all with sudo so that some queries don‚Äôt return only part of the results. Some commands might take a long time to finish:\nLook for the operating system of the host:\nnmap -O target_name For example to scan the google.com\nnmap -O google.com Starting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 12:10 EDT\nNmap scan report for google.com (142.250.201.46)\nHost is up (0.35s latency).\nrDNS record for 142.250.201.46: mrs08s20-in-f14.1e100.net\nNot shown: 998 filtered ports\nPORT STATE SERVICE\n80/tcp open http\n443/tcp open https\nWarning: OSScan results may be unreliable because we could not find at least 1 open and 1 closed port\nDevice type: general purpose\nRunning (JUST GUESSING): FreeBSD 9.X (86%)\nOS CPE: cpe:/o:freebsd:freebsd:9\nAggressive OS guesses: FreeBSD 9.1-PRERELEASE (86%)\nNo exact OS matches for host (test conditions non-ideal).\nOS detection performed. Please report any incorrect results at http://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 35.30 seconds\nNmap done: 1 IP address (1 host up) scanned in 35.30 seconds\nYou can also scan for your own machine, localhost.\nnmap -O localhost Starting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 12:13 EDT\nNmap scan report for localhost (127.0.0.1)\nHost is up (0.000013s latency).\nOther addresses for localhost (not scanned): 127.0.0.1\nNot shown: 997 closed ports\nPORT STATE SERVICE\n22/tcp open ssh\n25/tcp open smtp\n80/tcp open http\nDevice type: general purpose\nRunning: Linux 3.X\nOS CPE: cpe:/o:linux:linux_kernel:3\nOS details: Linux 3.7 - 3.9\nNetwork Distance: 0 hops\nOS detection performed. Please report any incorrect results at http://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 3.93 seconds\nScan without preforming a reverse DNS lookup on the IP address specified. This should speed up your results in most cases:\nsudo nmap -n google.com Output would be like this.\nStarting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 09:39 EDT\nNmap scan report for google.com (142.250.201.46)\nHost is up (0.35s latency).\nNot shown: 998 filtered ports\nPORT STATE SERVICE\n80/tcp open http\n443/tcp open https\nNmap done: 1 IP address (1 host up) scanned in 23.86 seconds\nScan a specific port instead of all common ports\nnmap -p 80 google.com Starting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 09:42 EDT\nNmap scan report for google.com (142.250.201.46)\nHost is up (0.35s latency).\nrDNS record for 142.250.201.46: mrs08s20-in-f14.1e100.net\nPORT STATE SERVICE\n80/tcp open http\nNmap done: 1 IP address (1 host up) scanned in 1.17 seconds\nTo scan only TCP connection.\nnmap -sT google.com Starting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 11:32 EDT\nNmap scan report for google.com (172.217.19.142)\nHost is up (0.35s latency).\nrDNS record for 172.217.19.142: par03s12-in-f142.1e100.net\nNot shown: 998 filtered ports\nPORT STATE SERVICE\n80/tcp open http\n443/tcp open https\nNmap done: 1 IP address (1 host up) scanned in 19.61 seconds\nTo scan only UDP connection.\nnmap -sU google.com Starting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 11:48 EDT\nNmap scan report for google.com (142.250.201.14)\nHost is up (0.35s latency).\nrDNS record for 142.250.201.14: mrs08s19-in-f14.1e100.net\nAll 1000 scanned ports on google.com (142.250.201.14) are open|filtered\nNmap done: 1 IP address (1 host up) scanned in 20.84 seconds\nScan for every TCP and UDP open port:\nnmap -sU -sT -n -PN -p- 103.146.242.22 Starting Nmap 6.40 ( http://nmap.org ) at 2022-09-18 11:58 EDT\nNmap scan report for 103.146.242.22\nHost is up (0.00030s latency).\nNot shown: 131065 closed ports\nPORT STATE SERVICE\n22/tcp open ssh\n80/tcp open http\n37700/tcp open unknown\n50390/tcp open unknown\n52924/tcp open unknown\nNmap done: 1 IP address (1 host up) scanned in 7.19 seconds","using-nmap#Using Nmap":"Part of securing a network involves doing vulnerability testing. This means trying to infiltrate your network and discover weaknesses in the same way that an attacker might.\nOut of all of the available tools for this,¬†nmap¬†is perhaps the most common and powerful.\nYou can install¬†nmap¬†on an Ubuntu or Debian machine by entering:\napt-get update apt-get install nmap A better port mapping file is one of the side effects of installing this software. If you look at this file, you can see a much more detailed list of the links between ports and services:\nless /usr/share/nmap/nmap-services This file has almost 20,000 lines, and it also has fields like the third one, which shows how often that port was found to be open during research scans on the Internet."},"title":"How To Use Nmap to Scan for Open Ports"},"/utho-docs/docs/linux/how-to-use-ps-kill-and-nice-to-manage-processes-in-linux/":{"data":{"":"\nIntroduction\nLike any modern computer, a Linux server can run more than one programme at the same time. These are called ‚Äúindividual processes‚Äù and are handled as such.\nLinux will take care of the low-level, behind-the-scenes parts of a process‚Äôs life cycle, such as starting up, shutting down, allocating memory, etc., but you will need a way to interact with the operating system to manage them at a higher level.\nThis guide will teach you about some of the most important parts of process management. Linux has a number of built-in tools that can be used for this.\nYou will try these ideas out in Ubuntu 20.04, but any modern Linux distribution will work the same way.\nPlease follow these instructions in order to use the ps, kill, and nice commands:","step1-how-to-view-linux-running-processes#Step.1 How to View Linux Running Processes":"Using the top command, you may view every process running on your server:\n# top # Output top - 15:14:40 up 46 min, 1 user, load average: 0.00, 0.01, 0.05 Tasks: 56 total, 1 running, 55 sleeping, 0 stopped, 0 zombie Cpu(s): 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st Mem: 1019600k total, 316576k used, 703024k free, 7652k buffers Swap: 0k total, 0k used, 0k free, 258976k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 24188 2120 1300 S 0.0 0.2 0:00.56 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.07 ksoftirqd/0 6 root RT 0 0 0 0 S 0.0 0.0 0:00.00 migration/0 7 root RT 0 0 0 0 S 0.0 0.0 0:00.03 watchdog/0 8 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 cpuset 9 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 khelper 10 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs The first few lines of output include CPU/memory load and running jobs.\nThere‚Äôs 1 operating process and 55 sleeping processes not requiring CPU cycles.\nThe output indicates running processes and their utilisation. By default, top sorts by CPU utilisation, so the busiest processes appear first. top will run until you exit it with Ctrl+C. This sends a kill signal to stop the process gracefully if possible.\nMost package repositories have the enhanced top programme, htop. Ubuntu 20.04 apt:\n# sudo apt install htop The htop command will thereafter be available:\n# htop Output Mem[||||||||||| 49/995MB] Load average: 0.00 0.03 0.05 CPU[ 0.0%] Tasks: 21, 3 thr; 1 running Swp[ 0/0MB] Uptime: 00:58:11 PID USER PRI NI VIRT RES SHR S CPU% MEM% TIME+ Command 1259 root 20 0 25660 1880 1368 R 0.0 0.2 0:00.06 htop 1 root 20 0 24188 2120 1300 S 0.0 0.2 0:00.56 /sbin/init 311 root 20 0 17224 636 440 S 0.0 0.1 0:00.07 upstart-udev-brid 314 root 20 0 21592 1280 760 S 0.0 0.1 0:00.06 /sbin/udevd --dae 389 messagebu 20 0 23808 688 444 S 0.0 0.1 0:00.01 dbus-daemon --sys 407 syslog 20 0 243M 1404 1080 S 0.0 0.1 0:00.02 rsyslogd -c5 408 syslog 20 0 243M 1404 1080 S 0.0 0.1 0:00.00 rsyslogd -c5 409 syslog 20 0 243M 1404 1080 S 0.0 0.1 0:00.00 rsyslogd -c5 406 syslog 20 0 243M 1404 1080 S 0.0 0.1 0:00.04 rsyslogd -c5 553 root 20 0 15180 400 204 S 0.0 0.0 0:00.01 upstart-socket-br htop improves CPU thread visibility, terminal colour support, and sorting options. It‚Äôs not always installed by default, but it can replace top. Ctrl+C exits htop like top. Learn top and htop.\nNext, learn how to query specific processes.","step2-how-to-list-processes-using-ps#Step.2 How to List Processes Using ps":"top and htop provide a dashboard-like interface for monitoring running processes, comparable to a graphical task manager. A dashboard interface can provide an overview, but it does not typically return directly actionable data. Linux has a second standard tool named ps for querying running processes.\nRunning ps without arguments offers minimal information.\n# ps Output PID TTY TIME CMD 1017 pts/0 00:00:00 bash 1262 pts/0 00:00:00 ps This output lists all processes related to the current user and terminal session. This makes sense if you are simply running the bash shell and this ps command in the terminal at the moment.\nTo obtain a more comprehensive view of this system‚Äôs processes, you can do ps aux:\n# ps aux Output USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.2 24188 2120 ? Ss 14:28 0:00 /sbin/init root 2 0.0 0.0 0 0 ? S 14:28 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? S 14:28 0:00 [ksoftirqd/0] root 6 0.0 0.0 0 0 ? S 14:28 0:00 [migration/0] root 7 0.0 0.0 0 0 ? S 14:28 0:00 [watchdog/0] root 8 0.0 0.0 0 0 ? S\u003c 14:28 0:00 [cpuset] root 9 0.0 0.0 0 0 ? S\u003c 14:28 0:00 [khelper] These parameters instruct ps to display processes owned by all users in a more readable format, regardless of their terminal relationship.\nUsing pipes, grep may be used to scan the output of ps aux in order to return the name of a specific process. This is useful if you suspect the programme has crashed or need to terminate it for whatever reason.\n# ps aux | grep bash Output microhost 41664 0.7 0.0 34162880 2528 s000 S 1:35pm 0:00.04 -bash microhost 41748 0.0 0.0 34122844 828 s000 S+ 1:35pm 0:00.00 grep bash This returns both the recently executed grep process and the currently running bash shell. It also returns their total memory and CPU consumption, how long they‚Äôve been running, and their process ID, which is shown in the result above. Each process in Linux and Unix-like systems is issued a process ID, or PID. This is how the operating system keeps track of and identifies processes.\nThe pgrep command provides a simple method for obtaining a process‚Äôs PID.\n# pgrep bash The PID ‚Äú1‚Äù is assigned to the init process, which is the initial process created upon system boot.\n# pgrep init This process spawns all system processes. Later processes have higher PIDs.\nThe process that spawned another is its parent. Parent processes have a PPID, which is visible in top, htop, and ps column headings.\nAny user-OS process communication involves translating process names and PIDs. These tools always output the PID. Next, you‚Äôll learn how to use PIDs to stop, resume, and other processes.","step3-how-to-send-signals-to-processes-in-linux#Step.3 How to Send Signals to Processes in Linux":"Signals control Linux processes. Signals are an OS-level technique of terminating or changing programme behaviour.\nKill is the most common programme signal. This utility‚Äôs default function is to kill a process.\n# kill PID_of_target_process It sends the process TERM. Term tells a process to end. So, the software may clean up and quit cleanly.\nIf a misbehaving programme doesn‚Äôt exit when given the TERM signal, send the KILL signal.\n# kill -KILL PID_of_target_process A specific signal not sent to the programme.\nThe process is shut down by the operating system kernel. This bypasses programmes that disregard signal input.\nEvery signal has a number that can be passed instead of the name. You can pass ‚Äú-15‚Äù for ‚Äú-TERM‚Äù and ‚Äú-9‚Äù for ‚Äú-KILL.‚Äù\nSignals don‚Äôt just stop programmes. Other actions can be taken with them.\nMany background programmes (called ‚Äúdaemons‚Äù) restart when given the HUP, or hang-up, signal. Apache usually works this way.\n# sudo kill -HUP pid_of_apache The command above reloads Apache‚Äôs configuration file and resumes service.\n-l lists all kill signals.\n# kill -l Although PIDs are the typical mechanism for transmitting signals, it is also possible to do so using ordinary process names.\nThe pkill command behaves nearly identically to kill, except it operates on a process name.\n# pkill -9 ping The preceding command is equal to:\n# kill -9 pgrep ping The preceding command is equal to:\n# killall firefox This command will deliver the TERM signal to all instances of Firefox currently operating on the system.","step4-modifying-process-priorities#Step.4 Modifying Process Priorities":"You may need to prioritise server processes.\nSome processes may be mission-critical, while others may be run when resources are available.\nNiceness controls Linux‚Äôs priority.\nPriority jobs don‚Äôt share resources properly, thus they‚Äôre less nice. Low priority operations take minimum resources, which is beneficial.\nWhen you run top, there was a ‚ÄúNI‚Äù column. Process value:\n# top [secondary_label Output] Tasks: 56 total, 1 running, 55 sleeping, 0 stopped, 0 zombie Cpu(s): 0.0%us, 0.3%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st Mem: 1019600k total, 324496k used, 695104k free, 8512k buffers Swap: 0k total, 0k used, 0k free, 264812k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1635 root 20 0 17300 1200 920 R 0.3 0.1 0:00.01 top 1 root 20 0 24188 2120 1300 S 0.0 0.2 0:00.56 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.11 ksoftirqd/0 Depending on the system, nice values can range from -19/-20 to 19/20.\nThe nice command runs a programme with a specified lovely value.\n# nice -n 15 command_to_execute Only fresh programmes function.\nTo change a program‚Äôs nice value, use renice:\n# renice 0 PID_to_prioritize Hopefully you have understand the above steps how to¬†use the ps, kill, and nice commands:\nMust read:- https://utho.com/docs/tutorial/linux-port-test-commandsredhat-7-centos-7-and-ubuntu-18-04/\nThankyou"},"title":"How To Use ps, kill, and nice to Manage Processes in Linux"},"/utho-docs/docs/linux/how-to-use-rsync-to-sync-local-and-remote-directories/":{"data":{"":"\nThis article will teach you How to Use Rsync to Synchronize Local and Remote Directories.¬†Introduction\nRsync, which stands for ‚Äúremote sync,‚Äù is a tool for keeping files in sync both remotely and locally. It uses an algorithm to move only the parts of files that have changed, which reduces the amount of data that needs to be copied.\nIn this tutorial, we‚Äôll explain what Rsync is, go over the syntax for using Rsync, show you how to use Rsync to sync with a remote system, and talk about other options you have.\nPrerequisites\nTo practise using rsync to sync files between a local system and a remote system, you‚Äôll need two computers, one to act as your local system and the other as your remote system. As long as they are set up the right way, these two machines could be virtual private servers, virtual machines, containers, or even personal computers.\nIf you want to use servers to follow this guide, it would be smart to give each of them administrative users and set up a firewall. Follow our First Server Setup Guide to set up these servers.\nTo follow this tutorial, you will need to have made SSH keys on both of the machines you use. This is true no matter what kind of machines you use. Then, follow Step 2 of that guide to copy each server‚Äôs public key to the authorized keys file of the other server.\nThis guide was tested on computers running Ubuntu 20.04, but it should work on most Linux-based computers with rsync installed if rsync is installed.\nTo Use Rsync to Sync Local and Remote Directories please read the below steps ‚Ä¶.","defination-rsync#Defination Rsync":"Rsync is a very flexible tool for synchronising files over a network. It comes with most Linux distributions by default because it is common on Linux and Unix-like systems and is a popular tool for system scripts.","keeping-a-local-system-in-synch-with-a-remote-one-using-rsync#Keeping a Local System in Synch with a Remote One Using Rsync":"To utilise rsync with a remote system, you need SSH access and rsync on both machines. Once SSH access is confirmed between two machines, use the following syntax to sync dir1 to a remote machine. In this example, you‚Äôll omit the trailing slash to transmit the real directory:\n#rsync -a ~/dir1 username@remote_host:destination_directory This technique ‚Äúpushes‚Äù a local directory to a distant machine. Pull syncs a remote directory with the local system. If dir1 were remote, the syntax would be:\n#rsync -a username@remote_host:/home/username/dir1 place_to_sync_on_local_machine Like cp and similar tools, source is always first, destination second.","recognizing-rsync-syntax#Recognizing Rsync Syntax":"rsync‚Äôs syntax is similar to that of other tools like ssh, scp, and cp.\nFirst, run the following command to go to your home directory:\n#cd after that create a test directory\n#mkdir dir1 Make a second test directoey:\n#mkdir dir2 Add some test files now:\n#touch dir1/file{1..100} There is currently a dir1 directory with 100 empty files in it. Verify by listing the files out:\n#ls dir1 Output file1 file18 file27 file36 file45 file54 file63 file72 file81 file90 file10 file19 file28 file37 file46 file55 file64 file73 file82 file91 file100 file2 file29 file38 file47 file56 file65 file74 file83 file92 file11 file20 file3 file39 file48 file57 file66 file75 file84 file93 file12 file21 file30 file4 file49 file58 file67 file76 file85 file94 file13 file22 file31 file40 file5 file59 file68 file77 file86 file95 file14 file23 file32 file41 file50 file6 file69 file78 file87 file96 file15 file24 file33 file42 file51 file60 file7 file79 file88 file97 file16 file25 file34 file43 file52 file61 file70 file8 file89 file98 file17 file26 file35 file44 file53 file62 file71 file80 file9 file99 dir2 is also empty. To sync dir1 to dir2 on the same system, execute rsync with the ‚Äúrecursive‚Äù -r flag:\n#rsync -r dir1/ dir2 -a is a combo flag that means ‚Äúarchive.‚Äù It syncs recursively and preserves symbolic links, special and device files, modification times, groups, owners, and rights. This flag is preferred over -r. Run the same command with the -a flag:\n#rsync -a dir1/ dir2 Please note that the previous two instructions have a trailing slash (/) after the first argument.\n#rsync -a dir1/ dir2 Trailing slash indicates dir1‚Äôs contents. Without the trailing slash, dir1 would be inside dir2. The result is a hierarchy:\nTrailing slash indicates dir1's contents. Without the trailing slash, dir1 would be inside dir2. The result is a hierarchy: Check your rsync command arguments before executing. Using Rsync‚Äôs -n or ‚Äîdry-run option does this. The -v flag, which means ‚Äúverbose‚Äù, is also necessary to get the appropriate output. You‚Äôll combine the a, n, and v flags in the following command:\n#rsync -anv dir1/ dir2 Output sending incremental file list ./ file1 file10 file100 file11 file12 file13 file14 file15 file16 file17 file18 Now compare that output to what you get when you remove the trailing slash, like in:\n#rsync -anv dir1 dir2 Output sending incremental file list dir1/ dir1/file1 dir1/file10 dir1/file100 dir1/file11 dir1/file12 dir1/file13 dir1/file14 dir1/file15 dir1/file16 dir1/file17 dir1/file18 This output shows that the directory was migrated, not just its files.","utilizing-alternative-rsync-methods#Utilizing Alternative Rsync Methods":"Rsync‚Äôs flag choices can change the utility‚Äôs default behaviour.\nText files that haven‚Äôt been compressed can be compressed with the -z option to minimise network transfer.\n#rsync -az source destination Also useful is -P. It‚Äôs ‚Äîprogress and ‚Äîpartial. The first flag shows transfer progress, and the second resumes interrupted transfers.\n#rsync -azP source destination #Output sending incremental file list created directory destination source/ source/file1 0 100% 0.00kB/s 0:00:00 (xfr#1, to-chk=99/101) sourcefile10 0 100% 0.00kB/s 0:00:00 (xfr#2, to-chk=98/101) source/file100 0 100% 0.00kB/s 0:00:00 (xfr#3, to-chk=97/101) source/file11 0 100% 0.00kB/s 0:00:00 (xfr#4, to-chk=96/101) source/file12 0 100% 0.00kB/s 0:00:00 (xfr#5, to-chk=95/101) If you execute the command once more, you will see a condensed version of the results because no modifications have been made since the last time. This exemplifies Rsync‚Äôs capability of making use of modification times in order to detect whether or not changes have been made:\n#rsync -azP source destination Output sending incremental file list sent 818 bytes received 12 bytes 1660.00 bytes/sec total size is 0 speedup is 0.00 Say you updated several files‚Äô modification time with this command:\n#touch dir1/file{1..10} If you run rsync with -azP again, you‚Äôll see how it intelligently recopies just modified files.\n#rsync -azP source destination #Output sending incremental file list file1 0 100% 0.00kB/s 0:00:00 (xfer#1, to-check=99/101) file10 0 100% 0.00kB/s 0:00:00 (xfer#2, to-check=98/101) file2 0 100% 0.00kB/s 0:00:00 (xfer#3, to-check=87/101) file3 0 100% 0.00kB/s 0:00:00 (xfer#4, to-check=76/101) To sync two directories, delete files from the destination if they‚Äôre removed from the source. rsync doesn‚Äôt remove files by default.\n‚Äîdelete modifies this behaviour. Before using this option, use -n, ‚Äîdry-run, to prevent data loss.\n#rsync -an --delete source destination You can exclude files or directories from a syncing directory by using the ‚Äîexclude= option and a comma-separated list.\n#rsync -a --exclude=pattern_to_exclude source destination You can override an exclusion pattern with the ‚Äîinclude= option.\n#rsync -a --exclude=pattern_to_exclude --include=pattern_to_include source destination The Rsync ‚Äîbackup option stores backups of essential files. It‚Äôs used with the ‚Äîbackup-dir option, which defines where backup files go.\n#rsync -a --delete --backup --backup-dir=/path/to/backups /path/to/source destination Rsync simplifies networked file transfers and local directory syncing. Rsync‚Äôs versatility makes it useful for many file-level tasks.\nMastering Rsync lets you construct elaborate backups and control how and what is shared.\nHope you have understand the above steps¬†To Use Rsync to Sync Local and Remote Directories\nMust read:- https://utho.com/docs/tutorial/find-multiple-ways-to-user-account-info-and-login-details-in-linux/\nThankyou"},"title":"How To Use Rsync to Sync Local and Remote Directories"},"/utho-docs/docs/linux/how-to-use-the-smtp-server-of-google/":{"data":{"":"","benefits#Benefits":"You can choose to have Google store and index the emails you send through its SMTP server. This will make all of your sent emails searchable and safe on Google‚Äôs servers. If you choose to use Gmail or G Suite for both outgoing and incoming email, all of your email will be in one place. Also, because Google‚Äôs SMTP server doesn‚Äôt use port 25, an ISP is less likely to block or mark your email as spam.","introduction#Introduction":"A little-known feature of Gmail and Google Apps email is Google‚Äôs portable SMTP server. You can configure Google‚Äôs SMTP server settings with whatever script or program you wish to send email. All you need is either (i) a free Gmail account, or (ii) a paid G Suite account.","sending-limits#Sending Limits":"Users can only send a certain amount of mail through Google‚Äôs portable SMTP server. With this limit, you can only send 99 emails per day. The limit is lifted automatically 24 hours after you send all 99 emails.\nMust Read : How to configure an external SMTP server in Plesk","settings#Settings":"Google‚Äôs SMTP server requires authentication, so here‚Äôs how to set it up in your mail client or application\nSMTP server (i.e., outgoing mail server):¬†[smtp.gmail.com¬†(http://smtp.gmail.com) SMTP username:¬†Your full Gmail or G Suite email address¬†(e.g.,¬†example@gmail.com¬†or¬†example@your_domain) MTP password:¬†Your Gmail or G Suite email password SMTP port:¬†465 SMTP¬†TLS/SSL required:¬†yes Before you begin, consider investigating your mail client or application‚Äôs security rating, according to Google. If you are using a program that Google does not consider secure, your usage will be blocked unless you generate an application-specific App Password.\nYou must also make sure that IMAP access is turned on for your account so that Google can automatically move your sent emails to the sent folder.\nTo do so, go to the Gmail settings and click on the¬†Forwarding and POP/IMAP¬†tab. Scroll down to the¬†IMAP Access¬†section and make sure that IMAP access is enabled for your account."},"title":"How to Use the SMTP Server of¬†Google"},"/utho-docs/docs/linux/how-to-verify-your-application-is-listening-on-the-correct-port/":{"data":{"":"","introduction#Introduction":"In this short article you will know to Verify Your Application is Listening on the Correct Port or not.\nIf you are troubleshooting a service and you are certain that it is operating normally, the next step in the process is to verify and make sure that there are no other issues. When you are debugging a service and you are certain that it is operating normally, the next step in the process is to check and make sure that the service is listening on the appropriate network port. Whenever you are debugging a service and you are certain that it is operating normally, the next step in the process is to check and make sure that the service is Only if you are satisfied that the service is operating normally is it required for you to go to the next stage. make sure that the service is configured to listen on the appropriate network port.\nOn a Linux server, the netstat command displays the services and ports that are actively listening, along with the specifics of any connections that are presently being made to those services and ports. It also displays any connections that have been made in the past to those services and ports. It is essential to pay attention to the addresses on which a network daemon is listening (including the port number), the daemon‚Äôs process identifier (PID), and the programme name when performing fundamental troubleshooting on a network daemon. These are the details regarding the link.\n-u¬†: Show UDP sockets.\n-p¬†: Show the PID and name of the program to which each socket belongs.\n-l¬†: Show only listening sockets.\n-n¬†: Show numeric addresses.\n-t¬†: Show TCP sockets.\nExample:\n# netstat -uplnt Examine this data to ensure that your application is listening on the correct local address and socket. If you submit a ticket for further debugging, please include this information.\nThank You üôÇ"},"title":"How to Verify Your Application is Listening on the Correct Port"},"/utho-docs/docs/linux/how-to-view-and-update-the-linux-path-environment-variable/":{"data":{"":"","step-3-adding-a-directory-to-path#Step .3 Adding a directory to PATH":"\nIntroduction\nCommand-line programmes may only be launched in their installation directory. PATH lets you run command-line programmes from any directory.\nThe PATH variable lists folders checked before executing an operation. By updating PATH, you can execute executables from any directory without inputting the absolute file path.\nInstead of typing:\nTo View and Update the Linux PATH Environment Variable please follow the below steps that will help you to increase you knowledge..\n# /usr/bin/python3 You can type this in its place because the PATH variable includes the /usr/bin directory:\n# python3 Priority directories are listed first.\nThis lesson updates the PATH variable.\nStep.1 Taking a Look at the PATH Variable #echo $PATH Unmodified PATH looks like this (file paths may vary by system):\nOutput /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games PATH includes some default directories, each separated by a colon :. The system checks these folders from left to right when launching an application.\nWhen a command-line programme isn‚Äôt in any of the specified folders, you may need to add it to PATH.\nStep.2 Setting the PATH environment variable to include a directory A directory can be added to PATH at the beginning or end.\nAdding a directory to PATH will make it checked first (/the/file/path, for example).\n# export PATH=/the/file/path:$PATH Putting a directory at the end of PATH means it will be checked last.\n# export PATH=$PATH:/the/file/path Multiple directories can be added to PATH by using a colon:\n# export PATH=$PATH:/the/file/path:/the/file/path2 Once the export command is executed, you can view the PATH variable to see the changes:\n# export PATH=$PATH:/the/file/path # echo $PATH You‚Äôll get something like this:\n# Output /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games:/the/file/path Only the current shell session can use this method. Once you exit and restart, the PATH variable will reset to its default value and no longer contain the added directory. PATH must be stored in a file to persist across shell sessions.\nStep .3 Adding a directory to PATH In this step, you‚Äôll add a directory to the shell configuration file, which is either /.bashrc or /.zshrc, depending on the shell you‚Äôre using. In this guide, /.bashrc will be used as an example.\nFirst, open the file called /.bashrc:\n# nano ~/.bashrc /.bashrc has existing info you won‚Äôt change. At the bottom of the file, add your new directory:\n# export PATH=$PATH:the/file/path Use the methods in the previous section to specify whether the new directory should be checked first or last.\nClose the file. A fresh shell session will alter the PATH variable. Source command applies modifications to current session.\n# source ~/.bashrc You can add new directories by opening this file and attaching colon-separated directories to the export command.\nHope you have understand the above steps to View and Update the Linux PATH Environment Variable..\nMust read:- https://utho.com/docs/tutorial/cheat-sheet-for-15-nmcli-commands-in-linux-rhel-centos/","step1-taking-a-look-at-the-path-variable#Step.1 Taking a Look at the PATH Variable":"","step2-setting-the-path-environment-variable-to-include-a-directory#Step.2 Setting the PATH environment variable to include a directory":""},"title":"How To View and Update the Linux PATH Environment Variable"},"/utho-docs/docs/linux/how-to-view-colored-man-page-documentation-in-linux/":{"data":{"":"","description#\u003cstrong\u003eDescription\u003c/strong\u003e":"","thank-you#\u003cstrong\u003eThank You\u003c/strong\u003e":"\nDescription In this article you will know How you can View Colored Man Page in linux. In computer operating systems that are similar to Unix, the phrase ‚Äúman page‚Äù refers to the documentation that is associated with a terminal-based application, tool, or utility (commonly known as a command). It includes the name of the command, the syntax for using it, an explanation of the possible options, the author, copyright information, related commands, and other information.\nThe following is an example of how to view the manual page for a command in Linux; this will display the man page for the df command:\n# man df The output of the man programme is typically formatted with the help of a terminal pager software such as more or less, and the default display for any kind of text is typically in white colour (bold, underlined etc..).\n*You may get nicely coloured man pages by making simple adjustments to the.bashrc file in your home directory by defining a colour scheme using a variety of LESS TERMCAP variables.\n# vi ~/.bashrc Add the following to your colour scheme. export LESS_TERMCAP_mb=$'e[1;32m' export LESS_TERMCAP_md=$'e[1;32m' export LESS_TERMCAP_me=$'e[0m' export LESS_TERMCAP_se=$'e[0m' export LESS_TERMCAP_so=$'e[01;33m' export LESS_TERMCAP_ue=$'e[0m' export LESS_TERMCAP_us=$'e[1;4;31m' The colour codes that were utilised in the setup that was just shown are shown below.\n31 ‚Äì red 32 ‚Äì green 33 ‚Äì yellow And here are the meanings of the escape codes that were utilised in the setup that was just presented:\n0 ‚Äì reset/normal 1 ‚Äì bold 4 ‚Äì underlined In addition to this, you can restart your terminal by typing reset, or you can switch to a different shell. Now when you try to see a man page for the df command, it should look like this instead of the default view. This view is much nicer.\n# man df You might also use the MOST paging application, which is compatible with operating systems that are similar to Unix and enables you to scroll both left and right as well as supporting multiple windows.\n# yum install most After that, you should include the following line in your /.bashrc file, then source the file as you did before and maybe reset your terminal.\nIn this article, we demonstrated how to colourize the man pages in Linux so that they look more appealing. Make use of the comment box down below to ask any questions you may have, as well as to contribute any helpful hints or techniques regarding the Linux shell.\nThank You "},"title":"How to View Colored Man Page Documentation in Linux"},"/utho-docs/docs/linux/install-apache-2-web-server-in-centos-5/":{"data":{"":"","configuration-options#Configuration Options":"","configure-apache#Configure Apache":"","configure-name-based-virtual-hosts#Configure Name-based Virtual Hosts":"","install-apache-http-server#Install Apache HTTP Server":"","install-apache-modules#Install Apache Modules":"","install-support-for-scripting#Install Support for Scripting":"","password-protecting-directories#Password Protecting Directories":"","rewriting-urls-with-mod_rewrite#Rewriting URLs with mod_rewrite":"This tutorial demonstrates how the Apache web server can be installed and configured on CentOS 5. Every setup is done via the terminal; make sure that you have logged in as root via SSH. If you have not followed the initiation guide, it is best to do so before this guide begins. Also note that you may want to consider using our CentOS LAMP Guide if you want to mount a full LAMP stack.\nSet the Hostname Please ensure that you follow our instructions for setting your hostname before downloading and configuring the components mentioned in this section. Make sure the following commands are set correctly:\nhostname hostname -f You will show your short hostname in the first command and your fully qualified FQDN domain name in the second command.\nInstall Apache HTTP Server Ensure that your system is up-to-date with a command:\nyum update You will want to make sure your installation of CentOS is configured to allow inbound traffic to port 80; by issuing a shell request, you can customize the built-in firewall.\nType the following order for Apache HTTP Server installation:\nyum install httpd Issue the following web server launch command:\n/etc/init.d/httpd start To make sure that Apache starts after the next reboot process, issue the command below:\nchkconfig httpd on Install Support for Scripting The following commands are optional and should be executed if you need support from Apache in PHP, Ruby, Python or Perl for server-side scripting.\nTo add support for Ruby, please issue the following command:\nyum install ruby Note that only Ruby programming language support is installed. If scripts and applications are to be run in ruby on web pages, some kind of CGI handler will be required.\nIssue the following order to enable Perl support:\nyum install mod_perl To add support for Python, edit the following command:\nyum install mod_python If you need MySQL support in Php, you do need Php MySQL support to install:\nyum install MySQL-python The following command is given to install PHP support, including can support bundles:\nyum install php php-pear If you still plan to run PHP with mysql, then install mySQL support too:\nyum install php-mysql Configure Apache The httpd.conf file located at: /etc / httpd / conf / httpd.conf includes all Apache settings. We recommend that you back up this file to your home directory, as follows:\ncp /etc/httpd/conf/httpd.conf ~/httpd.conf.backup All files in /etc / httpd / conf.d/ ended with the.conf extension are considered by default to be configuration files. We suggest adding the non-standard configuration options into these folders in the directory. Regardless of how you arrange your configuration files, it is strongly recommended that you make frequent backups of established working conditions.\nNow we‚Äôre going to set up virtual hosting to host many domains (or subdomains) with the server. Such websites can be managed by different users, or by a single user, if you wish.\nBefore beginning, we recommend you merge all virtual hosting configuration into a single vhost.conf file located in the /etc / httpd / conf.d/ directory. Open this file in your favorite text editor and we‚Äôll continue with virtual hosting setup.\nConfigure Name-based Virtual Hosts First, we have to configure Apache to ‚Äúlisten‚Äù only for IP address requests. Apache listens to requests on all IP addresses by default.\nNameVirtualHost \\*:80 Now, for any site you need to host with this api, you can build virtual host entries. Two examples of ‚Äúexample.org‚Äù and ‚Äúexample.net‚Äù websites are given here:\nServerAdmin admin@example.org ServerName example.org ServerAlias www.example.org DocumentRoot /srv/www/example.org/public_html/ ErrorLog /srv/www/example.org/logs/error.log CustomLog /srv/www/example.org/logs/access.log combined ServerAdmin webmaster@example.net ServerName example.net ServerAlias www.example.net DocumentRoot /srv/www/example.net/public_html/ ErrorLog /srv/www/example.net/logs/error.log CustomLog /srv/www/example.net/logs/access.log combined Notes concerning this configuration example:\nAll files for the pages you are hosting are located in the folders below /srv / www. Such directories can be symbolically connected to other locations if you choose to have them elsewhere. ErrorLog and CustomLog entries are suggested, but are not needed for more fine grained logging. The log directories must be generated before you restart Apache if they are specified (as shown above). Before using the above configuration, you will need to build the necessary folders. The following commands for the above configuration may be used for this:\nmkdir -p /srv/www/example.org/public_html mkdir -p /srv/www/example.org/logs mkdir -p /srv/www/example.net/public_html mkdir -p /srv/www/example.net/logs After your virtual hosts have been set up, first issue the following command to run Apache:\n/etc/init.d/httpd start Virtual domain hosting will now function-if you have configured DNS to show your domain at the IP address of your Instance. Note that as many virtual hosts as you need can be installed with Apache.\nWhenever you have modified an option in your vhost.conf file, remember to reload the configuration with the following command:\n/etc/init.d/httpd reload Configuration Options The huge amount of versatility offered in its configuration files is one of Apache‚Äôs strengths and obstacles. The main configuration file is located in /etc / httpd / conf / httpd.conf for the Apache 2 installation in CentOS 5, but the configuration for Apache is also downloaded in a certain order from directories at a variety of different locations. In the following order the configuration files are read, with later items preceding earlier and probably conflicting options listed.\n/etc/httpd/conf/httpd.conf In order to read.conf files, ordered alphabetically by name of file in /etc / httpd / conf.d/ directory. Note, subsequent files triumph over previously listed files. Files will be read in an order dependent on an alpha-numeric sort of file name in a directory of the specified configuration files.\nApache will follow symbolic links to read configuration files so that you can create links to files which are actually elsewhere in your file system in these directories and locations.\nFor most situations we are advised not modifying the default configuration file for compliance with best practice, because most Apache controls can be handled from files that are included in the conf.d/ directory, and administrators may avoid unintended conflicts. When you want to edit httpd.conf, backup the default set-up file and backups of known working states for your device. In case your modifications render unintended errors, you can easily return your server to a working state.\ncp /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd-conf.backup-1 As mentioned above, as in our LAMP guide for CentOS 5.2 configuration files for virtual host sites, you should be able to break site-specific configuration information into additional files, for instance in /etc / httpd / conf.d / vhost.conf.\nInstall Apache Modules One of the key strengths of Apache is its highly versatile configuration. There are few web-serving tasks which Apache can not perform with its support for a large number of modules. The /etc / httpd / modules/ directory is default for the modules. Configuration for standard modules is stored in /etc / httpd / conf / httpd.conf while configuration settings are usually set to .conf files in /etc / httpd / conf.d/ for optional modules that are enabled with Yum.\nCheck for lines beginning with LoadModule statements in the ‚ÄúConf‚Äù files to see if a module is available. A list of currently available modules should be created using the following two grep commands:\ngrep ^LoadModule /etc/httpd/conf/httpd.conf grep ^LoadModule /etc/httpd/conf.d/* Edit the related file and make a comment on the LoadModule statement by prefixing the line of the hash (e.g. #) for deactivation of an active module (at your own risk).\nUsing the following commands to obtain a list of Apache modules available in the CentOS repository:\nyum search mod_ Each of these modules can then be activated with the command:\nyum install mod_[module-name] The modules should be disabled and prepared for use after installation, but additional settings should be added to allow access to the features of the modules. For more detail on configuring different modules, see the Apache Module Documentation.\nUnderstanding .htaccess Configuration The.htaccess file is a webmaster and developer experience with the Apache configuration gui. In these files, configuration options allow you to monitor the actions of Apache by directory. You can lock a directory behind a password wall to prevent general access to it, for example. Furthermore, the .htaccess specific directory files are common places where URL re-writing rules are defined.\nNote that all directories below the file are protected by options listed in a.htaccess file. In addition, note that the higher level settings can be defined for all options provided in.htaccess files. You can set directory level options using \u003c Directory \u003e blocks within your virtual host if this kind of configuration structure is suitable for your setup.\nPassword Protecting Directories We need to create a.htpasswd file in a non-web accessible directory. For instance, if /srv / www / example.com / public html/ is the root of a video host document, use /srv / www / example.com/. Join the folder:\ncd /srv/www/example.com/ Using the htpasswd command, we will create a new password entry for a user named thisl:\nhtpasswd -c .htpasswd cecil Note, you can specify an alternate name for a password file (e.g .. htpasswd), which might be prudent if you wanted to store a number of.htpasswd files for different directories at the same location.\nThese usernames and passwords do not (and should not) match the usernames and passwords of the system. You can also specify how the passwords are encrypted / hashed with the-m flag for MD5, or-s for SHA hashes. Note that when you add additional users to the.htpasswd file, you should not use the-c option (which creates a new file)\nAdd the following lines to the.htaccess file for the directory you want to protect:\nAuthUserFile /srv/www/example.com/.htpasswd AuthType Basic AuthName \"Advanced Choreographic Information\" Require valid-user Note that the AuthName is presented to the user as an explanation in the authentication dialog for what they are asking to access on the server.\nRewriting URLs with mod_rewrite The mod rewrite engine is very powerful and is available for use by default. Although mod rewrite‚Äôs capabilities far exceed the scope of this section, we hope to provide a brief outline and some common use cases.\nEnable mod rewrite with the following line in the \u003c Directory \u003e block or.htaccess file:\nRewriteEngine on Now, a number of separate rewriting rules can be developed. Such rules provide a prototype in which the server compares incoming requests and the server offers an alternate page if a request fits a rewrite pattern. Here is a rewriting rule example:\nRewriteRule ^post-id/([0-9]+)$ /posts/$1.html Let this rule be analyzed. First of all the pattern to suit incoming requests is the first string. Within this second line the files to be handled are listed. Regular expressions are used for patterns in mod rewrite, which means that the ^ matches the beginning and the $matches the end of the string, which implies that the rewriter won‚Äôt rewrite strings partially matching the template.\nThis string rewrite all paths that start in /post-id/ with a number (eg. [0-9]+), which represent the corresponding.html file in the /posts/ directory. The word in the template or parenthetic words describes a variable that has been transferred to the second string as $1, $2, $3, etc.\nThere are many other options for using mod rewrite to allow users to see and interact with useful URLs while maintaining a file structure that makes sense from a development or deployment point of view.","set-the-hostname#Set the Hostname":"","understanding-htaccess-configuration#Understanding .htaccess Configuration":""},"title":"Install Apache 2 Web Server in CentOS 5"},"/utho-docs/docs/linux/install-cwp-in-centos-7/":{"data":{"":"CWP is an free and paid license based hosting panel for managing website database,emails,file,etc using a single Panel. In this article we will discuss how to install CWP in Centos 7","cwp-configuration-#CWP-Configuration:-":"You can Login into CWP admin Panel using url http://server-ip:2030/ or http://hostname:2030\n‚Äì Setup nameservers\n‚Äì Setup shared ip (must be your public IP address)\n‚Äì Setup at least one hosting package (or edit default package)\n‚Äì Setup root email\nNow CWP installation and Configuration is completed and CWP Panel is ready to use.\nThankyou..","cwp-installation-#CWP-Installation:-":"Step:-1. Access your server using putty or terminal with ssh by ssh command.\nStep:- 2. After access server run the below command for download and install CWP script.\nFor CentOS 6: New Installer with MARIA-DB 10-latest\n[root@CWP ~]# cd /usr/local/src [root@CWP ~]# wget http://centos-webpanel.com/cwp-latest [root@CWP ~]# sh cwp-latest For CentOS 7: Installer for CentOS 7\n[root@CWP ~]# cd /usr/local/src [root@CWP ~]# wget http://centos-webpanel.com/cwp-el7-latest [root@CWP ~]# sh cwp-el7-latest Step:- 3. After run the installer script wait for 30-40 minutes for completed CWP installation .\nStep:- 4. As CWP installation will complete. You need to reboot the server.\n[root@CWP ~]# reboot ","hardware-requirements#Hardware Requirements":"32 bit operating systems require a minimum of 512 MB RAM\n64 bit operating systems require a minimum of 1024 MB RAM (recommended)\nRecommended System: 4 GB+ RAM so you would have the full functionality such as Anti-virus scan of emails.","software-requirements#Software Requirements":"You must have a clean/fresh installation of supported operating systems:\nCentOS 6, RedHat 6 or CloudLinux 6, MINIMAL installation and English version only!\nCentOS 7 is also supported, we recommend minimal version.\nCentOS 8 is¬†NOT¬†supported yet."},"title":"How to install CWP in Centos 7"},"/utho-docs/docs/linux/install-ibm-websphere-application-server-in-linux/":{"data":{"":" How to install IBM WAS in linux\nIn this tutorial, you will learn how to install IBM Websphere Application Server in linux. WebSphere Application Server is the most popular JEE application platform in the world. Tens of thousands of businesses use it to run more than a million applications.","prerequisites#Prerequisites":" Need to have IBM id to install IBM WAS Super User or any normal user with SUDO privileges. IBM Installation Manager installed machine. You can install IBM installation manager using this guide. ","steps-to-install-ibm#Steps to install IBM":"Step 1. Installation of Installation manager, creates a new directory in /opt named as IBM. Now go to this directory and list all available files\ncd /opt/IBM/InstallationManager/eclipse/tools/ Step 2. Now find and save or copy either the latest online BASE repository or the composite repository to install WAS. At this time, following are the above mentioned repositories and THIS is the URL to find these repositories. In this tutorial we will use composite repositories to install IBM WAS.\nRepositories to install IBM WAS\nStep 3. List all the available packages in the given repository. As mentioned, we will use the composite repository. And at the time of writing this tutorial, https://www.ibm.com/software/repositorymanager/V9WASBase is the composite repository.\n./imcl listAvailablePackages -repositories https://www.ibm.com/software/repositorymanager/V9WASBase -prompt Note that, here you will be asked to enter the IBM login credentials, such as email id and password. To enter the same, press ‚ÄòP‚Äô then ‚Äòemail-id‚Äô and then password\nLists of available packages in repo\nIf you are using composite repositories, you will see java jdk, java websphere appClient, websphere base, websphere IHS etc.\nStep 4. Now install the IBM WAS of latest version and its corresponding Java version using the below command\n./imcl install com.ibm.websphere.BASE.v90_9.0.0.20160526_1854¬†com.ibm.java.jdk.v8_8.0.7016.20220915_1446 -repositories https://www.ibm.com/software/repositorymanager/V9WASBase -prompt -showProgress -acceptLicense Installation prompt to enter IBM login details\nAnd this is how you have learnt how to install IBM Websphere Application Server, WAS, successfully in linux using command line.\nOther useful article: Install IBM HTTP server in Linux or How to access IBM WAS admin console"},"title":"Install IBM Websphere Application Server ( IBM WAS) in Linux"},"/utho-docs/docs/linux/install-multiple-version-of-php-on-ubuntu-server/":{"data":{"":" Install multiple version of PHP on Ubuntu server\nIntroduction: In this tutorial, you will learn how to install multiple versions of php on Ubuntu server. PHP is a popular open source general-purpose scripting language that is especially well suited for web development and can be integrated into HTML. PHP is also known as PHP: Hypertext Preprocessor.\nPrerequisite: Before installing the PHP versions, we must satisfy the minimum requirement to do so.\nSuper user( root) or any normal user with SUDO privileges. Internet should be working on the machines. ","steps-to-follow#Steps to follow:":"Step 1: Install the dependencies which will be required to install multiple versions of PHP\n# apt-get install software-properties-common -y You will use the software-properties-common package‚Äôs apt-add-repository command-line application to add the ondrej/php PPA (Personal Package Archive) repository.\nStep 2: Now add the ppa:ondrej repository by below command\n# add-apt-repository ppa:ondrej/php Note: Here you will be asked to confirm whether you want to add the repository or not You can confirm the download by just clicking the Enter button. To cancel the download just press Ctrl + C\nStep 3: Now refresh list of available packages.\n# apt-get update -y Step 4: Install the required packages of php, in this example, we have installed php version 7.3\n# apt-get install php7.3 php7.3-fpm php7.3-mysql libapache2-mod-php7.3 libapache2-mod-fcgid -y Step 5: Similarly, install the second version of PHP. In this example, we will install PHP version 7.4\n# apt-get install php7.4 php7.4-fpm php7.4-mysql libapache2-mod-php7.4 libapache2-mod-fcgid -y Note: Here, you will see message, which says that PHP 7.4 FPM is not enabled by default. Therefore, we need to enable that manually.\nStep 6: Similarly, install the second version of PHP. We will install PHP version 8.1\n# apt-get install php8.1 php8.1-fpm php8.1-mysql libapache2-mod-php8.1 libapache2-mod-fcgid -y Again, you will see the same message, but this time it will be about php 8.1 version.\nStep 6.2: You can also, install some other basic dependency.\n# `apt-get install -y php php-cli php-common php-fpm php-mysql ph-zip php-gd php-mbstring php-curl php-xml php-bcmath openssl php-json php-tokenizer` Step 7: Now, finally, we will enable the PHP FPM version by using below command.\n# systemctl start php7.4-fpm ","systemctl-start-php81-fpm#systemctl start php8.1-fpm":" By looking at the status of both the FPMs, you will see that both versions are running and started\nNote: In order to update the default version of php:\nupdate-alternatives --config php In this tutorial, you have learned that how to install multiple version of php on Ubuntu server.\nAlso Read: How to install WordPress with LEMP on CentOS server, How to install Cockpit on Ubuntu server"},"title":"Install multiple version of PHP on Ubuntu server"},"/utho-docs/docs/linux/install-plesk-on-centos-7/":{"data":{"":"","install-plesk-using-browser#Install Plesk Using Browser":"If your host or IP address can be resolved and the 8447 port is available, you will be able to launch your Plesk Onyx installer through the browser.\n1. Download plesk GUI installer using following below command:\nwget https://autoinstall.plesk.com/plesk-installer 2. Set execution permissions on downloaded Plesk installer using command below.\nchmod +x ./plesk-installer 3. Initiate the Plesk installer with the following command.\n./plesk-installer ----web-interface 4. Access Installer via browser under port 8447 using IP address\nhttps://your-ip-address:8447 or host https://your-host-name:8447 5. Enter \"root\" user and password to access the web interface.\n6. Click on ‚ÄúInstall or upgrade product \"\n7. Choose the latest stable version of your product and press \"Continue\"\n8. Select one of the installation types that will identify the list of packages. Or you can just enable ‚ÄúSelect Preview Components‚Äù and choose all the components you like. Click ‚ÄúContinue‚Äù to proceed with the installation process.\n9. You will see the output of the console inside the web interface. Wait till the installation process is over.\n10. When you will get the message‚ÄùAll operations with products and components have been successfully completed.\" Click Ok\" and it will exit the installation page.\n11. Now reboot your server and after reboot configure your plesk panel using plesk url http://your-server-ip:8880 or https://your-server-ip:8443 by entering root user and password.\n12. After login in plesk enter the email address for admin notifications and password for admin user.\n13. Enter the Licnese key if you have plesk panel license key or go with ‚ÄúProceed with a full-featured trial license‚Äù and Check the button ‚ÄúI confirm that I've read and accepted the End-User License Agreement \"\n15. Click On enter plesk.\nThank You.","method-1#\u003cstrong\u003eMethod 1\u003c/strong\u003e":"","method-2#\u003cstrong\u003eMethod\u003c/strong\u003e 2":"\nPlesk is a private web host panel that provides users with the ability to administer websites, databases, emails and domains for their personal and/or custome clients. Allows simple point-and-click administration / maintenance via a browser.\nMethod 1 One Click Installation sh \u003c(curl [https://autoinstall.plesk.com/one-click-installer](https://autoinstall.plesk.com/one-click-installer) || wget -O - [https://autoinstall.plesk.com/one-click-installer](https://autoinstall.plesk.com/one-click-installer) --no-check-certificate) -‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\nMethod 2 "},"title":"Install Plesk on CentOS 7"},"/utho-docs/docs/linux/install-ssl-on-ubuntu-with-apache2/":{"data":{"":" How to install SSL on Ubuntu with Apache2\nWhat is SSL Before learning how to install SSL on Ubuntu with Apache2. Let‚Äôs first learn that SSL, or Secure Sockets Layer, is an Internet security technology based on encryption. It was created by Netscape in 1995 to provide privacy, authentication, and data integrity for Internet interactions. SSL is the forerunner of the current TLS encryption protocol.\nA certificate that is self-signed will encrypt communications between your server and any clients. Users cannot use the certificate to automatically authenticate the identity of your server, since it is not certified by any of the trustworthy certificate authorities available in web browsers.\nIf you do not have a domain name linked with your server and the encrypted web interface is not user-facing, a self-signed certificate may be suitable.","os-and-other-important-files#OS and other important files:":" We have used Ubuntu with Apache2 service Default Configuration file: /etc/apache2/apache.conf SSL Virtual Host file: /etc/apache2/sites-available/default-ssl.conf Steps to install the SSL: Step 1: Install the Apache2 server on your Ubuntu server using the below command.\n[console ]apt install apache2 -y[/console]\nStep 2: Start the Apache2 service on your server so that you can use it\nsystemctl start apache2 Step 3: Now to check whether your apache service is running fine or not, go to browser and search your server ip as shown below\nStep 4: Now if we want to test apache server with a sample source code. Go to the /var/www/html directory and create an index. html file and paste the content shown in the below screenshot.\nvim /var/www/html/index.html Step 5: Now generate the CSR and private key files using below command\nopenssl req -new -newkey rsa:2048 -nodes -keyout domain.com.key -out domain.com.csr Note:\nHere you will be asked to enter a few details. So please enter them as you want. Above command will generate the csr and private key in the present working directory In common name, for the subdomain, you must include an asterisk if you are requesting a unique wildcard SSL certificate. In such scenario, *.mydomain.com may serve as an example. Never enter any special characters in this area, including ‚Äúhttp://‚Äù, ‚Äúhttps://‚Äù, or any other variation. Never add text after the top-level domain. Your common name, for instance, should finish in.com,.net, or the other extension you are requesting. Congratulations, a CSR file has been generated.\nWhen getting an SSL certificate, you must copy and paste the whole contents of the CSR file to your Certificate Authority.\nThe lines that say ‚ÄúBEGIN CERTIFICATE REQUEST‚Äù and ‚ÄúEND CERTIFICATE REQUEST‚Äù must be included.\nStep 6: Download the validation file from the SSL panel.\nStep 7: After getting the validation file,\nStep 7.1: Your SSL/TLS site‚Äôs primary configuration file for the Ubuntu server with Apache2 is normally located in /etc/apache2/sites-enabled/default-ssl.conf\nRun the command below if it isn‚Äôt in the ‚Äúsites-enabled‚Äù directory.\na2ensite default-ssl.conf¬†Now identify the below lines and make sure to have these changes in your default conf file of the website.\nvim /etc/apache2/sites-enabled/default-ssl.conf \u003cVirtualHost *:443\u003e¬†DocumentRoot /var/www/path_to_source_code¬†SSLEngine on¬†SSLCertificateFile /path/to/your_domain_name.crt¬†SSLCertificateKeyFile /path/to/your_private.key\u003c/VirtualHost\u003e Note:\nValue of SSLEngine should be - ‚Äòon‚Äô In line SSLCertificateFile should be the path of ssl certificate\nIn the next line, SSLCertificateKeyFile, mention the path of key file of the SSL In some case, you can receive an error ‚ÄúCertificate Chain is incomplete, missing Certificate(s)‚Äù, in this case, follow this extra step. Append another line- SSLCertificateChainFile /path/to/gd.bunddle.crt Step 8: Now run these following commands to reflect the SSL Certificate on your server and reload the Apache2 service\na2enmod ssl Note: please mention here the ssl conf file which you used in your server\nsystemctl restart apache2 Step 9: Now Restart the apache service and go to your browser and browse https://server_Ip¬†Now click on advance, and then ‚Äúproceed to‚Ä¶(unsafe)‚Äù\nssl installed domain\nIn this tutorial, you have learned how to install SSL on Ubuntu with Apache2\nAlso Read: Install SSL on Ubuntu server using Nginx, How to install SSL on CentOS-7.3 with httpd server","prerequisites#Prerequisites:":" apt command should be working to install packages Internet should be working on the machine Super user or normal user with SUDO privileges "},"title":"How to install SSL on Ubuntu with Apache2"},"/utho-docs/docs/linux/install-vnstat-network-monitoring-on-centos-7/":{"data":{"":"","consolebasednetwork-traffic-monitoring#Console¬†based¬†network traffic monitoring":"This article will help you install VnStat network monitoring on your CentOS 7 server. VnStat is console¬†based¬†network traffic monitor for Linux. This will help monitor multiple interfaces at the same time. Several performance options can be configured based on user requirements.\nTo install VnStat, you need to enable the EPEL repositories package on your Linux server version.\nyum -y install epel-release You can now install VnStat with the following command.\nyum -y install vnstat Once the installation is completed, use the following command to start the vnstat service.\nsystemctl start vnstat systemctl enable vnstat chkconfig vnstat on Check the status of the service using the following command.\nservice vnstat status You can monitor the Network by the following commands from the command line.\nvnstat -----help Output:\nvnStat 1.16 by Teemu Toivola -q, --query query database -h, --hours show hours -d, --days show days -m, --months show months -w, --weeks show weeks -t, --top10 show top10 -s, --short use short output -u, --update update database -i, --iface select interface (default: eth0) -?, --help short help -v, --version show version -tr, --traffic calculate traffic -ru, --rateunit swap configured rate unit -l, --live show transfer rate in real time ","install-and-configure-vnstat-php-web-based-interface#Install and Configure VnStat PHP Web Based Interface":"VnStat also provides a web interface networking monitoring based¬†on php to display graphic statistics. In order to set up a vnStat web interface, required¬†Apache, php and php-gd packages on your system.\nIf you don‚Äôt have Apache, you need to install apache web server and php else skip apache and php installation.\nStep 1: Download vnStat Source Archive\nwget http://www.sqweek.com/sqweek/files/vnstat_php_frontend-1.5.1.tar.gz Step 2: Extract Archive\nExtract downloaded archive in apache default root directory /var/www/html/ using following command.\ntar xzf vnstat_php_frontend-1.5.1.tar.gz mv vnstat_php_frontend-1.5.1 /var/www/html/vnstat Step 3: Edit Configuration File\nEdit the¬†config.php**(/var/www/html/vnstat/config.php)**¬†file and set the below parameters as per your setup.\n$language = 'en'; $iface_list = array('eth0', 'sixxs'); $iface_title['eth0'] = 'Public Interface'; $vnstat_bin = '/usr/bin/vnstat'; Step 4: Now you can access vnStat in Web browser:- Using your cloud server ip or domain name.\nhttp://192.168.1.90/vnstat/ or http://web.microhost.com/vnstat/ "},"title":"How to install VnStat Network Monitoring on CentOS 7"},"/utho-docs/docs/linux/install-whm-cpanel-in-centos-7/":{"data":{"":"\nHardware requirements of WHM/Cpanel\nYou have sudo user access or root access to install whm/Cpanel.\nStep:- 1. Access your server using SSH from putty or any third party software.\nHere we are using default 22 port for SSH access, If you are using any custom port then mention that port number in port option.\nStep:- 2. Login as root and enter password of root user.\nStep:- 3. Set hostname of your Microhost cloud server to install Cpanel and hostname should be pointed to cloud server ip.\n[root@cloud ~]# hostnamectl set-hostname cloud.microhost.cloud Step- 4. Disable and stop Networkmanager using below command.\n[root@cloud ~]# systemctl stop NetworkManager [root@cloud ~]# systemctl disable NetworkManager Step- 5. Use below command to install WHM/Cpanel.\n[root@cloud ~]# cd /home \u0026\u0026 curl -o latest -L https://securedownloads.cpanel.net/latest \u0026\u0026 sh latest Step:- 6. WHM/Cpanel installation is in process and it will take 30 minutes to complete.\nStep:- 7. Once the installation is completed, the console will display a message like ‚Äúthe cPanel Installation has been completed‚Äù.\nStep- 8. WHM/Cpanel installation is completed and now you need to reboot the server and after that access WHM/Cpanel using below URL:-\nWHM:- serverip:2086 or hostname:2086 Cpanel:- serverip:2082 or hostname:2082 Step:- 9. Enter the private nameserver if you want to use private nameserver.\nStep:- 10. Now WHM/Cpanel has been configured and ready to use.\nThank you.."},"title":"Install WHM/Cpanel in Centos 7"},"/utho-docs/docs/linux/installation-and-configuration-of-apache-tomcat-9-on-centos-7/":{"data":{"":"Apache Tomcat program is an open-source application of Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies. Under the Java Community Process the Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket specifications are developing.\nThe Apache Tomcat software is developed and released under the¬†Apache License version 2 in an open and participatory environment. The Apache Tomcat project is designed to collaborate with the world‚Äôs best breed developers. We invite you to take part in this open project for development.\nApache Tomcat Software provides power over a wide range of industries and organizations to numerous large-scale mission-critical Web applications.","access-the-tomcat-admin-page#Access the Tomcat admin page":"You can access the web admin page through\n[root@Microhost ~]#http:// \u003cyour_domain_or_IP_address\u003e :8080 After executing the above url you will get the below output:\nThank You :)","changing-the-tomcat-port-from-8080-to-80#Changing the Tomcat port from 8080 to 80":"You can change the port of Tomcat in the configuration file server.xml by using the following command:\n[root@Microhost ~]#vi /opt/tomcat/conf/server.xml You will see the output like the screenshot given below:\nNow we have to change the port 8080 to 80 as per the given screenshot:\nSave the server.xml file using :wq and redirect to bin directory:\n[root@Microhost ~]# cd /opt/tomcat/bin/ First shutdown the Tomcat and start it again by using below command:\n[root@Microhost ~]#./shutdown.sh [root@Microhost ~]#./startup.sh ","configure-tomcat-web-management-interface#Configure Tomcat Web Management Interface":"At this time, Tomcat is set up and we can access it with a web browser on port 8080 or 80, but because we haven‚Äôt already created a user, we can never access the web management interface.\nIn the tomcat-users.xml file, Tomcat users and their roles are defined.\nWhen you open the file, you will notice that it contains comments and examples of how to configure the file. You can open the file by typing the following commnd:\n[root@Microhost ~]#vi /opt/tomcat/latest/conf/tomcat-users.xml After executing the above command the output will be shown like :\n\u003ctomcat-users\u003e \u003c!-- Comments --\u003e \u003crole rolename=\"admin-gui\"/\u003e \u003crole rolename=\"manager-gui\"/\u003e \u003cuser username=\"admin\" password=\"admin_password\" roles=\"admin-gui,manager-gui\"/\u003e \u003c/tomcat-users\u003e In order to add a new user, you need to define the user in the tomcat-users.xml (manager-gui and admin-gui) file, as shown below. Make sure the username and password are changed to something safer:\nThe web management interface of Tomcat is configured by default to allow localhost access only. You can open the following files and make the following changes if you want to access the website interface from a remote IP or from anywhere. However, public access is not recommended as it is a security risk.\nIf you need to access the web interface from anywhere, open the following files and comment or delete the lines that are in yellow:\n[root@Microhost ~]#vi /opt/tomcat/latest/webapps/manager/META-INF/context.xml After executing the above command the output will be shown like :\n\u003cContext antiResourceLocking=\"false\" privileged=\"true\" \u003e \u003c!-- \u003cValve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" /\u003e --\u003e \u003c/Context\u003e Now we have make the changes for host-manager :\n[root@Microhost ~]#vi /opt/tomcat/latest/webapps/host-manager/META-INF/context.xml After executing the above command the output will be shown like :\n\u003cContext antiResourceLocking=\"false\" privileged=\"true\" \u003e \u003c!-- \u003cValve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" /\u003e --\u003e \u003c/Context\u003e If you only have to access the web interface from a specific IP, add your public IP to the list instead of commenting on the blocks. Let‚Äôs say your public IP is 192.168.24.45, and just from that IP you want access:\n[root@Microhost ~]#vi /opt/tomcat/latest/webapps/manager/META-INF/context.xml \u003cContext antiResourceLocking=\"false\" privileged=\"true\" \u003e \u003c!-- \u003cValve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1 | 192.168.24.45\" /\u003e --\u003e \u003c/Context\u003e Now we have make the changes for host-manager :\n[root@Microhost ~]#vi /opt/tomcat/latest/webapps/host-manager/META-INF/context.xml \u003cContext antiResourceLocking=\"false\" privileged=\"true\" \u003e \u003c!-- \u003cValve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1 | 192.168.24.45\" /\u003e --\u003e \u003c/Context\u003e The list of IP addresses permitted is a list separated by a vertical bar. You may add individual IP addresses or use a standard term.\nTo make changes successful, restart the Tomcat service:\n[root@Microhost ~]#systemctl restart tomcat ","create-a-systemd-unit-file#Create a systemd unit file":"create a tomcat.service unit file in the /etc/systemd/system/ folder to make Tomcat run as a service\n[root@Microhost ~]# touch /etc/systemd/system/tomcat.service Now open your text editor and edit the file tomcat.service\n[root@Microhost ~]# vi /etc/systemd/system/tomcat.service Paste the following content:\n[Unit] Description=Tomcat 9 servlet container After=network.target [Service] Type=forking User=tomcat Group=tomcat Environment=\"JAVA_HOME=/usr/lib/jvm/jre\" Environment=\"JAVA_OPTS=-Djava.security.egd=file:///dev/urandom\" Environment=\"CATALINA_BASE=/opt/tomcat/latest\" Environment=\"CATALINA_HOME=/opt/tomcat/latest\" Environment=\"CATALINA_PID=/opt/tomcat/latest/temp/tomcat.pid\" Environment=\"CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC\" ExecStart=/opt/tomcat/latest/bin/startup.sh ExecStop=/opt/tomcat/latest/bin/shutdown.sh [Install] WantedBy=multi-user.target Save the File with :wq and exit.\nInstruct Systemd that a new unit file has been created by typing:\n[root@Microhost ~]# systemctl daemon-reload Now start and enable the Tomcat service by typing:\n[root@Microhost ~]# systemctl start tomcat [root@Microhost ~]# systemctl enable tomcat We can also start and stop the Tomcat service from the /bin directory:\n[root@Microhost ~]# cd /opt/tomcat/bin/ To start the Tomcat service use following command:\n[root@Microhost ~]#./startup.sh To stop the Tomcat service use following command:\n[root@Microhost ~]#./shutdown.sh [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNote‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=\"\" style=\"\" ]We suggest you to always start or stop Tomcat service from the bin directory after making any chages in cofiguration of Tomcat.[/ht_message]","create-tomcat-system-user#Create Tomcat system user":"To run Tomcat as the root user is a security risk and does not take best practice into account\nWe will create a new system user and group that will run the Tomcat service with a home directory /opt/tomcat:\n`[root@Microhost ~]#`useradd -m -U -d /opt/tomcat -s /bin/false tomcat ","download-tomcat-through-wget#Download Tomcat through wget":"We will download the version of Tomcat 9.0.33 from the Tomcat official downloads page. You should check the Tomcat 9 download page before proceeding with the next stage to see whether a new release is available. Download Tomcat zipper with the following wget command and navigate into /tmp directory:\n`[root@Microhost ~]#` wget [https://mirrors.estointernet.in/apache/tomcat/tomcat-9/v9.0.33/bin/apache-tomcat-9.0.33.tar.gz](https://mirrors.estointernet.in/apache/tomcat/tomcat-9/v9.0.33/bin/apache-tomcat-9.0.33.tar.gz) After completion of tomcat download extract the tar file as given below:\n`[root@Microhost ~]#` tar -xf apache-tomcat-9.0.33.tar.gz Move the Tomcat source files to it to the /opt/tomcat directory:\n[root@Microhost ~]# mv apache-tomcat-9.0.33 /opt/tomcat/ Run the following command to alter the user and group property of directory tomcat\n[root@Microhost ~]# chown -R tomcat: /opt/tomcat Make the scripts executable in the bin directory with the following chmod command:\n[root@Microhost ~]# sh -c 'chmod +x /opt/tomcat/latest/bin/*.sh' ","opening-of-port-8080-in-server-firewall#Opening of port 8080 in server firewall":"If you want to access the tomcat interface from outside your local network and your server is protected through a firewall, you must open port 8080.\nTo open the required port, use the following commands:\n[root@Microhost ~]#firewall-cmd --zone=public --permanent --add-port=8080/tcp [root@Microhost ~]#firewall-cmd --reload ","prerequisites#Prerequisites":" First, deploy the new server instance with Microhost.com Log in to the server with ssh(putty) with root privilege. Update the server using the command `[root@Microhost ~]#`Yum update Installation of open Jdk(java) Tomcat 9 includes Java SE 8 or later. We must install OpenJDK, an open-source Java Platform implementation, which is the default Java creation and runtime for CentOS 7.\nInstall Java by typing the command given below\n`[root@Microhost ~]#`yum install java-1.8.0-openjdk-devel "},"title":"INSTALLATION AND CONFIGURATION OF APACHE TOMCAT 9 ON CENTOS 7"},"/utho-docs/docs/linux/installation-of-lamp-stack-on-centos-7/":{"data":{"":"LAMP, which operates Linux as the operating system, is an open-source web-developing platform using Apache as the web-based server and PHP as an object-oriented scripting language. (Instead of PHP, Perl or Python are sometimes used.)","configuration-of-mysql-server#Configuration of Mysql server":"To run a security script, use this command:\n[root@Microhost ~]# mysql_secure_installation The output will be shown as below:\nCreate a new 12-character password with at least one uppercase letter, one lowercase letter, and one special character. When asked, type it again.\nYour new password is good and you‚Äôre asked to change it right away. You should receive feedback. You can easily say no because you have just done it:\nOutput: Estimated strength of the password: 100 Change the password for root ? (Press y|Y for Yes, any other key for No) : If we reject the request to rework the password, we must press Y and then Enter all of the following questions so that the anonymous users can be removed, the remote key can be disabled also test database will be removed.\nAfter completion of the installation we can enable the service of Mysql:\n[root@Microhost ~]# systemctl enable mysqld We can check the MySQL while login using below command:\n[root@Microhost ~]# mysql -u root -p ","install-php-modules#Install php modules":"We can optionally install additional modules to improve PHP‚Äôs functionality.\nYou can type this into your program to see the choices for PHP modules and libraries:\n[root@Microhost ~]# yum search php The consequence is that you can load all optional components. For each one, you will be given a brief description:\nphp-bcmath.x86_64 : A module for PHP applications for using the bcmath library php-cli.x86_64 : Command-line interface for PHP php-common.x86_64 : Common files for PHP php-dba.x86_64 : A database abstraction layer module for PHP applications php-devel.x86_64 : Files needed for building PHP extensions php-embedded.x86_64 : PHP library for embedding in applications php-enchant.x86_64 : Enchant spelling extension for PHP applications php-fpm.x86_64 : PHP FastCGI Process Manager php-gd.x86_64 : A module for PHP applications for using the gd graphics library . . . We can install the php modules by the following command\n[root@Microhost ~]# yum install php-fpm If you wish to install more than one module, you may do so by listing one module, separated by a window, according to the command yum install:\n[root@Microhost ~]# yum install module1 module2 ... We can test the php using test page. We will edit create a php file at given location:\n[root@Microhost ~]# vi /var/www/html/info.php Now copy the content as given below in the file and save the file by :wq\n\u003c?php phpinfo(); ?\u003e We can access the page by following url:\nhttp://your_server_ip/info.php Thank You :)","installation-of-apache-server#Installation of Apache Server":"The Apache web server is the most popular web server in the world, making hosting websites a great default option.\nWe can easily install Apache with the package manager of CentOS, yum. Use the below command to install Apache server .\n[root@Microhost ~]# yum install httpd Afterward, your web server has been installed.\nOnce it gets installed, you can start Apache on your server by the following command .\n[root@Microhost ~]# systemctl start httpd.service Also, enable the https service by following command so, it will start at boot time whenever the system gets rebooted.\n[root@Microhost ~]# systemctl enable httpd.service [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]If firewalld is inactive then skip all the firewall related commands.[/ht_message]\nWe need to check the firewall status. whether it is active or inactive. If firewall is in active mode then we have to add the port 80 or add the http service in the firewall .\n[root@Microhost ~]# systemctl status firewalld The output will be shown as below if the firewall is active.\nNow we have to add the https service in the firewall by following command.\n[root@Microhost ~]# firewall-cmd --permanent --add-service=http Reload the firewall rules by the following command.\n[root@Microhost ~]# systemctl restart firewalld [root@Microhost ~]# systemctl reload firewalld Now check the webserver while accessing the server IP address in the browser:\nhttp://server_ip The output will be shown as given below:","installation-of-mysql-server#Installation of Mysql server":"Download the MySQL packages from official website by the following command:\n[root@Microhost ~]# wget https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm Now that we have checked the file has not been corrupted or modified, we are going to install the package:\n[root@Microhost ~]# rpm -ivh mysql57-community-release-el7-9.noarch.rpm It introduces two additional MySQL yum repositories and we can use them in MySQL server installation:\n[root@Microhost ~]# yum install mysql-server To indicate you want to continue, click ‚Äòy‚Äô\nWith¬†the¬†following¬†command¬†we¬†will¬†start¬†the¬†daemon:\n[root@Microhost ~]# systemctl start mysqld Check the mysql service status by the following command:\n[root@Microhost ~]# systemctl status mysqld A temporary password for the MySQL root user is created during the installation process. Find it with the following command in mysqld.log:\n[root@Microhost ~]# grep 'temporary password' /var/log/mysqld.log The output will be shown as below:\n2020-04-22T09:53:31.440319Z 1 [Note] A temporary password is generated for root@localhost: Lktg)3NtgIlh Note the password that is required to protect your installation in the next step and where you are forced to change it. The default authentication policy requires 12 letters, with at least one letter from an upper case and one letter from a lowercase also a special character.","installation-of-php#Installation of Php":"PHP is the part of our setup that processes code for dynamic content display. It can run scripts, link to our MySQL databases, and view processed content to our web server.\nWe can use the following command for the installation of our components. The php-mysql kit will also be included:\n[root@Microhost ~]# yum install php php-mysql PHP should be built problem-free. To function with PHP we have to restart the Apache web server\n[root@Microhost ~]# systemctl restart httpd ","prerequisites#Prerequisites":" Before you begin with this guide, you should have root user account privilege set up on your server. Log in to the server with (SSH). Update the server packages using the command yum update -y . "},"title":"How to Install (Linux, Apache, MariaDB, PHP) LAMP Stack on CentOS 7"},"/utho-docs/docs/linux/installing-mongodb-on-centos-7/":{"data":{"":"\nMongoDB is a database engine that provides access to non-relational, document-oriented databases. It is known as a NoSQL database, because it is not based on a conventional table-oriented relational database framework.\nUnlike relational databases, before you add data to a database, MongoDB doesn‚Äôt require a predefined schema. You can alter the schema at any time and as often as is necessary without having to setup a new database with an updated schema.","add-mongodb-yum-repository#Add MongoDB Yum Repository":"Create a new file, vi /etc/yum.repos.d/mongodb-org-4.2.repo so that you can install the latest release using yum. Add the following contents to the file:\nvi /etc/yum.repos.d/mongodb-org-4.2.repo [mongodb-org-4.2] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.2/x86_64/ gpgcheck=enabled=gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc ","backup-and-restore-mongodb-database#Backup and restore MongoDB Database":"We use these tools:\nmongodump mongorestore Command to mongodb backup\nThe¬†mongodump¬†command will create a backup / dump of the MongoDB database with the name specified by the¬†--db [DB NAME]¬†argument.\nThe¬†--out /var/backups/`date +\"%Y%m%d\"¬†argument specifies the output directory as¬†/var/backups/[TODAY'S DATE]¬†e.g.¬†/var/backups/20190903\nsudo mongodump --db [DB NAME] --out /var/backups/date +\"%Y%m%d\" It assumes that you have a running mongodb instance on port 27017.\nMongodb backup when database is on remote server or port is different on localhost mongodump --host \u003chostname\u003e: --db \u003csource database name\u003e --authenticationDatabase \u003csource database name\u003e -u \u003cusername\u003e -p \u003cpassword\u003e Commands to restore mongodb database sudo mongorestore --db [DB NAME] /var/backups/[BACKUP FOLDER NAME] Thankyou.","configure-mongodb#Configure MongoDB":"The configuration file for MongoDB is located at¬†/etc/mongod.conf, and is written in YAML format.\nsecurity: authorization: enabled ","create-database-users#Create Database Users":"1. Open the mongo shell\nmongo 2. By default, MongoDB connects to a database called¬†test. Before adding any users, create a database to store user data for authentication:\nuse admin 3. Use this command to create an administrative user with the ability to create other users on any database. For better security, change the values¬†mongo-admin¬†and¬†password:\ndb.createUser({user: \"mongo-admin\", pwd: \"password\", roles:[{role: \"userAdminAnyDatabase\", db: \"admin\"}]}) Hold these credentials for future reference in a secure spot. All the information will be displayed to the database except the password.\n4. Exit the mongo shell:\nquit() ","install-mongodb#Install MongoDB":" sudo yum install mongodb-org ","start-and-stop-mongodb#Start and Stop MongoDB":"Start the MongoDB service with the systemctl utility:\nsudo systemctl start mongod To restart the mongod process\nsudo systemctl restart mongod The stop command halts all running mongod processes.\nsudo systemctl stop mongod MongoDB¬†can¬†also¬†be¬†started¬†on¬†boot:\nsudo systemctl enable mongod "},"title":"Installing MongoDB on CentOS 7"},"/utho-docs/docs/linux/installing-postgresql-on-ubuntu-20-04-step-by-step-instructions/":{"data":{"":"","step-1--installing-postgresql#\u003cstrong\u003eStep 1 ‚Äî Installing PostgreSQL\u003c/strong\u003e":"","step2-utilizing-postgresql-roles-and-databases#**Step.2 Utilizing PostgreSQL Roles and Databases":"\nIntroduction\nPostgreSQL implements the SQL querying language. Standards-compliant, it has reliable transactions and concurrency without read locks.\nThis guide shows how to install PostgreSQL and create a new user and database on an Ubuntu 20.04 server. See How to Install and Use PostgreSQL on Ubuntu 20.04 for a more detailed instruction.\nPrerequisites\nYou will need one Ubuntu 20.04 server that has been configured using our Initial Server Setup for Ubuntu 20.04 guide if you want to follow along with this tutorial. Your server ought to have a basic firewall and a non-root user with sudo capabilities after finishing this preparatory instruction.\nStep 1 ‚Äî Installing PostgreSQL In order to install PostgreSQL, you must first refresh the local package index on your server.\n#sudo apt update Install Postgres and -contrib, which adds utilities and functionality:\n#sudo apt install postgresql postgresql-contrib Check to see that the service has been initiated:\n#sudo systemctl start postgresql.service **Step.2 Utilizing PostgreSQL Roles and Databases **\nWhen it comes to handling authentication and authorisation, Postgres‚Äôs default settings make use of a concept known as ‚Äúroles.‚Äù These are comparable to the standard users and groups used by the Unix operating system in several respects.\nDuring the installation process, Postgres is configured to make use of the ident authentication method. This means that it links each of the Postgres roles to a corresponding Unix or Linux system account. If there is a role within Postgres with the same name as a Unix or Linux username, the user will have the ability to sign in as that role.\nDuring the course of the installation process, a user account with the name postgres was established. This account is linked to the default Postgres role. This account can be used in a number different ways in order to gain access to Postgres. Switching to the postgres account on your server can be done in a number of ways, one of which is to execute the following command:\n#sudo -i -u postgres After that, you can launch the Postgres prompt by typing:\n#psql With the PostgreSQL prompt now open, you can immediately start interacting with the database management system.\nRun these commands to leave the PostgreSQL prompt:\n#q After doing so, you will be brought back to the command prompt for postgres on Linux. Execute the exit command to return to your regular user account on the system:\n#exit You may also access to the Postgres prompt by executing the psql command with the sudo prefix while logged in as the postgres account:\n#sudo -u postgres psql By using this, you can log into Postgres without using a bash shell as a middleman.\nOnce more, you can end the interactive Postgres session by executing the commands below:\n#q "},"title":"Installing PostgreSQL on Ubuntu 20.04: Step-by-Step Instructions"},"/utho-docs/docs/linux/laravel-application-hosting-in-plesk/":{"data":{"":" How to install Laravel Application on Plesk serve\nDescription\nLaravel is an open-source web application framework that simplifies routine tasks that are integral to the majority of web development projects. These tasks include authentication and routing, as well as sessions and caching. It is the goal of Laravel to make the experience of application development more enjoyable for developers without compromising the capabilities of the applications themselves.\nFollow the below steps to install Laravel Application on Plesk server‚Ä¶..","install-laravel-application#Install Laravel application":"open Plesk and navigate to the ‚ÄòAdd Domain‚Äô button. From the drop-down menu that appears, select ‚ÄòLaravel site‚Äô to begin the process of establishing a new domain.\nAdd new domain\nDon‚Äôt be concerned if you don‚Äôt yet have a domain name for your website. For the time being, you can use a temporary domain name. Click on temporary domain name and you will get domain name as shown below Click add domain and you will see below screenshot After the website has been created, Plesk will prompt you to select between installing a default skeleton Laravel application or pulling the application from a remote Git repository. Your selection will be saved automatically. Installing the skeleton is the best way to get acquainted with the Laravel Toolkit‚Äôs capabilities for the time being. Wait for few seconds. And it is ready‚Ä¶.. Your very first Laravel application has been published to the world wide web! ","laravel-application-configuration#Laravel Application Configuration":" If you were to hand a mouse a Laravel application, it would probably ask for a database‚Ä¶ at least that‚Äôs what we assume would happen. In any case, let‚Äôs keep going and submit another application for your consideration. 1.Navigate to the Laravel dashboard labelled ‚ÄúInformation,‚Äù then click the ‚ÄúManage domain‚Äù button. Following this link will bring you to the ‚ÄúHosting‚Äù dashboard.\n2.After arriving at that location, select ‚ÄúDatabases,‚Äù and then after that, ‚ÄúAdd Database.‚Äù\nWhen you are finished creating the database, you will need to modify the configuration file for the Laravel application so that it contains information about how to connect to the newly created database. 1.Click the button labelled ‚ÄúManage Laravel Application‚Äù located on the ‚ÄúHosting‚Äù dashboard. Clicking on this link will take you to the Laravel card.\n2.After arriving at that location, open the.env file and add the database credentials.\nExecute the ‚Äòartisan migrate‚Äô command in order to create a database as the final step.\nAnd yes, the ‚Äòartisan‚Äô utility can be executed directly from within the Plesk user interface. You will need to select the‚Äômigrate‚Äô command from the list of preloaded commands after you have navigated to the ‚ÄòArtisan‚Äô tab. ","manage-laravel-application#Manage Laravel Application":" However, there is more to come! In particular, let‚Äôs take a look at some additional features that, when hosting Laravel applications, save you time.\nIt is important to point out that you can rapidly invoke the ‚Äòcomposer‚Äô and ‚Äònpm‚Äô commands by simply selecting them from the list of preloaded * commands, which I discovered to be an indispensable resource:\nIf your Laravel application makes use of Laravel Task Scheduling, then in addition to the typical ‚Äòartisan schedule:list‚Äô command, you have the ability to quickly review all scheduled jobs by switching to a different tab.\nLaravel is an open-source web application framework that simplifies routine tasks that are common in most web development projects. Authentication and routing, as well as sessions and caching, are examples of these tasks. Laravel‚Äôs goal is to make application development more enjoyable for developers while not compromising the capabilities of the applications themselves.\nHope you have understood all the things to install Laravel Application on Plesk server..\nMust read:- https://utho.com/docs/tutorial/how-to-check-disk-performance-iops-and-latency-in-linux/\nThankyou"},"title":"How to install Laravel Application on Plesk server"},"/utho-docs/docs/linux/learning-the-linux-alias-command-and-how-to-use-it/":{"data":{"":"\nDescription\nUsers of Linux often need to use the same command over and over again. Your productivity will suffer, and you will get distracted from the task at hand if you continue to type or duplicate the same instruction many times.\nBy generating aliases for the commands you use the most, you can save yourself a little bit of time. Aliases are similar to user-defined shortcuts in that they are used to represent a command (or group of commands) that may be carried out with or without the usage of user-defined parameters. On your Linux system, it is likely that you are already making use of aliases.\n*By running the alias command, you can see a list of the aliases you‚Äôve set up on your profile.\n#alias Here you can see the aliases that Centos 7.3 has already set up for your user.","creating-permanent-aliases#Creating Permanent Aliases":"You may save aliases in your user‚Äôs shell configuration profile file to maintain them across sessions. It might be:\nBash ‚Äì¬†~/.bashrc ZSH ‚Äì¬†~/.zshrc Fish ‚Äì¬†~/.config/fish/config.fish You should use a syntax that is almost the same as making a temporary alias. The only thing that will be different this time is that you will save it in a file. So, for example, you can use your favourite editor to open a.bashrc file in bash:\n#vi .bashrc Determine where in the file you wish to store the aliases that you have created. One possible location for their addition is at the very end of the file. You are able to include a note before your aliases for organisational reasons, and it should look something like this:\nPlease save the file. The file will be loaded immediately into your next session automatically. You will need to execute the following command in order to utilise the newly created alias in this session:\n#source .bashrc To delete an alias that was added through the command line, the unalias command may be used to remove the alias.\n#unalias alias_name If you want remove all aliases.\n#unalias -a This is a quick example of how to establish an alias for commonly used commands. Now you may build shell shortcuts for your most-used commands.\nThank You","creating-temporary-aliases#Creating Temporary Aliases":"In order to alias a command, you must first write the term ‚Äúalias,‚Äù then use the name you want to use to execute the command, followed by the equals sign (=), and then quote the command you want to alias.\nFollow the below syntex\nalias shortName=‚Äúyour custom command here‚Äù\nNote:If you start a new terminal session, you won‚Äôt be able to use the alias. You will need a permanent alias if you want your aliases to be saved between sessions.","how-to-make-an-alias#How to Make an Alias":"*Aliases are easy to set up and don‚Äôt take long. There are two kinds of aliases: ones that are temporary and ones that are permanent. We will talk about both."},"title":"Learning the Linux Alias Command and How to Use It"},"/utho-docs/docs/linux/linux-how-to-execute-a-command-with-a-time-limit-or-timeout/":{"data":{"":"","#":"Description Linux comes with a plethora of commands, each of which serves a specific purpose and has its own unique set of parameters. Linux is designed to make your computing experience as brisk and productive as it possibly can be. The maximum allowed amount of time is one of the characteristics of a Linux command. Any command that you run can have a timer attached to it if you so choose. If the specified amount of time elapses, the command will no longer be carried out.\nIn this brief tutorial, you are going to learn two different ways that you can implement a time limit into the commands that you write.\nTimeout Tool for Executing Linux Commands You are able to execute a command in Linux with a time limit because the operating system comes with a utility for the command line called a timeout.\nThe following is an example of its syntax:\n#timeout [OPTION] DURATION COMMAND [ARG]‚Ä¶ You must include a timeout value, measured in seconds, along with the command that you want to execute in order to use the command. You could, for instance, run the following command to timeout a ping command after 5 seconds.\n#timeout 5s ping google.com After the number 5, the s is optional. The following command remains the same and continues to function.\n#timeout 5 ping google.com Some other endings are:\nm representing minutes h representing hours d representing days Even after the timeout has sent the initial signal, the commands may continue to run in some circumstances. The ‚Äîkill-after option is available for use in situations like these.\nHere is the syntax for it.\n#-k, --kill-after=DURATION You are required to provide timeout with a duration so that it is aware after how much elapsed time the kill signal is to be transmitted.\nFor instance, the command that is being displayed will be aborted after a period of 8 seconds.\n#timeout 8s tail -f /var/log/messages The Timeout command is simple to use, while the Timelimit utility is more difficult to understand but provides more customization options. Your requirements will determine which of the available options is the best fit for you to select. Thank You "},"title":"Linux: How to Execute a Command with a Time Limit or Timeout"},"/utho-docs/docs/linux/linux-port-test-commandsredhat-7-centos-7-and-ubuntu-18-04/":{"data":{"":"Description\nKnow Linux port test commands (RedHat 7, CentOS 7, and Ubuntu 18.04).\nIn this article we will learn Linux port test commands RedHat 7, CentOS 7, and Ubuntu 18.04.\ncurl: is usually used from a Linux or Unix command line to download web pages or files. But the curl command has another great use: it can be used to test TCP port connectivity. As an example, let‚Äôs say you‚Äôre helping with some network changes and need to make sure that the connection from your server to a remote host and a specific TCP port still works.\ntelnet: The telnet command is used to talk to another host using the TELNET protocol in an interactive way. It starts in command mode, where it shows a ‚Äútelnet\u003e‚Äù prompt for entering commands. If telnet is called with a host argument, it automatically runs the open command.\nnc: The nc command is used to maintain networks and figure out what‚Äôs wrong with them. It can do things like read, write, and reroute data over the network, just like the cat command can be used to change files on a Linux system.","check-the-connection-to-the-ssh-port-using-curl#Check the connection to the SSH port using curl.":"using the curl command, which is detailed below.\ncurl -v telnet://youripaddress ","use-nc-to-test-the-ssh-port-connection#Use nc to test the SSH port connection.":" nc -v -z -u youripaddress Must Read : 2 Methods for Re-Running Last Executed Commands in Linux\nThankyou","use-telnet-to-test-the-connection-to-the-ssh-port#Use telnet to test the connection to the SSH port.":"You can test¬†using the telnet command, which is detailed below.\ntelnet youripaddress ","what-is-port#What is Port":"Ports are the communication endpoints of the network topology through which both inbound and outbound traffic passes.\nPrerequisites\nIn Linux, you absolutely need to have the netstat, curl, nc, and telnet commands.","what-is-tcp#What is TCP¬†":"Transmission Control Protocol (TCP) is what TCP stands for. Using this method, the system that is sending the data connects directly to the computer that is receiving it and stays connected while the data is being sent.\nUsing this method, the two systems can ensure that the data has been received safely and correctly without jeopardising packet integrity, and then they disconnect the connection.\nThis method of data transfer is faster and more reliable, but it places a greater load on the system because it must monitor the connection and the data flowing across it.\nHere‚Äôs how to do it with the curl command and its telnet functionality.","what-is-udp#What is UDP¬†":"User Datagram Protocol (UDP) is the name for UDP. With this method, the system sending the data puts the information into small packets and sends them out into the network, hoping that they will get to the right place.\nThis means that UDP doesn‚Äôt connect directly to the receiving computer like TCP does. Instead, it sends the data out and relies on the network devices between the sending system and the receiving system to get the data where it needs to go.\nWith this method, there is no guarantee that the data you send will ever get to where it needs to go. On the other hand, this way of sending information has very low costs, so it is often used for services that don‚Äôt have to work right away."},"title":"Linux port test commands(RedHat 7, CentOS 7, and Ubuntu 18.04)"},"/utho-docs/docs/linux/linux-top-command/":{"data":{"":"","arguments-for-the-top-command#Arguments for the Top command":"While you only invoke top when entering the name in a shell session, a few switches alter the actions of the utility\n-h: Display current version -c: This switches to the command column between the command and program name -d: Specify the delay time between screen refreshments -o: Sorts by the named field -p: Only show processes with specified process IDs -u: Show only processes by the specified user -i: Do not show idle tasks ","change-the-display-whilst-running-top#Change the Display Whilst Running Top":"When¬†the¬†command¬†is¬†in¬†the¬†foreground,¬†switch¬†on¬†and¬†off¬†many¬†of¬†the¬†features¬†by¬†pressing¬†relevant¬†keys.\nA Alternative display (default off) d Refresh screen after the specified delay in seconds (default 1.5 seconds) H Threads mode (default off), summarises tasks p PID Monitoring (default off), show all processes B Bold enable (default on), values are shown in bold text l Display load average (default on) t Determines how tasks are displayed (default 1+1) m Determines how memory usage is displayed (default 2 lines) 1 Single cpu (default off) - i.e. shows for multiple CPUs J Align numbers to the right (default on) j Align text to the right (default off) R Reverse sort (default on) - Highest processes to lowest processes S Cumulative time (default off) u User filter (default off) show euid only U User filter (default off) show any uid V Forest view (default on) show as branches x Column highlight (default off) z Color or mono (default on) show colors Thank You :)","examples-of-top-command#Examples of Top command":"Let‚Äôs¬†explore¬†a¬†number¬†of¬†ways¬†to¬†use¬†top¬†with¬†different¬†arguments.\nSPECIFY¬†A¬†TIME¬†DELAY¬†BETWEEN SCREEN¬†REFRESHES\nTo specify a delay time of 5 seconds between the screen refreshes use the following command:\n[root@Microhost ~]# top -d 5 Only show the processes for an individual user\nTo show only the processes of suraj user use the following syntax:\n[root@Microhost ~]# top -u suraj HIDE IDLE TASKS\nThe default top view may seem confusing,However you can then run the top command with the following function if you only want to see active processes (i.e. those that are not idle).\n[root@Microhost ~]# top -i ADDITION TO DISPLAY EXTRA COLUMNS\nYou can press the ‚ÄòF‚Äô key when running in the top, which displays the list of fields in the table: move up and down the field list via arrow keys.\nTo specify a field, press the ‚ÄòD‚Äô key on the screen. Click ‚ÄúD‚Äù again to delete the field. An asterisk (*) will appear next to displayed fields.\nThe field to sort the table can be set by simply pressing ‚ÄúS‚Äù on the field you are interested in sorting.\nTo commit your changes, press the enter key and click on ‚ÄúQ.‚Äù","first-row#First Row":" top - 16:42:25 up 13 days, 14:41, 1 user, load average: 0.09, 0.08, 0.05 top : Command Name\n16:42:25 : Current system time\nup 13 days , 14:41 : System up time\n1 user : Total logged users\nload average : 0.09, 0.08, 0.05 : Cpu load average in 1 min, 5 min, 15 min respectively.","fourth-row#Fourth Row":" KiB Mem : 2914368 total, 1965052 free, 81880 used, 867436 buff/cache KiB Mem : Shows the memory utilization on server\n2914368 total : Total system memory\n1965052 free : Current used memory by System\n81880 used : Free memory\n867436 buff/cache : Total memory used by Buffers/Cache","main-row#Main Row":" Process ID User Priority Nice level Virtual memory used by process Resident memory used by a process Shareable memory CPU used by process as a percentage Memory used by process as a percentage Time process has been running The Keyword list for Top command","second-row#Second Row":" Tasks: 65 total, 1 running, 64 sleeping, 0 stopped, 0 zombie Tasks : 65 total : Total process in active mode\n1 running : Current running process\n64 sleeping : Total process in sleep mode\n0 stopped : Total process in stopped mode\n0 zombie : Total process in zombie state","third-row#Third Row":" %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni, 87.5 id, 12.5 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu(s) : Shows cpu utilization status on server\n0.0 us : Cpu used by user process\n0.0 sy : Cpu used by system process\n0.0 ni : Cpu process used by setting nice value\n87.5 id : Cpu in idle state\n12.5 wa : Cpu waiting for I/O\n0.0 hi : Cpu used by hardware interrupts\n0.0 si : Cpu used by software interrupts\n0.0 st : Steal time\n[ht_message mstyle=‚Äúinfo‚Äù title=‚ÄúSteal time‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=\"\" style=\"\" ]Steal time is the time that a virtual CPU waits for a real CPU while the hypervisor is servicing another virtual processor.[/ht_message]","toggling-modes#Toggling Modes":"Press the key A to switch from the standard display to an alternative displaywhile running top.","top-command#Top Command":"Linux processes are displayed by top command. It gives the running system a dynamic real-time view. In general, the system summary information and the list of processes and threads currently managed by the Linux kernel are displayed by this command.\nOnce the command is executed, an interactive command mode will open, where the top half of the command will include process statistics and resource use. And in the lower half, there is a list of the processes underway. Press q just leaves the command mode.\nRow 1: top - 16:42:25 up 13 days, 14:41, 1 user, load average: 0.09, 0.08, 0.05 Row 2 : Tasks: 65 total, 1 running, 64 sleeping, 0 stopped, 0 zombie Row 3: %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni, 87.5 id, 12.5 wa, 0.0 hi, 0.0 si, 0.0 st Row 4: KiB Mem : 2914368 total, 1965052 free, 81880 used, 867436 buff/cache Row 5: PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9 root 20 0 0 0 0 S 0.3 0.0 21:55.05 rcu_sched "},"title":"Linux Top Command"},"/utho-docs/docs/linux/logical-volume-manager-in-linux-lvm-guide-for-beginners/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned Logical volume manager in linux (LVM) Guide for beginners.\nAlso Read:¬†How to Use Iperf to Test Network Performance\nThank You üôÇ","introduction#Introduction":"In this article you will learn Logical volume manager in linux (LVM) Guide for beginners.\nLinux‚Äôs block storage is managed by Logical Volume Manager (LVM). When compared to normal partitions, LVM manages storage in a different and more efficient way.\nLVM takes room from partitions or whole discs (called ‚ÄúPhysical Volumes‚Äù) and puts it together into a logical container (called a ‚ÄúVolume Group‚Äù). Logical Volumes are logical parts that make up the rest of the Volume Group.\nTo put it simply, LVM puts all of your storage space into a pool and lets you use that pool to make volumes (Logical Volumes).\nLVM is better than standard partitions because it gives you more options and freedom. It lets Logical Groups and Logical Volumes be changed in size online. So, if one of your logical sections runs out of room, you can easily make it bigger by using space from the storage pool.\nLayers of LVM LVM adds a layer of abstraction between the physical storage and the file system. This makes it possible to change the size of the file system, spread it over multiple physical discs, and use any amount of disc space.\nLVM makes divisions with three layers of abstraction.\nPhysical Volume, Volume Group, and Logical Volume.\nPHYSICAL VOLUME\nEverything starts with a disk. The physical volume is the first layer of abstraction that LVM uses to find the disk that has been marked for LVM actions. To put it simply, your disk should be set up as the physical drive if you want to work with LVM. It can be a whole hard drive or normal partitions made from that hard drive.\nVOLUME GROUP\nAll the actual volume is added up to make the volume group. Let‚Äôs say you have five disks that are each 1 TB in size. First, you will set up the five disks as real volumes, and then you will add them to the volume group.\nVolume group will hold 5TB of space, which is how much room is available from all the physical volumes. You can set up logical splits from the volume group.\nLOGICAL VOLUMES\nYou can make reasonable volumes out of the pool of space (Volume Group). Think of this as being the same as a typical disk divided.\nStep 1: Set the physical volume to zero I added three different-sized disks, and the total is 10G.\n# lsblk /dev/vd\\[b-e\\] Use the pvcreate command with the name of the device as the argument to set up a disk as a physical volume.\n# pvcreate /dev/vdb /dev/vdc /dev/vdd You can run any of the following tools to see a list of physical volumes. Different things will come out of each order.\nPVDISPLAY: The pvdisplay tool will tell you all about each physical volume, including which volume group it belongs to, its unique ID, and how much space it has.\n# pvdisplay PVS and PVSCAN are two tools that will tell you things like the size of the physical volume, the size of the volume group, and the size of the free space.\n# pvs # pvscan Step 2: Building combined a volume group Now I have three disks set up as a single physical drive that is 10GB in size. You should add these physical disks to a storage pool called a volume group.\nTo make a volume group, use the vgcreate command. You have to give the volume group a name. ‚Äúutho_files‚Äù is the name of the volume group in this case.\n# vgcreate utho\\_files /dev/vd\\[b-d\\] Use any of the following commands¬†to find out more about a volume group.\n# vgdisplay # vgdisplay utho\\_files Step 3: Show Volume Group Information The vgs and vgscan commands will tell you about all of the available volume groups, the amount of physical volumes and logical volumes, as well as the size of the allocated and free space in the volume group.\n# vgs # vgscan Step 4: Create Logical¬†volumes I already said that logical volumes are like disk sections. Now, the ‚Äúutho_files‚Äù pool (volume group) has almost 10GB of free room. On top of this volume group, we‚Äôll build logical volumes, format the volume with the ext4 file system, mount and use the volume.\nYou can use the lvcreate command to make the logical disk. Here is a general description of how the lvcreate tool works.\n# lvcreate -L \u003cvolume-size\u003e -n \u003clogical-volume-name\u003e \u003cvolume-group\u003e Here,\n-L \u003cvolume-size\u003e¬†=\u003e size in KB, MB, GB\n-n \u003clogical-volume-name\u003e¬†=\u003e Name for your volume\n\u003cvolume-group\u003e¬†=\u003e Which volume group to be used\nNow I am creating a logical volume with¬†3GB¬†size. I named the logical volume¬†‚Äúguide‚Äù.\n# lvcreate -L 3GB -n guide utho\\_files Step 5: Display information of logical volumes You can use any of the following commands to see details about logical volumes.\nThe lvdisplay command gives you specific information about the logical volume, the associated volume group, the volume size, the logical volume path, and so on.\n# lvdisplay # lvdisplay guide The lvscan and lvs commands will also give you some basic details about the logical volumes.\n# lvscan # lvs Step 6 - Format and Mount the Logical Volumes You need to set up a file system on the logical disk and mount it. Here, I‚Äôm using the ext4 file system to format the disk and mounting it under the /mnt/ directory.\nUnder /dev/volume-group/logical-volume, you should see the device file for logical volume. My device file will be called /dev/utho_files/guide.\n# mkfs.ext4 /dev/utho\\_files/guide Run the following command to mount the path. You can put the directory wherever you want to.\n# mount /dev/utho\\_files/guide /mnt/ To see the volumes that are mounted, run:\n# mount | grep -i guide You can also type df to get information about the mounted file system. You can see that the name of the file system is ‚Äúvolume name.‚Äù With these names, it will be easy for you to figure out what the base volume is and what its group is.\n# df -h /mnt/ You need to add an entry to the fstab in order to make the mount permanent even after you reboot the system."},"title":"Logical volume manager in linux (LVM) Guide for beginners"},"/utho-docs/docs/linux/mariadb-installation-on-centos-8/":{"data":{"":"\nDescription\nA wide variety of applications, including data warehousing, e-commerce, enterprise-level functionality, and logging programmes, make use of the MariaDB database. MariaDB will let you to fulfil all of your burden in an effective manner; it can function in any cloud database and can function at any scale, whether it be little or huge. What exactly is a database, then?","reset-the-mariadb-root-password#Reset the MariaDB Root Password":"Put a stop to the running instance of MariaDB server.\n#sudo systemctl stop mariadb Then run the following command to allow the database to start without loading the grant tables or connecting to the network.\n#sudo systemctl set-environment MYSQLD_OPTS=\"--skip-grant-tables --skip-networking\" Restart MariaDB:\n#sudo systemctl start mariadb This time, login to the MariaDB server with the root account without providing a password:\n#sudo mysql -u root Follow these steps to change root‚Äôs password. Change the password to a strong one:\n#FLUSH PRIVILEGES; #UPDATE mysql.user SET password = PASSWORD('password') WHERE user = 'root'; Make necessary adjustments to the authentication procedures for the root password:\n#UPDATE mysql.user SET authentication_string = '' WHERE user = 'root'; #UPDATE mysql.user SET plugin = '' WHERE user = 'root'; #exit; Restore the database‚Äôs environment settings to allow it to start with grant tables and networking:\n#sudo systemctl unset-environment MYSQLD_OPTS Then restart MariaDB:\n#sudo systemctl start mariadb Using your new root password, you should be able to access the database:\n#sudo mysql -u root -p Thank you","securing-the-installation#Securing the Installation":" #sudo mysql_secure_installation You will have the option of changing the MariaDB root password, removing anonymous user accounts, disabling root logins outside of localhost, and removing test databases. It is recommended that you select the appropriate options. More information about the script is available in the MariaDB Knowledge Base.\nRoot Login\n#sudo mysql -u root -p When prompted, enter the root password that you established prior to running the mysql secure installation script.\nAfter that, the MariaDB prompt and a welcome header will be displayed to you, as shown in the following example:\nMariaDB [(none)]\u003e Make a New Database and User in MariaDB\nRelog into the database if necessary. Enter a password if you set one above.\n#sudo mysql -u root -p Testdb is the database, testuser is the user, and password is the user‚Äôs password. You need a strong password.\n#CREATE DATABASE testdb; #CREATE user 'testuser'@localhost IDENTIFIED BY 'password'; #GRANT ALL ON testdb.* TO 'testuser' IDENTIFIED BY 'password'; This procedure can be sped up by creating the user and assigning database permissions at the same time:\n#CREATE DATABASE testdb; #GRANT ALL ON testdb.* TO 'testuser' IDENTIFIED BY 'password'; Then exit MariaDB as follows:\n#exit; ","setup-and-installation-of-mariadb#Setup and installation of MariaDB":"Use the package manager to set up MariaDB.\n#sudo yum install mariadb-server Enable MariaDB to start automatically during system startup, and then start the service:\n#sudo systemctl enable mariadb #sudo systemctl start mariadb MariaDB defaults to 127.0.0.1. Our MySQL remote access guide also applies to MariaDB.\nNote\nAlthough it is not recommended to give MariaDB unrestricted access on a public IP address, you can change the address it listens on by changing the bind-address parameter in /etc/my.cnf. Implement firewall rules that only permit connections from particular IP addresses if you choose to bind MariaDB to your public IP address."},"title":"MariaDB installation on CentOS 8"},"/utho-docs/docs/linux/methods-for-disabling-the-root-account-in-linux/":{"data":{"":"","change-the-root-users-shell#Change the root user\u0026rsquo;s shell.":"\nDescription\nDisabling the Root Account method will help you to Disabling the Root Account in Linux, The root account is the ultimate account on a Linux and other Unix-like operating systems. This account has complete read, write, and execute rights for all files and commands on a system, giving it access to all the system has to offer. It may be used to conduct any sort of action on a system; for example, it can be used to create/update/access/delete other users‚Äô accounts, install/remove/upgrade software packages, and so much more.\nBecause the root user has absolute privileges, all acts he/she takes are essential on a system. In this aspect, any faults committed by the root user might have significant repercussions for the system‚Äôs ability to function normally. In addition, this account may potentially be exploited by using it incorrectly or inappropriately either unintentionally, deliberately, or via faked ignorance of regulations.\nAs a result, it is recommended that you disable root access on your Linux server. As an alternative, you should create an administrative account that should be configured to gain root user privileges by using the sudo command. This account should then be used to carry out essential tasks on the server.\nIn this post, we will describe four methods to prevent root user account login in Linux.\n**NOTE:**Be careful to establish an administrative account using the useradd command that can utilise sudo to achieve root user rights before you limit access to the root account. Then, give this user account a strong password. Create User‚Äôs Home Directory is what the option -m does, and -c enables you to write a comment:\n# useradd -m -c \"Admin User\" microhost_com # passwd microhost_com Next, add this user to the right group of system administrators by using the usermod command with the -a and -G switches (wheel or sudo, depending on your Linux distribution):\n#usermod -aG wheel microhost_com Once a user with administrative capabilities has been established, switch to that account to prevent root access.\n# su microhost_com Change the root user‚Äôs shell. The most straightforward way to block root user login is to alter its shell from /bin/bash or /bin/bash (or any other shell that permits user login) to /sbin/nologin, in the /etc/passwd file, which you can open for editing using any of your favorite command line editors as shown.\n# sudo vim /etc/passwd Modify this line:\nroot:x:0:0:root:/root:/bin/bash to root:x:0:0:root:/root:/sbin/nologin Save and close the file.\nWhen a root user checks in from now on, he or she will get the notification ‚ÄúThis account is presently not available.‚Äù This is the default message, but you may customise it by editing the file /etc/nologin.txt.\nThis strategy works only with programmes that require a shell for user login; otherwise, sudo, ftp, and email clients can gain access to the root account.","disabl-ssh-root-login#Disabl SSH Root Login":"The commonest approach to getting to far off servers or VPSs is by means of SSH and to hinder root client login under it, you really want to alter the/and so /etc/ssh/sshd_config file.\n# sudo vim /etc/ssh/sshd_config Then uncomment (in the event that it is remarked) the order PermitRootLogin and set its worth to no as displayed in the screenshot.\nWhenever you are finished, save and close the document. Then restart the sshd administration to apply the new change in configurations.\n# sudo systemctl restart sshd As you may definitely be aware, this technique just influences openssh instruments set, projects, for example, ssh, scp, sftp will be obstructed from getting to the root account.","disable-root-login-via-console-device-tty#Disable root login via Console Device (TTY)":"The second way employs a PAM module called pam securetty, which grants root access only if the user is logging in on a ‚Äúsecure‚Äù TTY, as described by the listing in /etc/securetty.\nThe aforementioned file allows you to select which TTY devices the root user is authorised to login on; emptying this file forbids root login on any devices connected to the computer system.\nRun to create a blank file.\n# sudo mv /etc/securetty /etc/securetty.orig # sudo touch /etc/securetty # sudo chmod 600 /etc/securetty This technique has some restrictions in that it only impacts login, display managers (such as gdm, kdm, and xdm), and other network services that launch a TTY. The root account will be accessible to programmes such as su, sudo, ssh, and other relevant openssh tools.","restrict-root-acess-to-services-through-pam#Restrict root Acess to Services Through PAM":"Pluggable Authentication Modules (PAM in short) is a concentrated, pluggable, measured, and adaptable technique for authentication on Linux frameworks. PAM, through the/lib/security/pam_listfile.so module, bestows on extraordinary adaptability in limiting the honors of explicit records.\nThe above module can be used to reference a rundown of clients who are not allowed to sign in by means of some objective administrations, for example, login, ssh and any PAM mindful projects.\nFor this situation, we need to disable root client admittance to a framework, by confining admittance to login and sshd administrations. First open and edit the record for the objective help in the/etc/pam.d/directory as displayed.\n# sudo vim /etc/pam.d/login or\n# sudo vim /etc/pam.d/sshd Next, add the configuration underneath in both files.\nauth required pam_listfile.so onerr=succeed item=user sense=deny file=/etc/ssh/deniedusers At the point when you are finished, save and close each file. Then, at that point, make the plain file/etc/ssh/deniedusers which ought to contain one item for every line and not world intelligible.\nAdd the name root in it, then, at that point, save and close it.\n# sudo vim /etc/ssh/deniedusers Likewise set the required permissions on this file.\n# sudo chmod 600 /etc/ssh/deniedusers This strategy just influence projects and administrations that are PAM aware. You can obstruct root admittance to the system via ftp and email clients and more.\nFor more information, counsel the relevant man pages.\n# man pam_securetty # man sshd_config # man pam In this article, we have made sense of four different ways of disabling the root user login (or account) in Linux.\nMust Read : For Application Data Storage on Fedora 14, Use MongoDB\nThank You"},"title":"Methods for Disabling the Root Account in Linux"},"/utho-docs/docs/linux/migrate-your-google-cloud-platform-to-microhost-cloud/":{"data":{"":"","1-deploy-a-new-microhost-cloud-instance#1. Deploy a new Microhost Cloud instance":"\nThis tutorial will walk you through the suggested process for Migrate your Google Cloud Platform to Microhost Cloud.\nThe most typical method for moving websites from one hosting provider to another is to:\nCreate an instance of your services on Microhost Cloud, and then transfer over just the configuration and data that is relevant to those instances. As a consequence, this produces a Linux environment that has a 100% chance of starting up properly on the Microhost Cloud platform.\nEven while reinstalling your services might be a time-consuming process, any difficulties that arise when you are configuring your apps are often far simpler to debug than problems with low-level setup. When it comes to migration, this is the preferred technique.\nSteps to follow 1. Deploy a new Microhost Cloud instance When building a new Microhost Cloud, there are two factors to think about: the first is in which data centre the instnace should be located, and the second is the hardware resource plan the instance should operate under.\nLocation of Data Center\nWhen determining the bandwidth available between your location and the data centre, it is helpful to compare the speeds of download. These comparisons will indicate the latency between where you are located and the data centre; ideally, the latency should be as low as possible. Configuration of cloud instance\nChoose a plan that has a minimum amount of storage space that is sufficient for the data that is currently being stored on the GCP VM instance that you are using. Find out the Linux distribution the instance you currently have running on GCP is using, and install it on your new Microhost cloud. If your existing deployment utilises an earlier version of a Linux distribution, you should deploy the most recent version of that distribution that is available for your new cloud. This will guarantee that you have access to the most recent security upgrades and applications.\nPlease refer to¬†guide of how to create Microhost Cloud server¬†for further information on the deployment of your new Linux image.¬†","2--install-required-software-on-your-cloud-server#2- Install required software on your Cloud server":"Install the exact same software stack onto your new Microhost cloud server that is already running on your existing GCP instance. You may get a list of all installed packages by using the package manager that is associated with your GCP instance. For instance, if you are operating with Debian or Ubuntu, you may type in the following command:\n# apt list --installed And if you are operating with Redhat or CentOS machine:\n# yum list --installed You may refer to our Documents and Tutorials to learn how to set up your system‚Äôs software on your new cloud instance once you have determined which software you would want to move to your new Microhost Cloud server and after you have decided which software you would like to migrate.","3--identify-your-configuration-files-and-database-files#3- Identify your configuration files and database files":"Which of the software configuration options have to be kept (e.g. web server, virtual host information, database connection settings, and which files contain these settings, etc.).\nThe location on the disc where your data is stored (e.g. as files in a directory, in a database process, etc.).\nIt is quite probable that you will want a database dump in order to retrieve your data if it is stored in a database. When you do this, a file will be created on the disc that wraps the data from your database and can be moved over the network just like any other file:","4--transfer-you-data-from-gcp-to-your-microhost-cloud#4- Transfer you data from GCP to your Microhost Cloud":"You may use a network transfer programme such as rsync to move your data to the cloud instance you have created with Microhost.¬†Execute the following command from inside the GCP instance you are currently using. Replace instance-user with the Linux user that is logged into your Microhost Cloud instance, and replace cloud-ip with the IP address that is assigned to your Microhost Cloud instance.\n# rsync -avh /path/to/data_directory insatance-user@cloud-ip:/path/to/destination_directory Uploading files from the current host‚Äôs /path/to/data_directory to the new Microhost Cloud instance‚Äôs /path/to/destination_direcotory will occur when the command shown above is executed.\nIf you uploaded a database dump file to your new Instance, you must also restore the dump file in order for your database programme to utilise the data regularly.","5--test-your-new-environment#5- Test your new Environment":"When you have done configuring your programme and recovering your data, test the installation to ensure that it functions appropriately.\nUse this opportunity to load test your new service. If you notice that the hardware resource plan you initially selected is insufficient for completing these load tests, resize your plan and continue testing.\nWhen you‚Äôve completed testing, go to the next stage in the migration process: upgrading your DNS records.\nSo, in conclusion, we can say that the process for Migrate your Google Cloud Platform to Microhost Cloud is quite easy and simple","steps-to-follow#Steps to follow":""},"title":"Migrate your Google Cloud Platform to Microhost Cloud"},"/utho-docs/docs/linux/modify-file-permissions-with-chmod/":{"data":{"":"","#":"Chmod Helps you to alter Linux read and write permissions Unix-like systems, including those operating on Microhost platforms, have an extremely flexible mechanism for access control that enables system administrators to easily allow multiple users to have access to a single system without requiring all users to access all files in the file system. The simplest and fastest way to modify these file permissions is to use the chmod command.\nHow to Use chmod Chmod refers in this guide to recent chmod versions, like those provided by the GNU project. By default, chmod is included with all microhost images and included in the common ‚Äúbase‚Äù selection of packages available in almost all Linux-based operating system distributions.\nLinux File Permission Basics All Unix-like file system object is allowed to read, write, and execute access to three major types of permissions. The three possible classes are authorized: the user, the user group and all system users.\nUse to view a set of files‚Äô file permissions:\n[root@Microhost ~]# ls -lha There are ten characters representing the permission bits in the first column of the output. See the section on octal notation below to understand why they are called permission bits.\ndr-xr-x---. 3 root root 4.0K Apr 11 2018 . dr-xr-xr-x. 18 root root 4.0K May 11 03:35 .. -rw-------. 1 root root 1.3K Jan 18 2017 anaconda-ks.cfg -rw-------. 1 root root 597 May 9 12:29 .bash_history -rw-r--r--. 1 root root 18 Dec 28 2013 .bash_logout -rw-r--r--. 1 root root 176 Dec 28 2013 .bash_profile -rw-r--r--. 1 root root 176 Dec 28 2013 .bashrc -rw-r--r--. 1 root root 100 Dec 28 2013 .cshrc drwx------ 2 root root 4.0K May 9 03:34 .ssh -rw-r--r--. 1 root root 129 Dec 28 2013 .tcshrc The division of the bits into groups is one way of understanding the meaning of this column.\nThe first character is the file type. The remaining nine bits in three groups represent the user, group and worldwide permissions. Everyone means:\nr:¬†Read w:¬†Write x: eXecute [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]Note that access to files targeted by symbolic links is controlled by the permissions of the targeted file, not the permissions of the link object..[/ht_message]\nchmod Command Syntax and Options The¬†chmod¬†command¬†format¬†is¬†the¬†following:\n[root@Microhost ~]# chmod [who][+,-,=][permissions] filename Consider¬†the chmod command¬†below :\n[root@Microhost ~]# chmod g+w ~/microhost.txt This allows all user groups who have written permissions to the ~/microhost.txt file. Additional options for modifying targeted user permissions are:\nThe operator + allows, while the operator - removes permissions.you are also allowed to copy permissions:\n[root@Microhost ~]# chmod g=u ~/microhost.txt The parameter g=u means grant group permissions to be same as the user‚Äôs.\nThe following example can specify multiple allowances by separating with a comma:\n[root@Microhost ~]# chmod g+w,o-rw,a+x ~/microhost.txt In this way, the user groups add written permissions and remove reading and writing permissions from the ‚Äúother‚Äù system users. Finally, a+x gives all categories execute permissions. This value can also be set to + x. In the absence of a category, all permission categories shall be added or withdrawn.\nIn this notation, the owner of the file is referred to as the user (e.g. u+x).\n[root@Microhost ~]# chmod -R +w,g=rw,o-rw, ~/microhost.txt The -R option applies the amendment of permissions to the specified directory and all its contents recursively.\nHow to Use Octal Notation for File Permissions Octal notation is another way of setting permissions.\nThe following is an instance of a file authorization equivalent to chmod u=rwx, g=rx, o =\n[root@Microhost ~]# chmod 750 /microhost.txt The permissions for this file are¬†- rwx r-x ---.\nWithout considering the first bit, every bit with a-can be replaced with a 0 while r, w, or x is 1. The conversion is as follows:\nThe two other digits are separate from each other. Thus, 750 means the user could read , write or run, the group could not write, and other users could not read , write or run.\n744, the default type permit allows the user to read , write and execute licenses and group and ‚Äúworld‚Äù user read permissions.\nEither notation is equivalent and any form that expresses your permission requirements can be used more clearly.\nMaking a File Executable The following examples change the file permissions so that any user can execute the file ‚Äú~/microhost-project.py‚Äù:\nchmod +x ~/microhost-project.py\nThank You :)"},"title":"Modify File Permissions with chmod"},"/utho-docs/docs/linux/most-common-network-port-numbers-for-linux/":{"data":{"":"A port is a logical address that is typically assigned to a specific service or running application on a computer , and more specifically, TCP/IP and UDP networks. It is a connection endpoint that directs traffic to a specific operating system service. Ports are software-based and are typically associated with the host‚Äôs IP address.\nA port‚Äôs primary function is to ensure data transfer between a computer and an application. By default, specific services run on specific ports, such as web traffic on port 80 (443 for encrypted traffic), DNS on port 53, and SSH on port 22. Ports are typically linked to the IP addresses of the host systems that run the applications.\nPort numbers range from¬†0-65535¬†and are divided into three network ranges as shown:\nPorts that range from 1 to 1023 are known as system ports or well-known ports. These are ports that are reserved for running privileged services on a system. Ports numbers in the range of 1024 to 49151 are referred to as registered ports and are mostly used by vendors for their applications. They are available for registration at¬†IANA¬†which is an authority that oversees global IP address allocation. Ports numbers between 49151 and 65535 are referred to as dynamic ports. They cannot be registered with IANA and are mostly used for customized services. Commonly Use Network TCP Ports\nPort Description 20 FTP ( File Transfer Protocol ) port for data transfer between client and server. 21 FTP ( File Transfer Protocol ) port for establishing a connection between two hosts. It‚Äôs referred to as the command or control port. 22 SSH (Secure Shell) port. This is a secure remote login service where data is encrypted. 23 Telnet. This is a remote login service that is unencrypted. Data is sent in plain text and is hence considered insecure. It was deprecated in favor of SSH. 25 SMTP (Simple Mail Transfer Protocol). A protocol used by mail servers to send and receive mail. 53 DNS (Domain Name Service) Responsible for resolving a domain name to machine-readable IP addresses. 67 (UDP) Used by the DHCP server (Dynamic Host Configuration Protocol). 68 (UDP) Used by a DHCP client. 80 HTTP (Hyper Text Transfer Protocol) is used for unsecured web traffic. 443 HTTPS (Hyper Text Transfer Protocol Secure) is used for encrypted web traffic. 110 POP3 (Post Office Protocol). Protocol for unencrypted access to a mail server. 995 POP3S (Post Office Protocol Secure). Provides encryption for POP3 protocol. 123 (UDP) NTP (Network Time Protocol). 137 NetBIOS protocol used for File and Print Sharing. 143 IMAP (Internet Messaging Application Protocol) Manages electronic mail messages on the mail server. Does not provide encryption. 161/162 SNMP protocol is used for sending commands and messages. 993 IMAPS (Internet Messaging Application Protocol Secure) Secure protocol for IMAP and provides SSL/TLS encryption. 445 SMB (Server Message Block) Port. Used for file sharing. 465 SMTPS (Simple Mail Transfer Protocol Secure). Provides encryption for the SMTP Protocol. 631 Internet Printing Protocol. Thankyou"},"title":"Most Common Network Port Numbers for Linux"},"/utho-docs/docs/linux/multiple-user-account-creation-in-linux/":{"data":{"":"","#":"Description adduser and useradd are Unix/Linux user account creation programmes. These instructions add one user at a time. Multiple user accounts? You need newusers then.\nNewusers updates and creates new user accounts in bulk. It‚Äôs designed for big IT settings where a system administrator must update or create many user accounts in bulk. It reads stdin (by default) or a file to update or create user accounts.\nThis article explains how to create several Linux user accounts in batch mode using Newusers.\nTo create users in bulk, use the same format as the password file.\n#pw_name:pw_passwd:pw_uid:pw_gid:pw_gecos:pw_dir:pw_shell *pw_name: username\n*pw_passwd: user‚Äôs password\n*pw_uid: user‚Äôs ID\n*pw_gid: user‚Äôs group ID\n*pw_gecos: defines comments sections.\n*pw_dir: defines the home directory of the user.\n*pw_shell: defines user‚Äôs default shell.\n**Note:**Since the passwords in the input file are not encrypted, you need to protect it by giving it the right permissions. Only root should be able to read and write to it.\nFor instance, if you want to add user accounts named ravi and tecmint, you may create a file called users.txt and then follow the instructions given there.\n#sudo vim abc.txt Next, add the information of the user accounts to the file using the format that is shown here.\nSave the file and give it the rights it needs.\n#sudo chmod 0600 abc.txt Execute the newusers command using the input file in order to add all of the user accounts described above at once.\n#sudo newusers abc.txt After first making an attempt to create or update the accounts that have been specified, the newusers programme will then write these changes to the user or group databases. Changes are not committed to the databases in the event that any errors occur, with the exception of those that occur during the final writes to the databases. This is the straightforward operation of the newusers command.\nThe newusers program first attempts to create or update the specified accounts, and then writes these changes to the user or group databases. In the event of errors, except for final writes to the databases, no changes are made to the databases. This is just the way the newusers command works.\nIf the above command is successful, check the / etc / passwd and / etc / groups files to confirm that the user accounts have been added as shown.\n#cat /etc/passwd | grep -E \"gaurav|microhost\" *For more information, see the new user page.\n#info newuser Thank You "},"title":"Multiple User Account Creation in Linux"},"/utho-docs/docs/linux/mysql-1030-got-error-28-from-storage-engine/":{"data":{"":"","conclusion#conclusion":"Hopefully, you now understand how to resolve Mysql 1030 got error 28 from storage engine.\nThank You üôÇ","introduction#Introduction":"In this article, you have how to resolve Mysql 1030 got error 28 from storage engine.\nIf you have any experience hosting MySQL systems, you have most likely run into the following error at some point. The following are some suggestions for how to correct it.\nError 28 is a message that frequently appears when you use MySQL as your storage engine. This message indicates that the drive is full, which means that one or more of the partitions on your server have run out of space, and MySQL is unable to write to the disc as a result. In order for MySQL to run, you will need to free up space on your disc.\nIt‚Äôs possible that the query you ran caused MySQL to generate some temporary tables. In the default configuration, these tables will be created along with the other tables. This means that they will most likely be stored on your / partition, which only has 1.6 GB of free space remaining, despite the fact that these tables have the potential to grow much larger than that very quickly.\nSolution 1.\nInsufficient disc space is the cause of MySQL error ‚Äú28 from storage engine.‚Äù\nUse the following command to display available disc space.\n#df -h This is how the results must be.\nSolution 2.\nThe amount of available storage space must be increased, or unnecessary files must be deleted.\nSolution 3.\nRemove Items from the Temporary Directories\nSwitch to the /var/tmp folder.\n#cd /var/tmp Deleting everything in the current directory and its subfolders.\n#rm -rf * Or you can remove the files that are not useful to you, by the command:\n#rm -rf filename Solution 4.\nExplore the contents of /backup to discover if an unnecessary backup may be removed."},"title":"Mysql 1030 got error 28 from storage engine"},"/utho-docs/docs/linux/mysql-relational-databases-on-ubuntu-12-04/":{"data":{"":"","#":"","installing-mysql#*Installing MySQL":"","make-a-new-user-in-mysql-and-database#*Make a New User in MySQL and Database":"","mysql-server-secure-installation#*MySQL Server secure installation":"","password-reset-mysql-root-user#*Password-reset MySQL root user":"","produce-a-model-of-the-table#*Produce a Model of the Table.":"","root-login#*Root Login":"","tune-mysql#*Tune MySQL":"\n*Introduction MySQL is a relational database management system that is available for free online. You can say it like this: (RDBMS). The name comes from the combination of ‚ÄúMy,‚Äù which is the name of co-founder Michael Widenius‚Äôs daughter, and the acronym for Structured Query Language (SQL).\nWeb and server applications frequently use the well-liked database management system MySQL.\nInstalling, configuring, and managing MySQL will be covered in detail in this guide.\n*Installing MySQL #sudo apt-get install mysql-server During installation, you‚Äôll be prompted to set a MySQL root password. Keep your password safe.\n*MySQL Server secure installation #sudo mysql_secure_installation You can change MySQL‚Äôs root password, remove anonymous user accounts, disable root database login outside localhost, and delete test databases. Yes is recommended. The MySQL Reference Manual has more details.\n*Root Login #mysql -u root -p *After run this command you will face below interface\nmysql\u003e *Make a New User in MySQL and Database #create database microhostdb; #create user 'microhost'@localhost identified by 'password'; #grant all on microhostdb.* to 'microhost'; *Creating the user and assigning database access privileges at the same time can speed up the process significantly:\n#create database microhostdb; #grant all on microhostdb.* to 'microhost' identified by 'password'; #exit *Produce a Model of the Table. #mysql -u microhost -p #use microhostdb; #create table customers (customer_id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, first_name TEXT, last_name TEXT); #exit *Password-reset MySQL root user You can reset your root MySQL password if you forget it.\n*Stop the currently running MySQL server.\n#sudo service mysql stop *Use dpkg to re-run the setup process that MySQL goes through the first time it is installed. You‚Äôll be asked to set a root password once more.\n#sudo dpkg-reconfigure mysql-server-5.5 *You should now be able to log in once more by using the command mysql -u root -p.\n*Tune MySQL *From Ubuntu‚Äôs software repositories, install MySQL Tuner.\n#sudo apt-get install mysqltuner *To operate it:\n#mysqltuner The output will include two sections of interest: general recommendations and variables to tweak.\nThank You "},"title":"MySQL Relational Databases on Ubuntu 12.04"},"/utho-docs/docs/linux/nginx-installation-in-centos-7-2/":{"data":{"":"\nStep 1: Add Nginx repository\n# yum install epel-release Step 2:¬†Install Nginx using following command\n# yum install nginx Step 3: Start the service of Nginx\n# systemctl start nginx Step 4: Check status of Ngnix\n# systemctl status nginx Step 5: If you are running a firewall, run the following commands to allow HTTP and HTTPS traffic:\nCommand :\n# sudo firewall-cmd --permanent --zone=public --add-service=http¬†# sudo firewall-cmd --permanent --zone=public --add-service=https # sudo firewall-cmd --reload Step 6: Now check the webserver while accessing the server IP address in the browser.¬†URL: - http://server_domain_name_or_IP/\nThank you :)"},"title":"NGINX Installation in CentOS 7"},"/utho-docs/docs/linux/ntp-server-configuration-support-internal/":{"data":{"":"\nNTP Server Configuration\nTo Check The Date- Time Settings of Linux Machine\nThis Command is Used to list the Timezones\nThis Command is Used to set Required time zones\n‚Äúchrony‚Äù Package is Used To install NTP server\nNTP is a Network is time protocol which is Used to set the time of Linux machine according to\nNetwork time¬†After installing the chrony package\nAdd the server details in /etc/chrony.conf¬†file\nThis command is used to set ntp server start\nThank You!"},"title":"NTP Server Configuration"},"/utho-docs/docs/linux/python-3-installation-and-programming-environment-configuration-on-an-ubuntu-22-04/":{"data":{"":"","1-setup-python3#1. Setup Python3":"","2-setting-up-a-virtual-environment#2. Setting Up a Virtual Environment":"","3-creating-a-hello-world-program#3. Creating a ‚ÄúHello, World‚Äù Program":"\nIntroduction The Python programming language is an increasingly popular choice for both beginners and experienced developers. Flexible and versatile, Python has strengths in scripting, automation, data analysis, machine learning, and back-end development.\nPrerequisite On an Ubuntu 22.04 server, you will need a super user such as root or any non-root user with sudo privileges in order to follow this tutorial.\n1. Setup Python3 Python 3 comes pre-installed on Ubuntu 22.04 and other Debian Linux distributions. To ensure your versions are current, you should update your local package index\n# apt update Then upgrade the packages installed on your system to ensure you have the latest versions\n# apt upgrade -y Once the process is complete, check the version of Python 3 that is installed in the system by running the following\n# python3 -V output\nLet‚Äôs install pip, a tool for installing and managing programming packages we might want to use in our development projects, to manage Python software packages.\n# apt install -y python3-pip The following can be used to install Python packages:\n# pip3 install package_name Here, package name can be any Python package or library, like Django for web development or NumPy for scientific computing. So, if you want to install NumPy, you can use the command pip3 install numpy to do so.\nSimilarly, if you are venturing into python-based web scraping, you might find libraries like Scrapy or BeautifulSoup useful. However, modern-day web scraping has evolved to match with robust website anti-bot measures. Take ZenRows for instance, it‚Äôs a next-generation web scraping API that takes care of everything from rotating proxies to bypassing advanced anti-bot systems, easing the task for programmers. It‚Äôs a valuable tool to have in your python web scraping toolkit.\nYou still need to install a few more packages and development tools to make sure that your programming environment is set up well.\n# apt install -y build-essential libssl-dev libffi-dev python3-dev After setting up Python and installing pip and other tools, you can set up a virtual environment for your development projects.\n2. Setting Up a Virtual Environment Using virtual environments, you can give each of your Python projects its own space on your server. This way, each project can have its own set of dependencies that won‚Äôt affect any of your other projects.\nSetting up a programming environment gives you more control over Python projects and how different versions of packages are handled. This is very important when working with packages from other companies.\nAs many Python programming environments as you want can be set up. Each environment is basically a directory or folder on your server that has a few scripts in it to make it work as an environment.\nWhile there are a few ways to achieve a programming environment in Python, we‚Äôll be using the¬†venv¬†module here, which is part of the standard Python 3 library. Let‚Äôs install¬†venv¬†by running the following:\n# apt install -y python3-venv Once this is set up, you can start making environments. Let‚Äôs choose which directory we want to put our Python programming environments in, or use mkdir to make a new directory, as in:\n# mkdir environments Then go to the directory where you‚Äôll keep your programming environments:\n# cd environments Once you are in the directory where you want the environments to live, you can run the following command to create an environment:\n# python3 -m venv my_env Basically, pyvenv creates a new directory with a few files that we can look at with the ls command:\n# ls my_env These files work together to make sure that your projects are separated from the rest of your server, so that system files and project files don‚Äôt get mixed up. This is a good way to keep track of versions and make sure that each of your projects can use the packages it needs.\nTo use this environment, you need to turn it on. You can do this by typing the following command, which runs the activate script:\n# source my_env/bin/activate Your environment‚Äôs name, in this case my env, will now appear at the beginning of your command prompt. Your prefix may look a little different depending on which version of Debian Linux you are using, but the name of your environment in parentheses should be the first thing you see on your line:\n(my_env) root@ubuntu:~/environments$\nThis prefix tells us that the environment my env is currently running. This means that when we make programmes here, they will only use the settings and packages for this environment.\nAfter following these steps, your virtual environment is ready to use.\n3. Creating a ‚ÄúHello, World‚Äù Program Now that we‚Äôve set up our virtual environment, let‚Äôs make a simple ‚ÄúHello, World!‚Äù programme. This will let us try out our environment and give us a chance to learn more about Python, if we don‚Äôt already.\nTo do this, open up a command line text editor such as¬†vi and create a new file:\n# vi helloworld.py Once the text file opens in the terminal window, type out the programme:\nprint(\"Hello, World!\") Once you exit the editor and return to your shell, you can run the program:\n# python hello.py The output of the hello.py programme you made should look like this in your terminal:\nHello, World! Type the command deactivate to leave the environment and go back to your original directory.","introduction#Introduction":"","prerequisite#Prerequisite":""},"title":"Python 3 Installation and Programming Environment Configuration on an Ubuntu 22.04"},"/utho-docs/docs/linux/set-a-date-and-time-for-each-command-in-bash-history/":{"data":{"":"Description\nBy default, all commands executed on the command line by Bash are saved in the history buffer or recorded in a file called /.bash history. This means that a system administrator can view a list of commands executed by users on the system, or a user can check his/her command history using the history command.\nhistory The date and time when a command was executed are not displayed in the output of the history command above. This is the default setting on most, if not all, Linux distributions.\nThis article will describe how to specify time stamp information about when each command in the Bash history was executed to be displayed.\nBy setting the HISTTIMEFORMAT variable, the date and time associated with each history entry can be recorded to the history file, denoted with the history remark character.\nThere are two ways to do this: one is temporary, and the other is permanent.\nTo set HISTIMEFORMAT¬†variable temporarily, export it as below on the command line:\nexport HISTTIMEFORMAT='%F %T' The time stamp format in the export command above is:\n**%F**¬†‚Äì same, as¬†%Y-%m-%d¬†(year-month-date).\n%T¬†‚Äì same as¬†%H:%M:%S¬†(hour:minute:seconds).\nFor more information on how to use the date command, see the man page:\nman date Then, as follows, review your command history:\nhistory If you want to permanently configure this variable, open the file /.bashrc with your favourite editor:\nvi .bashrc And insert the following line (marked with a comment as your own configuration):\n#my config export HISTTIMEFORMAT='%F %T' After saving the file and exiting, run the command below to apply the modifications to the file:\nsource .bashrc That‚Äôs all! Please share any noteworthy history command tips and techniques, as well as your views on this article, in the comment area below.\nThankYou"},"title":"Set a date and time for each command in Bash History."},"/utho-docs/docs/linux/setup-software-raid-on-linux-server/":{"data":{"":" Setup Software RAID on Linux server\nIn this tutorial, we will learn how to setup software RAID on Linux server using MDADM utility.\nRedundant Array of Cheap Disks is what RAID stands for. RAID lets you use multiple physical hard drives as if they were just one virtual hard drive. RAID levels include RAID 0, RAID 1, RAID 5, RAID 10, etc.\nHere, we‚Äôll talk about RAID 1, also called ‚Äúdisk mirroring.‚Äù RAID 1 makes copies of data that are exactly the same. When you set up RAID 1, data will be written to both hard drives. Both hard drives have the same files on them.\nThe good thing about RAID 1 is that if one of your hard drives breaks, your computer or server will still work because you have a copy of the data on the other hard drive that is full and uncorrupted. You can take out the broken hard drive while the computer is working, replace it with a new one, and it will rebuild the mirror on its own.\nThe problem with RAID 1 is that it doesn‚Äôt give you any more disc room. If both of your hard drives are 1TB, the total amount of space you can use is 1TB, not 2TB.","prerequisites#Prerequisites:":" Super user or any normal user with SUDO privileges. Two additional of identical size and identical configuration, empty disks attached to your server. Upto date Linux server( CentOS or Ubuntu will be fine) ","steps-to-configure-software-raid#Steps to configure Software RAID:":"Step 1: Install the utility which will help us to configure the RAID: mdadm\nDebian/Ubuntu: # apt update \u0026\u0026 apt install mdadm CentOS/Redhat: # yum install mdadm Now, let‚Äôs examine the two fresh disks.\nlsblk Output of lsblk command\nmdadm --examine /dev/sdb /dev/sdc Details of the disk using mdadm\nIf you are facing any error like: mdadm: No md superblock detected on device. Then you need to make the label disk devices. You can do so using parted command.\nparted /dev/device1 mklabel msdos parted /dev/device2 mklabel msdos Replace the device1 and device2 with your storage devices.\nStep 2: Create RAID 1 logical drive using the mdadm\nmdadm --create /dev/md0 -l 1 --raid-devices=2 /dev/sdb /dev/sdc You can create any level of RAID using the above command. You just need to mention the raid level after -l option. Also do not forget to mention the disk devices according to RAID level.\noutput of the above command\nHere, the above command will create a raid array with name as md0 and it can be visible be as /dev/md0 in lsblk command\nlsblk lsblk command\nStep 3: Now confirm again with the below command\ncat /proc/mdstat content of mdstat\nStep 4: You can see that md0 is working and is set up as RAID 1. We can use the following tools to get more information about /dev/md0:\nDetails of RAID array device\nStep 5: Create the file system of the newly created device.\nmkfs.ext4 /dev/md0 Step 6: Mount this device on any directory.\nmount /dev/md0 /mnt And this is how you will setup software RAID on Linux server."},"title":"Setup Software RAID on Linux server"},"/utho-docs/docs/linux/speed-test-in-ubuntu-server/":{"data":{"":"\nSpeedtest.net is an online service that allows you to test your broadband connection by downloading a file from a speedtest.net server located near you. This utility allows you to use the command line to access the service.\nStep 1. Login into the server using root credentials on putty.\nStep 2. Install python pip packages in order to install SpeedTest-cli with the below command.\n# apt-get install python-pip Step 3. To install the SpeedTest client with pip, run the following command.\n# pip install speedtest-cli Step 4. At last , run speedtest-cli for upload and download speed test.\n# speedtest-cli Thank you!!"},"title":"Speed Test in Ubuntu server"},"/utho-docs/docs/linux/ssh-logins-with-banner-messages-issue-net-support-internal/":{"data":{"":"\nSSH Logins with Banner Messages (Issue.net)\nOne of the easiest ways to protect and secure SSH logins is by displaying warning messages to UNauthorized users or display welcome or informational messages to authorized users.\nThere are two ways to display messages one is using the issue.net file and second one is using the MOTD file.\nissue.net : Display a banner message before the password login prompt.\nmotd : Display a banner message after the user has logged in.\n# vi /etc/issue.net :: Now Paste the below command or any command you want to display on the server login screen then save the file and exit.\n######################################################### #Welcome to Microhost.Com Test Server¬†#¬†#¬†All connections are monitored and recorded¬†# #¬†Disconnect IMMEDIATELY if you are not an authorized user! # #¬†Thanks Microhost Support 01204840000 Ext:3 # ########################################################### # vi /etc/ssh/sshd_config Use above command to edit the ssh configuration file, now we have to find ‚ÄúBanner‚Äù comment on this file then untag it and give the following path display banner on ssh connection\nBanner /etc/issue.net\n:: Now we need to restart the ssh service in our system, use below command in centos 7 to execute the same.\n# systemctl restart sshd.service :: Now I need to check the server banner with new ssh connection.\nYeah! We got the notification message while login into the server.\nThank You‚Ä¶"},"title":"SSH Logins with Banner Messages (Issue.net)"},"/utho-docs/docs/linux/ssh-logins-with-banner-messages-motd-file-support-internal/":{"data":{"":"One of the easiest ways to protect and secure SSH logins is by displaying warning messages to unauthorized users or displaying welcome or informational messages to authorized users.\nThere are two ways to display messages one is using the issue.net file and second one is using the MOTD file.\nissue.net : Display a banner message before the password login prompt.\nmotd : Display a banner message after the user has logged in.\n# vi /etc/motd :: After executing the above command, a configuration file will open now you need to past what continent you want to display on the server login screen.\n############################################################### #¬†Welcome to Microhost.Com Test Server #¬†#¬†All connections are monitored and recorded¬†# #¬†Disconnect IMMEDIATELY if you are not an authorized user! # #¬†Thanks Microhost Support 01204840000 Ext:3 # ############################################################## :: Now check the server with a new session of SSH to verify that content is showing or not.\nThank you."},"title":"SSH Logins with Banner Messages (MOTD File)"},"/utho-docs/docs/linux/structure-of-apache-configuration/":{"data":{"":"In this article we will discuss Structure Of Apache Configuration, The \u003cVirtualHost\u003e¬†block allows administrators to change the web server behaviour, per host or per domain; all choices in the \u003cVirtualHost\u003e¬†block are available for the entire domain. We do not, however, have the option on a directory basis for defining options. Thankfully, Apache provides more configuration possibilities.\nThis article deals with many ways of configuring the web server per directory and per file level in very narrow terms. See our other Apache configuration guidelines or official Apache documentation for more information on different options","additional-information#Additional Information":" Directory blocks can‚Äôt be nested within each other. Blocks can be nested in the directory inside the blocks \u003cVirtualHost\u003e. The wildcard character can contain the path contained in a directory block. The asterisk matches any character sequence, and a question mark matches any character. This may be helpful if you need a DocumentRoot¬†option for all virtual hosts to control: Directory /srv/www/\\*/public_html ","directory-and-options#Directory and Options":"The \u003cDirectory\u003e¬†block refers to the filesystem directory and defines the behavior of Apache. This block is in angle brackets and begins with ‚ÄúDirectory‚Äù and the directory path within the file system. The directory and its sub directories as defined are the options set in a directory block. An example of a directory block is the following:\nOrder Allow,Deny Allow from all Deny 55.1 ","file-and-location-options#File and Location Options":"File Options Use the \u003cFiles\u003e¬†command if you need additional control over those files in a directory of your server. The web server behavior for a single file is governed by this directive. Every file with the given name is subject to the \u003cFiles\u003e¬†directives. For example, any file called roster.htm¬†in a file system is protected by the following example directive:\nOrder Allow,Deny Deny from all If the \u003cVirtualHost\u003e¬†block is present, all files called roster.htm¬†would have to be included in DocumentRoot or in DocumentRoot¬†directories. This may apply. When the\u003cFiles\u003e¬†directive is enclosed in the\u003cDirectory\u003e section, all roster.htm¬†files within or inside the directory sub-directories of the specified directory are subject to the options defined.\nLocation Options Although \u003cDirectory\u003e¬†and \u003cFiles\u003e¬†block the behavior of Apache for filesystem locations, the \u003cLocation\u003e¬†directive controls the behavior of Apache in relation to the specific route that the client asks for. If a user requests http://www.abc.com/webmail/inbox/,¬†the web server must look at/srv/www/abc.com/public_html/webmail/inbox/in the webmail/inbox/¬†directories of the DocumentRoot. One common use for this function is to allow a script to process requests made on a given path. For example, this block directs all path requests to a mod_python¬†script:\nSetHandler python-program PythonHandler modpython PythonPath \"['/srv/www/abc.com/application/inbox'] + sys.path\" Note, after the options in the \u003cDirectory\u003e blocks defined, the options defined in the \u003cLocation\u003e¬†directive are processed and can override any options set in these blocks.","match-directives-and-regular-expressions#Match‚Äù Directives and Regular Expressions":"Apache also allows server administrators some additional flexibility in how folders, files, and positions are defined, in addition to the simple guidelines mentioned above. These ‚ÄúMatch‚Äù blocks allow administrators to define one set of configuration options for a directory, file, and location class. Take an example here:\nOrder Allow,Deny Allow from all Deny 55.1 This block defines a set of options for any directory that matches the standard ^.+/images expression. In other words, any path which begins with a number of characters and ends with images will match these options, including the following paths: /srv/www/abc.com/public_html/images/, /srv/www/abc.com/public_html/objects/images, and /home/username/public/www/images.\nApache also allows an alternate syntax for defining paths within a standard directory block using regular expressions. Adding a tilde (e.g. ~)¬†between Directory¬†and the path causes the specified path to be read as a regular expression. Regular expressions are a basic pattern matching syntax, and normal and custom expression variants are provided by Apache, and Perl.\nThe following block is equivalent to the previous block while DirectoryMatch¬†is preferred:\nOrder Allow,Deny Allow from all Deny 55.1 Apache offers similar features to link a class of locations or files to a single collection of configuration directives using regular expressions. Consequently any of the following choices define appropriate configurations:\nOrder allow,deny Deny from all Order allow,deny Deny from all Order Deny,Allow Deny from all Allow 192. Order Deny,Allow Deny from all Allow 192.168 The directives above for \u003cFiles\u003e¬†and \u003cFilesMatch\u003e¬†are similar to those for \u003cLocation\u003e¬†and \u003cLocationMatch\u003e.","order-of-precedence#Order of Precedence":"With so many different configuration options locations, one option can be listed in one file or block only later to find it overridden by another. One of the key challenges of using the Apache HTTP server is how all the configuration choices that are disparately placed combine to generate different web server behaviors.\nThe following list offers a guide to the priority Apache uses when combining configuration options. Later operations can override options specified earlier.\n1. Blocks are first read in \u003cDirectory\u003e.\n2. .htaccess files are read simultaneously with \u003cDirectory\u003e¬†blocks, but can override the options defined in \u003cDirectory\u003e¬†blocks if AllowOverride option allows.\n3. Next read: \u003cDirectoryMatch\u003e¬†and \u003cDirectory ~\u003e.\n4. \u003cFiles\u003e¬†and \u003cFilesMatch\u003e are read after defining directory behaviors.\n5. Finally, one reads \u003cLocation\u003e and \u003cLocationMatch\u003e.\nIn general, the choices \u003cDirectory\u003e are sorted in sequence from the shortest to the longest. In other words, before setting options for/srv/www/abc.com/public_html/objects¬†, options for/srv/www/abc.com/public_html/objects/imageswill be handled regardless of the order they appear in the configuration file. All other instructions are interpreted in the order in the configuration file they appear in.\nThankyou‚Ä¶.","override-options-with-htaccess#Override Options with htaccess":"In addition to the above listed configuration methods, Apache reads configuration options by default for a directory from a file in that directory. This file is commonly known as.htaccess. Look for your httpd.conf¬†and related files for the following configuration options:\nAccessFileName .htaccess Order allow,deny Deny from all The first line tells Apache to view configuration options in publicly available directories in.htaccess¬†files. The second directive \u003cFiles ~ \"^\\.ht\"\u003e¬†tells Apache to refuse all requests for the use of any named file that starts with the characters.ht. It prohibits access to software choices for guests.\nYou can change the AccessFileName to specify another name where Apache can search for these configuration options. If you change this option, be sure to update the \u003cFiles\u003e¬†directive to prevent unintentional public access.\nAny option that can be specified in a block of \u003cDirectory\u003e¬†can be specified in a file.htaccess. .htaccess¬†files are particularly useful in cases where the website operator is a user who has access to edit files in the public directory of the website, but not to any of the Apache configuration files.\nFor every request,.htaccess¬†files are processed and refer to all requests made for files in the current directory. The directives also refer in the filesystem hierarchy to all folders ‚Äúbehind‚Äù the current directory.\nDespite the strength and versatility that.htaccess¬†files offer, there are drawbacks to using them:\nIf files with .htaccess¬†are allowed, Apache will need to check and process the directives in the .htaccess¬†file on any request. In addition, Apache will search for.htaccess¬†files in directories above the current file system directory. This can cause a minor degradation of performance if access files are allowed, and is true even if not used. Options set in files of .htaccess¬†can override options set in blocks of \u003cDirectory\u003e¬†which can cause confusion and unwanted actions. Any option set in a.htaccess¬†file can be placed in a block under \u003cDirectory\u003e. Allowing non-privileged users to change configurations of web servers poses a potential security risk, but the AllowOverride options may greatly reduce this risk. If you wish to disable.htaccess¬†files for a directory or directory tree, specify the option below in any directory block.\nAllowOverride None [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]For a directory that falls inside a directory where overrides have been disabled, you can define AllowOverride Everything.[/ht_message]"},"title":"Structure Of Apache Configuration"},"/utho-docs/docs/linux/update-dns-records-for-a-domain-using-plesk/":{"data":{"":"","#":"","1go-to-websites--domains-and-click-on-the-name-of-the-domain-you-want-to-manage-the-dns-settings-for#1.Go to Websites \u0026amp; Domains and click on the name of the domain you want to manage the DNS settings for.":"","2click-on-hostingdns-#2.Click on hosting\u0026amp;DNS .":"","3click-on-dns-settings#3.Click on DNS settings.":"","4after-clicking-add-record-you-will-be-brought-to-a-page-where-you-can-customise-the-dns-records-to-meet-your-needs#4.After clicking \u0026ldquo;add record,\u0026rdquo; you will be brought to a page where you can customise the DNS records to meet your needs.":"\nIntorduction The Domain name system, or DNS, is a naming framework that converts human-readable domain names into the numerical strings known as IP addresses that identify web sites. Resolving is the term for this type of translation. Plesk is the only option when adding a domain name (using Websites \u0026 Domains \u003e Add Domain) to manage resource resolution. It can fulfil three highly beneficial roles.\nDNS record DNS records, also known as zone files, are instructions that are stored in authoritative DNS servers. These instructions offer information about a domain, such as what IP address is connected with that domain and how to process requests for that domain. These entries are made up of a string of text files that are produced using DNS syntax. The DNS syntax is nothing more than a string of characters that functions as a set of instructions for the DNS server. Every DNS record also has something called a ‚ÄúTTL,‚Äù which stands for time-to-live and defines how frequently a DNS server will refresh that record. This value is included in all DNS records.\nA collection of DNS records can be compared to a listing for a company on Yelp.com. This directory will provide you with a lot of helpful information about a business, such as its location, hours of operation, and the services that it provides, among other things. There are a number of optional records that can be added to a domain that serve additional purposes. However, in order for a user to reach a website by using the domain name, it is necessary for the domain to include at least a few of the essential DNS records.\n1.Go to Websites \u0026 Domains and click on the name of the domain you want to manage the DNS settings for.\n2.Click on hosting\u0026DNS .\n3.Click on DNS settings.\n4.After clicking ‚Äúadd record,‚Äù you will be brought to a page where you can customise the DNS records to meet your needs.\n5.Fill in the blanks that need to be filled in, then click ok.\nYou can see that the record was successfully updated on the screenshot that was provided earlier.\n*Steps*\n1.Go to Websites \u0026 Domains and click on the name of the domain you want to manage the DNS settings for. 2.Click on hosting\u0026DNS . 3.Click on DNS settings. 4.After clicking ‚Äúadd record,‚Äù you will be brought to a page where you can customise the DNS records to meet your needs. 5.Fill in the blanks that need to be filled in, then click ok.\nYou can see that the record was successfully updated on the screenshot that was provided earlier.\nThank You ","dns-record#DNS record":""},"title":"Update DNS records For A Domain Using Plesk"},"/utho-docs/docs/linux/update-php-5-4-version-to-php-7-4/":{"data":{"":"","introduction-update-php-version-from-54-to-php-74#INTRODUCTION Update PHP version from 5.4 to PHP 7.4":"PHP is a general-purpose scripting language geared toward web development. It was originally created by Danish-Canadian programmer Rasmus Lerdorf in 1993 and released in 1995. The PHP reference implementation is now produced by The PHP Group. Update PHP version from 5.4 to PHP 7.4. Differences Between PHP5 and 7. PHP 5 uses the old version of the engine called Zend II, therefore its performance, in terms of speed is way below that of PHP 7. PHP 7 uses a brand new model of engine known as PHP-NG or Next generation. This engine considerably¬†enhances performance with optimized memory usage. PHP 7 enables programmers to declare the return type of the functions as per the expected return value. Thus, it makes the code robust and accurate.\nStep 1. Check the version of the php.\n# php - v¬†Current version is PHP 5.4¬†.\nStep 2. Install Remi Repository and EPEL Repository by using following commands . you must perform the following below given command.\n# wget [https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm](https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm) # wget [http://rpms.remirepo.net/enterprise/remi-release-7.rpm](http://rpms.remirepo.net/enterprise/remi-release-7.rpm) # rpm -Uvh remi-release-7.rpm epel-release-latest-7.noarch.rpm Step 3. After installing the repository, you must perform the following additional configurations.\n# yum install yum-utils # yum-config-manager --enable remi-php74 Step 4. For installation PHP 7.4. you must perform the following given command\n# yum install php php-opcache php-gd php-curl php-mysqlnd¬†Step 5. after the installation. You have to update the packages.\nStep 6. Check the version of PHP now. You must perform the following below given command.\n# php -v¬†PHP has been updated to 7.4.\nWe have learned in this article to update the PHP 5.4 to 7.4. PHP is a general-purpose scripting language geared toward web development. It was originally created by Danish-Canadian programmer.\nThank you!! 0000000000000000000000000000000000000000000000000000000000000000000000000000"},"title":"Update PHP 5.4 version to PHP 7.4"},"/utho-docs/docs/linux/upgrading-wordpress-manually-on-linux-sever/":{"data":{"":"\nStep 1. login to wordpress by accessing the server IP_address/wp-admin or domain_name/wp-admin in the browser.\nStep 2. check the current version of wordpress , i.e 5.0.13 here.\nStep 3. Download the most recent version of WordPress from the official WordPress website to your local machine.\nPlease visit https://wordpress.org/download/releases/ for more information.\nStep 4. extract the zip file and then connect the server using FileZilla.\nNOTE : hostname : serve_ip , username , password , port : 22. As shown in the below screenshot.\nStep 5. Navigate to your website‚Äôs root directory (for example, /var/www/html/wordpress) on the server side and delete the ‚Äòwp-admin‚Äô and ‚Äòwp-includes‚Äô directories.\nStep 6. Now, Copy the ‚Äòwp-includes‚Äô and ‚Äòwp-includes‚Äô directories from the unzipped new version WordPress to the website root directory(which replaces the directories deleted in step 5 at website root directory).\nStep 7 : Do not delete any files or directories in the website‚Äôs root directory after step 6. In this section just copy all of the files from the new version WordPress directory to the existing ‚Äòwordpress‚Äô website root directory (by this act it overwrite the existing files with same name).\nStep 8. Your wp-config.php file will be unaffected because the default config file for WordPress will be wp-config-sample.php. Examine wp-config-sample.php for any changes to the wp-config.php file. You can also edit database records \u0026 rename the wp-config-sample.php file as wp-config.php ,afterwards just delete the older file(wp-config.php).\nStep 9. Now, from your browser, navigate to the WordPress admin page at ‚Äòip address (or)domain name/wp-admin‚Äô and update the WordPress database.\nStep 10. Login into the wordpress and check the current version of wordpress .\nWordpress has been updated.\nThank you!!"},"title":"Upgrading WordPress Manually on Linux sever"},"/utho-docs/docs/linux/urls-redirect-with-apache-web-server/":{"data":{"":"You can learn how to redirect URLs with Apache in this section. Redirecting a URL allows you to return an HTTP status code that guides the client to a specific URL, making it useful for instances where a piece of content has been transferred. Redirect is a member of the mod alias module of Apache.","before-you-begin#Before You Begin":"1. This guide assumes you have followed our Guides to Getting Started and Securing Your Server and you have installed your Apache installation already. If you have not, please refer to our Apache or LAMP stack guides.\n2. You can change the configuration files for Apache in this guide so make sure you have the correct permissions to do so.\n3. Update your system.\nBased on the distribution of Linux, the Apache virtual host configuration files are located at various locations. CentOS 7, for example: /etc/httpd /conf.d/vhost.conf; Ubuntu 16.04: /etc/apache2/sites-available/abc.com.conf. For example. In order to be brief, the configuration file extracts from this guide will lead you to the configuration option for Apache.\nAfter making changes, remember to reload Apache configuration:\nCentOS 7\nsudo systemctl restart httpd Ubuntu 16.04\nsudo systemctl restart apache2 ","beyond-url-redirection#Beyond URL Redirection":"Apache also helps you to edit URLs using mod rewrite, in addition to redirecting users. Although the features are identical, the key difference being that rewriting a URL requires the server returning a request other than the one received by the client, while redirecting simply returns a status code, and the client then asks for the ‚Äúcorrect‚Äù answer.\nRewriting a request on a more realistic level does not affect the contents of the browser‚Äôs address bar, and can be useful in hiding URLs with sensitive or vulnerable data.\nEven though redirection helps you to quickly adjust the positions of different tools, in some cases you can find that rewriting suits your needs better. See Apache‚Äôs mod rewrite documentation for more detail.\nThankyou..","the-redirect-directive#The Redirect Directive":"You can find Redirect settings in your main Apache configuration file but we suggest that you keep them in your virtual host files or directory blocks. You can also use the Redirect statements in files with .htaccess. Here is an example of how Redirect could be used:\nRedirect /username http://team.abc.com/~username/ If no reason is provided, Redirect will submit a temporary (302) status code, and the client will be told that the /user name resource has temporarily moved to http:/team.abc.com/~user/.\nNo matter where they are located, Redirect statements must define the redirected resource‚Äôs complete file path, following the domain name. Such statements must provide even the full URL of the new location of the site.\nYou may also have a claim to return a HTTP status specific to:\nRedirect permanent /username http://team.abccom/~username/ Redirect temp /username http://team.abc.com/~username/ Redirect seeother /username http://team.abc.com/~username/ Redirect gone /username Permanent informs the customer the tool has permanently shifted. That returns a status code of 301 HTTP. Temp is the default action, which informs the client that the resource has temporarily relocated. This returns a 302 HTTP status code. Seeother informs the user that the requested tool was replaced with a different one. This returns a 303 HTTP status code. Gone informs the user that they have permanently removed the tool they are searching for. You don‚Äôt need to define a definite URL when using this statement. That returns a status code of 410 HTTP. The HTTP status codes can also be used as claims. Here is an example using the options with the status code:\nRedirect /username http://team.abc.com/~username/ Redirect /username http://team.abc.com/~username/ Redirect /username http://team.abc.com/~username/ Redirect /username RedirectPermanent and RedirectTemp can also execute permanent and temporary redirects, respectively:\nRedirectPermanent /username/bio.html http://team.abc.com/~username/bio/ RedirectTemp /username/bio.html http://team.abc.com/~username/bio/ Redirects can also be achieved using the regex patterns, using RedirectMatch:\nRedirectMatch (.\\*).jpg$ http://static.abc.com$1.jpg This match every request for a file with an extension of .jpg and replaces it with a position on a domain. The parentheses allow you to get a specific part of the request and insert it as a variable (specified by $1, $2, etc.) into the URL of the new site. For instance:\nA request is forwarded to http:/www.abc.com/avatar.jpg and to http:/static.abc.com/avatar.jpg Re-direct a submission for http:/www.abc.com/images/avatar.jpg to http:/static.abc.com/images/avatar.jpg. "},"title":"URLs Redirect with Apache Web Server"},"/utho-docs/docs/linux/uses-of-chage-command-in-linux/":{"data":{"":"","in-this-article-you-will-learn-how-to-use-chage-command-in-linux#\u003cstrong\u003eIn this article, you will learn how to use \u0026lsquo;chage\u0026rsquo; command in Linux.\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to use ‚Äòchage‚Äô command in Linux. Password expiration information can be viewed and updated with the chage command. This command is used when a user‚Äôs login is only going to be active for a short period of time, or if the user needs to have their password changed periodically. An account‚Äôs age, the time and date of its last password change, the time at which the account will be locked, and other similar details can all be viewed with the help of this command.\nThe help option can be used to see a complete list of the chage[change] command‚Äôs options.\n#chage -h If you want to see how old your accounts are (the -l option), you can do that. I‚Äôm using sudo to see the root‚Äôs age information. Enter :# sudo chage -l root\n#sudo chage -l root The second option, -d, allows you to change the last password to a date in the past that you specify in the command. By using the ‚Äúsudo‚Äù prefix, I am able to alter the root‚Äôs age data. In addition, I‚Äôm checking the modified date with the -l option. Copy and paste this command into the box: sudo change -d 2022-05-05 root\n#chage -d 2022-05-5 root #chage -l root By selecting the -E option, you can set the account‚Äôs expiration date. I‚Äôm using the command sudo to update the root user‚Äôs age. In addition, I‚Äôm checking the modified date with the -l option. Type: sudo change -E root\n#chage -E 2022-10-10 root #chage -l root The -M or -m option lets you set the maximum and minimum intervals between password changes. I‚Äôm using the command sudo to update the root user‚Äôs age. Additionally, I am checking out the new time frame with the -l option. Key in: sudo change -M 5 root\n#chage -M 5 root #chage -l root The -I switch allows you to set the inactivity period for your account after it has expired. If the user doesn‚Äôt log in after the password has expired, this command will prompt them to change it. If the password isn‚Äôt changed after this time, the account will be locked and the user will need to contact the administrator to have it unlocked. I‚Äôm using the command sudo to update the root user‚Äôs age. Additionally, I viewed the idle time by using the -l option. Copy and paste: sudo change -I 5 root\n#chage -I 5 root #chage -l root Password expiration notices can be issued in advance with the help of the -W option. The command takes a number representing the number of days before the expiration date as its input. I‚Äôm using the command sudo to update the root user‚Äôs age. To see the caution period, I am using the -l option. Just type: sudo change -W 2 root\n#chage -W 2 root #chage -l root ```![](images/chage-w.png) ## Conclusion Hopefully, you have learned how to use 'chage' command in Linux. Also read: [How to prevent a user from login in Linux](https://utho.com/docs/tutorial/how-to-prevent-a-user-from-login-in-linux/) Thank You üôÇ ","introduction#Introduction":""},"title":"How to use 'chage' command in Linux"},"/utho-docs/docs/linux/using-mysqldump-to-backup-mysql-databases/":{"data":{"":"","#":"\nDescription The mysqldump utility is part of MySQL and MariaDB. It makes it easier to make a backup of a database or system of databases. Using mysqldump makes a logical backup and generates the SQL statements needed to recreate the original database structure and data.\nNote\nThe database management system needs to be functioning and available because the mysqldump programme requires a connection to the database. You can instead make a physical backup, which is a copy of the file system directory holding your MySQL database, if the database is inaccessible for any reason.\nLog onto the system where you‚Äôll store backups. This computer needs MySQL CLI (which should come with the mysqldump utility). Verify mysqldump‚Äôs installation using this command:\n#mysqldump --version You‚Äôll need to know which version you‚Äôre using when referring to the documentation, so this should let you know. Consult the Installing MySQL guide if mysqldump and mysql are not already installed.\nSyntax for mysqldump in general\nThe following list shows numerous mysqldump commands. [options] contains all the backup command options you need. Common Command Options lists options.\nFollow the below steps to Backup MySQL Databases using mysqldump.","backup-restore#Backup Restore":" #mysql -u [username] -p [databaseName] \u003c [filename].sql Restore a DBMS backup‚Äôs entirety. You will be requested for the password for the MySQL root user:\nThis will overwrite all existing MySQL database system data.\n#mysql -u root -p \u003c full-backup.sql one-time database dump restoration The target database for the data import must already be empty or old, and the MySQL user you‚Äôre running the operation as must have write access to it:\n#mysql -u [username] -p db1 \u003c db1-backup.sql In order to restore a single table, you need to ensure that the destination database is prepared to accept the data:\n#mysql -u dbadmin -p db1 \u003c db1-table1.sql hope you have understood the all things to Backup MySQL Databases using mysqldump‚Ä¶.\nMust read:- https://utho.com/docs/tutorial/how-to-install-a-php-version-in-whm/\nThank you ","cron-based-backup-automation#Cron-based backup automation":"1.You can use the mysqldump command inside of a cron job to set up regular backups of your database.\n2.Using the mysql config editor set command, you can store your database credentials and connection information. Below is an example command, but make sure to change the values to your own. See How to Safely Store Credentials for more information and options.\n#mysql_config_editor set --login-path=[name] --user=[username] --host=[host] --password --warn 3.Make the folder where you want your backup files to go. This can be anywhere that your user can get to.\n#mkdir ~/database-backups 4.Edit the crontab file for your user.\n#crontab -e 5.Incorporate the cron job into the crontab file. Replace [name] with the login path name you intend to use and [database-name] with the database you desire to back up in the example below. As required, modify the location where the backups are stored, as well as any other options.\n0 1 * * * /usr/bin/mysqldump --login-path**=[**name**]** --single-transaction **[**database-name**]** \u003e ~/database-backups/backup-**$(**date +%F-%H.%M.%S**)**.sql ","each-database#Each database":" #mysqldump [options] --all-databases \u003e backup.sql Caution\nIf you wish to restore this database to a Utho MySQL Managed Database, do not use the ‚Äîall-databases option. It may delete existing users and restrict database access.\nNote\nIt can take a long to finish depending on how big the database is. Use the ‚Äîquick option to get rows one at a time rather than all at once if your table contains a lot of data.","multiple-particular-databases#Multiple particular databases:":" #mysqldump [options] --databases [database1_name] [database2_name] \u003e backup.sql ","one-database#One database:":" #mysqldump [options] [database_name] \u003e backup.sql Tables that are unique to a single database:\n#mysqldump [options] [database_name] [table_name] \u003e backup.sql "},"title":"Using mysqldump to Backup MySQL Databases"},"/utho-docs/docs/linux/using-the-carat-symbol-you-can-easily-correct-a-previous-commands-typo/":{"data":{"":"","#":"Description\nHave you ever typed a command and quickly pressed Enter before realising you made a mistake? While you can repair the misspelling and explore the command history by using the up and down arrows, there is a quicker and easier method.\nIn this article, we‚Äôll go over a quick and easy way to deal with a command line typo. Assume you wanted to see if a service was listening on port 22, but you entered nestat instead of netstat.\nYou may easily modify the incorrect command to the correct one and run it as follows:\nNOTE: Don‚Äôt Miss: The Power of the ‚ÄúHistory Command‚Äù in Bash Shell in Linux\nnestat -npltu | grep 22 ^nestat^netstat That‚Äôs correct. You can rectify the error and run the command automatically by using two carat signs (they should be followed by the typo and the proper word, respectively).\nIt is important to note that this method only works for the previous command (the most recent command executed); attempting to repair a typo for a command run earlier would result in an error from the shell.\nSummary This is an excellent approach for overcoming time-wasting behaviours. As you can see, finding and correcting a typo is more easier and faster than going through command history.\nSimply rectify the error with the caduceus signs, press the Enter button, and the proper instruction is executed immediately.\nThere could be various alternative methods for rectifying command line typos."},"title":"Using the Carat () Symbol, you can easily correct a previous command's typo."},"/utho-docs/docs/linux/using-the-terminal-in-linux-to-examine-the-websites-loading-time/":{"data":{"":"","#":"\nDescription The time it takes for a website to respond can have a big effect on how users feel, so if you are a web developer or a server administrator who is in charge of putting everything together, you need to make sure that users don‚Äôt get frustrated while trying to use your site. There is a real ‚Äúneed for speed.‚Äù\nYou will learn how to test the response time of a website using the command line in Linux by following the steps in this guide. In the following, we will demonstrate how to check the time required in seconds, specifically:\nfor the purpose of name resolution in order to establish a TCP connection to the server. for the process of transferring the file to start. for the first byte to be transferred during the process. for the entirety of the procedure. In addition, for HTTPS-enabled websites, we will also look at how to test the amount of time, in seconds, that it takes for a redirect as well as an SSL connection and handshake to be established with the server. It seems like a good idea, right? All right, then, let‚Äôs get started.\ncURL is a powerful command line tool that can transfer data from or to a server using a variety of different protocols, including FILE, FTP, FTPS, HTTP, HTTPS, and many others. The majority of the time, it is employed either as an HTTP header checker or as a command line downloader. Nevertheless, we are going to talk about one of its less well-known functionalities here.\ncURL has a useful option: -w, which prints information on stdout after a successful operation. It contains some variables that can be used to test the various response times listed above for a website. We will make use of a few of the time-related variables, each of which may be passed in a specific format either as a literal string or contained within a file.\nNow, open up your computer‚Äôs terminal and type in the following command:\n#sudo apt install curl -y After the installation was complete, we are now able to test the loading time of a website that we require. Therefore, to test a website, the command is\n#curl -s -w 'Testing Website Response Time for :%{url_effective}nnLookup Time:tt%{time_namelookup}nConnect Time:tt%{time_connect}nPre-transfer Time:t%{time_pretransfer}nStart-transfer Time:t%{time_starttransfer}nnTotal Time:tt%{time_total}n' -o /dev/null ADDRESS In the preceding command, ADDRESS can either be the website‚Äôs URL or its IP address; either one is acceptable.\nWe used three options in the above command:\n-s:¬†this will show an error if the command fails -w:¬†this will make the curl display result of the command -o:¬†output to a file *An HTTP website was put through its paces and the results are shown in the image below.\nLet‚Äôs say that in the event that we decide to test HTTPS websites, we want to include the following:\nThe total amount of time spent on the SSL connection (time appconnect) The time used to calculate redirection (time riderect) #curl -s -w 'Testing Website Response Time for :%{url_effective}nnLookup Time:tt%{time_namelookup}nConnect Time:tt%{time_connect}nAppCon Time:tt%{time_appconnect}nRedirect Time:tt%{time_redirect}nPre-transfer Time:t%{time_pretransfer}nStart-transfer Time:t%{time_starttransfer}nnTotal Time:tt%{time_total}n' -o /dev/null ADDRESS *So, if we don‚Äôt want to run such a long command, we can make it easier by putting it in a curl-formatted file and calling it from the command line.\nFrom this point on, we can observe how to make it. To begin, we need to input the command shown below in order to view the file using the vi text editor.\n#vi ~/curl-change.txt After that, copy the line and paste it into the file. After that, save the file and close it.\ntime_namelookup: %{time_namelookup}n time_connect: %{time_connect}n time_appconnect: %{time_appconnect}n time_pretransfer: %{time_pretransfer}n time_redirect: %{time_redirect}n time_starttransfer: %{time_starttransfer}n ----------n time_total: %{time_total}n Consequently, given that we have made the process simpler, we will need to carry out the command in the manner depicted in the following example.\n#curl -w \"@curl-change.txt\" -o /dev/null -s ADDRESS By utilising this command, we should be able to replace the ADDRESS with the URL or IP Address instead. This will work for websites that use either HTTP or HTTPS.\nWe will upload images by utilising the aforementioned script to run our website over HTTP and HTTPS.\nThe image below is a website that uses HTTP. The image below is a website that uses HTTPS. Thank You "},"title":"Using the Terminal in Linux to Examine the Website's Loading Time"},"/utho-docs/docs/linux/using-the-yum-command-install-google-chrome-on-centos-7/":{"data":{"":"\nDescription\nOne of the most widely used web browsers now available is Google Chrome. It has a market share of over 62% and offers far higher levels of both security and performance compared to any other browser. On the other hand, it is not completely open source and instead built on top of the open source Chromium project. Using the yum command, this tutorial will walk you through the process of installing the stable version of Google Chrome on CentOS 7.\nThe following steps should be taken in order to install the application on your desktop running CentOS Linux:","centos-7-install-google-chrome#CentOS 7 install Google Chrome":" # wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm ","chrome-installation-on-centos-7-desktop#Chrome installation on CentOS 7 desktop":" # sudo yum localinstall google-chrome-stable_current_x86_64.rpm Check out the screenshot below to see whether it‚Äôs already installed‚Ä¶.\nYour computer now has the Google Chrome web browser installed. And that is it.","download-google-chrome#download Google Chrome":" # wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm ","how-can-i-update-chrome-on-my-centos-7-computer#How can I update Chrome on my CentOS 7 computer?":"To update Google Chrome, simply use the yum command as detailed below:\n# sudo yum update or\n# sudo yum upgrade google-chrome-stable Find out more about the yum command.\n# man yum Conclusion\nOn this page, installation instructions for Google‚Äôs browser on a CentOS 7 workstation were provided. To learn more, have a look at the official Google Chrome website right here.\nThank You","how-do-i-launch-stable-google-chrome#How do I launch stable Google Chrome?":"At the prompt given by your shell, simply type the following command:\n# google-chrome \u0026 Alternately, you can begin the process from within the GUI itself:\nApplications \u003e Internet \u003e Google Chrome\nIt should look something like this when you use Google Chrome:","update-chrome#update Chrome":" # sudo yum upgrade google-chrome-stable ","yum-command-to-install-chrome-web-browser#yum command to install Chrome web browser":" # sudo yum localinstall google-chrome-stable_current_x86_64.rpm "},"title":"Using the yum command, install Google Chrome on CentOS 7."},"/utho-docs/docs/linux/virtualhost-creation-in-tomcat-9-8-7/":{"data":{"":" VirtualHost creation in Tomcat 10/9/8/7","creating-virtualhost-in-tomcat#Creating VirtualHost in Tomcat":"First, navigate to Tomcat‚Äôs install directory and edit /config/server.xml or /conf/server.xml file in the preferred editor to build virtual Tomcat hosts. For this tutorial, we will use vi. Then build your application‚Äôs virtual host. The virtual host below is:\nFirst application with domain name example.com and path /opt/tomcat/webapps/Dir1 document root. Second application with domain name mydomain.org and /opt/tomcat/webapps/Dir2 document root. vi /config/server.xml \u003cHost name=\"example.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cAlias\u003ewww.example.com\u003c/Alias\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"example_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /\u003e \u003cContext path=\"\" docBase=\"/opt/tomcat/webapps/Dir1\" debug=\"0\" reloadable=\"true\"/\u003e \u003c/Host\u003e \u003cHost name=\"mydomain.org\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cAlias\u003ewww.mydomain.org\u003c/Alias\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"mydomain_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /\u003e \u003cContext path=\"\" docBase=\"/opt/tomcat/webapps/Dir2\" debug=\"0\" reloadable=\"true\"/\u003e \u003c/Host\u003e After making the above changes we have to restart the Tomcat\nAnd now, you have learnt how to create VirutalHost in Tomcat 10/9/8/7\nThank You :)","prerequisites#Prerequisites":" One Ubuntu 20.04 server A super user ( root ) or any normal user with SUDO privileges. Entry for your domain in /etc/hosts file so that domains are reachable from tomcat server A ubuntu server with already installed tomcat server. Please follow the this guide to learn more about how to install tomcat 10 on ubuntu server ","what-is-tomcat#What is Tomcat":"In this guide, you will learn how to create VirutalHost in Tomcat 10/9/8/7. But before that, first learn that Java applications are served via Apache Tomcat, a web server and servlet container. It is an open source implementation of the Jakarta Servlet, Jakarta Server Pages, and other Jakarta EE platform technologies.","what-is-virtual-host#What is Virtual Host":"Virtual hosting allows everyone on a single server to host multiple domains (websites). This is a principle of sharing resources between multiple hosting accounts. Virtual hosting is typically used by common hosting servers where multiple users can host several websites on a single server.\nWe built an IP 192.168.10.45 Linux server for Tomcat hosting only. Tomcat 9 installed and set up to run on port 80. Following that, we used Tomcat Admin panel to deploy two java web applications on tomcat. All apps now running on the following URLs:\nhttp://192.168.10.45/Dir1 http://192.168.10.45/Dir2 We would now like to run all web applications on big domain names such as example.com and mydomain.org. End users may use the key domain name to access the web application."},"title":"VirtualHost creation in Tomcat 10/9/8/7"},"/utho-docs/docs/managed-database-cluster-on-utho-cloud/":{"data":{"":"","introduction#Introduction:":"Utho managed databases are a high-performance database cluster service that is completely managed. An effective substitute for manually installing, setting, maintaining, and safeguarding databases is to use managed databases. Every database cluster has automated failover, which means that malfunctioning or deteriorating nodes are automatically identified and replaced.","steps-to-setup-utho-managed-database-cluster#Steps to setup Utho managed database cluster:":"Step 1: Login to Your Utho Profile: Access the Utho Cloud platform and log in to your profile.\nStep 2: Select Database from the Side Menu: In the dashboard, navigate to the side menu and choose the ‚ÄòDatabase‚Äô option.\nStep 3: Click on ‚ÄúCreate Database Cluster‚Äù Button: Within the Database section, locate and click on the ‚ÄúCreate Database Cluster‚Äù button to initiate the setup process.\nStep 4: Select the Data Location: Choose the preferred data location where your database cluster will be hosted.\nStep 5: Select the MySQL Database Engine: Right now, we provide MySQL database engines. Opt for the MySQL database engine to power your cluster.\nStep 6: Select the Desired Configuration of the Cluster: Choose the configuration plan according to your requirements.\nStep 7: Select the Number of Nodes and Enter the Cluster Name: Define the number of nodes in your cluster and assign a unique name to it.\nStep 8: Click on ‚ÄúCreate Cluster‚Äù Button: Finalize the process by clicking on the ‚ÄúCreate Cluster‚Äù button, initiating the creation of your managed database cluster.\nAnd this is how easily you can setup a fully managed database cluster on Utho platform for your application."},"title":"Managed Database cluster on Utho Cloud"},"/utho-docs/docs/managing-resources-using-apache-mod_alias/":{"data":{"":"In certain cases all of the services that an Apache host uses are stored in the DocumentRoot of that server. The DocumentRoot is a directory listed in the block of configuration for \u003cVirtualHost\u003e. This directory is intended to represent the various files, folders, and services on the file system that users access via HTTP. However, it is normal for administrators to provide HTTP access to a resource that is not located in the DocumentRoot on file system. For certain cases, although Apache may obey symbolic connections, this could be hard to sustain. It allows Apache to assign an Alias that links a place in the request to an alternate location.\nThis document describes how to manage resources on the file system using the Alias¬†directive while still providing access through HTTP. In addition, this guide assumes that you have an Apache installation working and that you have access to change configuration files. If you have not installed Apache, you might want to find one of our installation guides for Apache or the installation guides for LAMP stacks. If you want a more comprehensive guide to Apache configuration, find the fundamentals of our Apache configuration and the layout of the Apache papers.","creating-aliases#Creating Aliases":"Virtual Host configurations usually specify a DocumentRoot¬†which, by default, specifies a directory called public_html/¬†or public/.¬†If the root document for the virtual host abc.com¬†is/srv/www/abc.com/public_html/,¬†then the file located at /srv/www/abc.com/public_html/index.htm will be returned by a request for http://www.abc.com/index.htm.\nIf the administrator were to retain the file system code/¬†resource at /srv/git/public/¬†but make it accessible at http://abc.com/code/,, an alias will be required. In this example this is accomplished:\nDocumentRoot /srv/www/abc.com/public_html/ Alias /code /srv/git/public Order allow,deny Allow from all Without the Alias¬†instruction, a http://abc.com/code/¬†request will return the resources available in the /srv/www/abc.com/public_html/code/. folder. The Alias will therefore direct Apache to serve content from the directory /srv/git/public. The segment \u003cDirectory\u003e¬†allows remote users to access the directory.\nThere are a variety of important factors to remember when using the Alias Directive:\nDirectory blocks need to be generated after Alias has been declared for the destination of Alias. This makes it possible to allow access to and otherwise regulate the actions of certain pieces. That would be /srv/git/public in the example above. In general, trailing slashes should be avoided in the Alias¬†Directives. If the above read Alias /code/ /srv/git/public/¬†the request for http://abc.com/code, code, without a trailing slash, it will be served from the DocumentRoot. The Alias directives must be generated either in the root-level configuration of the server (e.g. httpd.conf)¬†or in the \u003cVirtualHost\u003e configuration block. In addition to Alias, Apache provides for an AliasMatch Directive that provides similar functionality. AliasMatch¬†includes an additional opportunity to alias a class of requests for a given resource to a place outside the DocumentRoot. Consider another fictitious abc.com¬†virtual host configuration:\nDocumentRoot /srv/www/abc.com/public_html/ AliasMatch /code/projects/(.+) /srv/git/projects/$ Order allow,deny Allow from all In this example, resource requests for URLs such as http:/abc.com/code/projects/my_app and http:/abc.com/code/projects/my_app2 will be served in /srv/git/projects/my_app2 respectively. Nonetheless, http:/abc.com/code/projects will be accessed from /srv/www/abc.com/public html/code/projects/ instead of /srv/git/projects/ due to the /code/projects/(.+) trailing slash in the alias.\nAlthough the possible use case for Alias is quite limited, the feature is very powerful to maintain a stable and well-organized web server."},"title":"Managing Resources using Apache mod_alias"},"/utho-docs/docs/managing-two-factor-authentication-for-mobile-and-email-access-2/":{"data":{"":"","step-1-log-in-to-the-cloud-dashboard#\u003cstrong\u003eStep 1: Log in to the Cloud Dashboard\u003c/strong\u003e":"","step-2-access-your-profile#\u003cstrong\u003eStep 2: Access Your Profile\u003c/strong\u003e":"","step-3-locate-and-access-two-factor-authentication-settings#\u003cstrong\u003eStep 3: Locate and Access Two-Factor Authentication Settings\u003c/strong\u003e":"","step-4-configure-two-factor-authentication-code-delivery-preferences#\u003cstrong\u003eStep 4: Configure Two-Factor Authentication Code Delivery Preferences\u003c/strong\u003e":"","step-5-save-configuration-changes#\u003cstrong\u003eStep 5: Save Configuration Changes\u003c/strong\u003e":"Follow this step-by-step guide to effortlessly enhance the security of your Utho Dashboard account by managing Two-Factor Authentication preferences for mobile and email access.\nStep 1: Log in to the Cloud Dashboard 1.1 Login to your Utho Dashboard\nStep 2: Access Your Profile 2.1 Within the Cloud Dashboard interface, navigate to the top right corner.\n2.2 Click on your profile icon.\n2.3 In the dropdown menu, select ‚ÄúMy Profile.‚Äù\nStep 3: Locate and Access Two-Factor Authentication Settings 3.1 Within your profile settings on the Cloud Dashboard, scroll down to access additional options.\n3.2 Identify and click on the ‚ÄúTwo-Factor Authentication‚Äù section.\nStep 4: Configure Two-Factor Authentication Code Delivery Preferences 4.1 Within the Two-Factor Authentication settings, choose whether to enable or disable.\n4.2 Select the preferred delivery method(s) for receiving authentication codes.4.3 Ensure that at least one delivery method is chosen to receive 2-factor authentication codes.\nStep 5: Save Configuration Changes 5.1 After adjusting your Two-Factor Authentication settings, locate and click on the ‚ÄúSave Changes‚Äù button.\n5.2 Verify the successful saving of changes with the confirmation message indicating that the changes have been saved."},"title":"Managing Two-Factor Authentication for Mobile and Email Access"},"/utho-docs/docs/managing-two-factor-authentication-for-mobile-and-email-access/":{"data":{"":"","step-1-log-in-to-the-cloud-dashboard#\u003cstrong\u003eStep 1: Log in to the Cloud Dashboard\u003c/strong\u003e":"","step-2-access-your-profile#\u003cstrong\u003eStep 2: Access Your Profile\u003c/strong\u003e":"","step-3-locate-and-access-two-factor-authentication-settings#\u003cstrong\u003eStep 3: Locate and Access Two-Factor Authentication Settings\u003c/strong\u003e":"","step-4-configure-two-factor-authentication-code-delivery-preferences#\u003cstrong\u003eStep 4: Configure Two-Factor Authentication Code Delivery Preferences\u003c/strong\u003e":"","step-5-save-configuration-changes#\u003cstrong\u003eStep 5: Save Configuration Changes\u003c/strong\u003e":"Follow this step-by-step guide to effortlessly enhance the security of your Utho Dashboard account by managing Two-Factor Authentication preferences for mobile and email access.\nStep 1: Log in to the Cloud Dashboard 1.1 Login to your Utho Dashboard\nStep 2: Access Your Profile 2.1 Within the Cloud Dashboard interface, navigate to the top right corner.\n2.2 Click on your profile icon.\n2.3 In the dropdown menu, select ‚ÄúMy Profile.‚Äù\nStep 3: Locate and Access Two-Factor Authentication Settings 3.1 Within your profile settings on the Cloud Dashboard, scroll down to access additional options.\n3.2 Identify and click on the ‚ÄúTwo-Factor Authentication‚Äù section.\nStep 4: Configure Two-Factor Authentication Code Delivery Preferences 4.1 Within the Two-Factor Authentication settings, choose whether to enable or disable.\n4.2 Select the preferred delivery method(s) for receiving authentication codes.\n4.3 Ensure that at least one delivery method is chosen to receive 2-factor authentication codes.\nStep 5: Save Configuration Changes 5.1 After adjusting your Two-Factor Authentication settings, locate and click on the ‚ÄúSave Changes‚Äù button.\n5.2 Verify the successful saving of changes with the confirmation message indicating that the changes have been saved."},"title":"Enable/disable 2fa on mobile/email for Utho Dashboard"},"/utho-docs/docs/manojdhanda/":{"data":{"":"Step 1: Author: Manoj Dhanda ."},"title":"Manoj Dhanda"},"/utho-docs/docs/microhost-load-balancer/":{"data":{"":"\nLoad balancing is the process of distributing workloads/traffic across multiple computing resources. Load balancer is to save or protect a website from sudden outages. When the workload is distributed among various servers or network units, even if one node fails the burden can be shifted to another active node. It is used to maintain system firmness, improve system performance and to protect against system failures.\nAt first, you need to login Microhost Cloud Dashboard After successful login, we will move to the ‚ÄúLoad Balancers‚Äù section as given in the screenshot . In the ‚ÄúLoad balancer‚Äù option, we have to click on ‚ÄúAdd Load balancer‚Äù as given in the screenshot. 4. Output will be shown as similar like given in the screenshot, where we have to write the DC location , load balancer setting(algorithm) and load balancer name .\n5. After this click on create load balancer .Output will be shown as below-\nIn the backend section , you can choose the cloud server on which you want to apply the load balancer .\nIn the rules section , you can manage (Add/Delete) the rules as per your requirement.\nThankyou."},"title":"MicroHost Load balancer"},"/utho-docs/docs/nat-gateway-your-key-to-seamless-cloud-connectivity/":{"data":{"":"","how-does-combining-nat-gateway-with-services-like-load-balancers-or-firewall-rules-bolster-network-resilience-and-security#\u003cstrong\u003eHow does combining NAT Gateway with services like load balancers or firewall rules bolster network resilience and security?\u003c/strong\u003e":"","how-does-the-cost-structure-for-utilizing-a-nat-gateway-compare-across-different-cloud-service-providers-and-what-factors-influence-these-costs#\u003cstrong\u003eHow does the cost structure for utilizing a NAT Gateway compare across different cloud service providers, and what factors influence these costs?\u003c/strong\u003e":"","how-does-utho-cloud-improve-network-connectivity-and-security-for-businesses-in-the-cloud-with-its-nat-gateway-services#\u003cstrong\u003eHow does Utho Cloud improve network connectivity and security for businesses in the cloud with its NAT Gateway services?\u003c/strong\u003e":"In the world of cloud computing, ensuring smooth and uninterrupted connectivity is crucial. NAT Gateway plays a vital role in achieving this by seamlessly connecting your cloud resources to the internet while maintaining security and privacy. Join us as we explore the ins and outs of NAT Gateway and how it enhances your cloud networking experience.\nWhat does Cloud NAT entail? Cloud NAT, or Network Address Translation, is a service provided by cloud computing platforms like Google Cloud Platform. It enables virtual machine instances without external IP addresses to access the internet, as well as providing a means for instances with external IP addresses to communicate with those without.\nIn simpler terms, Cloud NAT allows virtual machines (VMs) in a cloud environment to connect to the internet or other resources outside of their network, even if they don‚Äôt have their own unique public IP address. Instead, Cloud NAT assigns a single public IP address to multiple VM instances within a private network, translating their internal IP addresses to the public one when accessing external services. This helps with security and efficiency by reducing the number of publicly exposed IP addresses while still allowing for internet connectivity.\nWhat are the primary benefits of using a NAT Gateway in cloud networking architectures? Using a NAT (Network Address Translation) Gateway in cloud networking architectures offers several key benefits:\nEnhanced Security: NAT Gateway acts as a barrier between your private subnet and the internet, hiding the actual IP addresses of your resources. This adds a layer of security by preventing direct access to your internal network.\nSimplified Network Management: It simplifies outbound internet connectivity by providing a single point for managing traffic from multiple instances in a private subnet. You don‚Äôt need to assign public IP addresses to each instance, reducing management overhead.\nCost-Effectiveness: NAT Gateway allows you to consolidate outbound traffic through a single IP address, which can be more cost-effective than assigning public IP addresses to each instance. This can result in savings, especially in scenarios with multiple instances requiring internet access.\nScalability: NAT Gateway can handle high volumes of outbound traffic and automatically scales to accommodate increased demand without intervention. This scalability ensures that your network remains responsive even during peak usage periods.\nImproved Performance: By offloading the task of address translation to a dedicated service, NAT Gateway can improve network performance and reduce latency compared to performing NAT functions on individual instances.\nOverall, integrating a NAT Gateway into your cloud networking architecture enhances security, simplifies management, reduces costs, and improves scalability and performance, making it a valuable component for cloud-based infrastructure.\nWhat are some real-world examples or use cases that illustrate the significance of NAT Gateway in contemporary cloud networking configurations? Real-world examples and use cases showcasing the importance of Network Address Translation Gateway in modern cloud networking setups include:\nSecure Internet Access: In a cloud environment hosting web applications, a NAT Gateway can ensure secure outbound internet access for instances in private subnets. This prevents direct exposure of internal resources to the internet while allowing them to access necessary external services, such as software updates or API endpoints.\nMulti-tier Applications: For multi-tier applications where different components reside in separate subnets (e.g., web servers in a public subnet and database servers in a private subnet), a NAT Gateway facilitates communication between these tiers while maintaining security. The web servers can access the internet via the NAT Gateway for updates or third-party services without exposing the database servers to external threats.\nHybrid Cloud Connectivity: Organizations with hybrid cloud architectures, where on-premises resources are integrated with cloud infrastructure, often use NAT Gateway to enable outbound internet connectivity for cloud-based resources while ensuring communication with on-premises systems remains secure.\nManaged Services Access: When utilizing managed services like AWS Lambda or Amazon S3 from instances in a private subnet, a NAT Gateway allows these instances to access the internet for invoking serverless functions, storing data, or retrieving configuration information without exposing them directly to the public internet.\nCompliance and Regulatory Requirements: In industries with strict compliance or regulatory requirements, such as healthcare or finance, NAT Gateway helps maintain security and compliance by controlling outbound traffic and providing a centralized point for monitoring and auditing network activity.\nThese examples highlight how NAT Gateway plays a crucial role in facilitating secure, controlled, and compliant communication between resources in cloud networking environments, making it an essential component of modern cloud architectures.\nHow does combining NAT Gateway with services like load balancers or firewall rules bolster network resilience and security? Integrating NAT Gateway with other cloud networking services, such as load balancers or firewall rules, enhances overall network resilience and security through several mechanisms:\nLoad Balancers: NAT Gateway can be integrated with load balancers to distribute incoming traffic across multiple instances in a private subnet. This integration ensures that inbound requests are evenly distributed while maintaining the security of internal resources by hiding their IP addresses. In the event of instance failure, the load balancer automatically routes traffic to healthy instances, improving application availability and resilience.\nFirewall Rules: By incorporating NAT Gateway with firewall rules, organizations can enforce fine-grained access controls and security policies for outbound traffic. Firewall rules can be configured to restrict outbound communication to authorized destinations, preventing unauthorized access and mitigating the risk of data exfiltration or malicious activity. Additionally, logging and monitoring capabilities provided by firewall rules enhance visibility into outbound traffic patterns, facilitating threat detection and incident response.\nNetwork Segmentation: NAT Gateway integration with network segmentation strategies, such as Virtual Private Cloud (VPC) peering or transit gateway, enables organizations to create isolated network segments with controlled communication pathways. This segmentation enhances security by limiting lateral movement of threats and reducing the attack surface. NAT Gateway serves as a gateway between segmented networks, enforcing access controls and ensuring secure communication between authorized endpoints.\nVPN and Direct Connect: NAT Gateway can be utilized in conjunction with VPN (Virtual Private Network) or Direct Connect services to establish secure, encrypted connections between on-premises infrastructure and cloud resources. This integration extends the organization‚Äôs network perimeter to the cloud while maintaining data confidentiality and integrity. NAT Gateway facilitates outbound internet access for VPN or Direct Connect connections, allowing on-premises resources to securely access cloud-based services and applications.\nOverall, the integration of NAT Gateway with other cloud networking services strengthens network resilience and security by providing centralized control, granular access controls, and secure communication pathways for inbound and outbound traffic. This comprehensive approach ensures that organizations can effectively protect their infrastructure and data assets in the cloud environment.\nHow does the cost structure for utilizing a NAT Gateway compare across different cloud service providers, and what factors influence these costs? The cost structure for using a NAT Gateway varies across different cloud service providers and is influenced by several factors:\nUsage Rates: Cloud providers typically charge based on the amount of data processed or bandwidth utilized by the NAT Gateway. This can vary depending on the region, with different rates for inbound and outbound data transfer.\nInstance Type: Some cloud providers offer different instance types for NAT Gateway, each with varying performance characteristics and associated costs. Choosing the appropriate instance type based on your workload requirements can impact the overall cost.\nData Transfer Pricing: In addition to NAT Gateway usage rates, data transfer pricing for transferring data between the NAT Gateway and other cloud resources, such as instances or external services, may apply. Understanding the data transfer pricing structure is essential for accurately estimating costs.\nHigh Availability Configuration: Deploying NAT Gateway in a high availability configuration across multiple availability zones may incur additional costs. Cloud providers may charge for redundant resources or data transfer between availability zones.\nData Processing Fees: Some cloud providers impose data processing fees for certain types of network traffic, such as processing NAT Gateway logs or performing network address translation operations.\nDiscounts and Savings Plans: Cloud providers often offer discounts or savings plans for long-term commitments or predictable usage patterns. Taking advantage of these discounts can help reduce the overall cost of utilizing Network Address Translation Gateway.\nComparing the cost structures of NAT Gateway across different cloud service providers involves evaluating these factors and determining which provider offers the most cost-effective solution based on your specific requirements and usage patterns.\nHow does Utho Cloud improve network connectivity and security for businesses in the cloud with its NAT Gateway services? Utho Cloud effectively facilitates NAT Gateway services to optimize network connectivity and enhance security for businesses operating in the cloud environment through the following mechanisms:\nSecure Outbound Connectivity: Utho Cloud‚Äôs NAT Gateway service allows businesses to securely connect their private subnets to the internet without exposing their internal IP addresses. This ensures that outbound traffic from resources in private subnets remains secure and private.\nCentralized Management: The NAT Gateway service in Utho Cloud provides a centralized point for managing outbound traffic from multiple instances in private subnets. This simplifies network management tasks and allows administrators to configure and monitor NAT Gateway settings easily.\nScalability: Utho Cloud‚Äôs NAT Gateway service is designed to scale automatically to handle increasing levels of outbound traffic. This ensures that businesses can maintain consistent network performance and responsiveness even during periods of high demand.\nHigh Availability: Utho Cloud offers NAT Gateway services with built-in redundancy and fault tolerance across multiple availability domains. This ensures high availability for outbound internet connectivity and minimizes the risk of downtime due to hardware or network failures.\nIntegration with Security Services: Utho Cloud‚Äôs NAT Gateway service can be integrated with other security services, such as Utho Cloud Firewall and Network Security Groups, to enforce access controls and security policies for outbound traffic. This helps businesses enhance their overall security posture in the cloud environment.\nOverall, Utho Cloud‚Äôs NAT Gateway services provide businesses with a secure, scalable, and easy-to-manage solution for optimizing network connectivity and enhancing security in the cloud environment.\nNetwork Address Translation is a crucial tool for building secure and efficient networks. Utho‚Äôs solutions include advanced NAT features that improve connectivity, security, and resource management in the cloud. This helps businesses make the most of cloud resources while keeping everything safe and private.\nUnderstanding NAT and its different forms is essential for network admins and IT professionals. It‚Äôs used for letting private networks access the internet, connecting different parts of a network, and managing IP addresses efficiently. In today‚Äôs networking world, NAT plays a big role in keeping things running smoothly and securely.","what-are-some-real-world-examples-or-use-cases-that-illustrate-the-significance-of-nat-gateway-in-contemporary-cloud-networking-configurations#\u003cstrong\u003eWhat are some real-world examples or use cases that illustrate the significance of NAT Gateway in contemporary cloud networking configurations?\u003c/strong\u003e":"","what-are-the-primary-benefits-of-using-a-nat-gateway-in-cloud-networking-architectures#\u003cstrong\u003eWhat are the primary benefits of using a NAT Gateway in cloud networking architectures?\u003c/strong\u003e":"","what-does-cloud-nat-entail#\u003cstrong\u003eWhat does Cloud NAT entail?\u003c/strong\u003e":""},"title":"NAT Gateway: Your Key to Seamless Cloud Connectivity"},"/utho-docs/docs/navigating-the-data-landscape-with-block-storage-solutions/":{"data":{"":"","how-do-businesses-utilize-technology-for-the-implementation-of-cloud-storage-solutions#\u003cstrong\u003eHow do businesses utilize technology for the implementation of cloud storage solutions?\u003c/strong\u003e":"","how-is-this-technology-applied-in-various-scenarios-for-businesses-utilizing-cloud-storage#\u003cstrong\u003eHow is this technology applied in various scenarios for businesses utilizing cloud storage?\u003c/strong\u003e":"Understanding the suitable storage option for your business is crucial. Whether you opt for local file storage or utilize off-server solutions like object storage or block storage, each comes with distinct features tailored to specific business requirements. To assist you in making an informed choice, we will delve into this cloud storage technology, examining its fundamental architecture and operational principles.\nWhat is the functioning mechanism of this technology in cloud storage solution? Block storage works by breaking down a file into chunks called blocks, each holding a set amount of data. These blocks don‚Äôt follow a specific order, and the data in one block isn‚Äôt necessarily connected to the data in nearby blocks. Each block has its own special ID. When you want to get a file, you send a request, and the system finds and puts together the needed blocks.\nHow do businesses utilize technology for the implementation of cloud storage solutions? Block storage proves to be an excellent choice for databases due to its high I/O performance and low-latency connections. It is suitable for RAID volumes that involve the combination of multiple disks. Applications like Java, PHP, and .Net, as well as critical ones like Oracle, SAP, Microsoft Exchange, and Microsoft SharePoint, benefit from the features of this technology.\nWhat are both the benefits and drawbacks of utilizing this technology? It comes with its own set of advantages. However, the merits are evident, especially when it comes to high performance and rapid data access.\nIncreased performance: It often outperforms other storage types due to its ability to provide quick access to data. Minimizing latency or response time is crucial in ensuring that your applications meet their performance goals.\nAbility to make incremental changes: Block storage enables the modification of a file without requiring the removal of all existing data, as is necessary in a traditional file system. For instance, you can make alterations by replacing, deleting, or inserting blocks. This feature makes it well-suited for frequently updated files, particularly those employed in databases.\nIncreased reliability: Block storage plays a vital role in maintaining the continuous availability of critical applications. In case of a failure, organizations can swiftly and effortlessly recover data from backup media.\nBlock storage comes with its limitations. It‚Äôs undeniable that it can incur higher costs compared to alternative storage methods, and it might not be the optimal solution for every workload.\nHigher cost: Block storage tends to be pricier than alternative storage options. For instance, acquiring and maintaining SANs can incur significant expenses.\nIncreased complexity: Administering block storage can be more intricate compared to alternative storage methods. For instance, effective management may necessitate additional training and/or experience.\nLimited metadata: In contrast to alternative storage types, block storage provides limited metadata support. This limitation can pose challenges in tracking and searching the data stored within a block storage system.\nHow is this technology applied in various scenarios for businesses utilizing cloud storage? Swift and high-performing block storage receive considerable acclaim. Here are potential use cases:\nDatabase Storage: Speed, performance, and reliability are the factors that make block storage excellent for databases and effective support for enterprise applications. The ease of modifying data blocks further enhances its suitability for frequently updated files.\nServer Storage: Block storage systems spread data across different sections. Making a block-based storage space is easy, and it works well as storage for virtual systems. You can attach a regular server to it and make lots of virtual machines. Many companies use block storage to set up storage spaces for all their virtual stuff.\nEmail Servers: Organizations frequently opt for high-performance and reliable block storage technology as the standard choice for storing emails.\nHow can utho support your block storage needs? The dedicated platform is designed to streamline block storage by offering limitless scalability, consolidating extensive data sets into a single, easily managed environment. The platform not only provides unlimited storage capacity but also incorporates additional features, including data protection and various comprehensive and intelligent management tools.","what-is-the-functioning-mechanism-of-this-technology-in-cloud-storage-solution#\u003cstrong\u003eWhat is the functioning mechanism of this technology in \u003cstrong\u003ecloud storage solution\u003c/strong\u003e?\u003c/strong\u003e":""},"title":"Navigating the Data Landscape with Block Storage Solutions"},"/utho-docs/docs/navigating-the-digital-highway-the-world-of-virtual-routers/":{"data":{"":"","how-do-virtual-routers-contribute-to-cost-savings-and-efficiency-in-network-management#\u003cstrong\u003e\u003cstrong\u003eHow do virtual routers contribute to cost savings and efficiency in network management?\u003c/strong\u003e\u003c/strong\u003e":"","how-does-a-virtual-router-operate#\u003cstrong\u003eHow does a virtual router operate?\u003c/strong\u003e":"","how-does-utho-cloud-ensure-the-security-and-reliability-of-its-virtual-router-offering#\u003cstrong\u003eHow does Utho Cloud ensure the security and reliability of its Virtual Router offering\u003c/strong\u003e?":"In today‚Äôs world where everything is connected through digital technology, the need for strong and adaptable networking solutions is greater than ever. Businesses of all sizes are always looking for ways to make their networks work better, so they stay connected without any interruptions. Virtual routers have become a big deal in this effort. This article dives into the world of virtual routers, looking at how they‚Äôve grown, what they offer now, why they‚Äôre useful, and what might be ahead for them.\nWhat do we mean by Virtual Routers? Virtual routers are software-based entities designed to replicate the functionalities of physical routers within a network. They operate on virtualized hardware and are managed through software interfaces. In simple terms, virtual routers are like digital versions of physical routers, serving as the backbone for routing network traffic without the need for dedicated hardware devices. They are commonly used in cloud computing environments, virtual private networks (VPNs), and software-defined networking (SDN) architectures.\nWhat are the benefits of using virtual routers? Using virtual routers offers several benefits:\nCost Savings: Virtual routers eliminate the need for purchasing dedicated physical hardware, reducing upfront costs. Organizations can leverage existing server infrastructure or cloud resources, leading to significant cost savings.\nScalability: Virtual routers can easily scale up or down based on network demands by allocating or deallocating virtual resources. This scalability allows organizations to adapt to changing requirements without investing in new hardware.\nFlexibility: Virtual routers offer flexibility in configuration and deployment options. They can be quickly provisioned, modified, or decommissioned to meet specific network needs, providing agility in network management.\nResource Utilization: By running on virtualized hardware, virtual routers can share resources such as CPU, memory, and storage with other virtual machines. This maximizes resource utilization and minimizes wasted capacity.\nEase of Management: Virtual routers are typically managed through software interfaces, offering centralized control and streamlined configuration. This simplifies network management tasks, reduces the need for manual intervention, and minimizes the risk of errors.\nHigh Availability: Virtualization technologies enable features such as failover clustering and live migration, enhancing the availability of virtual routers. This reduces downtime and associated costs related to network disruptions or hardware failures.\nTesting and Development: Virtual routers provide a cost-effective solution for creating test environments and conducting network experiments without disrupting production systems. They enable developers and network engineers to simulate various scenarios and validate configurations before deployment.\nSecurity: Virtual routers can be configured with security features such as access control lists (ACLs), firewall rules, and VPN encryption to protect network traffic. This enhances network security and compliance with regulatory requirements.\nOverall, using virtual routers brings cost savings, scalability, flexibility, and enhanced management capabilities to network environments, making them a preferred choice for modern enterprises.\nHow does a virtual router operate? The functioning mechanism of a virtual router involves several key components and processes:\nVirtualization Layer: Virtual routers operate within a virtualization layer, which abstracts hardware resources and provides a platform for running multiple virtual machines (VMs) on a single physical server.\nVirtual Machine Creation: A virtual router is created as a virtual machine instance within the virtualization environment. This involves allocating virtual CPU, memory, storage, and network resources to the virtual router VM.\nOperating System Installation: An operating system compatible with router software is installed on the virtual machine. Common choices include Linux-based distributions or specialized router operating systems like VyOS or pfSense.\nRouter Software Installation: Router software is installed on the virtual machine to provide routing functionality. This software could be open-source solutions like Quagga, proprietary router software, or purpose-built virtual router appliances provided by vendors.\nNetwork Configuration: The virtual router is configured with network interfaces, IP addresses, routing tables, and other parameters necessary for routing traffic within the network environment. This configuration is typically done through a command-line interface (CLI) or a web-based management interface.\nRouting Protocols: Virtual routers use routing protocols such as OSPF (Open Shortest Path First), BGP (Border Gateway Protocol), or RIP (Routing Information Protocol) to exchange routing information with neighboring routers and make forwarding decisions.\nPacket Forwarding: When a packet arrives at the virtual router, it examines the packet‚Äôs destination IP address and consults its routing table to determine the next hop for the packet. The virtual router then forwards the packet to the appropriate network interface or forwards it to another router based on routing protocol information.\nSecurity and Access Control: Virtual routers implement security features such as access control lists (ACLs), firewall rules, VPN encryption, and authentication mechanisms to protect network traffic and enforce security policies.\nMonitoring and Management: Virtual routers support monitoring and management functionalities for network administrators to monitor traffic, troubleshoot issues, and perform configuration changes. This includes features like SNMP (Simple Network Management Protocol), logging, and remote access interfaces.\nHigh Availability and Redundancy: Virtual routers can be configured for high availability and redundancy using techniques such as virtual machine clustering, load balancing, and failover mechanisms to ensure continuous operation and minimize downtime.\nBy orchestrating these components and processes, virtual routers emulate the functionality of physical routers within a virtualized environment, enabling efficient routing of network traffic in enterprise environments.\nHow do virtual routers contribute to cost savings and efficiency in network management? Virtual routers contribute to cost savings and efficiency in network management through several key factors:\nReduced Hardware Costs: Virtual routers eliminate the need for purchasing dedicated physical router hardware, which can be expensive. Instead, they utilize existing server infrastructure or cloud resources, reducing upfront hardware costs.\nResource Sharing: By running on virtualized hardware, virtual routers can share resources such as CPU, memory, and storage with other virtual machines. This maximizes resource utilization and minimizes wasted capacity, leading to cost savings.\nScalability: Virtual routers can easily scale up or down based on network demands by allocating or deallocating virtual resources. This scalability allows organizations to adapt to changing requirements without investing in new hardware, thereby saving costs.\nConsolidation: Multiple virtual routers can run on the same physical server or within the same virtual environment. This consolidation reduces the number of physical devices needed, simplifying network management and lowering operational costs.\nEase of Management: Virtual routers are typically managed through software interfaces, which offer centralized control and streamlined configuration. This simplifies network management tasks, reduces the need for manual intervention, and minimizes the risk of errors, leading to operational efficiency and cost savings.\nHigh Availability: Virtualization technologies enable features such as failover clustering and live migration, which enhance the availability of virtual routers. This reduces downtime and associated costs related to network disruptions or hardware failures.\nTesting and Development: Virtual routers facilitate easy creation of test environments and sandbox networks without the need for additional physical hardware. This accelerates testing and development cycles, leading to faster deployment of network changes and cost savings through improved efficiency.\nOverall, virtual routers offer cost savings and efficiency benefits by leveraging virtualization technologies to optimize resource utilization, streamline management, and enhance scalability and availability in network environments.\nWhat are some common use cases for virtual routers in enterprise environments? Virtual routers find numerous applications in enterprise environments due to their flexibility, scalability, and cost-effectiveness. Here are some common use cases.\nVirtual Private Networks (VPNs): Virtual routers are often deployed to provide secure remote access to corporate networks for remote employees or branch offices. They facilitate the establishment of encrypted tunnels, enabling secure communication over public networks.\nSoftware-Defined Networking (SDN): In SDN architectures, virtual routers play a crucial role in network abstraction and programmability. They help centralize network control and enable dynamic configuration changes based on application requirements.\nNetwork Segmentation: Enterprises use virtual routers to partition their networks into separate segments for security or performance reasons. This allows for the isolation of sensitive data, compliance with regulatory requirements, and efficient traffic management.\nLoad Balancing: Virtual routers can be employed to distribute network traffic across multiple servers or data centers to optimize resource utilization and improve application performance. They help ensure high availability and scalability for critical services.\nDisaster Recovery: Virtual routers are utilized in disaster recovery setups to replicate network infrastructure and ensure business continuity in case of outages or failures. They enable failover mechanisms and seamless redirection of traffic to backup sites.\nCloud Connectivity: Enterprises leverage virtual routers to establish connections between on-premises networks and cloud platforms, such as AWS, Azure, or Google Cloud. This enables hybrid cloud deployments and facilitates seamless data transfer between environments.\nNetwork Testing and Development: Virtual routers provide a cost-effective solution for creating test environments and conducting network experiments without disrupting production systems. They enable developers and network engineers to simulate various scenarios and validate configurations before deployment.\nTraffic Monitoring and Analysis: Virtual routers support the implementation of traffic monitoring and analysis tools, such as packet sniffers or intrusion detection systems (IDS). They enable real-time traffic inspection, logging, and reporting for network troubleshooting and security purposes.\nService Chaining: Enterprises deploy virtual routers in service chaining architectures to route network traffic through a sequence of virtualized network functions (VNFs), such as firewalls, load balancers, and WAN accelerators. This enhances network security and performance.\nEdge Computing: In edge computing environments, virtual routers are used to extend network connectivity to edge devices, such as IoT sensors or edge servers. They enable local processing of data and reduce latency for time-sensitive applications.\nBy addressing these use cases, virtual routers empower enterprises to build flexible, resilient, and efficient network infrastructures that meet their evolving business needs.\nHow does Utho Cloud ensure the security and reliability of its Virtual Router offering? Utho Cloud ensures the security and reliability of its Virtual Router offering through several measures:\nRobust Security Features: Utho Cloud incorporates robust security features into its Virtual Router offering, including encryption, authentication, and access controls. These features help safeguard data and prevent unauthorized access to network resources.\nCompliance Certifications: Utho Cloud adheres to industry standards and compliance certifications, such as ISO 27001 and SOC 2, to ensure the security and privacy of customer data. These certifications demonstrate Utho‚Äôs commitment to maintaining the highest standards of security and reliability.\nRedundant Infrastructure: Utho Cloud‚Äôs Virtual Router offering is built on redundant infrastructure to ensure high availability and reliability. This includes multiple data centers and network paths to mitigate the risk of downtime and ensure uninterrupted service.\nMonitoring and Management Tools: Utho Cloud provides comprehensive monitoring and management tools for its Virtual Router offering, allowing users to monitor network performance, detect potential security threats, and manage network configurations effectively.\nContinuous Updates and Patching: Utho Cloud regularly updates and patches its Virtual Router software to address security vulnerabilities and ensure optimal performance. These updates are applied automatically to minimize downtime and reduce the risk of security breaches.\nOverall, Utho Cloud prioritizes security and reliability in its Virtual Router offering by implementing robust security features, maintaining compliance certifications, leveraging redundant infrastructure, providing monitoring and management tools, and ensuring continuous updates and patching.\nAs organizations continue to navigate the digital highway, embracing the innovation of virtual routers opens up a world of possibilities for optimizing performance and staying ahead in the ever-evolving digital era. With the reliability and security measures in place, virtual routers pave the way for a smoother journey towards a connected future.","what-are-some-common-use-cases-for-virtual-routers-in-enterprise-environments#\u003cstrong\u003eWhat are some common use cases for virtual routers in enterprise environments?\u003c/strong\u003e":"","what-are-the-benefits-of-using-virtual-routers#\u003cstrong\u003eWhat are the benefits of using virtual routers?\u003c/strong\u003e":"","what-do-we-mean-by-virtual-routers#\u003cstrong\u003eWhat do we mean by Virtual Routers?\u003c/strong\u003e":""},"title":"Navigating the Digital Highway: The World of Virtual Routers"},"/utho-docs/docs/nginx-installation-in-centos-7/":{"data":{"":"Step 1: Add Nginx repository .\n# yum install epel-release Step 2:¬†Install Nginx using following command .\n# yum install nginx Step 3: Start the service of Nginx .\n# systemctl start nginx Step 4: Check status of Ngnix .\n# systemctl status nginx Step 5 : If you are running a firewall, run the following commands to allow HTTP and HTTPS traffic:\n# sudo firewall-cmd --permanent --zone=public --add-service=http¬†# sudo firewall-cmd --permanent --zone=public --add-service=https # sudo firewall-cmd --reload Step 6 : Now check the webserver while accessing the server IP address in the browser.¬†URL: - http://server_domain_name_or_IP/\nThank you :)"},"title":"NGINX Installation in CentOS 7"},"/utho-docs/docs/object-storage-gateway-to-modern-and-streamlined-data-management/":{"data":{"":"","how-does-utho-create-an-object-storage-solution-tailored-to-meet-your-requirements#\u003cstrong\u003eHow does Utho create an object storage solution tailored to meet your requirements?\u003c/strong\u003e":"In the current landscape of application-based businesses and services, flexible and scalable data storage has become a fundamental necessity. Given the complexity of contemporary deployments, involving containers and ephemeral infrastructure, storing data is no longer a straightforward task. Cloud providers address the storage requirements of modern application deployments by offering object storage solutions.\nWhat does the term refer to? Object storage involves storing extensive volumes of data, particularly unstructured data, in cloud environments. The unstructured data generated through various business activities, such as logs, videos, photos, sensor data, and webpages, is effectively managed by object storage. This approach distributes the data across numerous cloud servers, treating each file or data segment as an individual object. Each object is accompanied by metadata and a distinct name or identifier, facilitating seamless data retrieval.\nWhat is the functioning mechanism? Object storage diverges fundamentally from traditional file and block storage in its data handling approach. In this system, data is stored as discrete objects, each comprising the actual data and a distinctive identifier called an object ID. This unique identifier empowers the system to locate and retrieve objects without depending on hierarchical file structures or block mappings, resulting in expedited and more efficient data access.\nWhat are the benefits? Object storage presents numerous advantages compared to traditional storage solutions:\nScalability: Object Storage effortlessly scales to handle substantial data volumes, eliminating the need for intricate configurations or costly hardware upgrades.\nScale-Out architecture: Object storage facilitates a seamless starting point with the flexibility to expand gradually. In enterprise storage, a straightforward scaling model holds significant value. Scale-out storage exemplifies simplicity: by adding another node to the cluster, its capacity seamlessly integrates into the existing pool.\nPerformance: Object stores can deliver exceptional sequential throughput performance, rendering them ideal for efficiently streaming large files. Additionally, services play a crucial role in overcoming networking limitations by enabling parallel streaming of files over multiple channels, thereby enhancing usable bandwidth.\nDurability: Object Storage is crafted to ensure elevated levels of data durability, mitigating the risk of data loss caused by hardware failures or other potential issues.\nWhat are the use cases?\nPublic cloud environments often favor object storage, catering to various use cases such as cloud-native applications, content distribution, data archival, and beyond.\nRich media delivery: Managing and delivering rich media stands out as a major use case for object storage. Numerous applications leverage extensive object stores to store a variety of objects, including videos, photos, songs, and various file types.\nCloud data archival: Enterprises produce vast amounts of unstructured data, and object storage provides a cost-effective solution for archiving such data. The cloud has emerged as a preferred destination for enterprise archives. The immutability of objects aligns seamlessly with the archival use case, empowering enterprises to utilize cloud data archiving to meet regulatory requirements and governance mandates efficiently.\nDisaster recovery: Cloud data archival is gaining prominence in enterprise Business Continuity and Disaster Recovery (BCDR) planning. Employing object storage for cloud data backups not only controls backup costs but also ensures secure offsite storage for critical backup data. This approach holds the potential for swift recovery from outages by leveraging cloud resources.\nHow does Utho create an object storage solution tailored to meet your requirements? Utho provides accessible and scalable object storage solutions without straining your budget. Switching to Utho can result in savings of over 70%. Our offerings include robust data durability and security features such as data encryption. Embrace the next generation of storage technology with Utho.","what-are-the-benefits#\u003cstrong\u003eWhat are the benefits?\u003c/strong\u003e":"","what-does-the-term-refer-to#\u003cstrong\u003eWhat does the term refer to?\u003c/strong\u003e":"","what-is-the-functioning-mechanism#\u003cstrong\u003eWhat is the functioning mechanism?\u003c/strong\u003e":""},"title":"Object Storage: Gateway to Modern and Streamlined Data Management"},"/utho-docs/docs/object-storage-shaping-the-future-landscape-of-data-storage/":{"data":{"":"","what-are-the-advantages-it-offers-to-corporates#\u003cstrong\u003eWhat are the advantages it offers to\u003c/strong\u003e \u003cstrong\u003ecorporates?\u003c/strong\u003e":"","what-are-the-use-cases-and-examples-of-object-storage#\u003cstrong\u003eWhat are the use cases and examples of object storage?\u003c/strong\u003e":"","what-does-the-term-object-storage-refer-to#\u003cstrong\u003eWhat does the term \u0026ldquo;Object storage\u0026rdquo; refer to?\u003c/strong\u003e":"","what-lies-ahead-for-the-future-and-applications-of-object-storage#\u003cstrong\u003eWhat lies ahead for the future and applications of object storage?\u003c/strong\u003e":"Presently, numerous enterprises face challenges related to intricate and disorganized data storage, impeding their business expansion. Businesses are tasked with handling escalating data from diverse sources utilized across multiple applications and operational facets. In such scenarios, cloud object storage serves as a solution, offering a cost-effective data storage solution for diverse data types. It facilitates the storage of various objects such as video, audio, photos, static files, and more.\nWhat does the term ‚ÄúObject storage‚Äù refer to? Alternatively referred to as object-based storage, is an architectural framework for computer data storage specifically crafted to manage extensive volumes of unstructured data. Diverging from alternative architectures, it categorizes data into individual units, each accompanied by metadata and a distinctive identifier, facilitating the precise location and retrieval of each data unit.\nWhat are the advantages it offers to corporates? Leveraging object storage presents a multitude of primary advantages that cater to the dynamic and evolving landscape of modern data storage.\nMassive scalability: The flat architecture of object storage allows for seamless scaling without encountering the constraints faced by file or block storage. With this technology, scalability is virtually limitless, enabling data to expand to exabytes effortlessly by incorporating new devices.\nReduced complexity: Object storage eliminates the need for folders or directories, streamlining the system by removing the complexity associated with hierarchical structures. The absence of intricate trees or partitions simplifies file retrieval, as there is no requirement to know the precise location.\nSearchability: Metadata is integrated into objects, simplifying search and navigation without the necessity for a distinct application. This approach is notably more adaptable and customizable, allowing the tagging of objects with attributes and information, such as consumption, cost, and policies for automated processes like deletion, retention, and tiering.\nResiliency: Object storage has the capability to automatically replicate data, distributing it across various devices and geographical locations. This functionality serves to mitigate the impact of outages, fortify against data loss, and contribute to the implementation of effective disaster recovery strategies.\nCost efficiency: Object storage was designed with a focus on cost efficiency, offering storage for extensive data volumes at a more economical rate compared to file- and block-based systems. The cost structure of object storage is based on the actual capacity utilized, providing cost control benefits, particularly for substantial data storage requirements.\nWhat are the use cases and examples of object storage? It provides a diverse set of solutions that can be advantageous for an organization. Below are some typical examples and use cases.\nData archiving and backup: Object Storage is frequently employed for extended data retention due to its scalable capacity for storing substantial amounts of data and its high durability. This quality renders it well-suited for generating backups of critical data.\nMedia and entertainment: Object Storage is aptly designed for the storage and management of extensive sets of media files, including videos and music. Its capability to manage large file sizes, including 4K quality akin to Netflix standards, coupled with high data transfer rates, makes it particularly suitable for media file storage.\nIoT and sensor data: Object Storage is commonly employed for the storage and administration of the extensive data generated by Internet of Things (IoT) devices and sensors. Its proficiency in managing high data volumes and meeting the requirement for swift data access aligns well with the prevalent characteristics of IoT applications.\n**\nBig data and analytics:** Object Storage is ideally suited for the storage and administration of substantial volumes of unstructured data utilized in big data and analytics applications. Its scalability, enabling the storage of extensive data, coupled with its ability to provide rapid access when required for analysis, makes it well-matched for such applications.\nWhat lies ahead for the future and applications of object storage? The future prospects for the cloud Object Storage market appear optimistic. Factors such as the widespread adoption of cloud computing, the surge in unstructured data, and the demand for economical storage solutions contribute significantly to market expansion. Additionally, the anticipated surge in data from artificial intelligence (AI) and Internet of Things (IoT) technologies is poised to further propel the demand.\nHow has Utho‚Äôs cloud storage solution fueled its market growth amid rising demand? Utho‚Äôs cloud object storage solution provides scalable and resilient storage for unstructured data. The company has experienced substantial market expansion attributed to the growing demand for cloud storage solutions."},"title":"Object Storage: Shaping the Future Landscape of Data Storage"},"/utho-docs/docs/optimizing-sql-server-security-essential-best-practices/":{"data":{"":"","auditing#\u003cstrong\u003eAuditing\u003c/strong\u003e":"Every organization depends on its data, but often poorly protected databases are the reason for security breaches. This article explores the best ways to keep your SQL server secure and protect your data from intruders.\nData security focuses on three main things: keeping information private, making sure it‚Äôs accurate, and ensuring it‚Äôs available when needed. Let‚Äôs break down how to strengthen the security of your SQL Server, which is crucial in today‚Äôs database world.\nSQL Server Authentication Ensuring the security of data stored within SQL Server relies on the capability to authenticate access to designated datasets. In both Windows and Linux environments, SQL Server offers two authentication options:\nWindows/Linux Authentication\nSQL Server and Windows/Linux Authentication (commonly referred to as Mixed-mode)\nDuring the setup of SQL Server, you‚Äôll be prompted to choose one of these authentication modes.\nWindows or Linux Authentication Mode In this mode, when an installer accesses SQL Server, they use their Windows or Linux credentials. SQL Server then checks if the account name and password are valid through the Windows or Linux operating system. SQL Server doesn‚Äôt prompt for a password or handle the authentication process itself.\nWindows or Linux authentication relies on Active Directory (AD) accounts, which allow for centralized policy management. These policies cover things like password strength, expiration, account lockout, and group membership within Active Directory.\nWindows or Linux authentication is the default mode and provides higher security compared to SQL Server Authentication (which we‚Äôll discuss later). It uses the Kerberos security protocol to support these security features. A connection made using Windows or Linux authentication is often called a trusted connection because SQL Server trusts the credentials provided by the Windows or Linux operating system.\nSQL Server and Windows/Linux Authentication Mode (Mixed-Mode) When employing SQL Server Authentication, logins are established within SQL Server independently of Windows or Linux user accounts. SQL Server generates both the username and password, storing them internally. Users connecting via SQL Server Authentication must input their credentials (username and password) each time they access SQL Server.\nThis mode operates independently of the Windows or Linux Kerberos security protocol and is deemed less secure compared to Windows or Linux Authentication mode.\nSystem Administrator (SA) Account When using SQL Server with mixed-mode authentication, SQL Server automatically creates a System Administrator (SA) user login with full privileges. To enhance SQL Server security, follow these steps:\nRename the SA login to a less predictable name for added security.\nIf you won‚Äôt be using the SA account, consider disabling it entirely.\nChoose a strong and complex password for the SA (or renamed) account, including a mix of lowercase and uppercase letters, numbers, and special characters.\nMake sure that applications do not use the SA (or renamed) account in any part of the application connection string.\n**High-Privileged Operating System Accounts **\nTo operate, SQL Server requires a Windows or Linux account. Using high-privileged built-in accounts like Network Service or Local System for SQL Server services isn‚Äôt advisable. Unauthorized access to these accounts could lead to malicious activities in the database or server.\nAssign only the necessary security-level accounts for SQL Server services. Additionally, if there are high-privileged operating system accounts on the server hosting SQL Server that aren‚Äôt needed for operation, it‚Äôs best to disable them.\nRestrict SQL Traffic Database servers commonly receive connections from one or multiple servers. It‚Äôs imperative to restrict access to these servers exclusively to and from specified IP addresses. This measure serves to mitigate the risk of unauthorized access by malicious users.\nIn some scenarios, users of SQL Server may necessitate direct connections to the database. In such cases, it is recommended to confine SQL connections to the precise IP addresses (or, at the very least, IP class blocks or segments) that require access. This targeted approach enhances security by limiting connectivity to essential sources.\nIP restrictions can be administered using various solutions tailored to different platforms:\nOn Linux operating systems, traffic can be controlled using iptables. Additionally, alternatives such as UFW, nftables, and FirewallD are widely utilized.\nFor Microsoft platforms, utilize the Windows firewall or consider employing dedicated hardware firewalls.\nSQL Server Patches (Service Packs) Microsoft consistently releases SQL Server service packs and/or cumulative updates to address identified issues, bugs, and security vulnerabilities. It is strongly recommended to regularly apply SQL Server patching to production instances. However, prior to implementing a security patch on production systems, it is prudent to first apply these patches in a test environment. This step allows for the validation of patch changes and ensures that the database functions as intended under the updated conditions.\nBackups When managing SQL Server in production, it‚Äôs vital to set up a regular backup routine. A database backup essentially creates a copy of everything in the database, including its structure and data. These backups act as a safety net in case the database encounters problems like corruption, hardware failures, power outages, or disasters.\nBackups are also useful in scenarios where you need to roll back the database to a specific point in time, even when there‚Äôs no failure. It‚Äôs a good practice to do full database backups on a set schedule and incremental backups daily or at intervals throughout the day to ensure thorough coverage.\nSecuring your backups is crucial, but it‚Äôs an aspect that database professionals sometimes overlook. Key tasks include:\nRestricting access to backup files: Don‚Äôt give everyone in your organization full access rights (like creating, viewing, modifying, and deleting) to backup files.\nUsing strong encryption for backup files.\nStoring backups off-site: Depending on your organization‚Äôs policies and the importance of the database data, consider keeping backups of a certain age in secure off-site locations for safekeeping.\nAuditing Auditing is a critical part of SQL Server security. A dedicated database administrator or security team should regularly check the SQL Server auditing logs, paying close attention to any failed login attempts.\nSQL Server comes with built-in login auditing to keep an eye on all login accounts. These auditing tools carefully record incoming requests, noting both the username and the client‚Äôs IP address. By analyzing login failures, you can detect and address suspicious activities in the database. SQL Server audit logs can reveal various types of activity, such as:\nExtended Events: These provide essential data for monitoring and troubleshooting issues within SQL Server.\nSQL Trace: This is SQL Server‚Äôs built-in tool for monitoring and logging database activity. It allows you to see server activity, create filters for specific users, applications, or workstations, and even filter at the SQL command level.\nChange Data Capture (CDC): This records insertions, updates, and deletions in specific tables using a SQL Server agent.\nTriggers: These application-based triggers can be set up to track changes to existing records in designated tables.\nSQL Server-Level Audit Specifications: These specify which audit actions to monitor for the entire server or instance, including actions like table creation or server role modification.\nRegularly checking hardware and software firewall logs outside of SQL Server is also crucial to detect any attempts at unauthorized server access.\nProtecting SQL Server databases from security breaches is vital for organizations to safeguard their valuable data assets. By following best practices like robust authentication methods, careful management of system privileges, IP restrictions, regular patching, thorough backups, and vigilant auditing, organizations can strengthen their SQL Server environments against potential threats. It‚Äôs essential to stay proactive and diligent in security efforts to maintain a strong and resilient SQL Server infrastructure in today‚Äôs ever-changing digital landscape.","backups#\u003cstrong\u003eBackups\u003c/strong\u003e":"","high-privileged-operating-system-accounts#**High-Privileged Operating System Accounts":"","restrict-sql-traffic#\u003cstrong\u003eRestrict SQL Traffic\u003c/strong\u003e":"","sql-server-and-windowslinux-authentication-mode-mixed-mode#\u003cstrong\u003eSQL Server and Windows/Linux Authentication Mode (Mixed-Mode)\u003c/strong\u003e":"","sql-server-authentication#\u003cstrong\u003eSQL Server Authentication\u003c/strong\u003e":"","sql-server-patches-service-packs#\u003cstrong\u003eSQL Server Patches (Service Packs)\u003c/strong\u003e":"","system-administrator-sa-account#\u003cstrong\u003eSystem Administrator (SA) Account\u003c/strong\u003e":"","windows-or-linux-authentication-mode#\u003cstrong\u003eWindows or Linux Authentication Mode\u003c/strong\u003e":""},"title":"Optimizing SQL Server Security: Essential Best Practices"},"/utho-docs/docs/other/add-user-and-give-limited-permission-to-the-host-in-zabbix/":{"data":{"":"\nDescription\nIn this article we will learn how to add user and give limited permission to the host in Zabbix..\nwhat is Zabbix used for :- Zabbix is open source software for monitoring networks, servers, virtual machines (VMs), and cloud services, among other IT parts. Zabbix lets you keep an eye on things like network usage, CPU load, and disc space usage.\nFollow the below steps to Add user and give limited permission to the host in Zabbix..","step-1---add-user#Step 1 - Add user":"After the first installation, Zabbix only knows about two users. The ‚ÄúAdmin‚Äù user is a Zabbix ‚Äúsuperuser,‚Äù which means that they have full access. User ‚Äúguest‚Äù is a special default user. If a user doesn‚Äôt sign in, they‚Äôll be able to use Zabbix as a ‚Äúguest.‚Äù ‚Äúguest‚Äù has no rights to Zabbix objects by default.\nTo add a new user, navigate to Administration \u003e Users \u003e Users in the dropdown menu \u003e select ‚ÄúCreate User.‚Äù\nnavigate to Administration \u003e Users \u003e Users in the dropdown menu \u003e select \"Create User.\" Make sure to add your user to one of the existing groups, such as ‚ÄúTesting,‚Äù in the new user form.\nIn this pop-up, type the user‚Äôs email address. By default, a medium is always active, but you can set a time period for it to be active (see the Time period specification page for a description of the format). You can also change the severity levels where the medium will work, but for now, leave them all turned on. In the user properties, click Add, then click Save. The new user shows up in the list of users.","step-2---give-permission-for-limited-host#Step 2 - Give permission for limited host":"The following steps need to be taken in order to configure a host in the Zabbix frontend:\nNavigate to Configuration \u003e Hosts\nTo the right, click on ‚ÄúCreate Host‚Äù (or on the host name to edit an existing host). In the form, enter the host‚Äôs parameters.\nThe clone and complete clone buttons on an existing host can also be used to make a new host. When you select ‚Äúclone,‚Äù all host parameters and template links are kept (keeping all entities from those templates). Additionally, a full clone will keep things that are directly linked (applications, items, triggers, graphs, low-level discovery rules, and web scenarios).\nNote that the cloned version of a host will keep all template entities exactly the same as they were when they were first on the template. Any alterations that were made to those entities on the existing host level will not be cloned to the new host; instead, they will be the same as they were on the template. This includes updated item intervals, modified regular expressions, and prototypes added to the low-level discovery rule.\nNote: If you want to add this host to a particuler¬†host group, then you need to select an already existing host group as seen in the screenshot below. You may check out this screenshot for more information if you want to do this.\nNavigate to Configuration \u003e Hosts \u003e create host And now you can access your host that was added to a specific host group by going to configuration \u003e host group.¬†configuration \u003e host group After all, this is the last step to add limited host permission by user group, which means that users in that user group will only be able to access the hosts that you choose for them. follow the below steps.\nGo to Administrator \u003e user group \u003e select your user group \u003e select host permission To verify that everything you‚Äôve done is right, sign out of Zabbix and log in with your user and password, and then show the user the permissions you‚Äôve provided them. check the below screenshot.\nI really hope that you‚Äôve got all of those steps down. to Add user and give limited permission to the host in Zabbix.\nMust Read :- https://utho.com/docs/tutorial/how-to-install-zabbix-agent-on-centos-7/"},"title":"How to create user and give limited permission to the host in Zabbix"},"/utho-docs/docs/platform/activity-logs/":{"data":{"#":"Activity Logs Activity logs in the cloud refer to detailed records of actions and events that occur within a cloud environment. These logs capture activities such as resource provisioning, configuration changes, access attempts, and administrative actions. Activity logs are essential for auditing, compliance, troubleshooting, and security purposes. Activity logs in the cloud are essential for maintaining visibility, compliance, and security posture within your cloud environment. They enable organizations to monitor and analyze activities, detect unauthorized access, and respond to incidents effectively.\nActivity Logs offer several benefits: Detect unauthorized access Security Compliance Maintaining visibility Incremental Activity Logs Steps for approaching the Activity Logs: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform.\nOn L.H.S user will get an option of Activity Logs when scrolling down as shown in the snippet below. After that, a new page will open where the user will get all the details of the activities that have happened in their profile. Now on the homepage of Activity Logs user will get option for restoring and deleteing the Activity Logsshot as given in the snippet. THE END","activity-logs#Activity Logs":""},"title":"index"},"/utho-docs/docs/platform/auto-scaling/":{"data":{"auto-scaling#Auto Scaling":"","auto-scaling-offer-several-benefits#Auto Scaling offer several benefits:":"","steps-for-approaching-the-auto-scaling#Steps for approaching the Auto Scaling:":"Auto Scaling Auto scaling in cloud computing refers to the ability of a system to automatically adjust its computing resources based on workload changes. This is particularly useful in scenarios where the demand on the system fluctuates, such as web applications experiencing varying levels of traffic throughout the day.\nAuto Scaling offer several benefits: Monitoring Scaling Policies Scaling Actions Integration with Cloud Provider Services Cost Optimization Steps for approaching the Auto Scaling: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. On clicking on Auto Scaling button on the left hand side , it will redirect user to te homepage of auto scaling. After clicking on Auto Scaling it will redirect user to the homepage of auto scaling . Where user will get two option for creating auto scaling group. After clicking on create new, a new page will occur where user will choose the required details for there auto scalling. Select the DC Location as per the requirement. Select OS/Apps or select the stacks as per requirement. Now select the Plan Type as per the consumption. Now select the VPC network for your cluster. Now select the firewall for the server, it will keep the server away from the threats. Now Select LoadBalancer for your auto scaling group. Now provide the Instance Size:\nMin size Max size Desired size Here user can configure their auto scaling policy. Name Type Compare Value Adjust Period Cooldown Here user can configure their auto scaling schedules here: Name Desired Size Time zone Recurrence Day Time Start At Under server label user can provide the name for there auto scaling server. After filling all the required details click on Create Auto Scaling then auto scaling will be created. After clicking on create auto scaling it will redirect user to the homepage of Auto Scaling where user can see all then created auto scaling. Click on manage button to explore more about the Auto Scaling. "},"title":"index"},"/utho-docs/docs/platform/backups/":{"data":{"#":"Backups Backups in the cloud involve creating and maintaining copies of your data, applications, or entire systems in a remote cloud-based environment. Cloud backup solutions offer several advantages over traditional on-premises backup methods, including scalability, cost-effectiveness, and ease of management.\nBackups offer several benefits: Disaster Recovery Integration with Backup Software Cross-Region Replication Backupsshot-Based Backups Incremental Backups Automated Backup Policies Data Backup Steps for approaching the Backups : Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. On L.H.S user will get an option of Backups when scrolling down as shown in the snippet below. After that a new page will occur where user will get all the details of the his Backups he took during the process on different products like on Cloud instances we have option of Backups. Now on the homepage of Backups user wil get option for restoring and deleteing the Backups as given in the snippet. THE END","backups#Backups":""},"title":"index"},"/utho-docs/docs/platform/billings/":{"data":{"#":"Billings Billing in the cloud refers to the process of invoicing and charging for the usage of cloud computing services and resources. Billing in the cloud is a flexible and transparent process that allows customers to pay only for the resources they use, while providing visibility and control over their cloud spending.\nBillings offer several benefits: Cost Estimation Resource Pricing Billing Alerts Discounts/Coupon Steps for approaching the Billings: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Now on the L.H.S when scrolling down user will find an option of billings as shown in the snippet. After clicking on billings it will redirect user on the homepage of billings. Here in Add Funds tab , there are two options:\nAdding funds using different mode of payments.\nAdd credits with credit coupon. On clicking on the add funds button user will be able to add credits in his account.\nOn the other hand after applying the credit coupon and clicking on apply coupon the funds associated with the coupon will be automatically show in the available credit section.\nNow on navigating to Payment methods section, user will see an option for adding payment methods as shown below in the snippet. After clicking on add new card a tab of razorpay will occur where multiple payment methods are available.\nAnd after clicking on the paynow button it will follow the process and the credits added will be shown in Available credit section. Navigating to Invoice section , user can see all the invoices generated. NOTE: On every credit/coupon added user will get the receipt on e-mail too.\nTHE END","billings#Billings":""},"title":"index"},"/utho-docs/docs/platform/cloud-instances/":{"data":{"#":"Cloud Instances Cloud instances refer to virtual machines (VMs) or servers that are hosted in a cloud computing environment. Instead of running on physical hardware that‚Äôs located on-premises, these instances run on servers provided by a cloud service provider.\nCloud Instances offer several benefits: Scalability Flexibility Accessibility Reliability Pay-as-you-go pricing Steps for approaching the Cloud Instances : Visit on the link given below: This link will redirect you to the Dashboard after Login of the platform. Here you will get 2 options to manage the cloud instances tab.\nDeploy new (Dropdown)- To deploy a new instance\nLeft side menu tab Cloud instances- To deploy, view or manage the cloud instances.\nAfter selecting the left menu option it will redirect you to the cloud instances homepage.\nNow for creating a new cloud server click on create cloud server or deploy new.\nNow from Select DC location , select one required location where you want to deploy your server. In select OS/Apps choose any one from the given options:\nOperating System: Deploy server using available operating system on Utho platforms\nMarketplace: Deploy server using user‚Äôs private or globally available marketplaces teamplates\nStacks: Scripts to be executed after server deployment\nISO: Deploy server with Custom ISOs\nSnapshots: Deploy server with a snapshot of an existing or previosly existing server.\nBackups: Deploy server using the backup of an existing or previously existing server. NOTE: In the marketplace selected apps will be automatically assigned with the required operating system.\nNow select the Required Billing Cycle: Hourly\nWeekly\nMonthly\nOn selecting the billing cycle, the calculated price for that billing cycle can be seen in the Instance Quantity tab on the right-hand side From the Select Plan Type , choose the required plan from (Basic Plan, CPU Optimized, Memory Optimized).\nNow provide a password or ssh key for your cloud instance , If not provided then it will be auto generated from the backend with a random passcode. If required, choose VPC Network for the server or you create a new VPC by clicking on the highlighted button. After clicking on Add new VPC , it will redirect you to the page below. Fill all the required fields and click on Deploy VPC button. Then a new VPC will be created on the cloud instances tab.\nIf any additional features are required like auto backups or server management then click on the toggle button .\nThese add-on services are chargeable, which will affect the overall cost of your server. The pricing details can be viewed in the Instance Quantity tab on the right-hand side.\nEnter the server‚Äôs hostname in Server Hostname \u0026 Label. by default it will be auto generated.\nNow for deploy , Click on the Deploy Now button available in the Instance Quantity tab.\n18.After clicking on ‚ÄòDeploy‚Äô, except for hourly billing, a tab for payment will appear. An invoice slide will open upon clicking the ‚ÄòPay‚Äô button.\nWe will receive the various methods of payments for the product. After payment , payment confirmation message will be seen. Then it will redirect to the billings/invoice section on the dashboard.\nTHE END","cloud-instances#Cloud Instances":""},"title":"index"},"/utho-docs/docs/platform/container-registry/":{"data":{"#":"Container Registry A container registry is a repository for storing and managing container images. It‚Äôs a core component in modern cloud-native development and deployment workflows.\nContainer Registry offer several benefits: Scalability Cost-effectiveness Integration Storage and Organization Security Version Control Distribution Steps for approaching the Container Registry: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here on click on L.H.S tab , we will see the Container Registry. On clicking on the container registry it will redirect to the container registry home page as shown below: Now , Click on the Create Container Registry for creating repos. On click, It will redirect to the next page where we can start the process of craeting registry. Now, the user select the DC Location for which then need the container registry. Now the user will select the container registry type as per requirement from the dropdown as shown below. In this section user will provide a name for their registry container and then click on Create container button. After clicking on create container it will redirect us to the next page as shown below. Process involved after the creation of Container Registry. After the step (9) , the user will get 2 Registry :\nPublic Registry Private Registry In this the created registry will be stored according to the container registry type. Once the repository is created then click on edit icon shown in the snippet given below, With the help of that user can update there registry from public to private and vice-versa. Then a pop up will occur where user can see the update option. Click on the View Push Commands. A new slide window will open where Command references for pushisng an artifact to this project will be available as shown in the snippet below.\nAfter copying these commands user can easily push their item to the required place.\nBy clciking on this icon user can delete the repository. THE END","container-registry#Container Registry":""},"title":"index"},"/utho-docs/docs/platform/databases/mariadb/":{"data":{"#":"Databases (Maria DB) Databases in cloud servers refer to the storage and management of data within cloud computing environments. Traditionally, databases were hosted on physical servers maintained by organizations themselves. However, with the advent of cloud computing, databases can now be hosted on virtual servers provided by cloud service providers.\nDatabases offer several benefits: Scalability Cost-effectiveness Accessibility Reliability and redundancy Security Steps for approaching the Databases: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the cloud platform. Here we will get 2 options to reach the Databases tab.\nDeploy new (Dropdown) L.H.S tab After clicking it will redirect you to the database homepage. Now for creating a new database cluster click on create cluster or create database cluster. It will redirect you to the requirements and create cluster page. Where the user have to choose the data accordings to there requirements such as: Now from Select DC location , select one required location. Now moving forword we have to Select Database Cluster with there version which is required. In this user have to select which mode of billing cycle he/she prefer from the given options:\nHourly Monthly Yearly Now from the Select Plan Type , choose the required plan from the given options: Basic Plan CPU Optimized Memory Optimized If the user needs Set Number of Replica of his cluster , so that it can keep his website up even when his main wesite is facing issue. From here he/she can select maximum upto 3 Replica. VPC Network allows users to create and manage isolated networks within the cloud infrastructure. VPC networks enable users to deploy resources such as virtual machines (VMs), databases, and other cloud services securely and privately. In Add security group , user can create and select the firewalls for the server. User can name his cluster as per his perception in Name Your Cluster tab. After filling all the details then on clicking on the create cluster button. A new db cluster will be created and you will be automatically redirected to the homepage of database. After the cluster is created , its internal functionalities will be seen after clicking on the Manage Button as shown in the below snippet.\nAfter clicking on the manage a new screen will occur. Which contains various functionalities of Database stated below: After clicking on manage button from dashboard it will redirect to this page where user will get all the points mentioned below for in depth understanding of the product. Connection Details In cloud computing, ‚ÄúConnection Details‚Äù typically refer to the information required to establish connections between different components or services within a cloud environment. These details can include various parameters such as:\nEndpoint Addresses Port Numbers Protocol Authentication Credentials Security Settings Connection Timeout Settings Nodes With the help of Nodes we can add additional replica of our cluster in (postgre) anf for others here we can see and copy our IP‚Äôs either public or private. For both read only and edit too. Databases In databases user can create database and manage the permissions he want to provide to the reated database. Also user have the access to delete his created databases. On clciking on add database user can create a new database for him. After clicking on add database a new window will occur as shown in the snippet. And once the name if the database is filled \u0026 user clicked on add database a new database will be created as shown in the snippet. On clicking on Manage permissionn a side bar will occur where have to choose the database user and types of permission to give. After clicking on add permission the permission will be allocated to the database user. Also by cliciking on update you can update the access for database user as shown in the given snippet. 4. Users In this section user can create or add the user who can access that database for the work. Also owner have the access to delete his created user form databases by clicking on the delete button. On clicking on Add database user a side screen will occur where the you can create the users for your database as shown in the snippet below (2). After entering the name you have to click on add database user, then user will be added. Backups Backups are an essential aspect of maintaining data integrity and availability in cloud server environments. Cloud providers offer various backup solutions and features to help users protect their data from loss or corruption. User can add as many backups he want. With the help of backups user can save his cluster from any mishappening. On clicking on add new backup a backup will be created for the db. Firewall Firewalls in the cloud serve the same fundamental purpose as traditional network firewalls: to monitor and control incoming and outgoing network traffic based on predetermined security rules. However, in a cloud environment, firewalls are implemented and managed differently due to the distributed and dynamic nature of cloud computing. Here user can also add the firewall he needed. Here user can also add the firewall he needed. On clicking on add firewall a new sidebar will open where we have to choose the firewall created. Also on clicking on detach we can remove the firewall if not needed. Trusted Hosts In cloud computing, ‚Äútrusted hosts‚Äù typically refer to entities, such as servers, virtual machines, or services, that are deemed trustworthy within a cloud environment. These trusted hosts are often granted certain privileges or permissions based on their established trustworthiness. In this user can add the trusted IP address from where the server can get accessed. On clicking on Add IP user can add the new trusted ip address. Onclicking on detach user can remove that IP address. Destroy In destroy the user can destroy/delete his cluster permanently , which cannot be accessed again. Here , if the user didn‚Äôt need any of the cluster then user can destroy that cluster by clicking on destroy cluster button . After clicking on destroy button user will get a pop for the confirmation \u0026 warning. NOTE = Once the cluster is destroyed then in any case it will be not reterived. THE END","databases-maria-db#Databases (Maria DB)":""},"title":"index"},"/utho-docs/docs/platform/databases/mysql/":{"data":{"#":"Databases (MYSQL) Databases in cloud servers refer to the storage and management of data within cloud computing environments. Traditionally, databases were hosted on physical servers maintained by organizations themselves. However, with the advent of cloud computing, databases can now be hosted on virtual servers provided by cloud service providers.\nDatabases offer several benefits: Scalability Cost-effectiveness Accessibility Reliability and redundancy Security Steps for approaching the Databases: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the cloud platform. Here we will get 2 options to reach the Databases tab.\nDeploy new (Dropdown) L.H.S tab After clicking it will redirect user to the database homepage. Now for creating a new database cluster click on create cluster or create database cluster. It will redirect you to the requirements and create cluster page. Where the user have to choose the data accordings to there requirements such as: Now from Select DC location , select one required location. Now moving forword we have to Select Database Cluster with there version which is required. In this user have to select which mode of billing cycle he/she prefer from the given options:\nHourly Monthly Yearly Now from the Select Plan Type , choose the required plan from the given options: Basic Plan CPU Optimized Memory Optimized If the user needs Set Number of Replica of his cluster , so that it can keep his website up even when his main wesite is facing issue. From here he/she can select maximum upto 3 Replica. VPC Network allows users to create and manage isolated networks within the cloud infrastructure. VPC networks enable users to deploy resources such as virtual machines (VMs), databases, and other cloud services securely and privately. In Add security group , user can create and select the firewalls for the server. User can name his cluster as per his perception in Name Your Cluster tab. After filling all the details then on clicking on the create cluster button. A new db cluster will be created and you will be automatically redirected to the homepage of database. After the cluster is created , its internal functionalities will be seen after clicking on the Manage Button as shown in the below snippet. After clicking on the manage a new screen will occur. Which contains various functionalities of Database stated below: After clicking on manage button from dashboard it will redirect to this page where user will get all the points mentioned below for in depth understanding of the product. Connection Details In cloud computing, ‚ÄúConnection Details‚Äù typically refer to the information required to establish connections between different components or services within a cloud environment. These details can include various parameters such as:\nEndpoint Addresses Port Numbers Protocol Authentication Credentials Security Settings Connection Timeout Settings Nodes With the help of Nodes we can add additional replica of our cluster in (postgre) anf for others here we can see and copy our IP‚Äôs either public or private. For both read only and edit too. Databases In databases user can create database and manage the permissions he want to provide to the reated database. Also user have the access to delete his created databases. On clciking on add database user can create a new database for him. After clicking on add database a new window will occur as shown in the snippet. And once the name if the database is filled \u0026 user clicked on add database a new database will be created as shown in the snippet. On clicking on Manage permissionn a side bar will occur where have to choose the database user and types of permission to give. After clicking on add permission the permission will be allocated to the database user. Also by cliciking on update you can update the access for database user as shown in the given snippet. Users In this section user can create or add the user who can access that database for the work. Also ownwer have the access to delete his created user for databases. On clicking on Add database user a side screen will occur where the you can create the users for your database as shown in the snippet below (2). After entering the name you have to click on add database user, then user will be added. Backups Backups are an essential aspect of maintaining data integrity and availability in cloud server environments. Cloud providers offer various backup solutions and features to help users protect their data from loss or corruption. User can add as many backups he want. With the help of backups user can save his cluster from any mishappening. On clicking on add new backup a backup will be created for the db. Firewall Firewalls in the cloud serve the same fundamental purpose as traditional network firewalls: to monitor and control incoming and outgoing network traffic based on predetermined security rules. However, in a cloud environment, firewalls are implemented and managed differently due to the distributed and dynamic nature of cloud computing. Here user can also add the firewall he needed. On clicking on add firewall a new sidebar will open where we have to choose the firewall created. Also on clicking on detach we can remove the firewall if not needed. Trusted Hosts In cloud computing, ‚Äútrusted hosts‚Äù typically refer to entities, such as servers, virtual machines, or services, that are deemed trustworthy within a cloud environment. These trusted hosts are often granted certain privileges or permissions based on their established trustworthiness. In this user can add the trusted IP address from where the server can get accessed. On clicking on Add IP user can add the new trusted ip address. Onclicking on detach user can remove that IP address. Destroy In destroy the user can destroy/delete his cluster permanently , which cannot be accessed again. Here , if the user didn‚Äôt need any of the cluster then user can destroy that cluster by clicking on destroy cluster button . After clicking on destroy button user will get a pop for the confirmation \u0026 warning. NOTE = Once the cluster is destroyed then in any case it will be not reterived. THE END","databases-mysql#Databases (MYSQL)":""},"title":"index"},"/utho-docs/docs/platform/databases/postgre-sql/":{"data":{"#":"Databases (Postgre) Databases in cloud servers refer to the storage and management of data within cloud computing environments. Traditionally, databases were hosted on physical servers maintained by organizations themselves. However, with the advent of cloud computing, databases can now be hosted on virtual servers provided by cloud service providers.\nDatabases offer several benefits: Scalability Cost-effectiveness Accessibility Reliability and redundancy Security Steps for approaching the Databases: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the cloud platform. Here we will get 2 options to reach the Databases tab.\nDeploy new (Dropdown) L.H.S tab After clicking it will redirect you to the database homepage. Now for creating a new database cluster click on create cluster or create database cluster. It will redirect you to the requirements and create cluster page. Where the user have to choose the data accordings to there requirements such as: Now from Select DC location , select one required location. Now moving forword we have to Select Database Cluster with there version which is required. In this user have to select which mode of billing cycle he/she prefer from the given options:\nHourly Monthly Yearly Now from the Select Plan Type , choose the required plan from the given options: Basic Plan CPU Optimized Memory Optimized If the user needs Set Number of Replica of his cluster , so that it can keep his website up even when his main wesite is facing issue. From here he/she can select maximum upto 3 Replica. VPC Network allows users to create and manage isolated networks within the cloud infrastructure. VPC networks enable users to deploy resources such as virtual machines (VMs), databases, and other cloud services securely and privately. In Add security group , user can create and select the firewalls for the server. User can name his cluster as per his perception in Name Your Cluster tab. After filling all the details then on clicking on the create cluster button. A new db cluster will be created and you will be automatically redirected to the homepage of database. After the cluster is created , its internal functionalities will be seen after clicking on the Manage Button as shown in the below snippet.\nAfter clicking on the manage a new screen will occur. Which contains various functionalities of Database stated below: After clicking on manage button from dashboard it will redirect to this page where user will get all the points mentioned below for in depth understanding of the product. Connection Details In cloud computing, ‚ÄúConnection Details‚Äù typically refer to the information required to establish connections between different components or services within a cloud environment. These details can include various parameters such as:\nEndpoint Addresses Port Numbers Protocol Authentication Credentials Security Settings Connection Timeout Settings Nodes With the help of Nodes we can add additional replica of our cluster in (postgre) anf for others here we can see and copy our IP‚Äôs either public or private. For both read only and edit too. Databases In databases user can create database and manage the permissions he want to provide to the reated database. Also user have the access to delete his created databases. On clciking on add database user can create a new database for him. After clicking on add database a new window will occur as shown in the snippet. And once the name if the database is filled \u0026 user clicked on add database a new database will be created as shown in the snippet. On clicking on Manage permissionn a side bar will occur where have to choose the database user and types of permission to give. After clicking on add permission the permission will be allocated to the database user. Also by cliciking on update you can update the access for database user as shown in the given snippet. 4. Users In this section user can create or add the user who can access that database for the work. Also owner have the access to delete his created user form databases by clicking on the delete button. On clicking on Add database user a side screen will occur where the you can create the users for your database as shown in the snippet below (2). After entering the name you have to click on add database user, then user will be added. Connection Pool A connection pool in the context of cloud computing refers to a mechanism for efficiently managing and reusing database connections within cloud-based applications. It is a critical component for optimizing the performance and scalability of applications that interact with databases hosted in the cloud. In this, Connection pooling uses PgBouncer to manage backend processes. Each pool shares its assigned backend processes with up to 5000 client connections. We can create connection pool by clicking on the button marked below. Now fill the details marked below according to the requirement as shown in the snippet below. And after clicking on the create connection pool , pool will be generated as shown below. On clicking on edit button marked below user can edit there connection pool where as on clicking on connection details user can see all the connections of the network. Also on clicking on delete button the pool will be deleted. Backups Backups are an essential aspect of maintaining data integrity and availability in cloud server environments. Cloud providers offer various backup solutions and features to help users protect their data from loss or corruption. User can add as many backups he want. With the help of backups user can save his cluster from any mishappening. On clicking on add new backup a backup will be created for the db. Firewall Firewalls in the cloud serve the same fundamental purpose as traditional network firewalls: to monitor and control incoming and outgoing network traffic based on predetermined security rules. However, in a cloud environment, firewalls are implemented and managed differently due to the distributed and dynamic nature of cloud computing. Here user can also add the firewall he needed. Here user can also add the firewall he needed. On clicking on add firewall a new sidebar will open where we have to choose the firewall created. Also on clicking on detach we can remove the firewall if not needed. Trusted Hosts In cloud computing, ‚Äútrusted hosts‚Äù typically refer to entities, such as servers, virtual machines, or services, that are deemed trustworthy within a cloud environment. These trusted hosts are often granted certain privileges or permissions based on their established trustworthiness. In this user can add the trusted IP address from where the server can get accessed. On clicking on Add IP user can add the new trusted ip address. Onclicking on detach user can remove that IP address. Destroy In destroy the user can destroy/delete his cluster permanently , which cannot be accessed again. Here , if the user didn‚Äôt need any of the cluster then user can destroy that cluster by clicking on destroy cluster button . After clicking on destroy button user will get a pop for the confirmation \u0026 warning. NOTE = Once the cluster is destroyed then in any case it will be not reterived. THE END","databases-postgre#Databases (Postgre)":""},"title":"index"},"/utho-docs/docs/platform/databases/redis/":{"data":{"#":"Databases (Redis) Databases in cloud servers refer to the storage and management of data within cloud computing environments. Traditionally, databases were hosted on physical servers maintained by organizations themselves. However, with the advent of cloud computing, databases can now be hosted on virtual servers provided by cloud service providers.\nDatabases offer several benefits: Scalability Cost-effectiveness Accessibility Reliability and redundancy Security Steps for approaching the Databases: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the cloud platform. Here we will get 2 options to reach the Databases tab.\nDeploy new (Dropdown) L.H.S tab After clicking it will redirect you to the database homepage. Now for creating a new database cluster click on create cluster or create database cluster. It will redirect you to the requirements and create cluster page. Where the user have to choose the data accordings to there requirements such as: Now from Select DC location , select one required location. Now moving forword we have to Select Database Cluster with there version which is required. In this user have to select which mode of billing cycle he/she prefer from the given options:\nHourly Monthly Yearly Now from the Select Plan Type , choose the required plan from the given options: Basic Plan CPU Optimized Memory Optimized If the user needs Set Number of Replica of his cluster , so that it can keep his website up even when his main wesite is facing issue. From here he/she can select maximum upto 3 Replica. VPC Network allows users to create and manage isolated networks within the cloud infrastructure. VPC networks enable users to deploy resources such as virtual machines (VMs), databases, and other cloud services securely and privately. In Add security group , user can create and select the firewalls for the server. User can name his cluster as per his perception in Name Your Cluster tab. After filling all the details then on clicking on the create cluster button. A new db cluster will be created and you will be automatically redirected to the homepage of database. After the cluster is created , its internal functionalities will be seen after clicking on the Manage Button as shown in the below snippet. After clicking on manage button it will redirect us to the next interface shown below. Which contains various functionalities of Database stated below: After clicking on manage button from dashboard it will redirect to this page where user will get all the points mentioned below for in depth understanding of the product. Connection Details In cloud computing, ‚ÄúConnection Details‚Äù typically refer to the information required to establish connections between different components or services within a cloud environment. These details can include various parameters such as:\nEndpoint Addresses Port Numbers Protocol Authentication Credentials Security Settings Connection Timeout Settings Nodes With the help of Nodes we can add additional replica of our cluster in (postgre) anf for others here we can see and copy our IP‚Äôs either public or private. For both read only and edit too. Users In this section user can create or add the user who can access that database for the work. Also ownwer have the access to delete his created user for databases. On clicking on Add database user a side screen will occur where the you can create the users for your database as shown in the snippet below (2). After entering the name you have to click on add database user, then user will be added. Backups Backups are an essential aspect of maintaining data integrity and availability in cloud server environments. Cloud providers offer various backup solutions and features to help users protect their data from loss or corruption. User can add as many backups he want. With the help of backups user can save his cluster from any mishappening. On clicking on add new backup a backup will be created for the db. Firewall Firewalls in the cloud serve the same fundamental purpose as traditional network firewalls: to monitor and control incoming and outgoing network traffic based on predetermined security rules. However, in a cloud environment, firewalls are implemented and managed differently due to the distributed and dynamic nature of cloud computing. Here user can also add the firewall he needed. Here user can also add the firewall he needed. On clicking on add firewall a new sidebar will open where we have to choose the firewall created. Also on clicking on detach we can remove the firewall if not needed. Trusted Hosts In cloud computing, ‚Äútrusted hosts‚Äù typically refer to entities, such as servers, virtual machines, or services, that are deemed trustworthy within a cloud environment. These trusted hosts are often granted certain privileges or permissions based on their established trustworthiness. In this user can add the trusted IP address from where the server can get accessed. On clicking on Add IP user can add the new trusted ip address. Onclicking on detach user can remove that IP address. Destroy In destroy the user can destroy/delete his cluster permanently , which cannot be accessed again. Here , if the user didn‚Äôt need any of the cluster then user can destroy that cluster by clicking on destroy cluster button . After clicking on destroy button user will get a pop for the confirmation \u0026 warning. NOTE = Once the cluster is destroyed then in any case it will be not reterived. THE END","databases-redis#Databases (Redis)":""},"title":"index"},"/utho-docs/docs/platform/deploy-a-new-server-with-snapshot/":{"data":{"":"\nSnapshot is a feature in microhost cloud platform that is an instant ‚Äúpicture‚Äù of the file system of your server at a certain time period or we can say Cloning of a server of a particular time frame.\nA new server can be deployed with the snapshot. Sometimes, you need to create a server with a copy of another server, We are sharing with you the steps to create a server with snapshot. To know more about snapshot please Click here -\n1. At first, you need to login Microhost account\n2. Click on ‚ÄúDeploy a new cloud server‚Äù. First option is to select location, You need to select the same location at which your snapshot server is created.¬†Please note that - Both server must be at the same location.\n3. After selecting the size of server click on ‚Äúdeploy Cloud server‚Äù.\n4. Server will be created.\nThankyou."},"title":"Deploy a new server with snapshot"},"/utho-docs/docs/platform/dns-management/":{"data":{"":"\nDNS management is a free service offered to all the customers from¬†Microhost. You can use this functionality in terms of DNS Server which¬†will be using for pointing the DNS records as per your requirement. You¬†can add single as well as multiple domains in it. We will see, how we¬†can use this feature.¬†PREREQUISITE : Before we begin to Dns Management portion, First¬†Update the name server ns1.microhost.com \u0026 ns2.microhost.com from Domain control panel¬†.\nStep 1: First, we have to Login into Microhost Cloud Dashboard¬†Step 2: After Successful login, output will be shown as like given¬†screenshot. We have to move onto ‚ÄúManaged Dns‚Äù section then We¬†have to click on ‚ÄúAdd New Domain‚Äù Button.\nStep 3: We have to enter the domain name for which we want to use¬†the Dns Server. Afterward we will move to the next page, where we can add multiple record as per the requirement.\nStep 4: We can add records while putting the required details in the¬†given fields. Along with that, we can delete the added records while¬†clicking on the delete button as given in the screenshot.\nThankyou."},"title":"DNS Management"},"/utho-docs/docs/platform/firewall/":{"data":{"#":"Firewall Firewalls in the cloud function similarly to traditional network firewalls but are specifically designed to secure cloud-based infrastructure and applications. Cloud firewalls provide essential security controls to protect virtual networks, resources, and data within cloud environments. Cloud firewalls are essential components of cloud security architectures, helping organizations enforce security policies, control access to resources, and protect against a wide range of threats. They play a critical role in ensuring the security and compliance of cloud-based infrastructure and applications.\nFirewall offer several benefits: Managed Firewall Services Web Application Firewalls (WAF) Network Security Groups (NSGs) Security Groups Steps for approaching the Firewall: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here we will get 2 options to reach the Firewall tab.\nDeploy new (Dropdown) L.H.S tab After clicking it will redirect you to the Firewall homepage. Here user will get two options for creating the cloud firewall as shown in the snippet below. After clicking on this it will redirect user to the page where user can select/input the requirements for his firewall. In this page user will provide the name for his firewall and click on the Deploy firewall button. After deploying it will redirect user to the firewall management page where all deployed firewalls will be available as shown in the snippet below. Now on clicking on the manage button it will redirect user to the next page. On next page under Rules section user can create firewall for both Incoming rules and Outgoing rules. Adding firewall for Incoming. Here user have to fill the various details such as:\nServices Protocol Port Range Sources After selecting all the details user should click on Add New. Then a new custom rule will be applied to the firewall. Also on clicking on the delete button user can delete the applied rule. Adding firewall for Outgoing. Here user have to fill the various details such as: Services Protocol Port Range Sources After selecting all the details user should click on Add New. Then a new custom rule will be applied to the firewall. Also on clicking on the delete button user can delete the applied rule. Now moving to servers tab, In this clicking on the dropdown of add server and select the required server. Click on add server to attach that server to the firewall. After the firewall is attached with the cloud an option will occur to view cloud on clicking on that it will redirect the user to the cloud instance page where the firewall is attached. Also if the firewall is not attached wit any cloud then it will show option for creating the new cloud instances. Also if the server is not required then on clicking on delete button user can delete the server. Now moving to the Destroy tab, Here if there is no need of the firewall then on clicking on destroy button user can delete that firewall. Once deleted it can not be recovered. After deleting the firewall it wil redirect user to the firewall homepage.\nTHE END","firewall#Firewall":""},"title":"index"},"/utho-docs/docs/platform/how-to-access-a-server-through-password-less-authentication/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to access a server through password-less authentication.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to access a server through password-less authentication.\nSSH, also known as Secure Shell or Secure Socket Shell, is a network protocol that gives users, particularly system administrators, a secure way to access a computer over an unsecured network. SSH also refers to the suite of utilities that implement the SSH protocol. Secure Shell provides strong password authentication and public key authentication, as well as encrypted data communications between two computers connecting over an open network, such as the internet.\nStep 1. Login server via putty.\nStep 2. Generate SSH key at your server.\n# ssh-keygen Step 3. Enter file in which to save the key (/root/.ssh/id_rsa): press enter\nStep 4. Enter passphrase (empty for no passphrase): press enter\nStep 5. Enter same passphrase again: press enter\nNow your SSH key will be created on .ssh folder with name id_rsa.pub\nStep 6. Use the following command to go to the.ssh directory.\n# cd .ssh Step 7. Use this command to display the content of a folder.\n# cat id_rsa.pub [caption id=‚Äúattachment_9778‚Äù align=‚Äúalignleft‚Äù width=‚Äú641‚Äù] SSH KEY Command Output[/caption]\nStep 8. You should now enter your login information for your Microhost account and head over to the SSH key area.\nStep 9. To import an SSH key, select it and click Import.\nStep 10. At this point, you need to paste the SSH key shown in step no.7 and specify the name.\nStep 11. With the SSH key we just imported into our microhost account, we can now set up a new server.\nStep 12. Login to your existing server and run the following command:\n# ll # cat id\\_rsa Step 13. Save this private key in the file ‚Äútest.ppk‚Äù on your desktop. which is shown on the previous screenshot.\nStep 14. Now open the putty key generator and click on conversions. then select the import key.\nStep 15. Choose the test.ppk file, open it, and click ‚ÄúSave Private Key.‚Äù Then press OK.\nStep 16. Save the file on your desktop with the name test1.ppk\nStep 17. Open putty Click on ‚ÄúAuth‚Äù under the ‚ÄúSSH‚Äù option, browse to the ‚Äútest1.ppk‚Äù file, then click ‚ÄúOpen.‚Äù\nStep 18. Click on sessions. Write down your new server IP, which is deployed in step no. 11. then select open.¬†Step 19. Write ‚Äúroot‚Äù in your terminal and press Enter to see access to your server without a password."},"title":"How to access a server through password-less authentication"},"/utho-docs/docs/platform/how-to-add-additional-storage-in-the-cloud/":{"data":{"":"\nLogin to Microhost Cloud Dashboard Select the cloud server in which you want to add additional storage and then click on ‚ÄúActions‚Äù button. 3. When you click on ‚ÄúManage cloud‚Äù option , A window will appear shown in the image below. You need to select ‚ÄúStorage‚Äù option and then fill the select size in GB and then click on ‚ÄúAdd storage‚Äù.\n4. Additional storage will be added and you can check added additional storage by following process-\nIf you are using cloud server with windows operating system then you need to login in the server and click on the server manager -\u003e computer management -\u003e\nClick on Disk management.\nHowever, for Windows operating system . We will follow below steps for making additional disk online.\nTake the cursor on ‚Äúofline‚Äù and then right click on that.\nWhile clicking on that, output will be visible as below.\nWhile clicking on online the ‚Äúdisk‚Äù it would be online. However, we have to initialize the disk for creating the volume. Have a look on the below screenshot.\nAfter Initializing the disk, we have to format the disk first. Please see the below screenshot for reference.\nSelect the MBR option and then click on OK. Afterward , we can add a new volume drive by following steps. Click on ‚ÄúNew Simple Volume‚Äù .\nWhile clicking on ‚ÄúNew Simple Volume‚Äù a prompt will be shown like below.\nClick on Next as per the screenshot . In this section you can select the size of the partition. by default it will take complete space of drive.\nAfterward click on next to move further. In this section we can assign the name of new volume. Now click on next.\nIn this section we have format the disk with NTFS file system and then click on next.\nAfterward, we have to finish the process while clicking on finish.\nNow , A new drive will be visible in drive section of the system as per the given screenshot.\nIf you are using cloud server with linux operating system, then you need to follow these below mentioned steps.\nfdisk -l :‚Äì To list the all drives.\nfdisk /dev/vdb\nmount /dev/vdb1 /new/\nresize2fs /dev/vdb1\nmkfs.ext4 /dev/vdb1\nmount /dev/vdb1 /new/\nTHANK YOU :)"},"title":"How to add additional storage in the Microhost Cloud Server"},"/utho-docs/docs/platform/how-to-check-bandwidth-consumption-in-microhost-panel/":{"data":{"":" How to check Bandwidth consumption in Microhost panel\nStep 1. Login to Microhost Cloud Dashboard\nStep 2. Select the cloud server in which you want to add additional storage and then click on ‚ÄúActions‚Äù button.\nManage a cloud\nStep 3. When you click on ‚ÄúManage cloud‚Äù option , A window will appear shown in the image below. You need to select the ‚ÄúAnalysis‚Äù option (By default its already on Analysis) . You can examine your CPU, Memory ,read/write ,traffic and Bandwidth utilization of the server with the help of graph .\nOverview of server\nStep 4. When you scroll down , the last graph is of ‚ÄúDaily Public Bandwidth usages ‚Äù of the current month . You can see your input/output bandwidth day wise by just moving the cursor as shown in the below screenshot.\nThank You!!"},"title":"How to check Bandwidth consumption in Microhost panel"},"/utho-docs/docs/platform/how-to-deploy-a-cloud-server-with-custom-iso/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to deploy a cloud server with custom ISO.\nAlso read: How to take snapshot of a Microhost Cloud Server\nThankyou.","introduction#Introduction":"In this article, you will learn how to deploy a cloud server with custom ISO.\nWhy do we require custom ISO ?\nTo download and use the Custom/specific OS according to the client requirements .\nA Custom ISO file is an archive file that contains an identical copy (or image) of data found on an optical disc, like a CD or DVD. For any custom version of OS , you can download ISO file of that OS and Deploy the server with that file.\n1. At first, you need to login Microhost Cloud Dashboard\n2. Select the ISO section in Dashboard.\n3. You can download ISO file here . Choose the data center location , ISO file link and name of the server and click on Add ISO.\nNOTE : DOWNLOAD ISO FILE/LINK FROM THE OFFICIAL WEBSITE ONLY\nISO is downloaded and available .\n4. Now, create a new server and choose ISO section and that custom ISO file while deploying the cloud server .\nNOTE : DEPLOY THE SERVER ON THE SAME LOCATION AS ISO¬†5. Select cloud size ,options and enter host name of the server .¬†6. Click on deploy cloud server to deploy and mount the ISO .\n7. Server is in running status now :\n8. Now click on console and install the OS from the Cloud Console .\nConsole view : Here we are going to install centos 7 , You can install OS as per your requirement.\n9. After the full installation of OS, click on ISO section in the manage cloud and unmount the ISO by clicking on the unmount ISO\nServer deployed with Custom ISO ."},"title":"How to deploy a cloud server with custom ISO"},"/utho-docs/docs/platform/how-to-destroy-microhost-cloud-server/":{"data":{"":"","click-on-the-cloud-servers-option-present-on-the-top-left-#Click on the Cloud Servers option present on the Top left .":"","copy-and-paste-the-required-string-as-shown-below-and-click-on-the-destroy-server-button#Copy and paste the required string as shown below and click on the Destroy Server button.":"NOTE : Please make sure that you have data/backup of the server before destroying your server as it could not be restored again¬†.\nServer has been deleted.\nThankyou.","select-the-server-you-want-to-deletedestroy-by-clicking-on-the-manage--destroy-cloud-options-#Select the server you want to delete/destroy by clicking on the Manage \u0026lt; Destroy Cloud options .":"\nAt first, you need to login Microhost Cloud Dashboard Select the cloud server that you want to delete. Click on ‚Äúactions‚Äù button, Afterward click on ‚ÄúDestroy Cloud‚Äù option as shown in screenshot given below. Click on the Cloud Servers option present on the Top left . Select the server you want to delete/destroy by clicking on the Manage \u003c Destroy Cloud options . "},"title":"How to destroy MicroHost cloud server"},"/utho-docs/docs/platform/how-to-enable-weekly-backup-in-microhost-cloud-server/":{"data":{"":"\nWeekly backup is a system level backup in which an image copy of complete server is created. You can not retrieve a single file from the weekly backup. To retrieve a single file from backup you need to create a new server and then retrieve the file from server.\n1. At first, you need to login Microhost Cloud Dashboard\n2. Select the cloud server for which you want to enable backup.\n3. When you click on ‚ÄúManage cloud‚Äù option , A window will appear shown in the image below. You need to select ‚ÄúBackup‚Äù option and then click on ‚ÄúEnable Backup‚Äù.\n4. Backup will be created.\nThankyou"},"title":"How to enable weekly backup in Microhost Cloud server"},"/utho-docs/docs/platform/how-to-install-wine-on-rhel-8/":{"data":{"":" How to install Wine on RHEL 8","description#Description":"In this article we will learn how to install¬†wine¬†on RHEL 8.Starting with the very first step. Wine is a compatibility layer that was developed for multiple POSIX-based operating systems, such as Linux, Macintosh, and BSD, to enable these systems to run Windows-based software. Wine is available for free and is open source. Wine is a programme that, in its most basic form, automatically translates Windows API calls into POSIX calls. This eliminates the speed and memory penalties that are associated with using other methods and enables you to integrate Windows applications onto your desktop in an uncluttered manner. It is simple to understand and not too difficult to put into practise. Also, the installation process is relatively straightforward in virtually all of the well-known Linux variants.","step-1-update-server#Step 1: Update Server":" yum update ","step-2-install-epel-release-repository#Step 2: Install EPEL Release Repository":"You will only be able to obtain the wine package through the EPEL repository. In order to install and activate this repository, you will need to use the yum install epel-release command, as demonstrated further below.\nyum install epel-release ","step-3-install-wine-package#Step 3: Install Wine package":"With the yum install wine command, which will be demonstrated further down, you will be able to install the wine package from the EPEL repository. The package, as well as all of its dependencies, will be downloaded and installed as a result of this action.\nyum install wine ","step-4-verify-installation-of-package#Step 4: Verify Installation of package":"After the installation has been completed successfully, you will be able to validate all of the wine-related packages that have been installed by querying the rpm database using the rpm -qa | grep -i wine command, as will be demonstrated below.\nrpm -qa | grep -i wine ","step-5-check-version-of-package#Step 5: Check Version of package":"Using the wine ‚Äîversion command, as seen below, is another option for determining the currently installed version of wine.\nwine --version I really hope that you‚Äôve got all of those steps down for how to install¬†wine¬†on¬†RHEL 8"},"title":"How to install Wine on RHEL 8"},"/utho-docs/docs/platform/how-to-rebuild-microhost-cloud-server/":{"data":{"":"\nRebuilding a server means creating a server with¬†clean OS¬†installation and it will erase all current data from server. It is just like formating a server with same configuration.\nPlease Use¬†this¬†feature only¬†if¬†you¬†want¬†to¬†start your cloud server¬†all¬†with¬†clean OS¬†installation.¬†We are sharing with you the steps to rebuild a cloud server -\nLogin to Microhost Cloud Dashboard Click on cloud server and select the server that you want to rebuild. Click on Manage cloud -\u003e Rebuild Select the OS in which you want to rebuild Fill the text that ‚Äú I want delete my data from current server and build fresh server\" Click on ‚ÄúRebuild cloud server‚Äù. After rebuild server login details will be sent on registered email id.\nThankyou."},"title":"How to rebuild Microhost Cloud Server"},"/utho-docs/docs/platform/how-to-resize-upgrade-downgrade-cloud-server/":{"data":{"":" Resize Microhost Cloud instance\n1. First, You need to login¬†Microhost Cloud Dashboard.\n2. Select the cloud server that you want to upgrade.\n3. When you click on ‚ÄúManage‚Äù option , A window will appear shown in the image below. You need to select ‚ÄúResize‚Äù\n4. After getting above image you need to select the desired configuration, Suppose your requirement is 4vCPUs and 8GB RAM, then scroll down and select Resize cloud server.\n5. After Resize, your server will upgraded.\n6. After upgrade the cloud server, you can Downgrade also. First you need to select the configuration and scroll down and click on Resize Cloud server.\n7. After the resize cloud server, your server will be downgraded to your selected plan.\nThank you!!"},"title":"How to resize (upgrade/downgrade) cloud server."},"/utho-docs/docs/platform/how-to-take-snapshot-of-a-microhost-server/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned take snapshot of a Microhost Cloud Server.\nAlso read: How to deploy a cloud server with custom ISO\nThankyou.","in-this-article-you-will-learn-how-to-take-snapshot-of-a-microhost-cloud-server#\u003cstrong\u003eIn this article, you will learn how to take snapshot of a Microhost Cloud Server.\u003c/strong\u003e":"\nIntroduction In this article, you will learn how to take snapshot of a Microhost Cloud Server. Snapshot is a feature in microhost cloud platform that is an instant ‚Äúpicture‚Äù of the file system of your server at a certain time period or we can say Cloning of a server of a particular time frame. Snapshot images use minimal storage space.\n1. At first, You need to login to Microhost Cloud Dashboard - 2. Select your server of which you want to take snapshot and click on manage cloud.\n3. When you click on ‚ÄúManage cloud‚Äù option , A window will appear shown in the image below. You need to select ‚Äúsnapshot‚Äù option and then click on ‚ÄúTake live snapshot‚Äù.\n4. A pop-up will appear asking for extra charges for snapshot.\nClick OK and snapshot will be created.\n5. You can check created snapshots by this option-","introduction#Introduction":""},"title":"How to take snapshot of a Cloud Server"},"/utho-docs/docs/platform/kubernetes/kubernetes-guide/":{"data":{"#":"Kubernetes Navigating Kubernetes: A Step-by-Step Approach Step 1: Click on the provided link Console URL. This will direct you to the Kubernetes platform after logging in to the platform.\nStep 2: Here, you will find two options to navigate to Kubernetes, as indicated below:\nStep 3: Clicking on the button mentioned above will redirect you to the Kubernetes homepage.\nStep 4: Next, by clicking on the deploy button, the process of deploying the cluster will start.\nStep 5: Now, choose one required location from the ‚ÄúSelect DC location‚Äù dropdown menu.\nStep 6: Here, users can select the version and label for their cluster according to their requirements.\nStep 7: In this section, users can manage their node worker pools and add additional node pools if necessary. By selecting the node size, users can choose the required configuration. If there is no need for a node pool, users can delete the extra nodes by clicking on ‚ÄúDelete pool‚Äù.\nStep 8: In the Scaling Policy section, users can add a policy by selecting the data according to their requirements. Additionally, users have the option to delete the policy from this section.\nStep 9: In this step, users can select a load balancer for auto-scaling groups.\nStep 10: In this step, users can select the Virtual Private Cloud (VPC) for their cluster.\nStep 11: In this step, users can add security groups or firewalls for their server.\nStep 12: This marks the final step for deploying the cluster. Click on ‚ÄúDeploy Cluster‚Äù to proceed.\nOnce the cluster is deployed, you will be automatically directed to the next page where the installation of Kubernetes will occur. The status will change from pending to Active once the installation process is complete.\nSteps involved after clicking on manage section:- K8S Nodes In this step, users can add an additional node pool. They can also edit the available nodes by clicking on ‚ÄúOpen Scale Pool,‚Äù filling out the required data, and then clicking on ‚ÄúUpdate‚Äù to retrieve the updated data. VPC A Virtual Private Network (VPC) provides a secure environment for business-critical applications and data. Users can create and configure a VPC to enhance security. Firewall Firewalls in the cloud are security mechanisms used to control and monitor incoming and outgoing network traffic. Users can add as many firewalls as needed for their cluster and delete unnecessary ones by clicking on ‚ÄúDetach.‚Äù LoadBalancer Load balancers distribute incoming network traffic across multiple servers to ensure high availability and scalability of applications. Users can use load balancers to distribute loads on servers. Destroy ‚ÄúDestroy‚Äù refers to permanently deleting or terminating a cloud resource. Users can delete the cluster if it‚Äôs no longer needed by clicking on ‚ÄúDestroy Cluster.‚Äù THE END","kubernetes#Kubernetes":""},"title":"index"},"/utho-docs/docs/platform/kubernetes/what-and-why-kubernetes/":{"data":{"#":"Kubernetes Kubernetes, often abbreviated as K8s for its abbreviation pattern (K-eight-s), is an open-source platform designed to oversee containerized applications in diverse environments, including private, public, and hybrid clouds. It‚Äôs widely utilized for managing microservice architectures and can be deployed across various cloud providers.\nKubernetes empowers application developers, IT system administrators, and DevOps engineers to automate the deployment, scaling, maintenance, scheduling, and operation of multiple application containers across node clusters. While containers share a common operating system (OS) on host machines, they remain isolated from each other by default, unless explicitly connected by a user.\nWhy you need Kubernetes and what it can do Containers offer a convenient way to package and execute applications. However, in a production setup, managing these containers and ensuring continuous operation is crucial. For instance, when a container crashes, another should swiftly take its place. Wouldn‚Äôt it be convenient if this process were automated?\nEnter Kubernetes! Kubernetes offers a robust framework for running distributed systems with resilience. It handles tasks like scaling, failover management, and deploying patterns for your applications. For instance, Kubernetes effortlessly orchestrates canary deployments, enabling seamless updates for your system.\nKubernetes offer several benefits: Service Discovery and Load Balancing: Easily expose containers using DNS names or IP addresses, and Kubernetes efficiently distributes network traffic to maintain deployment stability.\nStorage Orchestration: Automatically mount storage systems of your choice, including local storage and various public cloud providers.\nAutomated Rollouts and Rollbacks: Describe your container‚Äôs desired state, and Kubernetes smoothly transitions it to the specified state at a controlled pace. This includes creating new containers, removing existing ones, and transferring resources seamlessly.\nAutomatic Bin Packing: Specify CPU and memory requirements for containers, and Kubernetes optimizes resource usage by efficiently allocating containers to nodes within your cluster.\nSelf-Healing: Kubernetes ensures the reliability of your containers by automatically restarting or replacing failed containers and handling unresponsive ones, ensuring they‚Äôre ready to serve before being advertised to clients.\nSecret and Configuration Management: Securely store and manage sensitive information like passwords and tokens, deploy and update secrets and configurations without rebuilding container images, and maintain confidentiality in your stack configuration.\nBatch Execution: Kubernetes effectively manages batch and CI workloads, replacing failed containers as needed.\nHorizontal Scaling: Easily scale your application up or down using simple commands, a user-friendly interface, or automatic scaling based on CPU usage.\nIPv4/IPv6 Dual-Stack: Allocate both IPv4 and IPv6 addresses to Pods and Services, ensuring compatibility across different network protocols.\nExtensibility: Extend the capabilities of your Kubernetes cluster by adding features without the need to modify the upstream source code.\nHow Kubernetes works Kubernetes simplifies managing a cluster of computing instances by handling container deployment and scaling. Containers are organized into logical groups called pods, allowing you to run and scale one or multiple containers together. The Kubernetes control plane software takes care of crucial tasks like determining when and where to deploy your pods, managing traffic routing, and adjusting pod scaling based on resource utilization or other metrics you specify. It automatically initiates pods on your cluster according to their resource needs and restarts them if either the pods or the instances they run on encounter failures. Each pod receives its own IP address and a unique DNS name, which Kubernetes utilizes for connecting your services internally and routing external traffic. This streamlined approach ensures seamless communication and management within your Kubernetes environment.\nKubernetes Advantages The Kubernetes platform offers several key advantages that have contributed to its widespread adoption:\nPortability: Containers are easily transportable across various environments, from virtual setups to bare metal. Since Kubernetes is supported on major public clouds, you can run containerized applications on Kubernetes in diverse environments.\nIntegration and Extensibility: Kubernetes seamlessly integrates with existing solutions such as logging, monitoring, and alerting services. Additionally, the Kubernetes community actively develops open-source solutions that complement Kubernetes, resulting in a robust and rapidly expanding ecosystem.\nCost Efficiency: Kubernetes optimizes resource usage, automates scaling, and provides flexibility to run workloads where they‚Äôre most beneficial, giving you control over your IT spending. Scalability: Cloud-native applications scale horizontally, and Kubernetes employs auto-scaling mechanisms to dynamically spin up additional container instances and scale out in response to demand.\nAPI-Based: Kubernetes operates on a REST API foundation, enabling comprehensive control over the Kubernetes environment through programming.\nSimplified CI/CD: Kubernetes facilitates Continuous Integration and Continuous Deployment (CI/CD) practices, automating the building, testing, and deployment of applications to production environments. Enterprises are integrating Kubernetes with CI/CD to establish scalable pipelines that dynamically adapt to workload demands.\nCommon Kubernetes terms Here are some key Kubernetes terms that can help you understand how Kubernetes operates:\nCluster: The core of the Kubernetes engine, where containerized applications run. It consists of a group of machines that manage and execute applications.\nNode: Worker machines within a cluster that execute tasks.\nPod: Collections of containers deployed together on the same host machine to perform related tasks.\nReplication Controller: An abstraction used to manage the lifecycle of pods, ensuring a specified number of pod replicas are running at any given time.\nSelector: A system used to find and categorize specific resources based on predefined criteria.\nLabel: Key-value pairs assigned to resources for filtering, organizing, and performing operations on resource sets.\nAnnotation: Similar to labels but with a larger data capacity, annotations provide additional information about resources.\nIngress: An API object that manages external access to services within a cluster, typically for HTTP traffic. It facilitates name-based virtual hosting, load balancing, and Secure Sockets Layer (SSL) termination.\nTHE END","kubernetes#Kubernetes":""},"title":"index"},"/utho-docs/docs/platform/loadbalancer/":{"data":{"#":"","loadbalancer#LoadBalancer":"","steps-for-approaching-the-loadbalancer#Steps for approaching the LoadBalancer:":"LoadBalancer Load balancing means spreading out incoming network traffic across a bunch of backend servers or resources.\nLoad Balancer works at layer 4 of the OSI model. Think of it as the middleman between clients and servers. When traffic comes in, the load balancer sends it to different backend servers based on rules you set up, like how healthy each server is. These backend servers could be Utho Virtual Machines or part of a Virtual Machine Scale Set.\nWhy use Load Balancer? With Load Balancer, you can easily scale your applications and ensure they stay available. It supports both inbound and outbound traffic scenarios, offering fast response times and high data transfer rates. Plus, it can handle millions of connections for all TCP and UDP applications.\nHere are some key things you can do with Load Balancer:\nBalance both internal and external traffic to Utho virtual machines. Improve reliability by spreading resources across different zones. Set up outbound connections for virtual machines. Monitor the health of your balanced resources using health probes. Access virtual machines in your network through public IP addresses and ports using port forwarding. Support IPv6 for load balancing. Balance services across multiple ports, IP addresses or both. Types of Load Balancers ‚Äì Based on Functions Various load-balancing techniques exist to tackle specific network issues.\nNetwork Load Balancer (Layer 4 - L4):\nNetwork Load Balancers distribute traffic based on network variables like IP addresses and destination ports. They operate at the transport layer (TCP - level 4), making routing decisions without considering application-level parameters such as content type, cookies, or headers. These balancers focus solely on network layer information to direct traffic.\nApplication Load Balancer (Layer 7 - L7):\nApplication Load Balancers, at the highest level of the OSI model, distribute requests based on various application-level parameters. They analyze a broader range of data, including HTTP headers and SSL sessions, to distribute server load based on multiple variables. This enables them to manage server traffic based on individual usage and behavior.\nGlobal Server Load Balancer (Multi-site Load Balancer)\nGlobal Server Load Balancers (GSLBs) extend the capabilities of L4 and L7 load balancers across multiple data centers, especially in cloud environments. They efficiently distribute load globally, ensuring optimal performance for end users across different geographical locations. Additionally, they facilitate quick recovery and seamless business operations in case of server disasters by redirecting traffic to alternative data centers. Understanding Load Balancing Algorithms\nLoad balancing algorithms consist of rules guiding a load balancer in selecting the optimal server for various client requests. These algorithms are broadly categorized into two main types.\nDynamic Load Balancing Algorithms Explained:\nLeast Connection\nRoutes traffic to servers with the fewest open connections, assuming similar processing power for all connections.\nWeighted Least Connection\nAllows assigning different weights to servers, accommodating varying connection-handling capacities.\nWeighted Response Time\nConsiders both server response time and active connections to direct traffic to servers with the quickest responses, ensuring faster service.\nResource-Based\nDistributes load based on available server resources like CPU and memory. A specialized software agent on each server measures resource availability, guiding the load balancer in traffic distribution.\nStatic load balancing Algorithms Explained:\nStatic Load Balancing Algorithms Explained:\nRound Robin:\nDistributes traffic to servers in rotation using the Domain Name System (DNS). Each DNS query receives a different server address. Weighted Round Robin:\nLets administrators assign different weights to servers, allocating more traffic to those with higher capacity. Weighting is configurable within DNS records. Steps for approaching the LoadBalancer: Steps to Interact with the Load Balancer for Better Performance and Resource Management in Your Organization.\nStep 1: Click on the provided link Console URL.Upon logging in to the platform, This link will redirect you to the Dashboard.\nStep 2: Click on the left-hand sidebar and navigate through the dropdown menu to access the Load Balancer options. Step 3: Select ‚ÄúLoad Balancer‚Äù to proceed, which will redirect you to the Load Balancer homepage. Step 4: Click on the ‚ÄúCreate LoadBalancer‚Äù button as indicated. Step 5: After clicking, you‚Äôll be directed to a page where you‚Äôll need to select the required details for your Load Balancer. Step 6: Choose the DC Location according to your requirements. Step 7: Select the load balancer setting/type from the dropdown menu. Step 8: Provide an appropriate name for the load balancer. Step 9: Click on the ‚ÄúLoad Balancer‚Äù button to create the load balancer. Step 10: Once the load balancer is created, you‚Äôll be redirected to the homepage where all load balancers will be displayed. Step 11: Click on the ‚ÄúManage‚Äù section to delve into the details and functionality of the Load Balancer. Upon clicking ‚ÄúManage,‚Äù you‚Äôll be redirected to the next page. To add a frontend, click on the ‚ÄúAdd Frontend‚Äù button. This action will prompt a new page where you must provide various details:\nFrontend Name Protocol Port Algorithm Redirect to HTTPS: Selecting this option ensures that only HTTPS links will access your website. On selecting this only https links will hit our website not others. Sticky Sessions: This feature preserves user data in the form of cookies. It keeps the data of user in the form of cookies. After completing the necessary details, click on ‚ÄúAdd Frontend.‚Äù You‚Äôll observe a pop-up indicating the successful addition process. Once the frontend is added, navigate to the ‚Äú(+)‚Äù sign as shown in the snippet. Clicking on the ‚Äú(+)‚Äù sign leads to a new page containing user details. Here, you can update frontend details by clicking on ‚ÄúUpdate Setting.‚Äù ACL (Access Control List) rules in cloud computing control access to resources. To add a new ACL rule, click on ‚ÄúAdd New Rule.‚Äù A sidebar will appear where you can input the necessary details/conditions for the new rule.\nAdvanced routing in cloud computing efficiently manages and directs network traffic within a cloud environment. Additionally, users can delete created frontends by clicking on ‚ÄúDelete Frontend.‚Äù\nHow Can Utho Help Utho is a leading choice for load balancing, trusted by many high-traffic websites. With numerous websites worldwide relying on Utho, it ensures fast, reliable, and secure content delivery. Unlike hardware-based options, Utho is a software-based load balancer, offering significant cost savings while providing similar capabilities. Its wide range of load-balancing features allows you to create a finely-tuned application delivery network.\nWhen you use Utho as your load balancer in front of your application and web server farms, it improves your website‚Äôs efficiency, performance, and reliability. By using Utho, you can boost customer satisfaction and maximize returns on your IT investments."},"title":"index"},"/utho-docs/docs/platform/microhost-cloud-firewall/":{"data":{"":"\nCloud Firewall is an extra layer of security for the cloud server which prevent¬†unauthorized/unwanted traffic for your cloud server. It will only allow the traffic¬†which is known to it. Inbound and Outbound both connection can be controlled¬†through the cloud firewall. We will See, how we can create and configure the¬†Cloud Firewall.\nStep 1: First, we have to Login into Microhost Cloud Dashboard -\nStep 2: After successful login, we will move to the Cloud Firewall section as given in the screenshot:¬†Step 3: In the Cloud Firewall option, we have to click on ‚ÄúCreate New Firewall‚Äù as¬†given in the screenshot:\nStep 4: Output will be shown as similar like given in the screenshot, where we have to write the name of the Firewall. It could be anything like (Test, Myfirewall¬†etc.)¬†Step 5: After clicking on create firewall, a firewall with some predefined policies¬†would be created as shown in the screenshot. You can manage those policies as per your requirement.¬†Suppose if you want to open port 443 for service ‚Äúhttps‚Äù then you can create a rule as shown in screenshot.\nStep 6: After creating a firewall we have to add the server respectively for which we want to apply these policies. We can add single or multiple servers for single firewall as per the requirement.¬†Step 7: We can remove the added server from firewall and delete the firewall as well.¬†Thank you."},"title":"Microhost Cloud Firewall"},"/utho-docs/docs/platform/microhost-product-details/":{"data":{"":"\nA cloud server is a virtual server that runs on a cloud computing environment (rather than a physical server). It is built and hosted through the Internet and can be accessed on a cloud computing platform from a remote protocol. Also called virtual servers:¬†DEPLOYING CLOUD SERVERS USING MICROHOST CLOUD DASHBOARD¬†Step 1: Firstly, we have to login into the Microhost Cloud Dashboard Afterward, you will see the interface like below:\nStep 2: Now, we have to click on Deploy Cloud Server as per the screenshot given below.\nStep 3: Afterward, we will see a new page, where we have to select the configuration and data center location as per given in the screenshot.\nStep 4: In the below screenshot we have to select the configuration\nStep 5: Now after going below on the same page , we will get the option of writing server hostname along with the button of deploy cloud server. Please see the screenshot for your reference.¬†After clicking on ‚Äúdeploy cloud server‚Äù , the server will be created of selected configuration. We can see the IP and other details in the ‚Äúcloud server‚Äù section of the dashboard.\nThankyou"},"title":"How to create Microhost cloud server"},"/utho-docs/docs/platform/object-storage/":{"data":{"#":"Object Storage Object storage in the cloud refers to a method of storing and managing data in a distributed, scalable, and highly available manner. Instead of organizing data in a hierarchical file system structure like traditional storage systems, object storage systems store data as objects within a flat namespace. These services are widely used for storing various types of data, including images, videos, documents, backups, and log files, among others.\nObject Storage offer several benefits: Cost-effectiveness Access via APIs Durability Scalability Steps for approaching the Object Storage: Visit on the link given below: Console url\nFirst, login to your Utho profile using the link mentioned above. This link will redirect you to the Dashboard of your profile. Here we will get the options to reach the Object Storage tab.\nDeploy new (Dropdown)- To deploy a Object storage.\nLeft side menu option Object Storage- To view and manage your object storage.\nAfter clicking on the option from the left side menu, it will redirect user to the Object Storage homepage. Here user will have two options:\nCreate Buckets Create Access Keys Here moving forward with creating Bucket. On homepage user will have two ways to create bucket as shown below in the snippet, On clicking it will redirect user to the requirement page. Now user will choose the location from DC Location. Here user will select the storage Plan Type according to the requirements. If user want then he/she can provide the bucket name or else it will be generated automatically. After that user will click on create storage as shown below. After clicking on create storage it will redirect user to the next page where all created storage will be seen. Click on manage button for in depth understanding of any specific Object Storage. On clicking the Manage button it will redirect user to the next tab.\nUnder Object section on clicking on Upload files -\u003e Attach files then user can upload his files in the bucket created. Also on clicking on new folder user can create the folder of his choice. Under Access Control section user can update the access control for there files by choosing the option from dropdown and clicking on save changes button. Once user select there access control there he can see more information about the different permission. Here on clicking on update permissions user can update \u0026 manage the permissions given. On clicking on destroy button user can delete the object storage. Now for the process involved in the Create Access Keys: Navigate to Access Keys section and click on create access keys as marked in the snippet below. On click a sidebar will open where we have to provide the access key name and click on create key button. After clicking on the create button a new access key will be created. Here, the user will be prompted with the newly created access and secrent key. Copy this secret key somewhere safe because after this page, user cannot see or generate the same secret key.\nHere on clicking on edit button a new sidebar. Where user can update there access key by choosing te required status and clicking on update access key. After the update the same will be reflected on the homepage of access key. THE END","object-storage#Object Storage":""},"title":"index"},"/utho-docs/docs/platform/resource-transfer/receive-resource/":{"data":{"#":"Receive Resource Receiving resources in the cloud typically involves accepting data, files, or other digital assets into your cloud environment from external sources.\nReceive Resource offer several benefits: Processing Monitoring and Management Data Ingestion Security Steps for approaching the Receive Resource: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here on L.H.S user will get the option of Receive Resource tab as shown below in the below snippet. On clicking on that it will redirect user on the next tab which is homepage of Receive Resource. Here user will get 2 options:\nReceive Resource\nRecieve Resource\nNow starting with Receive Resource: In this we are receiving resources from other user. After choosing user have to click on continue as shown in the snippet. After clicking on continue user will be navigated to the next page where user have to enter details like resource id and resource token. Now user have to provide resource type, resource id, resource token in another account as shown below in the snippet. And click on attach resource. After clicking on attach resource a new tab with the resource received will occur. Hence the receiving of resource is successful.\nTHE END","receive-resource#Receive Resource":""},"title":"index"},"/utho-docs/docs/platform/resource-transfer/transfer-resource/":{"data":{"#":"Resource Transfer Resource Transfer in the cloud computing involves the delivery of computing services over the Internet, which can include storage, databases, networking, software, and more. Resource transfer in the cloud typically refers to the movement of data, files, or other resources between different components or services within a cloud infrastructure.\nResource Transfer offer several benefits: Data Transfer Costs Optimization Security Steps for approaching the Resource Transfer: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here on L.H.S user will get the option of Resource transfer tab as shown below in the below snippet. On clicking on that it will redirect user on the next tab which is homepage of resource transfer. Here user will get 2 options:\nResource Transfer Recieve Resource Now starting with Resource Transfer: In this we are transfering our resources to any other user. After choosing user have to click on continue as shown in the snippet. After clicking on continue user will be navigated to the next page where he have to choose the Resource type. Once the resource type is selected all the server will be shown below in get token to share your resource tab for that particular resource. After that on clicking on get token , resource id will be sent on the mail. Aslo a confirmationmessage will be shown on the above screen that resource id sent to email. Now the further process will take place on Receive Resource.\nTHE END","resource-transfer#Resource Transfer":""},"title":"index"},"/utho-docs/docs/platform/snapshots/":{"data":{"#":"Snapshots A snapshot is a point-in-time copy of a data volume or disk. It captures the entire state of the volume at the moment the snapshot is taken, including the data, configuration, and metadata associated with it.\nSnapshots offer several benefits: Cloning Cost Optimization Disaster Recovery Data Backup Data Recovery Steps for approaching the Snapshots : Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. On L.H.S user will get an option of snapshots when scrolling down as shown in the snippet below. After that a new page will occur where user will get all the detils of the his snapshots he took during the process on different products like on Cloud instances we have option of Snapshots. Now on the homepage of Snapshots user wil get option for restoring and deleteing the snapshot as given in the snippet. THE END","snapshots#Snapshots":""},"title":"index"},"/utho-docs/docs/platform/sqs/":{"data":{"#":"SQS (Simple Queue Service) Simple Queue Service allows you to decouple and scale microservices, distributed systems, and serverless applications by providing a reliable and highly scalable messaging system.SQS supports both standard and FIFO (First-In-First-Out) queues, allowing you to choose the message ordering and delivery behavior that fits your use case. Overall, UTHO SQS is a reliable and scalable messaging service that helps you build loosely coupled, distributed systems in the cloud. It‚Äôs commonly used for asynchronous communication between microservices, decoupling components of a system, and managing workloads in serverless architectures.\nSQS offer several benefits: Cost-effectiveness Fully Managed Reliable Scalability Simple Flexible Steps for approaching the SQS: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here on L.H.S sidebar user will get an option for SQS as marked in the snippet below. On clicking it will redirect user to the homepage of SQS. Here user can create new SQS by clicking on create SQS button marked in the attached snippet. On clicking it will redirect user to the deploy page. Here user will select the Data Center location from the given options. Now user will select the Plan Type according to their consumption. Now user can give the name for there SQS if not given then it will generate automatically and then click on create SQS button as shown in the snippet below. After clicking on create SQS deployment of SQS will be done and it will redirect user to the homepage of SQS. Where on clicking on manage SQS a new tab will occur. Then user will get the option to create the queue by clicking on the create queue button. On clicking on it a sidebar will open , here the process of Adding a new queue will start. Now user will select the queue type for there application. Now user will set the configuration like message size, visibility to other consumers and message retention. Then user will give a proper name for there queue and click on create queue. After clicking it will redirect user to the homepage of queue. Here user will get the option for sending message by clicing on send message tab as shown in the snippet below. Here user can send the message from there queue by clicking on send message button. Now on clicking on delete icon the created queue will be deleted. THE END","sqs-simple-queue-service#SQS (Simple Queue Service)":""},"title":"index"},"/utho-docs/docs/platform/stacks/":{"data":{"#":"Stacks Stacks generally refer to collections of related services or resources that work together to provide a specific functionality or solution. These stacks can include various components such as virtual machines, databases, networking configurations, security policies, and application code.\nStacks offer several benefits: Application Stacks DevOps Stacks Serverless Stacks Data Stacks Steps for approaching the Stacks: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here we will get 2 options to reach the stacks tab.\nDeploy new (Dropdown) L.H.S tab After clicking on this tab it will redirect user to the homepage of stacks. For creating the stack user should click on create stacks as shown in the snippet below. After clicking on create stack it will redirect user to next page where it will ask user for the detils required for creating the stacks. After filling all the details user should click on create stack. Then user will be redirected to homepage of stacks where all created stacks will be available. On clicking on eye button stack label will be seen. And on clicking on edit button user will be able to edit the stacks created where on clicking on update stack created stack will be updated. And on clicking on the deploy server on the homepage of stack it will redirect user to the cloud instances deploy server page. On deploy server page user will get stack selected automtically and that can be deployed after clicking on the deploy button on the cloud instances product page. And in community stacks in the context of cloud computing typically refer to pre-configured, community-driven templates or configurations that are shared among users for common use cases or scenarios. THE END","stacks#Stacks":""},"title":"index"},"/utho-docs/docs/platform/steps-to-activate-microhost-vpn/":{"data":{"":"\nA¬†virtual private network¬†(VPN) gives you online privacy and anonymity by creating a private network from a public internet connection. VPNs mask your internet protocol (IP) address so your online actions are virtually untraceable.\n1. At first, Access Microhost cloud platform\n2. Now click on VPN option and Create VPN\n3. Select DC Location and Enter VPN name, number of users, billing cycle after that click on Deploy\n4. Once you click on deploy VPN it will redirect you to the payment page. After the payment, VPN would be created and ready to use. Now click on Manage VPN option from action section.¬†5. Now from Add user section you can create vpn user.¬†6. After adding users you can see user details from the manage user section and download the ovpn file after clicking on download option.¬†7. Now you need to download the open vpn client connect¬†a) For windows https://openvpn.net/downloads/openvpn-connect-v3-windows.msi\n8. After downloading and installing, you can see the openvpn shortcut on your desktop. run openvpn client, it will ask for the URL import and file import option, You need to select the file import option and import ovpn file which you downloaded.\n9. After importing the ovpn file, you will see the message ‚Äúprofile imported successfully‚Äù. you have to add a profile and connect vpn.\n10. Once you import and add profile. You will be able to see the connect option.\nThank you."},"title":"Steps to Activate Microhost VPN"},"/utho-docs/docs/platform/subuser/":{"data":{"#":"Subuser Subuser refers to a secondary user account or profile created within a primary user account in a cloud service provider‚Äôs platform. This concept is often used to manage permissions, access, and resources within the cloud environment. Overall, subusers provide a flexible and scalable way to manage users, access, and resources within a cloud environment, facilitating collaboration, security, and resource optimization.\nSubuser offer several benefits: Billing and Cost Management Flexibility Resource Management Access Control Team Collaboration Steps for approaching the Subuser : Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here user will get an option for Subuser on the L.H.S tab after scrolling down. Click on the Subuser button as marked below. After clicking on the subuser button it will redirect user on then next page which is homepage of the Subuser. Here user will get 2 option to create the subuser.\nCreate subuser Add Subuser On clicking on create subuser a popup will occur where the information about the subuser will be asked for creation of subuser. Here user have to enter the name, mobile number \u0026 email id of the user also the owner(user) have to provide the permission. Like for which products subuser will be having the access. Snippet is attached below for the references. After filling all the details click on the Add user button given below. After clicking on add user it will redirect to the account management page where owner can see all the subusers added. Click on the manage button for more functionalities. After clicking on manage button it will redirect user to the next page. Here on clicking on Update permission owner can update the permissions for the subuser. .\nHere user can see all the permissions provided to the subuser also owner can update the permissions for the exixting subusers. After selecting all the permissions owner have to click on save permissions given below for updating the permissions. Also on clicking on Disable or Destroy button owner/user can delete the subuser. THE END","subuser#Subuser":""},"title":"index"},"/utho-docs/docs/platform/target-group/":{"data":{"steps-for-approaching-the-target-group#Steps for approaching the Target Group:":"TARGET GROUP A ‚ÄúTarget Group‚Äù typically refers to a component of a load balancer service, particularly in UTHO . Here‚Äôs a breakdown of what a target group is and its role in cloud infrastructure. Also target group is a logical grouping of targets. It acts as a destination for requests routed by the load balancer based on configured rules.\nTarget Group offer several benefits: Health Checks Routing Policies Path-Based Routing Integration with Auto Scaling Security Steps for approaching the Target Group: Visit on the link given below: Console url\nThis link will redirect you to the Dashboard after Login of the platform. Here on click on L.H.S bar we will get two options to navigate to Target Group as shown below. Now for creating the target group click on the marked point given on the snippet. After clicking on that button a side bar will open where the user have to fill all the details related to Target Group. NAME: This is a user-defined name for the target group, which helps identify it within your system. It‚Äôs usually a descriptive label to indicate the purpose or function of the target group. PROTOCOL: This specifies the communication protocol used for routing traffic to the targets in the target group. Common protocols include HTTP, HTTPS, TCP, and SSL. PORT: This is the port number on which the targets in the target group are listening for incoming traffic. For example, if your application runs on port 80 for HTTP traffic, you would specify port 80 here. HEALTH CHECK PATH: This is the URL path that the load balancer periodically checks on each target to determine its health status. It‚Äôs typically a specific endpoint on your application server that returns a health check response (e.g., ‚Äú/health‚Äù or ‚Äú/status‚Äù). HEALTH CHECK PROTOCOL: This specifies the protocol used for health checks. It‚Äôs often the same as the protocol used for routing traffic to the targets (e.g., HTTP or HTTPS), but it can be different if necessary. Health Check Interval: This is the time interval at which the load balancer sends health check requests to the targets to verify their health status. It‚Äôs usually specified in seconds. Health Check Timeout: This is the maximum amount of time the load balancer waits for a response from a target before considering the health check failed. It‚Äôs specified in seconds. Healthy Threshold: This is the number of consecutive successful health checks required for a target to be considered healthy. Once a target reaches this threshold, it‚Äôs considered healthy and can receive traffic. Unhealthy Threshold: This is the number of consecutive failed health checks required for a target to be considered unhealthy. Once a target reaches this threshold, it‚Äôs considered unhealthy and is removed from the pool of available targets until it becomes healthy again. After filling all the details click on Create Target Group. 5. After clicking on it will redirect to the homepage of target group. 6. Where on clicking on manage it will navigate to new page given below also from there user can upadte his created target group in the Configuration section. After configuration user will move to target section where the user have to click on add target. After clicking on add target a new tab will occur where user have to fill all the required data‚Äôs like (Server type, Protocol, Port, Choose Backend Server) and click on Add server. OR\nBy clicking on the custom IP user can add custom server. After clicking on this a new page will occur where user have to fill all the required data‚Äôs as given in the snippet. THE END","target-group#TARGET GROUP":"","target-group-offer-several-benefits#Target Group offer several benefits:":""},"title":"index"},"/utho-docs/docs/platform/vpc/":{"data":{"#":"VPC A Virtual Private Cloud (VPC) is a service provided by public cloud providers, enabling enterprises to create their own private cloud-like computing environment within shared public cloud infrastructure.\nWhy Choose a Virtual Private Cloud (VPC)? Enhanced Security\nA VPC offers robust security features, allowing you to safeguard your virtual networking environment, including IP addresses, subnets, and network gateways. This enables you to securely isolate critical resources like databases in private-facing subnets, minimizing exposure to potential internet threats.\nData Control\nWith a VPC, your network is isolated from other cloud environments, granting you full control over your data and preventing any intermingling with data from other entities. This segregation mitigates the risk of data compromise often associated with public cloud platforms.\nImproved Performance\nVPCs enable you to prioritize network traffic for specific applications, optimizing their performance and minimizing congestion or potential bottlenecks. This ensures consistent and reliable performance for your critical workloads.\nOn-Demand Flexibility\nYou have the flexibility to tailor your cloud architecture according to your company‚Äôs specific requirements. For instance, you can configure your VPC to facilitate individual direct connections for contractors, bypassing your internal network and providing efficient access to cloud resources.\nBenefits of Using a VPC: Enhanced Security: VPCs offer isolation within public clouds, enhancing protection for sensitive data and workloads.\nCustomizable Networks: Organizations can tailor network configurations to meet specific security and operational needs.\nImproved Performance: Segmented network resources optimize application performance and user experience. Cost Efficiency: VPCs leverage pay-as-you-go models and economies of scale for cost savings.\nHybrid and Multi-Cloud Support: VPCs facilitate secure connections between on-premises networks and cloud environments, supporting multi-cloud strategies.\nScalability and Flexibility: VPCs provide scalability and flexibility for resource management and response to demand changes.\nApproach to Setting Up a VPC: Step 1: Click on the provided link Console URL.Upon logging in to the platform, this link will redirect you to the Dashboard. Step 2: At this stage, you will encounter two options to access the VPC tab:\n‚ÄúDeploy new‚Äù (from the dropdown menu) Left-hand side (LHS) tab Step 3: Upon selection, you will be redirected either to the VPC homepage or the VPC deployment page. Step 4: Proceed by selecting the Data Center (DC) Location according to your requirements. Step 5: Provide the necessary values to define the VPC Network and input all details required to configure your VPC. Step 6: Click on ‚ÄúDeploy VPC‚Äù to initiate the deployment process for your VPC. Step 7: Once the VPC deployment is completed, you will be redirected to the VPC homepage, where you can view all deployed VPCs. Step 8: To access more information about the functioning of the VPC, click on the ‚ÄúManage‚Äù button. Upon clicking the ‚ÄúManage‚Äù button, you will be redirected to the next tab, where detailed information about the VPC is provided.\nTHE END","vpc#VPC":""},"title":"index"},"/utho-docs/docs/platform/vpn/":{"data":{"#":"VPN Also known as a Virtual Private Network, a VPN establishes an encrypted connection via the Internet between a device and a network. This encryption is key in securely transmitting sensitive data, preventing unauthorized individuals from intercepting the traffic. Moreover, a VPN enables users to work remotely, providing a secure means of conducting tasks from a distance. Notably, VPN technology finds extensive usage within corporate environments.\nWhy Consider Using a VPN? When your browser connects to a web server, it first checks with Domain Name Services (DNS) servers to find the server‚Äôs IP address. While most connections are encrypted using SSL/TLS, there are still vulnerabilities, especially on public Wi-Fi networks. For example, a skilled attacker could weaken the encryption protocol, making communications vulnerable to brute-force attacks.\nUsing a VPN adds an extra layer of security to the connection. The VPN service encrypts data and sends it across the network. As a result, the targeted server sees the VPN‚Äôs public IP address instead of the user‚Äôs. If there‚Äôs a hijacked connection and someone tries to eavesdrop, strong VPN encryption prevents potential brute-force attacks, protecting data from exposure in an insecure environment.\nWhat does a VPN (virtual private network) do? Instead of connecting directly to the internet, your web traffic is routed through a VPN server. This process conceals your IP address, making it appear as though your web activity originates from your VPN service provider‚Äôs network rather than your actual location. This additional layer of protection between your device, whether it‚Äôs a computer or smartphone, allows you to browse and use the internet securely and with enhanced privacy.\nAdvantages of Using a VPN: VPNs offer users and organizations several benefits, including:\nSecure Connectivity: VPNs encrypt connections, ensuring that third parties cannot intercept data without the encryption keys. Streamlined Distributed Networks: Site-to-site VPNs create a virtual link between networks, allowing them to use private IP addresses for internal traffic, simplifying network management. Access Control: VPNs enable remote users to access internal resources securely, as the VPN endpoint is within the network firewall, allowing authorized access without exposing resources to the public. Approach to Setting Up a VPN: Step 1: Click on the provided link Console URL.Upon logging in to the platform, this link will redirect you to the Dashboard. Step 2: On the Databases tab, two access options are available:\n‚ÄúDeploy new‚Äù (from the dropdown menu) Left-hand side (LHS) tab Step 3: Selecting either option will direct you to the VPN homepage. Step 4: On the homepage, two options for creating the VPN are provided. Step 5: Upon selection, you‚Äôll be redirected to the next page to specify details according to your requirements. Step 6: Choose the Data Center (DC) Location based on your needs. Step 7: Select the VPN users, name, and billing cycle as indicated. Click on ‚ÄúDeploy VPN‚Äù to initiate the deployment process. A bill will be generated; proceed to payment by clicking ‚ÄúPay.‚Äù Select the desired mode of payment.\nOnce payment is completed, the VPN will be deployed, and you will receive an email with the invoice details.\nTHE END","vpn#VPN":""},"title":"index"},"/utho-docs/docs/renew-with-ease-lets-encrypt-certificate-guide/":{"data":{"":"This article covers the process of renewing Let‚Äôs Encrypt SSL certificates installed on your instance. Please note that it does not apply to Let‚Äôs Encrypt certificates managed by Utho for load balancers.\nLet‚Äôs Encrypt utilizes the Certbot client for installing, managing, and automatically renewing certificates. If your certificate doesn‚Äôt renew automatically on your instance, you can manually trigger the renewal at any time by executing:\nsudo certbot renew If you possess multiple certificates for various domains and wish to renew a particular certificate, utilize:\ncertbot certonly --force-renew -d example.com The ‚Äú‚Äìforce-renew‚Äù flag instructs Certbot to request a new certificate with the same domains as an existing one. Meanwhile, the ‚Äú-d‚Äù flag enables you to renew certificates for multiple specific domains.\nTo confirm the renewal of the certificate, execute:\nsudo certbot renew --dry-run If the command executes without errors, the renewal was successful.\nRenewing Let‚Äôs Encrypt certificates doesn‚Äôt have to be daunting. By following the steps outlined in this comprehensive guide, you can ensure your certificates remain up-to-date and your websites stay secure. Whether it‚Äôs automating the renewal process or manually triggering it when needed, maintaining SSL certificates doesn‚Äôt have to be a hassle. With the right tools and knowledge at your disposal, you can keep your online presence protected without any fuss."},"title":"Renew with Ease: Let's Encrypt Certificate Guide"},"/utho-docs/docs/reserved-ip-redefining-the-future-of-cloud-scalability/":{"data":{"":"","how-do-reserved-ips-impact-cost-optimization-strategies-within-cloud-infrastructure-deployments#\u003cstrong\u003eHow do Reserved IPs impact cost optimization strategies within cloud infrastructure deployments?\u003c/strong\u003e":"","how-does-utho-clouds-reserved-ip-service-stand-out-for-businesses-and-what-advantages-does-it-offer-over-competitors#\u003cstrong\u003eHow does Utho Cloud\u0026rsquo;s Reserved IP service stand out for businesses, and what advantages does it offer over competitors?\u003c/strong\u003e":"","what-are-reserved-ips-in-the-context-of-cloud-infrastructure-and-how-do-they-contribute-to-scalability-and-flexibility#\u003cstrong\u003eWhat are\u003c/strong\u003e R\u003cstrong\u003eeserved IPs in the context of cloud infrastructure, and how do they contribute to scalability and flexibility?\u003c/strong\u003e":"","what-are-some-examples-of-scenarios-where-reserved-ip-addresses-have-significantly-enhanced-the-performance-of-cloud-infrastructure#\u003cstrong\u003eWhat are some examples of scenarios where Reserved IP address\u003c/strong\u003ees \u003cstrong\u003ehave significantly enhanced the performance of cloud infrastructure?\u003c/strong\u003e":"","what-future-trends-or-advancements-can-we-expect-to-see-in-the-realm-of-reserved-ip-usage-within-cloud-infrastructure#\u003cstrong\u003eWhat future trends or advancements can we expect to see in the realm of Reserved IP usage within cloud infrastructure?\u003c/strong\u003e":"In today‚Äôs rapidly evolving business environment, companies need to be agile, quickly responding to new demands. Reserved IPs are crucial for this, giving businesses the ability to easily grow and adjust within cloud setups. Stay with us as we explore how Reserved IPs boost scalability and flexibility in today‚Äôs cloud systems.\nWhat are Reserved IPs in the context of cloud infrastructure, and how do they contribute to scalability and flexibility? Reserved IPs in the context of cloud infrastructure are specific IP addresses set aside for particular resources or purposes within a cloud environment. These IP addresses are allocated or reserved by users or administrators for exclusive use by certain virtual machines, applications, or services.\nHere‚Äôs how Reserved IPs contribute to scalability and flexibility in cloud infrastructure:\nPredictable Access: Reserved IP address ensure consistent and predictable access to specific resources within the cloud environment. By assigning fixed IP addresses to critical components, such as databases or application servers, organizations can maintain stable connections without worrying about IP address changes.\nScalability: Reserved IPs facilitate scalability by allowing organizations to easily scale their cloud resources up or down without affecting connectivity. When scaling resources, such as adding more virtual machines or instances, Reserved IPs can be quickly reassigned to new instances, maintaining continuity in service without disrupting ongoing operations.\nFlexibility: Reserved IPs provide flexibility in managing cloud resources. Administrators can allocate and reallocate IP addresses based on changing requirements or workload demands. This flexibility enables efficient resource utilization and adaptation to evolving business needs without constraints imposed by fixed IP assignments.\nLoad Balancing and Failover: Reserved IP address play a crucial role in load balancing and failover configurations. By associating Reserved IPs with load balancers or failover clusters, organizations can distribute incoming traffic across multiple instances or redirect traffic to backup resources in case of failures, ensuring high availability and reliability of services.\nNetworking and Security: Reserved IPs simplify networking configurations and enhance security in cloud environments. Administrators can implement firewall rules, access controls, and routing policies based on Reserved IP addresses, enabling granular control over network traffic and strengthening overall security posture.\nReserved IPs in cloud infrastructure contribute to scalability and flexibility by providing predictable access, facilitating scalability, offering flexibility in resource management, supporting load balancing and failover mechanisms, and enhancing networking and security configurations. These capabilities empower organizations to efficiently manage their cloud resources, adapt to changing demands, and maintain reliable and resilient cloud-based services.\nWhat limitations or challenges are associated with utilizing Reserved IP addresses in cloud infrastructure? When using Reserved IPs in cloud infrastructure, there are several limitations and challenges to consider:\nLimited Availability: Depending on the cloud provider and region, there might be limitations on the availability of Reserved IP addresses. In some cases, there may be scarcity, especially in popular regions or during peak usage periods.\nManagement Overhead: Managing a large number of Reserved IP addresses can become complex, especially in environments with frequent resource provisioning or scaling. Administrators need to track and manage IP allocations, which can increase operational overhead.\nScaling Challenges: In dynamic cloud environments where resources frequently scale up or down, managing Reserved IPs can pose challenges. Ensuring that Reserved IPs are properly allocated to new instances or services during scaling events requires careful coordination and automation.\nIP Address Exhaustion: Similar to traditional networking environments, there‚Äôs a risk of IP address exhaustion, especially in large cloud deployments. Organizations must effectively plan and manage IP address allocations to avoid running out of available addresses.\nNetwork Segmentation: Implementing network segmentation with Reserved IPs can be challenging, particularly when dealing with multiple virtual networks or complex network topologies. Ensuring proper isolation and security between different network segments requires meticulous planning and configuration.\nVendor Lock-In: Utilizing Reserved IPs from a specific cloud provider may result in vendor lock-in, making it difficult to migrate to another provider in the future. Organizations should consider the long-term implications of relying heavily on provider-specific Reserved IP features.\nRegulatory Compliance: Compliance requirements, such as data sovereignty regulations, may impose constraints on the use of Reserved IPs. Organizations operating in regulated industries must ensure that their use of Reserved IPs complies with relevant regulations and standards. Addressing these limitations and challenges requires careful planning, effective management practices, and potentially leveraging automation and orchestration tools to streamline IP address management in cloud environments.\nWhat are some examples of scenarios where Reserved IP addresses have significantly enhanced the performance of cloud infrastructure? Reserved IPs have proven to be instrumental in various scenarios for enhancing the performance of cloud infrastructure. Here are some examples:\nHigh Availability Configurations: In scenarios where high availability is critical, Reserved IP address are often used in conjunction with load balancers and failover mechanisms. By associating Reserved IPs with redundant instances or servers, organizations can ensure continuous availability of services even in the event of hardware failures or maintenance activities. This setup improves performance by minimizing downtime and distributing incoming traffic efficiently across healthy instances.\nDatabase Clustering: Reserved IPs are commonly employed in database clustering setups where multiple database nodes work together to handle queries and maintain data consistency. By assigning Reserved IP address to each node within the cluster, organizations can achieve seamless failover and load balancing. This configuration enhances database performance by distributing workloads evenly and providing rapid failover capabilities in case of node failures.\nContent Delivery Networks (CDNs): CDNs rely on Reserved IPs to deliver content efficiently to users across the globe. By using Reserved IPs to map content servers to geographically distributed edge locations, CDNs can reduce latency and improve content delivery speeds. Reserved IPs allow CDNs to route user requests to the nearest edge server, minimizing the distance data travels and enhancing overall performance for end users.\nMicroservices Architecture: In microservices-based applications, Reserved IP address play a crucial role in facilitating communication between individual microservices. By assigning reserved IPs to each microservice instance, organizations can establish reliable communication channels and streamline service discovery processes. This setup improves application performance by reducing latency and ensuring seamless interactions between microservices, leading to faster response times and improved scalability.\nAPI Gateways: Reserved IPs are commonly used in API gateway configurations to provide a stable entry point for accessing backend services. By assigning Reserved IPs to API gateway instances, organizations can ensure consistent API endpoints for clients, regardless of changes in backend infrastructure or scaling activities. This setup enhances performance by reducing the overhead associated with endpoint management and improving the reliability of API interactions.\nReserved IP address play a significant role in improving cloud infrastructure performance across various scenarios, including high availability configurations, database clustering, content delivery networks, microservices architecture, and API gateway setups. By leveraging Reserved IPs effectively, organizations can enhance reliability, scalability, and responsiveness in their cloud-based applications and services.\nHow do Reserved IPs impact cost optimization strategies within cloud infrastructure deployments? Reserved IPs can have a significant impact on cost optimization strategies within cloud infrastructure deployments in several ways:\nReduced Variable Costs: By Reserving IP addresses for long-term use, organizations can benefit from discounted pricing offered by cloud providers. Reserved IPs typically involve a one-time fee or lower hourly rates compared to dynamically assigned IPs, resulting in cost savings over time.\nPredictable Billing: Reserved IPs provide cost predictability by offering fixed pricing for the duration of the reservation period, which can range from months to years. This predictability allows organizations to budget more effectively and avoid unexpected fluctuations in expenses related to IP address usage.\nAvoidance of Overage Charges: In environments with fluctuating workloads or dynamic resource provisioning, using Reserved IP address can help avoid potential overage charges associated with exceeding allotted IP address quotas. By Reserving IP addresses in advance, organizations can ensure they have sufficient capacity to meet their needs without incurring additional charges for exceeding limits.\nOptimized Resource Utilization: Reserved IP addresses encourage efficient resource allocation by incentivizing organizations to plan their IP address usage strategically. By reserving only the necessary number of IPs for anticipated workloads, organizations can optimize resource utilization and avoid unnecessary expenses associated with unused or underutilized IP addresses.\nLong-Term Cost Savings: While Reserved IPs may involve upfront costs, they often result in long-term cost savings for organizations with stable or predictable workloads. By committing to Reserved IPs for extended periods, organizations can capitalize on lower rates and avoid paying higher prices for dynamically allocated IPs over time.\nSimplified Cost Management: Reserved IP address contribute to simplified cost management by consolidating IP address-related expenses into a single, predictable billing structure. This simplicity enables organizations to track and manage IP address costs more effectively, streamlining financial processes and reducing administrative overhead.\nReserved IP address play a crucial role in cost optimization strategies within cloud infrastructure deployments by reducing variable costs, providing predictable billing, avoiding overage charges, optimizing resource utilization, generating long-term savings, and simplifying cost management processes. By leveraging Reserved IPs strategically, organizations can optimize their cloud spending while maintaining reliable and scalable infrastructure deployments.\nHow does Utho Cloud‚Äôs Reserved IP service stand out for businesses, and what advantages does it offer over competitors? Utho Cloud distinguishes its Reserved IP address services by offering a range of features tailored to meet the diverse needs of businesses and companies. Some specific features and advantages of Utho Cloud‚Äôs Reserved IP services compared to other cloud providers include:\nFlexible Reservation Options: Utho Cloud provides flexible reservation options for IP addresses, allowing businesses to Reserve IPs for varying durations based on their specific requirements. Whether organizations need short-term or long-term reservations, Utho Cloud offers customizable options to accommodate different use cases and budgetary considerations.\nPredictable Pricing: Utho Cloud offers predictable pricing for Reserved IPs, enabling businesses to budget more effectively and avoid unexpected fluctuations in costs. With fixed pricing for the duration of the reservation period, organizations can accurately forecast their expenses related to IP address usage and avoid surprises on their bills.\nIntegration with Networking Services: Utho Cloud‚Äôs Reserved IP address services seamlessly integrate with its comprehensive suite of networking services, including virtual networking, load balancing, and DNS management. This integration simplifies network configuration and management, allowing businesses to leverage Reserved IPs alongside other networking features to optimize performance and reliability.\nHigh Availability and Reliability: Utho Cloud ensures high availability and reliability of Reserved IPs by providing redundancy and failover capabilities across its global network infrastructure. Businesses can rely on Utho Cloud‚Äôs robust architecture to maintain continuous access to Reserved IPs, minimizing downtime and ensuring uninterrupted connectivity for critical applications and services.\nAdvanced Security Features: Utho Cloud incorporates advanced security features into its Reserved IP addresses services, including network isolation, access controls, and encryption capabilities. By leveraging Utho Cloud‚Äôs security enhancements, businesses can protect their reserved IPs and data from unauthorized access, ensuring compliance with regulatory requirements and safeguarding sensitive information.\nScalability and Elasticity: Utho Cloud‚Äôs Reserved IP address services offer scalability and elasticity to accommodate evolving business needs and workload fluctuations. Businesses can easily scale their reserved IP resources up or down as required, allowing them to adapt to changing demand patterns and optimize resource utilization efficiently.\nUtho Cloud differentiates its Reserved IP address services by offering flexible reservation options, predictable pricing, seamless integration with networking services, high availability and reliability, advanced security features, and scalability. These features and advantages enable businesses to effectively meet their diverse networking requirements while leveraging the benefits of Utho Cloud‚Äôs robust and comprehensive cloud platform.\nWhat future trends or advancements can we expect to see in the realm of Reserved IP usage within cloud infrastructure? In the realm of Reserved IP usage within cloud infrastructure, several future trends and advancements are anticipated:\nIncreased Automation: Expect to see advancements in automation tools for managing Reserved IPs. Automation will streamline the process of provisioning, releasing, and managing IP addresses, reducing manual effort and potential errors.\nDynamic IP Management: Future advancements will likely focus on dynamic IP management, where IPs are assigned and released automatically based on demand. This dynamic allocation will optimize resource utilization and improve scalability.\nIntegration with SDN: Reserved IP usage will likely integrate more seamlessly with Software-Defined Networking (SDN) technologies. This integration will provide greater flexibility and control over network configurations, enhancing overall network performance and security.\nIPv6 Adoption: As the adoption of IPv6 continues to grow, cloud providers will offer more support for Reserved IPv6 addresses. This shift will accommodate the increasing number of devices connected to the internet and provide a larger address space for future expansion.\nEnhanced Security Features: Future trends may include the integration of enhanced security features into Reserved IP management, such as built-in DDoS protection, firewall management, and encryption capabilities. These features will help safeguard against cyber threats and ensure the integrity of network communication.\nMulti-Cloud Compatibility: With the rise of multi-cloud environments, expect advancements in Reserved IP management tools that offer compatibility across different cloud providers. This interoperability will simplify network management for organizations operating in hybrid or multi-cloud architectures.\nCost Optimization Tools: Future advancements will likely include tools and algorithms for optimizing Reserved IP usage to minimize costs. These tools may analyze usage patterns and suggest adjustments to Reserved IP allocations to avoid over-provisioning and reduce expenses.\nEnhanced Monitoring and Analytics: Expect advancements in monitoring and analytics capabilities for tracking Reserved IP usage and performance metrics. These insights will enable better resource planning, troubleshooting, and optimization of network infrastructure.\nOverall, the future of Reserved IP usage within cloud infrastructure will be characterized by increased automation, dynamic management, enhanced security, and compatibility across diverse environments, driven by the evolving needs of modern digital businesses.\nWith their ability to facilitate seamless scalability and adaptability, they empower businesses to navigate the complexities of the digital landscape with agility and efficiency. By understanding and harnessing the power of Reserved IPs, organizations can unlock new levels of scalability and flexibility, ensuring they stay ahead in today‚Äôs dynamic business landscape.","what-limitations-or-challenges-are-associated-with-utilizing-reserved-ip-addresses-in-cloud-infrastructure#\u003cstrong\u003eWhat limitations or challenges are associated with utilizing Reserved IP address\u003c/strong\u003ees \u003cstrong\u003ein cloud infrastructure?\u003c/strong\u003e":""},"title":"Reserved IP: Redefining the Future of Cloud Scalability"},"/utho-docs/docs/reset-your-forgotten-utho-cloud-instance-password/":{"data":{"":"","step-1-login-to-utho-cloud-dashboard#\u003cstrong\u003eStep 1: Login to Utho Cloud Dashboard\u003c/strong\u003e":"","step-2-click-on-instances#\u003cstrong\u003eStep 2: Click on \u0026ldquo;Instances\u0026rdquo;\u003c/strong\u003e":"","step-3-choose-the-desired-server-and-click-on-manageoverview#\u003cstrong\u003eStep 3: Choose the Desired Server and Click on Manage/Overview\u003c/strong\u003e":"","step-4-navigate-to-the-power-tab-for-password-reset#\u003cstrong\u003eStep 4: Navigate to the \u0026ldquo;Power\u0026rdquo; Tab for Password Reset\u003c/strong\u003e":"","step-5-scroll-down-and-select-reset-server-password#\u003cstrong\u003eStep 5: \u003cstrong\u003eScroll down and select \u0026ldquo;Reset Server Password\u003c/strong\u003e\u003c/strong\u003e":"","step-6-receive-the-new-password-on-your-registered-email-id#\u003cstrong\u003eStep 6: Receive the New Password on Your Registered Email ID\u003c/strong\u003e":"In the event of a forgotten password, the following steps guide you through the process of resetting your cloud server password for seamless access and security\nStep 1: Login to Utho Cloud Dashboard 1.1 Login to your Utho Dashboard\nStep 2: Click on ‚ÄúInstances‚Äù 2.1 Within the Cloud Dashboard interface, locate the navigation menu.\n2.2 Identify and select the ‚ÄúInstances‚Äù option.\n2.3 Click on the ‚ÄúInstances‚Äù link or button to access the dedicated section.\nStep 3: Choose the Desired Server and Click on Manage/Overview 3.1 Navigate to the ‚ÄúInstances‚Äù or relevant section of the Cloud Dashboard.\n3.2 Identify and select the specific server from the list.\n3.3 Once the server is highlighted, locate and click on the ‚ÄúManage‚Äù or ‚ÄúOverview‚Äù option.\nStep 4: Navigate to the ‚ÄúPower‚Äù Tab for Password Reset 4.1 Access the server management interface on the Cloud Dashboard.\n4.2 Identify and select the navigation menu or tabs within the interface.\n4.3 Locate and choose the ‚ÄúPower‚Äù tab.\n4.4 Navigate to the ‚ÄúPassword Reset‚Äù or equivalent option within the ‚ÄúPower‚Äù tab.\nStep 5: Scroll down and select ‚ÄúReset Server Password 5.1 Within the server management interface, locate the relevant section.\n5.2 Scroll down to access additional options.\n5.3 Identify and select the ‚ÄúReset Server Password‚Äù link.\nNote: Resetting the server password will initiate a server restart.\nStep 6: Receive the New Password on Your Registered Email ID 6.1 Following the server password reset, check your registered email inbox.\n6.2 Look for an email containing the new password information.\n6.3 Get the new password from the email you received and make sure to store it securely."},"title":"Reset your forgotten Utho Cloud Instance password"},"/utho-docs/docs/resiliency-in-the-cloud-what-it-is-and-why-it-matters/":{"data":{"":"","conclusion#\u003cstrong\u003eConclusion\u003c/strong\u003e":"Cloud computing has become an essential aspect of modern business, enabling organizations to easily store and access their data and applications in remote servers. However, as with any technology, there is always a risk of downtime or data loss. This is where cloud resiliency comes in.\nResiliency in the cloud\nCloud resiliency is the ability of a cloud infrastructure to continue operating even in the face of disruptions or failures. In other words, it is the capacity of a cloud system to maintain its core functions and data availability even during adverse conditions. This could be due to natural disasters, cyber-attacks, hardware failures, software bugs, or human errors.\nWhy cloud resiliency matters. With businesses relying heavily on cloud technology to keep their operations running, any downtime or data loss can have severe consequences, such as revenue loss, damage to reputation, and legal liabilities. Cloud resiliency helps to minimize these risks by ensuring that the system remains operational and data is accessible even during disruptions.\nFor example, if there is a power outage at a data center, a resilient cloud system would automatically switch to an alternative data center, ensuring that applications and data remain available to users without any interruption.\nAnother advantage of cloud resiliency is that it helps to reduce recovery time in case of disruptions. Instead of spending time and resources on restoring data and systems, a resilient cloud system can quickly recover from failures and resume operations.\nIn addition, it can also help businesses to comply with regulatory requirements related to data availability and disaster recovery.\nConclusion In today‚Äôs digital age, cloud resiliency has become a critical component of business continuity planning. By ensuring that cloud systems are designed to withstand disruptions and failures, businesses can minimize downtime, reduce recovery time, and protect themselves against potential risks. Therefore, it‚Äôs important for businesses to consider resiliency in cloud when selecting a cloud provider and designing their IT infrastructure.","why-cloud-resiliency-matters#\u003cstrong\u003eWhy cloud resiliency matters\u003c/strong\u003e.":""},"title":"Resiliency in the cloud: What it is and why it matters"},"/utho-docs/docs/revealing-ssl-crafting-a-web-connection-with-security/":{"data":{"":"","how-does-utho-elevate-data-protection-strategy-to-the-next-level#\u003cstrong\u003eHow does Utho elevate data protection strategy to the next level?\u003c/strong\u003e":"In our increasing dependence on the internet, the specter of cybercrime looms larger than ever. The repercussions of cybercrimes go beyond mere financial losses, encompassing risks such as reputational damage, operational disruptions, compromised business continuity, reduced productivity, and, most critically, the potential loss of online infrastructure. Given the escalating threats to businesses, a robust website security solution is absolutely imperative. This is precisely where Secure Sockets Layer steps in.\nWhat does an SSL certificate entail? An SSL certificate is a digital certificate that verifies a website‚Äôs identity and facilitates an encrypted connection. Secure Sockets Layer, is a security protocol establishing encrypted communication between a web server and a browser. Businesses use Secure Sockets Layer certificates on their websites to ensure the security of online transactions and protect customer information, maintaining privacy and security.\nWhat are the benefits of a Secure Sockets Layer certificate? Encryption:¬†Secure Sockets Layer certificates encrypt sensitive data, rendering it unreadable to third parties. This secure data transmission reassures end-users that their information is protected, fostering positive interactions with the website.\nSecure Icon: Secure Sockets Layer triggers a secure icon in the address bar, providing visitors assurance that they are on a secure site, safeguarding their credentials. This fosters trust, leading to an increased return on investment (ROI) over time as more customers engage with the site.\nPCI DSS Standards: Secure Sockets Layer certificates adhere to specific regulations known as PCI DSS standards. Government-mandated PCI DSS rules require the utilization of the latest encryption standards and a secure connection on the website, ensuring the safety of visitors and customers.\nBusiness Validation:¬†Secure Sockets Layer certificate authorities validate business credentials by verifying related documents, assuring end-users that they are interacting with the correct and verified organization. This instills confidence that their data is secure on the website, protected by robust encryption.\nWhat occurs when your SSL certificate reaches its expiration? Using an expired SSL certificate triggers browser warnings, causing user apprehension about sharing details on the website due to the risk of eavesdropping by third parties. Eavesdroppers may spy on information and misuse it against users. Swift renewal of an expired Secure Sockets Layer certificate is crucial to maintain user trust; otherwise, it can create a false impression of the company, negatively impacting overall ROI.\nWhat are the various types of Secure Sockets Layer certificates? Different SSL certificates cater to enterprise needs, involving a validation process for domain ownership and organizational details. Certification types include:\nExtended Validation SSL certificate: This Secure Sockets Layer version is crucial for companies aiming to enhance website legitimacy and build visitor trust. To acquire it, organizations must undergo a validation process, affirming domain ownership. This SSL certificate is particularly vital for conducting financial transactions or collecting highly sensitive data on a website.\nOrganization Validated SSL certificate: This type of Secure Sockets Layer isn‚Äôt designed for financial transactions; rather, it encrypts user activity data transmitted between the server and the web browser.\nDomain Validation SSL certificate: This type of Secure Socket Layer certificate provides only basic encryption with low-level security. It is suitable for blogs and personal websites, but for those needing higher levels of SSL encryption, alternative options should be considered.\nWildcard Validation SSL certificate: This Secure Sockets Layer allows enterprises to use a single SSL protocol certificate purchased for their domain across all subdomains. It presents a cost-effective alternative to obtaining a certificate for each individual subdomain.\nUnified Communications SSL certificate: This Secure Sockets Layer version offers SSL certification for multiple domains owned by an organization, covering up to 100 domains with a single certificate.\nHow does Utho elevate data protection strategy to the next level? Ensuring security in the cloud and cloud computing is paramount. Utho provides convenient access and streamlined validation processes for all your Secure Sockets Layer protocol needs. We also offer tailored recommendations for the best-suited Secure Sockets Layer offerings based on your specific requirements. Utho extends essential security features to our clients, ensuring the latest cybersecurity protection across every aspect of your organization.","what-are-the-benefits-of-a-secure-sockets-layer-certificate#\u003cstrong\u003eWhat are the benefits of a Secure Sockets Layer certificate?\u003c/strong\u003e":"","what-are-the-various-types-of-secure-sockets-layer-certificates#\u003cstrong\u003eWhat are the various types of Secure Sockets Layer certificates?\u003c/strong\u003e":"","what-does-an-ssl-certificate-entail#\u003cstrong\u003eWhat does an SSL certificate entail?\u003c/strong\u003e":"","what-occurs-when-your-ssl-certificate-reaches-its-expiration#\u003cstrong\u003eWhat occurs when your SSL certificate reaches its expiration?\u003c/strong\u003e":""},"title":"Revealing SSL: Crafting a Web Connection with Security"},"/utho-docs/docs/rewrite-urls-using-mod_rewrite-and-apache/":{"data":{"":"","before-you-begin#Before You Begin":"","redirection-codes-in-mod_rewrite#Redirection Codes in mod_rewrite":"You‚Äôll learn how to rewrite the URLs using mod rewrite and Apache in this article. Rewrite a URL is a server-side operation which can serve contents from a position of a file system that does not exactly suit the client‚Äôs request. It could be helpful to enhance the readability of URLs by search engines and users, or to upgrade tools when the layout of your site changes.\nBefore You Begin 1. This guide assumes that you have followed our Getting Started and Securing Your Server guides, and that you have already configured your Apache installation. If you haven‚Äôt, please refer to our Apache guides or LAMP stack guides.\n2. We‚Äôre going to change Apache configuration files in this guide so make sure you have the correct permissions to do so.\n3. Update your system.\nsudo apt-get update \u0026\u0026 sudo apt-get upgrade Rewrite URLs In a \u003cDirectory\u003e block (usually a virtual host file) or a .htaccess file, enable mod rewrite:\nRewriteEngine on You can set up several separate rules for rewrite. Such rules include a template comparable by the server against incoming requests. If a request fits a pattern for rewrite, the server will change the request as defined in the rule and will process the request. Here is an example of a law on rewrite:\nRewriteRule ^post-id/([0-9]+)$ /posts/$1.html Let‚Äôs explain this rule: the first string is a pattern to match incoming requests. The second string specifies the actual files that are to be served. Use regular expression syntax to rewrite patterns. The ^ defines the beginning of the string, and the $defines the end of the string, which means that the rewrite engine will not rewrite strings that match only part of the pattern.\nThe string rewrites all URLs specifying paths beginning with /post-id/ and containing one or more numbers (e.g. [0-9]+) in the /posts/ directory, providing a corresponding .html file. The parenthetical term or terms in the pattern define a variable that is passed as $1, $2, $3 and so on to the second string, depending on how many parentheticals are given in the first string.\nYou can build and implement several rewrite rules, then sequentially implement those rules. The order in which you define RewriteRules will affect the rules match.\nOptionally, you can insert a directive on RewriteBase to change the rewrite rules actions. Suppose we:\nThe /srv/www/abc.com/public html/ directory defines such directives. Some users make a request in the form http:/abc.com/post-id/200, where 200 may be more than one digit in number. Some users send requests in the http:/abc.com/page/title-of-page type, where ‚Äúpage title‚Äù may represent any string of characters. The files are stored on the /srv/www/abc.com/public html/objects/ directory and fit the requested object in name, but have an extension of .html. RewriteEngine on RewriteBase /objects RewriteRule ^post-id/([0-9]+)$ $1.html RewriteRule ^page/([^/]+)$ $1.html The rewrite rules outlined above will apply for:\nhttp:/abc.com/post-id/200/ and the /srv/www/abc.com/public html/ objects/200.html file http:/abc.com/page/free-toast/ and support the /srv/abc.com/public html/objects/free-toast.html file This is useful if the file system positions don‚Äôt match the URLs as requested by the client. This is also particularly useful when a single file, for example index.php, dynamically generates all requests.\nRewrite URLs Under Specific Conditions You can set the conditions under which a RewriteRule is to be used with the parameter RewriteCond. Let us take the following example from the WordPress application‚Äôs default rewrite rules:\nRewriteEngine On RewriteBase / RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteRule . /index.php [L] In this example, all requests that begin with the top level of the context are affected by the rewrite rules. This is defined in the directive RewriteBase /. The context is determined by where the rewrite rules are specified in the virtual host, directory block, or .htaccess file.\nThe statements in RewriteCond instruct Apache to enforce only the rule that follows them if their requirements are fulfilled. The requested file name in the above example does not need to match an existing file on the file system! (-f) and does not match an existing directory on the file system! (-d).\nIf both conditions are true and there is no file or directory that matches the request, Apache will apply the rewrite rule. For example, if the user asks for http:/abc.com/? Post=123 or http:/abc.com/post/123 will the server return the result for index.php? Post=123 or the.php/post/123 index, respectively.\nMultiple RewriteCond is connected to logical AND operators in such a way that all conditions are true in order for the RewriteRule to apply to that request. You may also add a [OR] statement at the end of the RewriteCond directive to add a list of conditions to the logical OR and create a number of possible conditions where the request is rewritten by a single RewriteRule. For more information on rewrite conditions, see the Apache mod rewrite documentation.\nRedirection Codes in mod_rewrite Finally, there are a number of codes that you can add to the RewriteRule that change the behavior of the rewrite. Rewrite Rule in the previous example. /index.php [L], we used the [L] option that stands for ‚Äúlast rule.‚Äù This prevents Apache from applying any additional rewrite rules to that rule. There are a few common options:\nF tells the client that the URL is forbidden to respond with HTTP code 403. N forces mod rewrite to restart the rewrite process, and enables multi-stage rewrite. R informs the client that the requested page has passed, with the temporary redirection of HTTP code 302. Using \"R=301\" when the page has relocated permanently. You may also specify multiple options at the end of a RewriteRule separating them with commas: [L,R=301]\nThankyou..","rewrite-urls#Rewrite URLs":"","rewrite-urls-under-specific-conditions#Rewrite URLs Under Specific Conditions":""},"title":"Rewrite URLs using mod_rewrite and Apache"},"/utho-docs/docs/rule-based-access-control-for-apache/":{"data":{"":"\nApache provides a set of tools that allow administrators to manage server-provided access to specific resources. You can already know about authentication-based access controls, which allow visitors to authenticate the server before accessing resources.\nThe rule-based access control implemented by Apache allows you to determine which visitors have access to certain resources at a very granular level. You may build rules that block a specified set of IPs from your web server, or access a particular resource, or even just access a certain virtual host.\nThe most fundamental application of rule-based access control is to set firm limits on which resources are accessible through network link. The Web server refuses all users access to all files on the network in the default Apache configuration. Instead, Apache helps administrators to control unique tools.\nAdditional uses for such access rules include blocking unique IP ranges that have been responsible for malicious traffic and restricting ‚Äúinternal users‚Äù access to a specified resource or collection of resources, among a variety of other possibilities.\nAdditional uses for these access rules include blocking specific IP ranges that were responsible for malicious traffic and limiting access to a given resource or set of resources to ‚Äúinternal users‚Äù among a number of other options.","additional-access-control-rules#Additional Access Control Rules":"By changing and broadening the example above, you may define granular access control rules for your resources. The notes and suggestions below provide some insight into some of the more advanced features that such access control systems can provide.","advanced-access-control#Advanced Access Control":"Although IP address is by far the simplest way to use these access control rules to manage access, Apache offers a variety of additional methods.\nFirstly, Apache allows administrators to require or deny access based on the requester‚Äôs hostname. This forces Apache to look up the hostname running the request to do a reverse DNS (rDNS) lookup and then allow or refuse access based on that information. Consider the following example:\nOrder Deny,Allow Deny from all Allow from hostname.abc.com In this setup, Apache only allows requests from the hostname.abc.com machine with valid rDNS to access the resource.\nSecondly, you can create access rules in the HTTP session around environment variables. This enables you to allow and deny access to resources on the basis of variables such as browser (user agent) and referrer. Take, for example, the following:\nSetEnvIf Referer searchenginez.com search_traffic Order Deny,Allow Deny from all Allow from env=search_traffic This access control rule works with mod_setenvif of Apache. First, if a request reference matches searchenginez.com the search_traffic environment variable is set. Next, all hosts are denied resource access. Finally, requests with the search_traffic environment variable set allow access to the resource. For more information on setting and using environment variables, please review the official Apache documentation for mod_setenvif.\nThankyou..","controlling-access-for-a-range-of-ips#Controlling Access for a Range of IPs":"Apache allows this with the following syntax if you want to manage access for a number of IP addresses and not for a single address:\nOrder Deny,Allow Deny from all Allow from 192.Allow from 10 The statements above provide for all addresses starting with 192.168 and 10. Such IP ranges are usually reserved for local networking, and are not adresses that can be routed publicly. If used, certain access control regulations can only permit traffic on the network from ‚Äúlocal sources.‚Äù\nHere is an additional example of an access rule:\nOrder Allow,Deny Allow from all Deny from 185.201.1 This rule requires access to the provided resource for all, and then refuses access to all IP addresses beginning with 185.201.1. That statement will include all traffic originating from the 185.201.1.0 to 185.201.1.255 IP address range.\nBe very careful that these directives are placed in the proper sense when developing access control rules, especially those that use the Allow from all directives.","examples-of-rule-based-access-control#Examples of Rule Based Access Control":"You may want to consult our Apache Configuration Structure Guide to see a number of examples of these directives in practice.\nHere is an example of a basic rule:\nOrder Deny,Allow Deny from all Allow from 192.168.2.101 To analyze this in simpler terms:\nThe regulation Order Deny, Allow inform the web server that ‚ÄúDeny‚Äù rules should be processed prior to Allow rules. The Deny from all directives inform the web server that access to the provided resource should be denied to all users. First of all, this rule is processed. The Allow from directive informs the web server that it will accept requests originating at IP address 192.168.2.101. This is handled last, and reflects a derogation the¬†Deny from all¬†rule. In short, access to the resource is denied to all hosts except for 192.168.2.101."},"title":"Rule-based Access Control for Apache"},"/utho-docs/docs/secure-and-govern-the-lifecycle-of-data-with-snapshots-protection/":{"data":{"":"","heading#":"As mission-critical data volumes rise, the need for protection grows. Traditional disk-to-disk copies are time and space-intensive, leading to increased storage costs. The technology emerges as an efficient solution, offering data protection, mining, and cloning support. Many storage vendors now integrate snapshot technology to provide advanced data protection for critical business needs.\nWhat does Snapshots refer to? These are essentially instantaneous disk images capturing the state of a server, virtual machine, or storage system at a particular point in time. As the term implies, they represent a quick capture of the server‚Äôs files and configurations, preserving system settings for potential future use. Beyond facilitating rollbacks, It proves valuable for duplicating settings to deploy on additional servers or storage systems.\nWhat purposes do snapshots serve? It find applications in version control, acting as a safeguard against potential system damage during activities such as upgrades, software installations, and component uninstallations. Consequently, their widespread utilization in development and testing is driven by their ability to restore recently modified data.\nWhat are the distinct categories of snapshots? While the implementation of a data snapshot may differ among vendors, several widely used techniques exist for generating and integrating snapshots.\nCopy-on-write: The copy-on-write snapshots contain metadata detailing the altered data blocks (copies on writes) since its creation. These are nearly instantaneous as they avoid duplicating the metadata. However, their performance is resource-intensive, demanding three I/O operations for each write‚Äî one read and two writes.\nRedirect-on-write: Redirect-on-write snapshots employs pointers to indicate snapshot-protected blocks, allowing the original copy to retain point-in-time snapshot data while altered data is stored in the snapshot storage. This method is more efficient in terms of performance resources, as each modified block triggers only a single write IO. Nevertheless, if a snapshot is deleted, the reconciliation process between multiple new blocks and the original block becomes intricate and perplexing.\nContinuous data protection (CDP): CDP snapshots are generated in real-time, updating the snapshot of the original copy whenever a change occurs. This facilitates ongoing capturing and monitoring of data modifications, automatically preserving every version of the data created by the user, either locally or at a target repository. However, the frequent creation and updates of snapshots can impact network performance and consume bandwidth.\nClone/mirroring: A clone or mirror snapshots constitutes an exact replica of the entire storage volume, rather than just snapshots of updated data. This approach allows for straightforward data recovery, replication, and archiving, as the complete volume remains accessible even if the primary/original copy is compromised. However, the process of saving such extensive data volumes tends to be slow and demands substantial storage space.\nWhat are the advantages and disadvantages of using this technology?\nKey benefits of utilizing storage snapshots for backup and recovery.\nAllow for quicker restoration to a previous point in time when compared to backups.\nEffortlessly created with swift execution, It has no impact on the production server.\nBy eliminating the necessity for Windows native backup solutions, it contributes to a reduction in the total cost of ownership (TCO).\nYet, it has drawbacks worth considering before relying solely on them for backup and recovery. Susceptible to disruptions impacting the production server.\nEngages a significant portion of the primary storage capacity.\nLacks granularity, requiring the recovery of data in its entirety as individual files cannot be restored from snapshots.\nHow Does Utho provide comprehensive data protection?\nUtho provides cutting-edge data protection through its snapshot-based backup and recovery solutions. By leveraging the advantages and generating full backups, it enables swift and dependable recovery with the flexibility of single-file restoration. Additionally, it supports full data restoration and live mounts, allowing for the restoration of a complete virtual machine from a backup in just seconds.","what-are-the-distinct-categories-of-snapshots#\u003cstrong\u003eWhat are the distinct categories of snapshots?\u003c/strong\u003e":"","what-does-snapshots-refer-to#\u003cstrong\u003eWhat does Snapshots refer to?\u003c/strong\u003e":"","what-purposes-do-snapshots-serve#\u003cstrong\u003eWhat purposes do snapshots serve?\u003c/strong\u003e":"","yet-it-has-drawbacks-worth-considering-before-relying-solely-on-them-for-backup-and-recovery#\u003cstrong\u003eYet, it has drawbacks worth considering before relying solely on them for backup and recovery.\u003c/strong\u003e":""},"title":"Secure and Govern the Lifecycle of Data with Snapshots Protection"},"/utho-docs/docs/securing-cloud-perimeter-digital-backbone-to-your-business/":{"data":{"":"","what-does-the-term-cloud-virtual-firewall-mean#\u003cstrong\u003eWhat does the term Cloud Virtual Firewall mean?\u003c/strong\u003e":"In today‚Äôs digital landscape, where businesses heavily rely on cloud infrastructure and services, establishing a robust security perimeter is crucial to safeguard private data and defend against cyber threats.¬†Firewalls, integral to network security, play a crucial role in protecting cloud environments, ensuring the security, privacy, and availability of critical resources. This discussion highlights the significance of firewalls in cloud-based managed services and their role in establishing a secure perimeter.\nWhat does the term Cloud Virtual Firewall mean? Cloud Firewalls, also referred to as Firewall-as-a-Service (FWaaS), are security solutions deployed in the cloud. These software-based products play a crucial role in preventing cyber-attacks by creating a protective barrier around cloud assets. These assets include cloud platforms, stored data, infrastructure, and applications, shielding them from potentially harmful internet traffic. Cloud-based firewalls extend their protective measures to both the internal/private network and on-premise assets. Often provided as a service by third-party vendors, these solutions contribute to enhancing overall cybersecurity.\nWhat are the benefits of utilizing a Cloud Virtual Firewall? **In this segment, we will explore the advantages of opting for a cloud firewall.\n**\nEnhanced Security: Cloud firewalls enhance security for cloud-based resources by offering an additional layer of protection. They play a key role in safeguarding against unauthorized access, mitigating DDoS attacks, and identifying and blocking malicious traffic. Through the monitoring and filtering of both incoming and outgoing network traffic, It contribute significantly to establishing a robust security posture for organizations.\nScalability: Exhibit scalability in alignment with the requirements of the cloud environment. They efficiently manage substantial volumes of network traffic, guaranteeing optimal performance and minimal latency. As businesses extend their cloud infrastructure and experience growth, cloud firewalls can flexibly adjust to meet escalating demands, delivering seamless security at scale.\nCentralized Management: Cloud firewalls provide centralized management and control of security policies. Administrators have the capability to define and enforce consistent security rules across various cloud instances, regions, or even different cloud providers. This streamlines security management, diminishes administrative overhead, and guarantees uniform security configurations across the organization‚Äôs cloud infrastructure.\nFlexibility and Agility: Cloud firewalls enable organizations to tailor security policies to meet their specific requirements. Administrators can establish rules to permit or restrict specific protocols, ports, or IP addresses, offering granular control over network traffic. This flexibility empowers organizations to align their security measures with their distinct business needs and compliance requirements.\nMigration Security: Migration, especially to the cloud, carries security risks as data traverses from one location to another. Cloud firewalls mitigate these concerns by offering inherent end-to-end security throughout the data migration process. Data is safeguarded at the source, during transit, and at the destination, ensuring a secure and smooth cloud migration akin to a protected convoy for your data.\nWhat are the drawbacks of using Cloud Virtual Firewalls? While this security perimeter come with their benefits, they, like all things positive, also come with drawbacks.\nInherent Complexity: Configuring and managing cloud firewalls can pose complexity, especially for organizations with limited security expertise. Establishing intricate security rules and ensuring accurate configurations demand a profound understanding of network protocols and potential threats. To effectively manage and optimize cloud virtual firewall deployments, organizations might need to invest in training or enlist external expertise.\nPotential False Positives: Depend on predefined rules and heuristics to detect and block potentially malicious traffic. However, these rules may at times produce false positives, incorrectly flagging legitimate network traffic as suspicious or harmful. False positives can disrupt regular business operations or lead to unnecessary inconvenience, necessitating extra time and effort for investigation and resolution.\nPerformance Impact: The incorporation of a cloud firewall into the network path has the potential to introduce latency and affect network performance. The inspection and filtering of network packets demand computational resources, and high traffic volumes or intricate rule sets may potentially degrade network throughput. Organizations should conscientiously evaluate the performance implications and optimize firewall configurations to mitigate any impact on network performance**.**\nWhat are the different types? There are two categories of cloud firewalls, distinguished by the specific security needs they address.\nSaaS Firewalls: SaaS Firewalls aim to secure an organization‚Äôs network and users, similar to traditional on-premises hardware or software firewalls, with the distinction that it is deployed off-site in the cloud. This category of firewall is alternatively referred to Software-as-a-service firewall (SaaS firewall), Security-as-a-service (SECaaS), or¬†Firewall-as-a-service (FWaaS).\nNext-Generation Firewalls: Next Generation Firewalls are cloud-based services designed for deployment within a virtual data center, safeguarding an organization‚Äôs servers in a platform-as-a-service (PaaS) or infrastructure-as-a-service (IaaS) model. The firewall application operates on a virtual server, ensuring the security of incoming and outgoing traffic between cloud-based applications.\nHow does Utho fortify enterprise cloud security amidst today‚Äôs risk landscape? Utho offers an AI-powered, scalable Next-Generation Firewall, serving as the optimal solution for maintaining enterprise network security resilience in today‚Äôs risk landscape. Utho integrates visibility, simplicity, control, and protection into a unified package, designed to secure, converge, and scale enterprise security. Empowered by AI and automation, Utho is crafted to provide the most effective network protection in the industry."},"title":"Securing Cloud Perimeter: Digital Backbone to your Business"},"/utho-docs/docs/securing-connectivity-power-of-ssh-keys-for-network-safety/":{"data":{"":"","how-can-ssh-keys-be-utilized#\u003cstrong\u003eHow can SSH Keys be utilized?\u003c/strong\u003e":"","what-advantages-does-ssh-key-authentication-offer-compared-to-password-authentication#\u003cstrong\u003eWhat advantages does SSH key authentication offer compared to password authentication?\u003c/strong\u003e":"","what-is-a-secure-shell-#\u003cstrong\u003eWhat is a Secure Shell ?\u003c/strong\u003e":"","what-is-the-functioning-mechanism-of-ssh-key-authentication#\u003cstrong\u003eWhat is the functioning mechanism of SSH key authentication?\u003c/strong\u003e":"","what-sets-utho-apart-in-terms#\u003cstrong\u003eWhat sets Utho apart in terms?\u003c/strong\u003e":"Enterprises universally depend on Secure Shell (SSH) keys for authenticating privileged users and ensuring trusted access to vital systems like application servers, routers, firewalls, virtual machines, cloud instances, and various devices. Not solely restricted to privileged administrative operations, It play a pivotal role in secure machine-to-machine automation for critical business functions. Once established for client authentication, facilitate seamless, password-free, and automated connections between systems.\nWhat is a Secure Shell ? An SSH key is a secure access credential for the SSH (secure shell) network protocol. This authenticated and encrypted protocol is utilized for remote communication between machines on an unsecured open network. SSH serves as a secure solution for remote file transfer, network management, and remote operating system access. The term ‚ÄúSSH‚Äù also refers to a collection of tools designed for interaction with the SSH protocol.\nWhat is the functioning mechanism of SSH key authentication? It consist of a public-private pair. The public key is publicly available, whereas the private key remains confidential. The public key encrypts data, and only the corresponding private key can decrypt it. This ensures that only the owner, holding the private key, can access the information. Even if a third party intercepts the public key, forging a connection is impossible without the private key.\nHow can SSH Keys be utilized? The company utilizes an SSH key for secure online authentication and communication. This provides users with easy access, bypassing the complexities associated with traditional operations.\nEnsuring secure communication between local computers and remote hosts enables the establishment of connections and the issuance of necessary commands.\nSafely perform tasks like updates, backups, software installations, system repairs, and other remote administration operations.\nSSH establishes a secure, automated connection with the server for performing operations such as making adjustments, uploading files, creating backups, changing configurations, and more.\nThe SSH key provides access without passwords for two or more user accounts through a single sign-on.\nWhat advantages does SSH key authentication offer compared to password authentication? SSH key authentication offers various benefits compared to using passwords.\nHighly secure authentication method: For businesses with sensitive data, using SSH keys for SFTP servers adds strong security. The keys can be extremely long, making them highly resistant to hacking, equivalent to a password with at least 12 uncommon characters.\nMitigates vulnerabilities associated with passwords: Traditional passwords, often easy to guess or reused, pose security risks. SSH-keys, automatically generated and unique, eliminate these vulnerabilities.\nGuarantees the use of only authorized devices: Logging in with usernames and passwords allows access from any device, even insecure ones. SSH-keys, on the other hand, only permit access from approved devices with stored keys, preventing unauthorized use.\nKeeps things secure during an attack: Even with your best efforts, data breaches can happen. If you use passwords and the server is compromised, hackers can steal the password. By disabling passwords and requiring SSH keys, even during a hack attempt, the attacker won‚Äôt be able to access the user account.\nWhat sets Utho apart in terms? At Utho, we‚Äôve streamlined the process, eliminating the complexity of setting up the Public through the Command Line Interface. Now, you can effortlessly assign your Public SSH key directly from the Cloud Console."},"title":"Securing Connectivity: Power of SSH Keys for Network Safety"},"/utho-docs/docs/simplify-and-deploy-effortless-kubernetes-on-bare-metal/":{"data":{"":"","benefits-of-running-kubernetes-directly-on-bare-metal#\u003cstrong\u003eBenefits of Running Kubernetes Directly on Bare Metal\u003c/strong\u003e":"","choosing-between-vms-and-bare-metal-factors-to-consider#\u003cstrong\u003eChoosing Between VMs and Bare Metal: Factors to Consider\u003c/strong\u003e":"","comparing-kubernetes-deployment-choices-bare-metal-vs-virtual-machines#\u003cstrong\u003eComparing Kubernetes Deployment Choices: Bare Metal vs. Virtual Machines\u003c/strong\u003e":"","heading#":"","optimal-strategies-for-running-kubernetes-on-bare-metal#\u003cstrong\u003eOptimal Strategies for Running Kubernetes on Bare Metal\u003c/strong\u003e":"","simplify-bare-metal-kubernetes-management-with-utho#\u003cstrong\u003eSimplify Bare Metal Kubernetes Management with Utho\u003c/strong\u003e":"Running Kubernetes directly on bare metal servers presents several benefits, such as improved performance and reduced costs. While it may not always be the ideal choice, especially without the right infrastructure, it‚Äôs a worthwhile option to explore.\nIn the following guide, we‚Äôll discuss important factors to consider when opting for bare-metal Kubernetes deployment. Additionally, we‚Äôll outline the steps to plan and set up a bare-metal Kubernetes cluster, ensuring optimal performance and cost-effectiveness.\nComparing Kubernetes Deployment Choices: Bare Metal vs. Virtual Machines Deploying Kubernetes on bare metal means setting up a cluster using physical servers instead of virtual machines.\nIf you‚Äôre deploying Kubernetes on-premises or creating a self-managed cluster on a cloud provider‚Äôs infrastructure-as-a-service (IaaS) platform, you have the option to configure your nodes on bare-metal servers instead of VMs. This approach allows you to run Kubernetes directly on bare metal.\n**(All Kubernetes nodes run on bare-metal servers. The distinction lies in whether the node is bare metal or VM-based. In the case of bare metal, there‚Äôs no hypervisor separating the node from the underlying server. Conversely, a VM-based node operates on top of a physical server.)\n**\nBenefits of Running Kubernetes Directly on Bare Metal Running Kubernetes on bare metal is preferred for two primary reasons: cost-effectiveness and enhanced performance.\n**Cost\n**\nOpting for bare-metal nodes in your Kubernetes cluster can lead to lower total cost of ownership due to several factors:\nNo Virtualization Software Costs: You eliminate expenses associated with virtualization software.\nSimplified Management: Maintenance becomes easier without a virtualization layer, reducing labor costs.\nNo Hypervisor Overhead: All server resources are dedicated to running workloads, resulting in lower infrastructure costs.\nThe savings from avoiding virtual machines (VMs) for Kubernetes can be substantial. According to Utho, Kubernetes running on VMs can incur up to 30% higher costs compared to bare metal. However, actual savings vary based on workload types and cluster configurations.\nManagement\nBare-metal Kubernetes offers enhanced control and streamlines administration in several key aspects:\nNetwork Configuration: With virtualized infrastructure removed, setting up networking becomes simpler in bare-metal Kubernetes.\nTroubleshooting: Bare-metal infrastructure reduces complexity, making troubleshooting more straightforward. Identifying issues is easier without the added layer of virtualization.\nAutomation and Deployment: Automating services and deploying software on bare-metal clusters is often simpler due to the absence of a virtualized infrastructure layer.\nPerformance\nWhen it comes to performance, bare-metal Kubernetes offers significant advantages.\nAccording to trusted sources, that bare metal Kubernetes experiences three times lower network latency. Additionally, a study by trusted sources found that containers running on bare metal outperform those on VMs by 25-30%. It‚Äôs important to highlight that the research focused on standalone Docker containers, not those operating within a Kubernetes cluster.\nThese performance disparities between bare metal and VM-based environments might seem surprising, considering that hypervisor overhead typically consumes only around 10% of total infrastructure resources.\nHowever, performance hits in virtualized environments aren‚Äôt solely due to hypervisor overhead. You also incur resource consumption from guest operating system environments, which utilize memory and CPU even during idle periods. Moreover, in multi-tenant VM environments, noisy-neighbor issues can arise, impacting performance across VMs sharing the same server. If VMs are managed by an orchestrator on the host server, additional resource consumption occurs.\nDeploying bare-metal servers within an edge architecture can significantly boost performance by leveraging the efficiency of bare metal and the low latency characteristic of edge servers.\nAnother critical factor affecting performance is the application‚Äôs reliance on access to bare-metal resources. If your Kubernetes-hosted applications can benefit from direct access to hardware devices such as GPUs, deploying on bare metal can lead to substantial performance gains.\nAdditionally, it‚Äôs essential to recognize that virtualization introduces another layer to your software stack, which could potentially cause performance issues if it malfunctions. A hypervisor crash that affects a node could disrupt the resources provided to your Kubernetes cluster, potentially impacting the performance of applications running within Kubernetes.\nConsiderations and Challenges in Bare-Metal Kubernetes Deployment\nBare-metal Kubernetes clusters have two main drawbacks: management complexity and resilience to node failures.\nManagement\nIn comparison to bare-metal servers, managing VMs tends to be simpler, as IT professionals are more familiar with tools for virtualized and cloud-based environments. With scripts or VM orchestration tools, you can swiftly deploy numerous VMs using prebuilt images. These images can also be used for VM backups and restoration in case of failure. Most virtualization platforms offer snapshotting features, enabling you to save VM states at different points in time, along with automated failover tools for restarting failed VMs.\nWhile similar functionality exists for bare-metal servers, implementation is notably more complex. It‚Äôs possible to create Linux server images for provisioning additional servers, or to develop custom Bash scripts for automated failover. However, building and maintaining such tooling demands significant effort and isn‚Äôt as straightforward as with VM platforms.\nIt‚Äôs important to note that certain VM platforms offer more advanced management and orchestration capabilities than others.\nConfiguration\nIn addition to the straightforward management of VM images, VMs typically offer greater configuration flexibility. Setting up networking, storage, and other resources might be more complex with bare-metal servers, especially if they have unconventional interfaces or lack adequate support. On the other hand, mainstream virtualization platforms are generally compatible with various Kubernetes configurations. Many also provide multiple types of virtual network interfaces, enhancing flexibility even further.\nChoosing Between VMs and Bare Metal: Factors to Consider In summary, when deciding between Kubernetes on bare metal and Kubernetes on VMs, consider the following factors:\nCost: If you‚Äôre on a tight budget or if your chosen virtualization platform is costly, bare metal might be the better option.\nPerformance: Are you aiming for maximum performance for your applications, or are you willing to accept a potential performance hit of up to 30% with VMs?\nHardware Acceleration**:** If any of your applications require direct access to hardware resources, bare metal is the preferred choice.\nManagement: Consider your readiness and capacity to handle the additional management complexity associated with bare-metal servers.\nResiliency: Evaluate how many node failures your cluster can withstand. If your tolerance is limited, VMs might be preferable to distribute risk across multiple nodes.\nOptimal Strategies for Running Kubernetes on Bare Metal For maximizing the benefits of bare-metal nodes, consider the following strategies:\nOpt for smaller nodes: Generally, smaller bare-metal nodes are preferable. Having a larger number of lower-power nodes enhances resilience compared to a smaller number of high-end nodes.\nChoose standardized hardware: Select standard, mainstream servers to avoid hardware compatibility issues. Avoid obscure vendors and overly cutting-edge hardware that may lack adequate support.\nExplore cloud options: If managing on-premises bare-metal servers is challenging, consider deploying bare-metal server instances in a public cloud. This approach alleviates much of the management burden by outsourcing physical hardware maintenance.\nMaintain consistent OS versions: Simplify server management by ensuring each node runs the same version of the same operating system.\nUtilize bare-metal management tools: Employ management solutions specifically designed for bare metal to streamline operations and mitigate risks.\nSimplify Bare Metal Kubernetes Management with Utho If the idea of manually configuring and overseeing each bare-metal node feels overwhelming, Utho‚Äôs managed bare metal service might be the solution. Utho offers a bare metal controller that automates server management, along with a user-friendly SaaS management platform for environment administration.\nWith Utho‚Äôs managed bare metal service, you can easily convert bare-metal servers into a Kubernetes cluster in just minutes. Experience the cost-efficiency and performance advantages of bare-metal Kubernetes without the hassle of manual management."},"title":"Simplify and Deploy: Effortless Kubernetes on Bare Metal"},"/utho-docs/docs/tuning-of-your-apache-server/":{"data":{"":"","apache-mod_status#Apache mod_status":"Your Apache configuration settings have a significant impact on the output of your Microhost Cloud server. There are several tools that can be used to further examine the performance of your Apache server and make educated choices on how to start tuning Apache configurations. The guide include an overview of some of the process monitoring and device resource usage that can be used to inspect how Apache affects the output of your Microhost Cloud Server. You will also learn about important Apache modules, such as Multi-Processing modules, which will allow you to make use of Apache‚Äôs power and flexibility.\nTools [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]The steps in this guide claim the rights of origin. Be sure to run the steps below either as root or with the sudo prefix. Please see our Users and Groups Guide for more detail on rights.[/ht_message]\nThere are a number of tools that can help decide whether resource settings need to be modified, including the top command and the Siege load-testing system. Microhost cloud server‚Äôs own Longview software will also aid in the control of servers. A good place to start is getting acquainted with your server‚Äôs RAM and CPU use.\nDiscover consumption information with the ps command variations below. The ps command is used to produce a report about the processes running on your Microhost cloud server:\necho [PID] [MEM] [PATH] \u0026\u0026 ps aux | awk '{print $2, $4, $11}' | sort -k2rn | head -n 20 ps -eo pcpu,pid,user,args | sort -k 1 -r | head -20 Apache mod_status The Apache Status module, mod_status, offers comprehensive status page output information about your server.\n1. Open the Configuration file for your website. This file is located on Debian / Ubuntu systems at /etc/apache2/sites-available/hostname.abc.com.conf, or /etc/httpd/conf.d/vhost.conf¬†on CentOS/Fedora systems.\n2. Add the below to the¬†\u003cvirtual_hosts\u003e¬†block:\nSetHandler server-status Order Deny,Allow Deny from all Allow from localhost 3. Apache mod_status also offers an ExtendedStatus option, which provides additional details about every request that Apache has made. Edit your Apache configuration file to allow ExtendedStatus, and add the following line:\nExtendedStatus On [ht_message mstyle=‚Äúalert‚Äù title=‚ÄúNOTE‚Äù \" show_icon=‚Äútrue‚Äù id=\"\" class=‚Äú‚Äústyle=‚Äù‚Äù ]Enabling ExtendedStatus takes extra machine resources[/ht_message]\n4. Restart Apache:\nDebian/Ubuntu: ``` systemctl restart apache2 - CentOS/Fedora: ``` systemctl restart httpd 5. To view the created file, you can download Lynx, a web browse in text mode:\nDebian/Ubuntu: ``` apt-get install lynx - Fedora/CentOS: ``` yum install lynx 6. Open the file:\nlynx http://localhost/server-status ","apache2buddy#Apache2Buddy":"Similar to MySQLTuner, the Apache2Buddy script tests your Apache setup, and makes recommendations based on your Apache process memory and overall RAM. While it is a fairly simple program which focuses on the MaxClients directive, it is useful to use Apache2Buddy. The script can be executed with the following command:\ncurl -sL https://raw.githubusercontent.com/richardforth/apache2buddy/master/apache2buddy.pl | sudo perl ","event#Event":"The event module is available on Apache 2.4 only, and is based on the MPM worker. Like the worker, it creates multiple threads per child process, with a thread dedicated to KeepAlive connections which will be passed on to child threads once the request is made. This is good for multiple simultaneous connections, especially those that are not all simultaneously active but make the occasional request. In the case of SSL connections, the MPM event works the same as the worker.","keepalive#KeepAlive":"KeepAlive allows connecting clients to make several requests using a single TCP connection, instead of opening one new one for each request. It decreases page load times and increases the use of the CPU for your web server, at the cost of increasing the usage of RAM on your computer. For the MaxConnectionsPerChild a KeepAlive connection will be counted as a single ‚Äúorder.‚Äù\nIn the past, this setting was mostly disabled to maintain RAM use, but server resources have become less costly, and Apache 2.4 now allows the option by default. Enabling KeepAlive will improve the user experience of your site considerably, so be careful not to disable it without checking the effects of doing so. In your web server setup, or within a Virtual Host block, KeepAlive can be allowed or deactivated.\nThankyou‚Ä¶","maxconnectionsperchild#MaxConnectionsPerChild":"MaxConnectionsPerChild restricts the amount of requests that a child process interacts with over his lifetime. After reaching the mark, the child cycle dies. The child cycle shall never expire if set to 0. With this the recommended value is a few thousand, to prevent memory leakage. Be mindful that setting this too low can slow the system down, as developing new processes takes on resources. MaxRequestsPerChild was named for this setting in versions lower than Apache 2.3.9.","maxrequestworkers#MaxRequestWorkers":"This parameter, formerly known as MaxClients (Apache 2.3.13 or lower), shows the maximum quantity of requests that can be served concurrently, with any amount going beyond the queued limit. The queue size is dependent on directive ListenBacklog. If MaxRequestWorkers is set too low, connections will ultimately be sent time-out to the queue; but, if set too high, this will cause the memory to swap. If that value is increased past 256, the value of ServerLimit must be increased as well.\nOne way to determine the best value for this is to divide the amount of RAM each Apache process uses by the available amount of RAM, leaving some space for other processes. Using Apache2Buddy, or the commands below, to help evaluate certain values.\nThe following command is given for determining the RAM each Apache method uses. The value of the Resident Set Size (RSS) shows the RAM currently being used in kilobytes by a device. On Debian or Ubuntu systems replace httpd with apache2:\nps -ylC httpd --sort:rss To convert it to megabytes divide the number under the RSS column by 1024.\nTo get the memory usage information:\nfree -m Use the top command to get a more detailed view of the resources which Apache uses.","maxspareservers#MaxSpareServers":"This parameter sets maximum number of idle processes for children. If more idle processes exist than this number, then they will be terminated. This number shouldn‚Äôt be set too high unless your website is incredibly busy, because even idle processes consume resources.","minspareservers#MinSpareServers":"Sets minimum number of idle processes for child process. If there are less processes than the MinSpareServer number, then Apache 2.2 or lower produces more processes at a rate of one per second. With Apache 2.4, this rate increases exponentially starting with 1 and ending with 32 spawned children per second. The advantage of this value is that it can take an empty thread when a request comes in; should a thread not be available, Apache will need to spawn a new kid, take up resources and prolong the time it takes for the request to go through. Remember that too many idle processes on the server will have an adverse impact too. It would only be appropriate to tune this value on very busy sites. Changing the value to a large number isn‚Äôt recommended.","module-values#Module Values":"After you have selected your MPM, the values within the configuration need to adjust. Such settings are found on Debian / Ubuntu in /etc/apache2/apache2.conf, and on CentOS / Fedora in /etc/httpd/conf/httpd.conf. Here is an example of MPM Prefork Module configuration:\nStartServers MinSpareServers MaxSpareServers MaxRequestWorkers MaxConnectionsPerChild 4500 Replace \u003cIfModule mpm_prefork_module\u003e¬†with \u003cIfModule mpm_worker_module\u003e¬†or \u003cIfModule mpm_event_module\u003e for use with the worker or event modules, respectively.\nNext, the module settings you‚Äôve added in the previous phase should be updated. In do so, you should bear in mind what every interest does, and how best in change it. It is recommended that you make small adjustments to the software settings and track the results afterwards.\nThe following sections provide an overview of any MPM module environment","multi-processing-modules#Multi Processing Modules":"Apache version 2.4 provides three Multi Processing Modules (MPMs) for the settings management. Every module produces processes for children, but varies in the way they interact with threads.","prefork#Prefork":"Upon launch the prefork module produces a number of child processes, each child manages just one thread. Since these systems deal with just one thread at a time, request speed may be affected if there are too many requests at the same time. If this occurs, certain requests will simply wait in line for action to be taken. You may increase the number of child processes that are spawned in order to manage this, but this increases the amount of RAM used. Prefork is the easiest module to use for non-thread-safe libraries.","serverlimit#ServerLimit":"The ServerLimit setting configures the maximum value for MaxRequestWorkers over the entire life of the Apache httpd method in the sense of the prefork module. When you need to increase MaxRequestWorkers over 256, then increasing the corresponding ServerLimit.\nWhen using the worker and event modules, the maximum value for MaxRequestWorkers for the length of the Apache httpd phase is calculated by ServerLimit and ThreadLimit. Notice that unused shared memory will be set aside if ServerLimit is set to a value higher than the required.","startservers#StartServers":"The StartServers value shows the number of child processes generated at startup, and is managed dynamically depending on the load. Often there is little need to adjust this number, unless your server is regularly restarted and includes a large number of requests upon reboot.","tools#Tools":"","worker#Worker":"The child processes of the worker module spawn several threads per process with each thread ready to assume new requests. This enables the entry of a larger number of simultaneous requests, and in effect it is easier to use RAM on the server. Overall, the worker module offers higher performance, but is less secure than the prefork module and cannot be used with modules that are not thread safe."},"title":"Tuning Of Your Apache Server"},"/utho-docs/docs/upload-ssl-certificate-on-utho-cloud/":{"data":{"":"","introduction#Introduction:":"Securing your website with an SSL certificate is crucial for safeguarding sensitive data and ensuring a trusted connection. On the Utho Cloud platform, adding an SSL certificate is a straightforward process, especially when preparing to attach it to a load balancer.","steps-to-upload-ssl-certificate#Steps to upload SSL certificate:":"Step 1: To begin, navigate to the Utho Cloud dashboard and locate the SSL certificate section. Here, you‚Äôll find an option to upload your certificate and its associated private key. Ensure that the certificate is in the correct format, commonly PEM or DER, and includes the full certificate chain.\nStep 2: Once ready, select the ‚ÄòAdd SSL Certificate‚Äô option and provide the necessary details.\nStep 3: Utho Cloud will guide you through the process, ensuring that the certificate and private key are securely added to the platform. As shown below in the image, first mention the SSL certificate name. Next, paste the content of .crt file you should have received when you applied for the SSL. And now, paste the content of the private key you generated while creating the CSR for your SSL. Then click on the ‚ÄúAdd SSL Certificate‚Äù button.\nStep 4: That‚Äôs is it. You have successfully uploaded the SSL certificate on your profile."},"title":"Upload SSL certificate on Utho Cloud"},"/utho-docs/docs/use-lighttpd-web-server-in-ubuntu-16-04-xenial-xerus/":{"data":{"":"Lighttpd provides a lightweight web server that can serve high loads with less memory than Apache servers. It is widely used on high-traffic platforms such as WhatsApp and xkcd.\nThis guide describes how the lighttpd (‚Äúlight‚Äù) web server is installed and configured on Ubuntu 16.04 (Xenial Xerus). See links at the end for more detail on other services typically available in web server stacks.","before-you-begin#Before You Begin":" Complete the Getting Started Guide and set your Instance hostname and timezone. Lighttpd is a network-oriented application that can expose you to vulnerabilities if your server is not protected. Search for a standard user account in the Securing Your server guide, harden SSH access and delete unnecessary networking services. If you switch from another web server, such as Apache, remember to turn off the other server for test purposes, or to configure lighttpd to use an alternative port until it is properly configured. Update your system: apt-get update \u0026\u0026 apt-get upgrade ","configure-lighttpd#Configure Lighttpd":"The key lighttpd configuration file is at /etc/lighttpd/lighttpd.conf. This file lists the application modules that can be loaded and allows you to change the web server ‚Äôs global settings.\nThe first configuration command is server.module, which lists the modules to be loaded when the lighttpd server is started or reloaded. To deactivate a function, add a # at the start of the line to comment on the module. Delete the # for module activation. This list can also include modules. For example, in the default file you can enable the mod rewrite module (URL requests rewriting) by uncommenting the appropriate line. Note that in the order they appear, these modules will be loaded.\nThe server.modules block is a list of other application and modules configuration settings. Most instructions are self-explanatory, but you may want to include not all of the options available by default. Several output settings you might want to add:\nServer.max-connections ‚Äì Defines how many connections are allowed concurrently Server.max-keep-alive-requests ‚Äì Sets the maximum number of requests to keep the link alive until the link ends. Server.max-worker-Enter the number of spawning worker processes. If you know Apache, a worker is similar to a child operation. Server.bind-Sets the lighttpd socket address, hostname, or path. You can create several lines to listen to different IP addresses. Binding to all interfaces is the default mode. Some configurations depend on certain modules. For example, url.rewrite requires mod rewrite to be allowed, as it is module specific. However, for ease of use, most modules have their own configuration files and can be enabled and disabled via command line rather than by editing the configuration file.","create-virtual-host-directories#Create Virtual Host Directories":"Whether you are using simple-vhost or evhost, before lighttpd can serve content, you need to create directories. Once you have installed the necessary instructions, build the necessary directories to replace abc.com with your name of domain:\nmkdir -p /var/www/html/abc .com/htdocs/ The following command generates two additional virtual hosts for the top-level fields.net and.org:\nmkdir -p /var/www/html/{abc.net/htdocs, abc.org/htdocs} The following command creates two additional virtual hosts from the evhost abc for the subdomains:\nmkdir -p /var/www/html/{abcSub/htdocs,abcSub2/htdocs} ","enable-and-disable-modules-via-command-line#Enable and Disable Modules via Command Line":"You may want to allow and disable modules via the command line for easy use. Lighttpd provides a simple way to do this so that each time a new module is needed the configuration must not be edited.\nRun light-enabled-mod from the command line to see the list of modules available, a list of modules already enabled, and a prompt to enable a module. This can be done in one line as well. To activate the authentication module, for example:\nlighty-enable-mod auth This command creates a symbolic connection to the /etc/ lighttpd/conf-enabled configuration file of the module read in the main configuration file. Check for the .conf file in /etc/lighttpd/conf-available to change the configuration of a particular module.\nThere are many other modules in separate Ubuntu packages. Some good stuff are:\nlighttpd-mod-mysql-vhost-Uses MySQL database to control virtual hosts. This module works well when a large number of virtual hosts are managed. lighttpd-mod-webdav Apache supports WebDAV extensions for distributed Apache content authoring lighttpd-mod-magnet Checks the request management module When these packages are mounted, you can enable them with light-enable-mod.\nRestart lighttpd to load changes:\nsystemctl restart lighttpd.service ","install-lighttpd#Install Lighttpd":"Load server from the list of Ubuntu packages:\napt-get install lighttpd Upon deployment, make sure the server is running and activated. Visit http:/198.51.100.10:80 and your IP address will be replaced by 198.51.100.10. If you set up lighttpd for testing on an alternate port, please replace 80 with this version. You will see a lighttpd placeholder page containing some important information:\nConfiguration files in /etc/lighttpd are stored. The ‚ÄúDocumentRoot‚Äù (which includes all HTML files) is located in the /var/www directory by default. You should customize this later. Ubuntu provides help scripts for allowing and deactivating server modules without editing the config file directly: light-enable-mod and light-disable-mod. ","run-scripts-with-fastcgi#Run Scripts with FastCGI":"You can execute these scripts with FastCGI, if you need your web server to execute dynamic content. In order to run a script, FastCGI outputs the Web server interpreter rather than run the ‚Äúinside‚Äù scripts on the Web server. This is in contrast to approaches such as mod perl, mod python, and mod php, but in high-traffic circumstances this way is often more effective.\nTo set up FastCGI, make sure that an interpreter for your chosen language is installed on your system:\nTo install Python:\napt-get install python To install Ruby:\napt-get install ruby To install PHP 7 for CGI interfaces\napt-get install php7.0-cgi Perl version 5.22.1 is included by default in Ubuntu 16.04. You will also need to install and set up a database system depending on the program you plan to run.\nOn the basis of file extensions that can be sent to individual managers, Lighttpd sends CGI requests to CGI managers. You can also submit requests for an extension to several servers and lighttpd loads these FastCGI connections automatically.\nInstalling a php7.0-cgi package and allowing FastCGI with light-enable-mod quickcgi-php for instance, configures the standard FastCGI handler in /etc/lighttpd/conf-enabled/15-fastcgi-php.conf. Although the handler probably needs to be changed, the default settings are an effective example:\nfastcgi.server += ( \".php\" =\u003e (( \"bin-path\" =\u003e \"/usr/bin/php-cgi\", \"socket\" =\u003e \"/var/run/lighttpd/php.socket\", \"max-procs\" =\u003e 1, \"bin-environment\" =\u003e ( \"PHP_FCGI_CHILDREN\" =\u003e \"4\", \"PHP_FCGI_MAX_REQUESTS\" =\u003e \"10000\" ), \"bin-copy-environment\" =\u003e ( \"PATH\", \"SHELL\", \"USER\" ), \"broken-scriptfilename\" =\u003e \"enable\" )) ) Add the following entry to your configuration file to map more than one file extension to a single FastCGI handler:\nfastcgi.map-extensions = ( \".[ALT-EXTENSION]\" =\u003e \".[EXTENSION]\" ) ","things-to-keep-in-mind#Things to Keep in Mind":"Though lighttpd is an efficient and capable web server, its behavior has two warnings:\nServer side does not function in lighttpd in the same way as in Apache‚Äôs mod ssi, which allows you to dynamically insert content from one file into another. Even though this is an effective way to assemble content quickly, lighttpd ‚Äôs handling of the script via SSI is not a recommended workflow. See project documentation for lighttpd on mod ssi.\nBecause of how FastCGI works, the operation of lighttpd web applications requires extra configuration, particularly for users who use web server-embedded interpreters (e.g. mod perl, mod python, mod php). See the lighttpd project documentation on optimizing FastCGI output for more detail.\nThankyou..","virtual-host-setup-with-enhanced-vhost#Virtual Host Setup with Enhanced Vhost":"Enhanced virtual hosting works somewhat different from Simple by building a document root based on a wildcard pattern. Before beginning, make sure all other virtual hosting modules are disabled.\nTo allow the enhanced virtual hosting module, run the following command: ``` lighty-enable-mod evhost 2. Restart lighttpd to load the configuration changes: ``` systemctl restart lighttpd.service You must modify /etc/lighttpd/conf-available/10-evhost.conf to achieve the same directory structure by evhost as with simple-vhost above: evhost.path-pattern = \"/var/www/html/%0/htdocs/\" 4. Modify the server.document-root configuration file in the main lighttpd:\nserver.document-root = \"/var/www/html/abc.com/htdocs\" When abc.com is requested and /var/www/html/abc.com/htdocs/ is found, the configuration set to Steps 3 and 4 is the root of the document when you send requests. The 0 percent of the pattern means that a request will be verified for domain and Top Level Domain (TLD) called host files. The.document-root server directive defines a default host used when there is no corresponding directory.\n5. Restart lighttpd to load the configuration changes:\nsystemctl restart lighttpd.service The convention on naming these virtual hosts is derived from the domain names. Take an example of the following web address: http:/abcSub2.abcA.com/ We read domain names from top level on the right to bottom level on the left. Therefore, com is the TLD, abc A is the domain, abcSub is subdomain 1 and abcSub2 is the subdomain 2.\nTo change the lighttpd format of the host directory, define the pattern to the directory in which the content lives. The table below shows the host directory format used for each pattern as root document. It also shows which host file will be used to serve content, with an example request from the above URL:","virtual-host-setup-with-simple-vhost#Virtual Host Setup with Simple Vhost":"This segment covers a basic virtual hosting setup. The Simple Vhost module lets you set virtual hosts in a user defined directory named for the domains below a root server with their respective document roots. Make sure all other virtual hosting modules are disabled before continuing.\nStart by allowing simple-vhost: ``` lighty-enable-mod simple-vhost 2. Restart lighttpd to load your changes: ``` systemctl restart lighttpd.service Modify the following settings in your¬†/etc/lighttpd/conf-available/10-simple-vhost.conf¬†file: simple-vhost.server-root = \"/var/www/html\" simple-vhost.document-root = \"htdocs\" simple-vhost.default-host = \"abc.com\" The server root specifies the base directory for all host directories.\nThe document-root determines the subdirectory in which the pages to be served are located. In some Apache configurations, this is similar to the public html directory, but in the above configuration it is called htdocs.\nWhen lighttpd receives a request and is unable to find a compatible directory, the content is served from the default host.\nRequests are tested in the above configuration for existing directory names within /var/www/html. If a directory exists which matches the requested domain, the result is served by the respective htdocs. If it does not exist, content from htdocs is served in the default host directory.\nIn order to explain this definition, presume that /var / www / html only contains the abcA.com and abc.com directories, both of which contain content htdocs folders:\nWhen an application for an abcA.com URL is made, the content of /var/www/html/abcA.com / htdocs is served. When a request is made for a server URL that does not have a directory, contents will be served with /var/www/html/abc.com/htdocs as abc.com is the default host. Build host directories for each subdome in the same way for subdomains. For example , in order to use abcSub as a subdomain of abc A.com, create an abcSub. abc A.com directory with a content htdocs directory. Make sure that any subdomains you want to use are added to DNS records.\n4. Restart the web server again to reload changes:\nsystemctl restart lighttpd.service ","virtual-hosting-best-practices#Virtual Hosting Best Practices":"The way you set up virtual hosting on your web server depends, what sites you host, how many domains you traffic, and how many workflows you run. We suggest that you host all of your domains into a single directory (e.g. /var/www/html) and connect the directories symbolically to useful locations.\nFor example, you can build a series of user accounts for the ‚Äúblog editor.‚Äù You can then connect the document root of each domain to a folder in the editor‚Äôs home folder. For the abc-user user account that manages the abc.com website:\nln -s /home/abc-user/abc.com/ /var/www/html/abc.com You may also use symbolic connections to host the same files for many essentially host domains. For example , in order to see abc .org for example in the files of abc .com, build the following link:\nln -s /var/www/html/ abc .org/ /var/www/html/ abc .com No matter what you decide, we advise you to build a structured method to manage virtual hosting to simplify any device modifications."},"title":"Use lighttpd Web Server in Ubuntu 16.04 (Xenial Xerus)"},"/utho-docs/docs/use-lsyncd-to-sync-directories-on-rockylinux/":{"data":{"":" use lsyncd to sync directory on RockyLinux","important-points#Important points":" Two server with password-less authentication enabled on both server for each other.\nNeed either a normal user with sudo privileges or sudo user.\nServer 1- 103.127.28.167 and Server 2- 103.209.147.242\nDirectory 1- /var/www/ to be sync with /var/www/ directory of other server\nDirectory 2- /var/lib/mysql to be sync with /var/lib/mysql directory of other server","steps-to-enable-mirror-synchronisation#Steps to enable mirror synchronisation":"Setup Password-less authentication Step 1: Now, generate the SSH key to be used for authentication. Execute the below command on both servers.\nssh-keygen After executing the above command, you just need to press enter for rest of the prompts you will get.\nStep 2: Copy the public key on other servers to complete the password-less authentication.\nssh-copy-id root@server-ip . Please change ‚Äúserver-ip‚Äù to the IP address of a computer other than the one you are using to run this command. Here, you will be asked to enter the ‚Äúserver-ip‚Äù password.\nInstall \u0026 configure lsyncd binary Step 3: Now, install lsyncd on your both machines. To install lsyncd, you need to install epel-release first.\nyum install -y epel-release yum -y install lua lua-devel pkgconfig gcc asciidoc lsyncd\nStep 4: Now, open the lsyncd configuration file and comment out the line you see using two dashs(--). Then, copy the text below and paste it into your file. vi /etc/lsyncd.conf\n\u003cfigure\u003e ![](images/image-1208-1024x724.png) \u003cfigcaption\u003e Configuration file of lsync \u003c/figcaption\u003e \u003c/figure\u003e substitute the target ip to the IP address of the remote computer you want to sync your directory with. Also, in source, you need to define the directory you want to sync with remote server's directory. \u003e Content of /etc/lsyncd configuration file. \u003e \u003e ``` \u003e settings { \u003e logfile = \"/var/log/lsyncd/webserver.log\", \u003e maxDelays = 1 \u003e } \u003e sync { \u003e default.rsync, \u003e source=\"/var/www\", \u003e target=\"103.127.28.212:/var/www\", \u003e rsync = { \u003e compress = true, \u003e acls = true, \u003e verbose = true, \u003e owner = true, \u003e group = true, \u003e perms = true, \u003e rsh = \"/usr/bin/ssh -p 22 -o StrictHostKeyChecking=no\" \u003e } \u003e } \u003e ``` Step 5: Now, restart your server lsyncd services on both of your server. systemctl restart lsyncd\nAnd this is how you will mirror sync your directory with the directory of remote server or you can say you have learnt how to use lsyncd to sync directories on RockyLinux ","what-is-lsync#What is lsync?":"Before learning how to use lsyncd to sync directories on RockyLinux first learn about lsync. Although rsync is a superb and flexible backup solution, it does have one drawback: you must manually launch it whenever you want to back up your data. Yes, you can use cron to set up scheduled backups, but even this approach is unable to offer live synchronisation that is smooth. The lsyncd tool, a command-line application that uses rsync to synchronise (or rather mirror) local folders with a remote machine in real time, is what you need if this is what you want.\nLsyncd keeps an eye on the inotify or fsevents local directory trees event monitor interface. For a brief period of time, it gathers and integrates events before spawning one (or more) processes to synchronise the adjustments. By default, rsync is used. Lsyncd is a lightweight live mirror solution that is relatively simple to install, doesn‚Äôt require new filesystems or block devices, and doesn‚Äôt degrade the speed of the local filesystem.\nInstead of sending the move destination over the wire again, Rsync+ssh is an advanced action configuration that leverages SSH to act on file and directory moves directly on the target."},"title":"Use lsyncd to sync directories on RockyLinux"},"/utho-docs/docs/use-nginx-as-a-reverse-proxy-2/":{"data":{"":"","#":"\nWhat is a Reverse Proxy? A reverse proxy is a server between internal and external clients which transmits clients requests to a different server. Although other standard applications, such as Node.js, can support themselves, NGINX has a range of advanced load balancing, safety and speed features that most specialized applications do not have. The reverse proxy of NGINX helps you to apply these functions to any program.\nA simple Node.js app to illustrate how to configure NGINX as reverse proxy is used for this tutorial.\nInstall NGINX These measures are used to install NGINX Mainline from the official NGINX Inc repository on Ubuntu. See the NGINX admin guide for other distributions. See our Getting Started with the NGINX series for information about configuring NGINX for production environments.\n1. In the text editor, Open¬†/etc/apt/sources.list¬†and add the next line to the right. Substitute¬†CODENAME¬†by your Ubuntu release codename in this case. For eg, the bionic insert in place of¬†CODENAME¬†below for Ubuntu 18.04 named¬†bionic¬†1. Beaver:\ndeb¬†[http://nginx.org/packages/mainline/ubuntu/](http://nginx.org/packages/mainline/ubuntu/)¬†CODENAME nginx 2. Import the signing key for the repository and link it to¬†apt:\n#sudo wget [http://nginx.org/keys/nginx_signing.key](http://nginx.org/keys/nginx_signing.key) #sudo apt-key add nginx_signing.key 3. Install NGINX:\n#sudo apt update ``` ``` #sudo apt install nginx 4. Ensure that NGINX works and can automatically continue when reboot:\n#sudo systemctl start nginx ``` ``` #sudo systemctl enable nginx] Create an Example App Install Node.js\n1. Using curl to access the NodeSource installation file. Replace the version of the node with the version you want to install in the curl command:\n#curl -sL [https://deb.nodesource.com/setup_9.x](https://deb.nodesource.com/setup_9.x) -o nodesource_setup.sh 2. Run the script:\n#sudo bash nodesource_setup.sh 3. The setup script will run an¬†apt-get update¬†it automatically, so you can install Node.js right away:\n#sudo apt install nodejs npm Next to Node.js will be installed the Node Package Manager (NPM).\nConfigure the App 1. Make a directory for the example app:\n#mkdir nodeapp \u0026\u0026 cd nodeapp 2. Initialize a Node.js app within the directory:\n#npm init Accept all the defaults when prompted.\n3. Install Express.js:\n#npm install ‚Äìsave express 4. Use a¬†text¬†editor to create app.js and add¬†the below¬†content:\nconst express = require(‚Äòexpress‚Äô) const app = express() app.get(‚Äò/‚Äô, (req, res) =\u003e res.send(‚ÄòMicrohost Cloud!‚Äô)) app.listen(3000, () =\u003e console.log(‚ÄòNode.js app listening on port 3000.‚Äô)) 5. Run the app:\n#node app.js 6. In a new terminal window, use curl to verify that the app is working¬†on localhost\n#curl localhost:3000 Microhost Cloud!\nConfigure NGINX You can configure Node.js to use a sample app on the public IP address of your cloud server to view the device on the internet. This segment then configures NGINX such that all requests from the public IP address are forwarded to the server on¬†localhost¬†already listening.","basic-configuration-for-nginx-reverse-proxy#Basic Configuration for NGINX Reverse Proxy":"1. Create a configuration¬†file¬†for the app in¬†/etc/nginx/conf.d/. Replace¬†abc.com¬†into your app‚Äôs domain or public IP address:\nserver { listen 80; listen [::]:80; server_name abc.com; location / { proxy_pass¬†http://localhost:3000/; b } } This setup is a reverse proxy through the¬†proxy_pass¬†command. It specifies to forward to port¬†3000¬†on locals all applications corresponding to the location block (in this case, the root / path) on the¬†localhost, where the Node.js application is running.\n2. Disable or delete default Welcome to NGINX page:\n#sudo mv /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/default.conf.disabled 3. Test the configuration:\n#sudo nginx -t 4. Reload the new configuration if no errors are reported:\n#sudo nginx -s reload 5. In a browser, navigate¬†your¬†cloud server‚Äôs public IP address. Now it should be show the ‚ÄúMicrohost cloud‚Äù message displayed.\nAdvanced Options The¬†proxy_pass¬†directive is enough for a simple application. More complex apps can however require additional guidance. For instance, Node.js is often used for applications that involve several real-time interactions. Disable NGINX buffering feature to accommodate:\nlocation / { proxy_pass¬†http://localhost:3000/; proxy_buffering off; } You may also modify or delete the headers forwarded with the¬†proxy_set_header¬†request:\nlocation / { proxy_pass¬†http://localhost:3000/; proxy_set_header X-Real-IP $remote_addr; } This configuration uses the¬†$remote_addr¬†variable to give the original client IP address to the proxy host.\nConfigure HTTPS with Certbot A reverse proxy has the advantage that HTTPS can easily be set up with a TLS certificate. Certbot is a device that enables you to get free Let‚Äôs Encrypt certificates quickly. On Ubuntu 16.04 this guide is going to use Certbot, but the official website holds complete installation and guidance for all major districts.\nUse below these steps to receive a Certbot certificate. In order to use the new certificate, Certbot automatically updates your NGINX settings files:\n1. Install the Certbot and¬†internet¬†server-specific¬†packages, then run Certbot:\n#sudo apt-get update ``` ``` #sudo apt-get install software-properties-common ``` ``` #sudo add-apt-repository ppa:certbot/certbot ``` ``` #sudo apt-get update ``` ``` #sudo apt-get install python-certbot-nginx ``` ``` #sudo certbot ‚Äìnginx 2. Certbot is going to ask for website details. As part of the certificate, the answer will be saved:\n# sudo certbot ‚Äìnginx Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator nginx, Installer nginx What names do you want HTTPS allowed for? 1: abc.com\n2: www.abc.com\nSelect the correct commas and/or spaces separated numbers, or leave the input\nSelect all of the choices displayed blankly (Enter ‚Äò c ‚Äò to cancel):\n3. Certbot will also ask if you would like to routinely redirect HTTP visitors to HTTPS visitors. It is suggested that you choose this option.\n4. Once the tool is completed, Certbot will store all of the created keys and certificates into/etc/letsencrypt/live/$domain¬†, where¬†$domain¬†is the name of the domain entered during the generation stage of the certificate.\nNote:- Certbot recommends pointing your¬†web¬†server configuration to the default¬†certificate directory¬†or¬†creating¬†symlinks. Keys and¬†certificate¬†should¬†not¬†be moved to a¬†special directory.\nFinally, Certbot will replace your internet server configuration in order that it makes use of the new certificate, and also redirects HTTP visitors to HTTPS if you selected that option.\n5. If you‚Äôve a firewall configured on your Cloud server, you need to add a firewall rule to allow incoming and outgoing connections to the HTTPS service. On Ubuntu, UFW is a commonly used and simple tool for handling firewall rules. for HTTP and HTTPS traffic Install and configure UFW:\n# sudo apt install ufw ``` ``` #sudo systemctl start ufw ``` ``` #sudo systemctl enable ufw ``` ``` #sudo ufw allow http:// ``` ``` #sudo ufw allow https: ``` ``` #sudo ufw enable Thank You :)"},"title":"Use NGINX as a Reverse Proxy"},"/utho-docs/docs/utho-object-storage-get-started/":{"data":{"":"","#":"An object storage system that works with S3 and is intended for cloud-based unstructured data management, access, and storage\nEnable Object Storage Object storage is disabled by default. Simply create a bucket in the Utho Cloud dashboard to enable object storage.\nCreate a bucket The Utho Cloud Manager provides a web interface for creating buckets. To create a bucket:\nStep 1: Login to the Utho cloud dashboard.\nStep 2: Click on the¬†Object Storage menu option in the sidebar, and then click on¬†Create Bucket.\nStep 3: The¬†Create a bucket page appears.\nStep 4: Determine which cluster the bucket will live in and how much data it will hold.\nStep 5: Now, give the name you want to give to your Bucket\nStep 6: Click¬†Submit. Congratulations! You are now ready to¬†upload objects to your bucket.\nGenerate an Access Key Step 1: Login to the Utho cloud dashboard.\nStep 2: Click on the¬†Object Storage¬†link in the sidebar, and then click the¬†Create Access Key¬†link.\nStep 3: Type the key‚Äôs name here. In the Utho Cloud Manager, you can refer to your key pair by this title. Next, select ‚ÄúCreate Key.‚Äù\nStep 4: Your secret key and access key are displayed in a window. Put these in writing somewhere safe. The access key is displayed in the Utho Cloud Manager, but once the window is closed, you are unable to get your secret key back.\nUpload an Object to a Bucket Step 1: If you have not already, log into the¬†Utho Cloud Dashboard.\nStep 2: On the sidebar, select the link for ‚ÄúObject Storage.‚Äù All of your buckets are listed. To start uploading items, click on the bucket of your choice. Drag and drop a file from your computer into your object storage bucket using the Upload Files Pane.\nStep 3: Alternatively, you can use the Browse Files option to open the file browser on your computer and choose a file to upload to your bucket.\nStep 4: When the upload is complete, your object should be visible on the¬†Objects Listing Page. It is possible to drag and drop more than one file at once into the Upload Files Panel.\nAccess Control \u0026 Permission Sharing access to objects and buckets with other Utho¬†Object Storage users is possible. To configure sharing, two methods are available: bucket policies and access control lists (ACLs). Both of these technologies have similar purposes in that they can be used to provide and restrict access to resources stored in Object Storage. Although ACLs do not provide as many fine-grained access modes as bucket policies, they can still be used to restrict or give access to specific items.\nUsing bucket policies is highly recommended if you can group items with comparable permission requirements into separate buckets. If you are unable to arrange your items in this way, ACLs remain a useful choice.\nComparing ACLs to bucket policies, you can manage permissions more precisely using bucket policies. Select bucket policies over ACLs if you need more granular permissions than just read and write access.\nApplying a written bucket policy file to the bucket is another way to construct bucket policies. This file can‚Äôt be larger than 20 KB. You might wish to use ACLs instead if your policy has a long list of policy rules.\nBucket policies and ACLs can be utilised simultaneously. Any rule that restricts access to an Object Storage resource takes precedence over a rule that authorises access in this scenario. For example, if a bucket policy prohibits a user from accessing a bucket that an ACL permits, the user will not be able to access the bucket."},"title":"Utho Object Storage - Get Started"},"/utho-docs/docs/virtualization-the-key-to-efficiency-in-cloud-computing/":{"data":{"":"","how-does-utho-cloud-contribute-to-enhancing-scalability-and-flexibility-in-the-cloud-through-virtualization#**How does Utho Cloud contribute to enhancing scalability and flexibility in the cloud through virtualization?":"In today‚Äôs world of cloud computing, virtualization is a game-changer. It‚Äôs all about making the most of your resources by turning one physical machine into many virtual ones. This helps businesses save money, scale up easily, and run more efficiently. Let‚Äôs dive into how virtualization makes cloud computing work better for everyone.\nHow does virtualization contribute to the efficiency of cloud computing systems? Virtualization plays a crucial role in enhancing the efficiency of cloud computing systems by enabling the creation of virtual instances of computing resources such as servers, storage, and networking components. Here‚Äôs how virtualization contributes to this efficiency:\nResource Utilization: Virtualization allows for the efficient utilization of physical hardware resources by dividing them into multiple virtual machines (VMs) or containers. This means that a single physical server can host multiple virtual servers, optimizing resource allocation and reducing hardware underutilization.\nScalability: With virtualization, cloud computing systems can quickly scale resources up or down based on demand. Virtual machines and containers can be provisioned or decommissioned dynamically, allowing for rapid response to changing workloads and ensuring optimal resource allocation.\nIsolation: Virtualization provides isolation between different virtual instances, ensuring that each application or workload operates independently without interfering with others. This isolation enhances security and stability within the cloud environment by minimizing the impact of failures or security breaches.\nFlexibility: Virtualization enables flexibility in deploying and managing diverse workloads within the cloud environment. Users can deploy various operating systems and applications on virtual machines or containers, adapting to specific requirements without constraints imposed by physical hardware limitations.\nResource Consolidation: Virtualization facilitates resource consolidation by enabling multiple virtual instances to share underlying physical resources. This consolidation reduces the number of physical servers required, leading to cost savings in terms of hardware procurement, maintenance, and energy consumption.\nVirtualization enhances the efficiency of cloud computing systems by optimizing resource utilization, enabling scalability, providing isolation between virtual instances, offering flexibility in workload deployment, and facilitating resource consolidation. These benefits contribute to improved performance, agility, and cost-effectiveness in cloud environments.\nHow does virtualization help in reducing hardware costs and improving cost-effectiveness in cloud computing? Virtualization helps in reducing hardware costs and improving cost-effectiveness in cloud computing in several ways:\nConsolidation of Resources: Virtualization allows multiple virtual machines (VMs) to run on a single physical server, consolidating resources such as CPU, memory, storage, and network interfaces. This consolidation reduces the number of physical servers required, leading to savings in hardware costs.\nOptimized Resource Utilization: By dividing physical servers into multiple VMs, virtualization optimizes resource utilization. It ensures that resources are allocated dynamically based on workload demands, reducing underutilization and maximizing the efficiency of hardware resources.\nEnergy Efficiency: Virtualization contributes to energy efficiency by reducing the number of physical servers needed to support workloads. Consolidating resources onto fewer servers results in lower energy consumption, leading to cost savings on power and cooling expenses.\nReduced Maintenance Costs: With fewer physical servers to manage, organizations can reduce maintenance costs associated with hardware procurement, installation, maintenance, and upgrades. Virtualization simplifies IT management tasks, allowing administrators to focus on higher-level activities.\nExtended Hardware Lifespan: Virtualization prolongs the life of hardware components by making sure they‚Äôre used to their fullest extent. Instead of replacing hardware as frequently, organizations can leverage virtualization to prolong the lifespan of existing infrastructure, further reducing costs.\nOverall, virtualization significantly contributes to cost reduction and improved cost-effectiveness in cloud computing by enabling resource consolidation, optimizing resource utilization, enhancing energy efficiency, reducing maintenance costs, and extending hardware lifespan. These benefits make virtualization a key technology for driving efficiency and cost savings in modern cloud environments.\nWhat are some common virtualization techniques used in cloud environments, and how do they optimize efficiency? In cloud environments, several common virtualization techniques are employed to optimize efficiency:\nHardware Virtualization: This technique involves creating virtual instances of physical hardware components, such as CPUs, memory, and storage devices. It allows several virtual machines (VMs) to operate on just one physical server, making the most of resources and cutting down hardware expenses.\nHypervisor-Based Virtualization: Also known as full virtualization, this technique utilizes a hypervisor to abstract and manage physical hardware resources. The hypervisor acts as a virtualization layer between the physical hardware and the VMs, allowing multiple operating systems to run concurrently on the same physical server.\nContainerization: Containerization is a lightweight virtualization technique that encapsulates applications and their dependencies into self-contained units called containers. Containers share the host operating system‚Äôs kernel and resources, making them more efficient and faster to deploy compared to traditional VMs. Containerization optimizes efficiency by reducing overhead and enabling rapid application deployment and scalability.\nPara-Virtualization: In para-virtualization, guest operating systems are adjusted to recognize the virtualization layer. This allows the guest OS to communicate directly with the hypervisor, improving performance and efficiency compared to full virtualization techniques.\nHardware-Assisted Virtualization: Hardware-assisted virtualization leverages specialized hardware features, such as Intel VT-x or AMD-V, to improve virtualization performance and efficiency. These features offload virtualization tasks from the CPU, reducing overhead and improving overall system performance.\nNetwork Virtualization: Network virtualization abstracts network resources, such as switches, routers, and firewalls, to create virtual networks within a physical network infrastructure. This technique enables the creation of isolated virtual networks, improving security, scalability, and flexibility in cloud environments.\nStorage Virtualization: Storage virtualization abstracts physical storage resources to create logical storage pools that can be dynamically allocated to different applications and users. This technique improves storage efficiency, scalability, and flexibility by centralizing management and simplifying data migration and provisioning.\nThese virtualization techniques optimize efficiency in cloud environments by enabling better resource utilization, reducing hardware costs, improving scalability and flexibility, enhancing performance, and simplifying management and deployment processes. By leveraging these techniques, organizations can maximize the benefits of cloud computing while minimizing costs and complexity.\nWhat security considerations are associated with virtualization in cloud computing, and how are they addressed? Security considerations associated with virtualization in cloud computing include:\nHypervisor Security: The hypervisor, which manages virtual machines (VMs) on physical servers, is a critical component of virtualization. Vulnerabilities in the hypervisor could potentially lead to unauthorized access or control over VMs. To address this, organizations implement stringent access controls, regularly patch and update hypervisor software, and utilize secure hypervisor configurations.\nVM Isolation: Ensuring strong isolation between virtual machines is crucial to prevent unauthorized access and data breaches. Hypervisor-based security features, such as VM segmentation and access controls, are employed to enforce isolation and prevent VM-to-VM attacks.\nVM Sprawl: VM sprawl occurs when a large number of unused or unnecessary virtual machines are created, increasing the attack surface and management overhead. To mitigate this risk, organizations implement policies for VM lifecycle management, regularly audit and decommission unused VMs, and employ automation tools for VM provisioning and deprovisioning.\nResource Segregation: In multi-tenant cloud environments, ensuring segregation of resources between different users or tenants is essential to prevent unauthorized access to sensitive data. Techniques such as network segmentation, VLANs, and virtual firewalls are used to enforce resource segregation and isolate tenant environments.\nData Protection: Protecting data within virtualized environments is critical to maintaining confidentiality, integrity, and availability. Encryption of data at rest and in transit, strong access controls, and regular data backups are essential measures to mitigate data security risks in virtualized cloud environments.\nVulnerability Management: Regular vulnerability assessments and patch management are essential to address security vulnerabilities in virtualized environments. Organizations deploy security patches and updates promptly, conduct regular vulnerability scans, and implement security best practices to reduce the risk of exploitation.\nVirtualization Management Interfaces: Secure management of virtualization platforms and tools is essential to prevent unauthorized access and control over virtualized resources. Strong authentication mechanisms, role-based access controls (RBAC), and auditing capabilities are employed to secure management interfaces and monitor for unauthorized activities.\nCompliance and Regulatory Requirements: Compliance with industry regulations and data protection laws is critical in virtualized cloud environments. Organizations ensure adherence to regulatory requirements by implementing security controls, conducting regular compliance audits, and maintaining documentation of security measures and controls.\nBy addressing these security considerations through a combination of technical controls, best practices, and proactive risk management strategies, organizations can enhance the security posture of their virtualized cloud environments and mitigate potential security risks and threats.\n**How does Utho Cloud contribute to enhancing scalability and flexibility in the cloud through virtualization? **\nUtho Cloud enhances scalability and flexibility in the cloud through virtualization in several ways:\nElastic Compute: Utho Cloud offers a range of compute options, including virtual machines (VMs) and bare metal instances, that can be quickly provisioned and scaled up or down based on demand. This elasticity allows organizations to dynamically adjust their compute resources to meet changing workload requirements, optimizing performance and cost-effectiveness.\nAutomated Scaling: Utho Cloud provides automated scaling capabilities that allow users to define scaling policies based on predefined triggers, such as CPU utilization or incoming traffic. This automated scaling ensures that resources are allocated efficiently, minimizing manual intervention and maximizing uptime and availability.\nVirtual Networking: Utho Cloud‚Äôs virtual networking features enable organizations to create and manage virtual networks, subnets, and security groups to isolate and secure their cloud resources. This flexibility allows users to design custom network architectures that meet their specific requirements, optimizing performance and security in the cloud environment.\nStorage Flexibility: Utho Cloud offers a variety of storage options, including block storage, object storage, and file storage, that can be easily provisioned and scaled to accommodate changing storage needs. Users can leverage these flexible storage solutions to store and manage data effectively, ensuring scalability and performance in the cloud.\nIntegrated Services: Utho Cloud provides a comprehensive suite of integrated services, including database, analytics, and application development tools, that can be seamlessly integrated with virtualized infrastructure. This integration simplifies the deployment and management of cloud applications, enabling organizations to innovate faster and drive business growth.\nOverall, Utho Cloud‚Äôs robust virtualization capabilities empower organizations to scale their infrastructure dynamically, adapt to changing business requirements, and achieve greater agility and efficiency in the cloud. By leveraging virtualization technology, Utho Cloud enables organizations to maximize the benefits of cloud computing while minimizing complexity and cost.\nVirtualization is the backbone of efficiency in cloud computing. It helps businesses use resources better, scale up easily, and save money. As technology evolves, virtualization will continue to play a vital role in making cloud computing even more effective and innovative for everyone.","how-does-virtualization-contribute-to-the-efficiency-of-cloud-computing-systems#\u003cstrong\u003eHow does virtualization contribute to the efficiency of cloud computing systems?\u003c/strong\u003e":"","how-does-virtualization-help-in-reducing-hardware-costs-and-improving-cost-effectiveness-in-cloud-computing#\u003cstrong\u003eHow does virtualization help in reducing hardware costs and improving cost-effectiveness in cloud computing?\u003c/strong\u003e":"","what-are-some-common-virtualization-techniques-used-in-cloud-environments-and-how-do-they-optimize-efficiency#\u003cstrong\u003eWhat are some common virtualization techniques used in cloud environments, and how do they optimize efficiency?\u003c/strong\u003e":"","what-security-considerations-are-associated-with-virtualization-in-cloud-computing-and-how-are-they-addressed#\u003cstrong\u003eWhat security considerations are associated with virtualization in cloud computing, and how are they addressed?\u003c/strong\u003e":""},"title":"Virtualization: The Key to Efficiency in Cloud Computing"},"/utho-docs/docs/web-servers/nginx-enable-tls-or-https-connections/":{"data":{"":"\nThe Secure Socket Layer (SSL) is the successor to Transportation Layer Security (TLS). It provides stronger and more powerful HTPS and includes non-SSL improvements such as Forward Confidentiality, modern OpenSSL cipher suites, and HSTS-compatibility.\nA single NGINX installation may host a number of websites and any number that use the same TLS certificate and key. This guide describes different scenarios for adding a TLS certificate to the NGINX setup of your domain.","before-you-begin#Before You Begin":" You need root user accesssudo privileged user account. A TLS certificate and key are required for your site. If you are a private or internal location, or are simply experimenting, the certificate can be signed by yourself. If this is what your website wants, you can can use a commercial certificate chain. See our instructions to build a self-signed certificate or a signing certificate submission, if you don‚Äôt already have a certificate and server key. Make sure the NGINX has been compiled with - http ssl module if you have compiled from source code. Check for nginx -V efficiency. ","configure-a-single-https-site#Configure a Single HTTPS Site":"Scenarios: You have a domain certificate, and you would like NGINX to run over HTTPS on a single website. Scenario:\nJust use the setup of the http block in the previous section with only one site to deal with. You should not include ssl * instructions in the website configuration file in this case. Nonetheless, you will tell NGINX that port 443 for HTTPS connections should be listening instead of port 80. For further information, see the SSL module for NGINX docs.\n1. For example, below is a simple site configuration that works with the above-mentioned http block. This server block provides your site with IPv4 and IPv6 support, but you have no HTTP support only over HTTPS. In order to access your website, you need to type https:/ in the browser.\nIt is just a first step; without HSTS or by redirecting HTTP requests to port 443 you probably would not like to use this configuration.\nserver { listen ssl default_server; listen [::]:ssl default_server ; server_name abc.com www.abc.com; root /var/www/abc.com; } 2. After changes to the NGINX config files, reload your setup:\nnginx -s reload 3. Go to the web browser of your site or IP of cloud server to ensure that https:// is defined in the URL. Your HTTPS website will load. The browser warns you of an insecure link if you have a self-signed certificate. Activate and link the advertising.","configure-multiple-sites-with-a-single-certificate#Configure Multiple Sites with a Single Certificate":"Scenario: You have a multi-domain certificate, such as a Wildcard certificate or a SubjectAltName certificate.\nIn this case, the HTTP Server configure instructions are the same in the http domain. Two separate /etc/nginx/conf.d/, configuration files are needed, one for each site which is secured by credentials. In it, the IP address of each site with the listen¬†directive needs to be specified. If you have two different websites with different IPs, you don‚Äôt want to use default_server¬†.\nThe sites abc1.com, abc2.com, and /root/certs/abc.com/¬†are provided by the same certificate and key.\nserver { listen 203.0.113.30:ssl; listen [2001:DB8::5]:ssl; server_name abc1.com www.abc1.com; root /var/www/abc1.com; } server { listen 203.0.113.30:ssl; listen [2001:DB8::5]:ssl; server_name abc2.com www.abc2.com; root /var/www/abc1.com; } 2. Reload the configurations:\nnginx -s reload 3. HTTPS should now be accessible for both sites. You can see that the cert that supports both sites uses your browser to inform certificate property.","configure-multiple-sites-with-different-ssl-certificates#Configure Multiple Sites with Different SSL Certificates":"Scenario: You have two, or more, totally independent TLS certificate/key pairs websites that you want to serve on.\n1. Make sure that your storage certificate is well ordered. An example is given below:\n/root/certs/ ‚îú‚îÄ‚îÄ abc1.com/ ‚îÇ ‚îú‚îÄ‚îÄ abc1.com.crt ‚îÇ ‚îî‚îÄ‚îÄ abc1.com.key ‚îî‚îÄ‚îÄ abc2.com/ ‚îú‚îÄ‚îÄ abc2.com.crt ‚îî‚îÄ‚îÄ abc2.com.key 2. Configure your nginx.conf http block as stated above but without the certificate and key locations. Instead, these are in the¬†server¬†block of the individual website because the positions on each site vary. The result should be:\nhttp { ssl_ciphers EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH; ssl_protocols TLSvTLSv1.TLSv1.2; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; } 3. Add to each server block the ssl_certificate¬†and ssl_certificate_key¬†directive with the proper path of the certificate and key file at each domain .\nserver { listen 203.0.113.55:ssl; listen [2001:DB8::7]:ssl; server_name example1.com www.abc1.com; root /var/www/abc1.com; ssl_certificate /root/certs/abc.com/abc1.com.crt; ssl_certificate_key /root/certs/abc.com/abc1.com.key; }\nserver { listen 203.0.113.65:ssl; listen [2001:DB8::8]:ssl; server_name example2.com www.abc2.com; root /var/www/abc2.com; ssl_certificate /root/certs/abc2.com/abc.com.crt; ssl_certificate_key /root/certs/abc2.com/abc.com.key; }\n4. Reload your configuration:\nnginx -s reload 5. Both sites should be HTTPS-accessible, but by inspecting the certificates using your browser, abc1.com¬†uses abc1.com.crt and abc2.com¬†usesabc2.com.crt.\nThankyou","configure-the-http-block#Configure the http Block":"Guidelines to extend NGINX to all web domains, like SSL / TLS instructions, would go to the http¬†block in nginx.conf, The following instructions presume the same certificate and key for a particular website or all pages on the list.\nhttp { ssl_certificate /root/certs/abc.com/abc.com.crt; ssl_certificate_key /root/certs/abc.com/abc.com.key; ssl_ciphers EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH; ssl_protocols TLSv1.TLSv1.2; ","credentials-storage-location#Credentials Storage Location":"No official or favorite place to store the TLS certificate and the key on your site is safe. The certificate is sent to each server computer, and it is not a private file. The secret is private, however.\nYou want them to remain untouched and safe against other device users, wherever you want to store your certificate / key pair. We‚Äôre going to save them as an example in /root/certs/ , but you can save that folder whatever place you want.\n1. Create storage folder:\nmkdir /root/certs/abc.com/ 2. Shift into that folder your certificate(s) and key(s).\n3. Limit the following file permissions:\nchmod 400 /root/certs/abc.com/abc.com.key "},"title":"NGINX : Enable TLS or HTTPS Connections"},"/utho-docs/docs/web-servers/nginx-installation-and-basic-setup/":{"data":{"":"","before-you-begin#Before You Begin":" You need root access of the server or a sudo¬†privileged user account. Set the hostname of your cloud server. Update your server. ","binary-versus-compiling-from-source#Binary Versus Compiling from Source":"Below are the three ways to install NGINX Open Source are available:\nA pre-built binary from your Linux distribution‚Äôs repositories. This is the best way to install because you can install the nginx package using your package manager. However, for distributions that supply binaries, you are going to run an older version of NGINX as opposed to the current stable or mainline version. Patches can also be slower to land in upstream distro repositories.\nA pre-built binary from NGINX Inc.‚Äôs repository. That is the way that sequence is built. It is still a simple installation process that just needs to be connected to your device and then configured as usual. The most beneficial approach is the traditional upstream setup with quicker updates and new releases than a Linux distribution. Compile time options are commonly different from NGINX binaries in distribution repositories and nginx -V¬†can be used to display the built-in of your binary.\nCompiling from source.¬†It is the most complex, but not yet impractical deployment approach after the NGINX documentation. Source code is always modified by patches and maintained on the newest stable or mainline updates, so construction can be automated easily. This is the most flexible installation method because any compiling options and flags you select can be included or skipped. For instance, one common reason why people compile their own NGINX build is that they can use a server with a newer OpenSSL version than their Linux distribution.","configuration-notes#Configuration Notes":"As the use of the NGINX web server has grown, NGINX, Inc. has worked to distance NGINX from configurations and terminology that were used in the past when trying to ease adoption for people already accustomed to Apache.\nIf you know Apache well, you will know that multiple site configurations are stored in /etc/apache/sites-available/, (so called virtual hosts in Apache terminology), which are symlinked to files inside /etc/apache/sites-enabled/. However, multiple NGINX guides and blog posts recommend the same settings. As you might expect, that lead to some confusion, with the hypothesis that NGINX uses the ../sites-available/¬†and ../sites-enabled/ and users of www-data¬†regularly. That‚Äôs not the case.\nSure, it may. The NGINX packages of deposits in Debian and Ubuntu have modified their configuration for quite a while to that purpose, so it is definitely a working configuration to support sites that are stored in /sites-available/ and symlinked with /sites-enabled/. But, it‚Äôs unnecessary and the only one that does it is the Debian Linux family. Do not push Apache settings on NGINX.\nMultiple site configuration files can then be stored as an abc.com.conf, or abc.com.disabled in /etc/nginx/conf.d/. Do not add server¬†blocks to /etc/nginx/nginx.conf directly, as your configuration is fairly straightforward. This file is intended to configure the server process rather than individual websites.\nThe NGINX method also acts as the ngnix user in the nginx community, so note when you change permissions to website folders. See Creating NGNIX Plus Configuration Files¬†.\nUltimately, as the NGINX docs point out, Virtual Host is an Apache concept, even though it is used in the Debian and Ubuntu file nginx.conf and some of the old documentation of NGINX. The NGINX equivalent is a Server Block, so this is the phrase you will see on NGINX in this show.","configuration-recap#Configuration Recap":"To summarize where we have been so far:\nThe stable version of NGINX Open Source has been installed from the nginx.org repository. A basic website can be accessed: The root directory is located in /var/www/example.com/\nThe configuration file is located in /etc/nginx/conf.d/abc.com.conf.\nserver { listen default_server; listen [::]:default_server; server_name abc.com www.abc.com; root /var/www/abc.com; index index.html; gzip on; gzip_comp_level 3; gzip_types text/plain text/css application/javascript image/\\*; } Changes that we want NGINX to apply universally are in the /etc/nginx/nginx.conf¬†http block. Our additions are at the bottom of the block, so we know what‚Äôs added compared to what‚Äôs provided by default. Now, nginx.conf¬†looks like the following example. Please note that nginx.conf¬†does not contain any server¬†blocks:\nuser nginx; worker_processes auto; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main ‚Äò$remote_addr - $remote_user [$time_local] ‚Äú$request‚Äù ' ‚Äò$status $body_bytes_sent ‚Äú$http_referer‚Äù ' ‚Äò\"$http_user_agent\" ‚Äú$http_x_forwarded_for‚Äù‚Äô;\naccess_log /var/log/nginx/access.log main;\nsendfile on; #tcp_nopush on;\nkeepalive_timeout 65;\n#gzip on;\ninclude /etc/nginx/conf.d/*.conf;\nserver_tokens off;\n} Thankyou‚Ä¶","disable-server-tokens#Disable Server Tokens":"The version number of NGINX is available by default when the server is linked, whether by good 201 curl or 404 returned to a client. Disabling server tokens makes it harder to evaluate versions of NGINX and hence harder for an attacker to execute version-specific attacks.\nServer tokens enabled:\nServer tokens disabled:\nAdd the below line to the¬†http¬†block of¬†/etc/nginx/nginx.conf:\nserver_tokens off; ","install-nginx#Install NGINX":"","installation-instructions#Installation Instructions":"The NGINX admin guide provides simple and specific instructions for any installation method and version of NGINX you like, so we won‚Äôt represent it here. When your installation is done, return to the show.","nginx-configuration-best-practices#NGINX Configuration Best Practices":"There are a wide variety of tweaks to NGINX to best suit your needs. Nonetheless, others are unique to your use; what works well for one person can not work for another.\nThis series provides configurations that are fairly generic for use in almost any production scenario, but on which you can create for your own specialized setup. None of the following is considered best practice, and none of them are mutually related. They are not necessary to the operation of your site or server, but unintended and unwanted effects if they are not taken into account.\nTwo short points:\nFirst preserve your default nginx.conf¬†file so that you have something to recover if your personalization is so difficult that NGINX breaks. ``` cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.backup-original - After applying the following update, reload your configuration with: ``` nginx -s reload ","serve-content-over-ipv4-and-ipv6#Serve Content Over IPv4 and IPv6":"Add a second IPv6 listen¬†Directive to the /etc/nginx/conf.d/abc.com.conf: server¬†block:\nlisten [::]:80; If your site is using SSL/TLS, you would add:\nlisten [::]:443 ssl; ","set-your-sites-root-directory#Set Your Site‚Äôs Root Directory":"The NGINX directory serves sites that vary depending on how you have installed them. NGINX supplied from NGINX Inc. uses /usr/share/nginx/ at the time of this writing.\nNGINX docs warn of the loss of site data when upgrading NGINX by depending on the default location .. You can use /var/www/,/srv/, or any other location where package or device updates are not affected . See Using the default document root¬†and¬†Not using standard document root locations.\nThe /var/www/abc.com/¬†sequence will be used in its examples. Replace abc.com¬†, which can be accessed with your Cloud server‚Äôs IP address or domain name.\n1. The root directory of your site or sites should be linked to /etc/nginx/conf.d/abc.com.conf corresponding server block:\nroot /var/www/abc.com; 2. Then make that directory:\nmkdir -p /var/www/abc.com ","stable-versus-mainline#Stable Versus Mainline":"The first decision on your installation is whether you want NGINX Open Source stable or mainline version. Stable is recommended and what this set of guides uses should be. Information on NGINX here.","static-content-compression#Static Content Compression":"You do not want to uniformly allow gzip compression because you run the risk of vulnerability to the CRIME¬†and BREACH¬†exploits depending on the content and session cookies.\nIn NGINX compression has been disabled for years now by default, so that it is not vulnerable out of the box to CRIME. Modern browsers have also taken steps to combat these attacks, but web servers may be irresponsibly configured.\nAt the other hand, if you completely disable gzip compression, you remove such vulnerabilities and use less CPU cycles at the cost of reducing the efficiency of the web. Different mitigations are possible on the server and the introduction of TLS 1.3 will contribute further to this. For now, the only way, unless you know what you are doing, is only to compress static site content like images, Text, and CSS.\nBelow is an example of how to use cat /etc/nginx/mime.types to display the mime forms available. If you want the gzip¬†guidelines to extend to all sites supported by NGINX in the http¬†domain, it is better to use them for specific sites and content types only in the server domain.\ngzip on; gzip_types text/html text/plain text/css image/*; If NGINX serves many websites, some use SSl / TLS and some don‚Äôt, an example would look as in the following example. The gzip¬†directive is applied to the server¬†block of the HTTP site to keep it disabled for the HTTPS domain.\nserver { listen 80; server_name example1.com; gzip on; gzip_types text/html text/css image/jpg image/jpeg image/png image/svg; } server { listen ssl; server_name example2.com; gzip off; } There are several other options available for the gzip module of NGINX. See NGINX docs for more detail, and you can also use the ngx http gzip static module which suits static content compression if you want to compile NGINX builds.","use-multiple-worker-processes#Use Multiple Worker Processes":"Add or edit the below line in¬†/etc/nginx/nginx.conf, in the area just before the¬†http¬†block. This is the called¬†main¬†block, or context, though it‚Äôs not marked in¬†nginx.conf¬†like the¬†http¬†block is. The first choice would be to set it to¬†auto, or the amount of CPU cores available to your cloud server.\nworker_processes auto; For more details, see the sections on worker processes in¬†the NGINX docs¬†and¬†this NGINX blog post."},"title":"NGINX: Installation and Basic Setup"},"/utho-docs/docs/web-servers/use-nginx-as-a-reverse-proxy/":{"data":{"":"A reverse proxy is a server between internal and external clients which transmits clients requests to a different server. Although other standard applications, such as Node.js, can support themselves, NGINX has a range of advanced load balancing, safety and speed features that most specialized applications do not have. The reverse proxy of NGINX helps you to apply these functions to any program.\nA simple Node.js app to illustrate how to configure NGINX as reverse proxy is used for this tutorial.","advanced-options#Advanced Options":"The proxy_pass¬†directive is enough for a simple application. More complex apps can however require additional guidance. For instance, Node.js is often used for applications that involve several real-time interactions. Disable NGINX buffering feature to accommodate:\nlocation / { proxy_pass http://localhost:3000/; proxy_buffering off; } You may also modify or delete the headers forwarded with the proxy_set_header request:\nlocation / { proxy_pass http://localhost:3000/; proxy_set_header X-Real-IP $remote_addr; } This configuration uses the $remote_addr¬†variable to give the original client IP address to the proxy host.","basic-configuration-for-nginx-reverse-proxy#Basic Configuration for NGINX Reverse Proxy":"1. Create a configuration file for the app in /etc/nginx/conf.d/. Replace abc.com¬†into your app‚Äôs domain or public IP address:\nserver { listen 80; listen [::]:80; server_name abc.com; location / { proxy_pass http://localhost:3000/; b } } This setup is a reverse proxy through the proxy_pass¬†command. It specifies to forward to port 3000¬†on locals all applications corresponding to the location block (in this case, the root / path) on the localhost, where the Node.js application is running.\n2. Disable or delete default Welcome to NGINX page:\nsudo mv /etc/nginx/conf.d/default.conf /etc/nginx/conf.d/default.conf.disabled 3. Test the configuration:\nsudo nginx -t 4. Reload the new configuration if no errors are reported:\nsudo nginx -s reload 5. In a browser, navigate your cloud server‚Äôs public IP address. Now it should be show the ‚ÄúMicrohost cloud‚Äù message displayed.","configure-https-with-certbot#Configure HTTPS with Certbot":"A reverse proxy has the advantage that HTTPS can easily be set up with a TLS certificate. Certbot is a device that enables you to get free Let‚Äôs Encrypt certificates quickly. On Ubuntu 16.04 this guide is going to use Certbot, but the official website holds complete installation and guidance for all major districts.\nUse below these steps to receive a Certbot certificate. In order to use the new certificate, Certbot automatically updates your NGINX settings files:\n1. Install the Certbot and internet server-specific packages, then run Certbot:\nsudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install python-certbot-nginx sudo certbot --nginx 2. Certbot is going to ask for website details. As part of the certificate, the answer will be saved:\n# sudo certbot --nginx Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator nginx, Installer nginx What names do you want HTTPS allowed for? 1: abc.com 2: www.abc.com Select the correct commas and/or spaces separated numbers, or leave the input Select all of the choices displayed blankly (Enter ' c ' to cancel): 3. Certbot will also ask if you would like to routinely redirect HTTP visitors to HTTPS visitors. It is suggested that you choose this option.\n4. Once the tool is completed, Certbot will store all of the created keys and certificates into/etc/letsencrypt/live/$domain¬†, where $domain¬†is the name of the domain entered during the generation stage of the certificate.\nNote:- Certbot recommends pointing your web server configuration to the default certificate directory or creating symlinks. Keys and certificate should not be moved to a special directory.\nFinally, Certbot will replace your internet server configuration in order that it makes use of the new certificate, and also redirects HTTP visitors to HTTPS if you selected that option.\n5. If you‚Äôve a firewall configured on your Cloud server, you need to add a firewall rule to allow incoming and outgoing connections to the HTTPS service. On Ubuntu, UFW is a commonly used and simple tool for handling firewall rules. for HTTP and HTTPS traffic Install and configure UFW:\n# sudo apt install ufw sudo systemctl start ufw sudo systemctl enable ufw sudo ufw allow http sudo ufw allow https sudo ufw enable Thankyou‚Ä¶‚Ä¶.","configure-nginx#Configure NGINX":"You can configure Node.js to use a sample app on the public IP address of your cloud server to view the device on the internet. This segment then configures NGINX such that all requests from the public IP address are forwarded to the server on localhost already listening.","configure-the-app#Configure the App":"1. Make a directory for the example app:\nmkdir nodeapp \u0026\u0026 cd nodeapp 2. Initialize a Node.js app within the directory:\nnpm init Accept all the defaults when prompted.\n3. Install Express.js:\nnpm install --save express 4. Use a text editor to create app.js and add the below content:\nconst express = require('express') const app = express() app.get('/', (req, res) =\u003e res.send('Microhost Cloud!')) app.listen(3000, () =\u003e console.log('Node.js app listening on port 3000.')) 7 5. Run the app:\nnode app.js 6. In a new terminal window, use curl to verify that the app is working on localhost\ncurl localhost:3000 Microhost Cloud! ","create-an-example-app#Create an Example App":"Install Node.js 1. Using curl to access the NodeSource installation file. Replace the version of the node with the version you want to install in the curl command:\ncurl -sL https://deb.nodesource.com/setup_9.x -o nodesource_setup.sh 2. Run the script:\nsudo bash nodesource_setup.sh 3. The setup script will run an¬†apt-get update¬†it automatically, so you can install Node.js right away:\nsudo apt install nodejs npm Next to Node.js will be installed the Node Package Manager (NPM).","install-nginx#Install NGINX":"These measures are used to install NGINX Mainline from the official NGINX Inc repository on Ubuntu. See the NGINX admin guide for other distributions. See our Getting Started with the NGINX series for information about configuring NGINX for production environments.\n1. In the text editor, Open¬†/etc/apt/sources.list¬†and add the next line to the right. Substitute CODENAME¬†by your Ubuntu release codename in this case. For eg, the bionic insert in place of CODENAME¬†below for Ubuntu 18.04 named bionic¬†Beaver:\ndeb http://nginx.org/packages/mainline/ubuntu/ CODENAME nginx 2. Import the signing key for the repository and link it to apt:\nsudo wget http://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key 3. Install NGINX:\nsudo apt update sudo apt install nginx 4. Ensure that NGINX works and can automatically continue when reboot:\nsudo systemctl start nginx sudo systemctl enable nginx "},"title":"Use NGINX as a Reverse Proxy"},"/utho-docs/docs/windows/how-to-add-ssl-biniding-in-windows-server/":{"data":{"":"\nStep 1. Bind the certificate.\nOpen bindings options in default web sites. As shown in the below screenshot .\nClick on ‚ÄúBindings‚Ä¶‚Äù\nStep 2 : After opening the bindings , Click on add/edit option to add the domains for the SSL\nAdd a new rule.\nOr edit the existing rule.¬†Step 3: Choose the type, IP, port, and SSL certificate and then click on OK.\nSSL has been successfully installed on the domain , Please verify through URL .\nThank you."},"title":"How to add SSL biniding in windows server"},"/utho-docs/docs/windows/how-to-allocate-unallocated-disk-space-in-windows/":{"data":{"":"\nIntroduction Unallocated space, also referred to as ‚Äúfree space,‚Äù is the area on a hard drive where new files can be stored. Conversely, allocated space is the area on a hard drive where files already reside. Think of ‚Äúallocated‚Äù storage space as already filled with data and not to be overwritten with other newer data, while ‚Äúunallocated‚Äù space is available to store new data even though it may contain old data which would be overwritten by new data. In this blog, we will how to allocate unallocated disk space in Windows Server.\nStep 1. Login to your Windows server\nStep 2. Open¬†Command Prompt¬†(cmd)\nStep 3. run¬†diskpart\nC:UsersAdministrator\u003ediskpart Step 4. run list volume\nDISKPART\u003e list volume Step 5. run¬†select volume 0\nDISKPART\u003e select volume 0 Step 6. run¬†extend filesystem allocate unallocated disk space\nDISKPART\u003e extend filesystem Disk Allocation: Successful\nHow to add additional storage to your server.\nThank You!"},"title":"How to allocate unallocated disk space in Windows Server"},"/utho-docs/docs/windows/how-to-allow-icmpv4ping-in-windows-firewall-using-powershell/":{"data":{"":"","introduction#INTRODUCTION":"ICMPv4¬†Internet Control Message Protocol¬†version 4 is definitely a Network layer protocol. And its job is to report the error to the source if any problem arises while delivering the datagram. The ICMPv4 is a message-oriented protocol. It‚Äôs a protocol of version 4 of the TCP/IP protocol suite. In this article we will learn How to allow ICMPv4(PING) in Windows Firewall using Powershell.\nStep 1. Open PowerShell and an Administrator allow ICMPv4(PING) in Windows Step 2. Run the command to add rule of ICMPv4 Step 3. Set a display Name open ICMPv4(PING) in Windows A Step 4. Check if the rule is properly added.\nRule added successfully.\nThank You!"},"title":"How to allow ICMPv4(PING) in Windows Firewall using PowerShell"},"/utho-docs/docs/windows/how-to-allow-multiple-rdp-sessions-for-the-single-user-in-windows-server/":{"data":{"":"\nStep 1. Login to your windows server\nStep 2. Launch the Registry Editor in Windows Server, go to¬†Start¬†and then¬†Run. Type regedit in run menu and hit enter. To launch Registry Editor in Windows Server 2012 R2, press¬†Windows key¬†+ R. Type regedit¬†in run menu and hit enter.\nStep 3. Once Registry Editor window is launched, navigate to¬†HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\TerminalServer.\nStep 4. By selecting the¬†Terminal Server¬†registry, you would see the registry key¬†fSingleSessionPerUser¬†on the right panel.\nStep 5. Right-click on¬†fSingleSessionPerUser¬†key and then click on¬†Modify.¬†Step 6. Change the key value from 1 to 0 as shown in the screenshot below, and close the Registry Editor. The value 1 denotes single session for each remote desktop user, and 0 denotes multiple sessions for each user.\nMultiple RDP sessions for the single user enabled.\nNote:\nIf¬†fSingleSessionPerUser¬†key is not available then you will need to create it manually. To add a¬†new key, click on¬†Terminal Server¬†\u003e\u003e¬†New¬†\u003e\u003e¬†New¬†DWORD (32) Value. Name this key as¬†fSingleSessionPerUser, set its value to 0 and close the Registry Editor.\nThank You."},"title":"How to allow multiple RDP sessions for the single user in Windows Server"},"/utho-docs/docs/windows/how-to-block-or-allow-tcp-ip-port-in-windows-firewall/":{"data":{"":"","allow-port-in-firewall#Allow port in Firewall":"Step 1. Navigate to the Windows Firewall and advanced settings.\nStep 2. To see the list of rules, select ‚ÄúInbound Rules‚Äù from the menu that appears on the left side of the window. then click on the new rule.\nStep 3. Select port and then press the next button.¬†Step 4. Click on ‚ÄúSpecific local ports‚Äù and choose a port number (e.g., 80). then click on ‚Äúnext.‚Äù\nStep 5. Choose Allow the Connection and then click Next.¬†Step 6. Apply Your New Rule to Each of the Different Types of Profiles. To apply your rule to each of the three kinds of profiles (domain, private, and public), under the Profile window, you must check the appropriate boxes. To proceed, click the ‚ÄúNext‚Äù button.\nStep 7. Give your new rule a name. You can also add a description to your rule if you want to. Click ‚ÄúFinish‚Äù when you‚Äôre done to set up the settings.","block-port-in-firewall#Block port in Firewall":"Step 1. Navigate to the Windows Firewall and advanced settings.\nStep 2. To see the list of rules, select ‚ÄúInbound Rules‚Äù from the menu that appears on the left side of the window. then click on the new rule.\nStep 3. Select port and then press the next button.¬†Step 4. Click on ‚ÄúSpecific local ports‚Äù and choose a port number (e.g., 80). then click on ‚Äúnext.‚Äù\nStep 5. Choose Block the Connection and then click Next.¬†Step 6. Apply Your New Rule to Each of the Different Types of Profiles. To apply your rule to each of the three kinds of profiles (domain, private, and public), under the Profile window, you must check the appropriate boxes. To proceed, click the ‚ÄúNext‚Äù button.\nStep 7. Give your new rule a name. You can also add a description to your rule if you want to. Click ‚ÄúFinish‚Äù when you‚Äôre done to set up the settings.","conclusion#Conclusion":"Hopefully, you have learned how to block or allow ports in windows firewall.\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to block or allow ports in windows firewall.\nYour Windows machine can be protected from any threats posed by the network by using the Windows Firewall. You have the option of controlling who is granted to enter your system as well as the level of accessibility that is granted."},"title":"How to Block or Allow TCP/IP Port in Windows Firewall"},"/utho-docs/docs/windows/how-to-boot-windows-server-2012-into-safe-mode/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to boot windows server into safe mode.\nAlso read: How to Install OpenSSH on Windows Server\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to boot windows server into safe mode.\nSafe Mode is an unique Windows boot-up mode that can be used to restart the computer once a major problem has occurred that interferes with normal Windows functions and activities. Safe Mode enables users to debug and find the root cause of a fault.\nIn safe mode, an operating system has reduced functionality; however, the process of isolating problems is facilitated by the fact that many non-essential components, such as sound, are disabled. An installation that will only boot into safe mode typically has a significant issue, such as disc corruption or the installation of software with improper configuration, which prevents the operating system from successfully booting into its normal operating mode and allowing the user to use the computer as intended.\nA user is normally granted access to utility and diagnostic programs¬†when operating in safe mode. This enables the user to investigate and resolve issues that prevent the operating system from functioning normally. The functionality of the system is not meant to be used while in safe mode, and user access to features is severely restricted.\nYou have to login into your window server.\nStep 1. On the right side of the window, search cmd and click on Command Prompt.\nStep 2. Now run the msconfig command.\n# msconfig Step 3. Now click Left on the boot option, which is next to the general.\nStep 4. Go to the Boot menu, and then choose ‚ÄúSafe boot‚Äù from the list of ‚ÄúBoot settings.‚Äù\nStep 5. After pressing OK, the below screenshot will show on your screen. Select the Restart option to apply the changes.\nStep 7. After selecting Restart, you have to log into your server again and you will see a black screen on your server. That means your server is now in safe mode."},"title":"How to Boot Windows Server into Safe Mode"},"/utho-docs/docs/windows/how-to-change-default-shell-from-cmd-to-powershell-in-windows-server/":{"data":{"":"\nFollow these steps to change default shell from cmd to PowerShell in the following article..\nStep 1. Login to your Windows Server.\nStep 2. Start PowerShell as Administrator on the OpenSSH server and run the following command:\nGet-Command powershell | Format-Table -AutoSize -Wrap CommandType Name Version Source\n-‚Äî‚Äî‚Äî‚Äî- ‚Äî‚Äî ‚Äî‚Äî- ‚Äî‚Äî\nApplication powershell.exe 10.0.17763.1 C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\nStep 3. Set DefaultShell=PowerShell in the OpenSSH registry entry. PowerShell PATH specifies the PATH confirmed above\nNew-ItemProperty -Path \"HKLM:SOFTWAREOpenSSH\" -Name DefaultShell -Value \"C:WindowsSystem32WindowsPowerShellv1.0powershell.exe\" -PropertyType String -Force DefaultShell : C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\nPSPath : Microsoft.PowerShell.Core\\Registry::HKEY_LOCAL_MACHINE\\SOFTWARE\\OpenSSH\nPSParentPath : Microsoft.PowerShell.Core\\Registry::HKEY_LOCAL_MACHINE\\SOFTWARE\nPSChildName : OpenSSH\nPS Drive : HKLM\nPSProvider : Microsoft.PowerShell.Core\\Registry\nSo, this is how you can change default shell from cmd to PowerShell in Windows Server.\nThank You!"},"title":"How to change default shell from cmd to PowerShell in Windows Server"},"/utho-docs/docs/windows/how-to-change-rdp-port-in-windows-support-internal/":{"data":{"":"\nStep1 :: Need to connect windows server with Administrator privilege, then type below command in search box ‚Äúregedit‚Äù\nStep2 :: You need to follow the below path to change the RDP Port in the Windows system.\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\nStep3 ::After following above path steps, need to right click on on port number then provide the wanted port number for connection.\nStep4 :: Need to follow below steps to change the port number in registry.\nNow give your desired port number to connect to the windows server‚Ä¶\nStep5 :: Now I need to exit the registry then reboot the server to see the changes on it.\nStep6 :: Now we have to check the server connection with the new RDP port.\nThanks :)"},"title":"How to change RDP port in Windows Server"},"/utho-docs/docs/windows/how-to-change-rdp-port-via-powershell-in-windows-server/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to change RDP port via PowerShell in Windows server.\nAlso read: How to Block or Allow TCP/IP Port in Windows Firewall\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to change RDP port via PowerShell in Windows server.\nThe Remote Desktop Protocol, also known as RDP, is a proprietary protocol developed by Microsoft that enables users to remotely connect to other computers, most commonly through TCP port 3389. A distant user can gain access to the network using its encrypted communication channel.\nThe Windows Remote Desktop (RDP) port 3389 is a popular target for hackers. So, it may make sense to alter the RDP port. This article describes how to change¬†RDP port via PowerShell in Windows server 2012/2016/2019.\nStep 1. In the search box, type PowerShell.\nStep 2. Run PowerShell as an administrator.\nStep 3. To determine the current RDP port, use the command below.¬†# Get-ItemProperty -Path 'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\Terminal Server\\\\WinStations\\\\RDP-Tcp' -name \"PortNumber\" Step 4. To change the RDP port, run the below command and change the port as per your own.\n# Set-ItemProperty -Path 'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\Terminal Server\\\\WinStations\\\\RDP-Tcp' -name \"PortNumber\" -Value $portvalue=new\\_port\\_number Step 5. Run the below command to check the newly set RDP port.\n# Get-ItemProperty -Path 'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\Terminal Server\\\\WinStations\\\\RDP-Tcp' -name \"PortNumber\" Step 6. To reflect the changes, restart the RDP services from the Task Manager."},"title":"How to change RDP port via PowerShell in Windows server"},"/utho-docs/docs/windows/how-to-configure-a-dns-reverse-lookup-zone-in-windows-server-2019/":{"data":{"":"\nStep 1. Login to your win server via¬†RDP\nStep 2. Open¬†Server Manager\nStep 3. Go to¬†tools¬†and open¬†DNS¬†Step 4. In the DNS Manager, under your server,¬†right-click¬†on¬†Reverse Lookup zones,¬†and click on¬†New Zone‚Ä¶\nStep 5. In NEW ZONE wizard, click¬†next\nStep 6. Select¬†Primary Zone¬†and click¬†Next.\nStep 7. Select¬†IPv4 Reverse Lookup Zone¬†and click¬†Next.\nStep 8. Select¬†Network ID,¬†input your server IP and click¬†Next.\nStep 9. Select¬†Create a new file name. A file name should automatically be created. Click Next.\nStep 10. Click¬†Finish.\nStep 11. Expand¬†Reverse Lookup Zones¬†and click on the zone you just added.\nStep 12. Open¬†cmd¬†and input the command:\nC:UsersAdminnistrator\u003e**ipconfig /registerdns** Step 13. Add¬†the pointer.¬†Go to¬†DNS Manager,¬†right click on the mid screen, and click on ‚ÄúNew Pointer (PTR)‚Ä¶‚Äù.\nStep 14.Add¬†Host IP Address and Hostname, then click¬†OK.\nPointer Added\nStep 15. Exit your server and go to your main desktop. Open¬†cmd¬†and type the command¬†C:UsersAdministrator\u003e **nslookup your_server_ip** Example:\nC:UsersDivyanshu\u003e**nslookup 103.127.30.85** Name: 1031273085.network.microhost.in Address: 103.127.30.85 Thank you!"},"title":"How to configure a DNS Reverse Lookup Zone in Windows Server 2019"},"/utho-docs/docs/windows/how-to-configure-ftp-server-on-windows-server-2019/":{"data":{"":"\nStep 1. Log into tyour windows server.\nStep 2. Open Server Manager\nStep 3. Go to Add Roles and features Configure FTP Server on Windows\nSelect IIS and FTP server under IIS\nClick Install. IIS and FTP server will be installed.\nStep 4. Go to TOOLS and open IIS\nStep 5. Right-click on sites and click on Add FTP Site‚Ä¶\nInput Site name and it‚Äôs physical path Configure FTP Server on Windows\nSelect the server‚Äôs IP address in bindings, set authentication and read/write permissions and you are good to go.\nFTP folder has been set in the server.\nstep 6. Go to File manager of your local desktop and hit ftp://server_ip/ to access the FTP location.\nFTP folder accessed successfully.\nThank You!"},"title":"How to Configure FTP Server on Windows Server 2019"},"/utho-docs/docs/windows/how-to-configure-ip-manually-on-windows-server/":{"data":{"":"\nStep 1. Log into your Windows Server\nStep 2. Open Run and type CMD, click ok\nStep 3. In CMD, type ncpa.cpl and hit enter\nStep 4. Go to properties of the Network Adapter\nStep 5. Double Click on IPv4 config.\nStep 6. Enter IP, Netmask, Gateway and DNS and click OK\nIP configured.\nThank You."},"title":"How to configure IP manually on Windows Server"},"/utho-docs/docs/windows/how-to-connect-sftp-using-filezilla/":{"data":{"":"In this tutorial, we will learn how to connect a Linux server with SFTP using FileZilla client.\nYou need to enable SFTP or SSH access in your Microhost cloud server before logging in to SFTP.\n1. Open¬†FileZilla\n2. Click on file tab on top in FileZilla.\n3. After that click file open site manager and click on new site.\n4. Open new site and set site name(For Eg:- site1) and change protocol from FTP to SFTP.\n5. Enter your server ip or hostname in host field and username and password(root user or normal user which one you have assigned to your website folder) and click on connect.\nThankyou.."},"title":"How to connect SFTP using FileZilla"},"/utho-docs/docs/windows/how-to-connect-to-a-windows-server-using-remote-desktop-protocol-rdp/":{"data":{"":"\nconnect to a Windows server\nStep 1. Open Run and type ‚Äúmstsc‚Äù, click enter\nStep 2. enter the server IP along with the RDP port\nStep 3. Enter username and password of the server and you will be connected to your Windows Server\nWindows Server Connected.\nThank You!"},"title":"How to connect to a Windows server using Remote Desktop Protocol (RDP)"},"/utho-docs/docs/windows/how-to-connect-virtual-server-remotely-using-rdp-in-windows-os/":{"data":{"":"You can connect to other computers or devices connected to local network or on the Internet with a Public IP Address via Windows, Remote Desktop Connection application. For example, when you‚Äôre using another computer or device, you can use the remote desktop connection app to connect to your computer desk‚Äôs or work computer.","connect-another-computer-remotely-on-the-public-network#Connect another computer remotely on the public network":"Step 1: Go to the search bar of the taskbar as following :\nStep 2: Type remote desktop connection in the search bar and click on the icon :\nThe output will be shown as following:\nStep 3: First you need to enter the IP address in the white bar as shown in above image. If you are using RDP default port then enter only the IP address. However, if you are using any custom port for RDP connection then you have to enter the IP address including the port number.\nIn the above image IP Address is ( 51.89.143.218) and port is ( 4489 ). You can use only IP Address in case of default port.\nNow you will get the prompt to enter the Username \u0026 Password as following:\nEnter the password and sometime you will get the Warning of SSL Security. You need to enter YES as following:\nNow you are connected with the remote PC and can use it as your local PC.\nThank You :)"},"title":"How to Connect Virtual Server Remotely Using RDP in Windows OS"},"/utho-docs/docs/windows/how-to-create-mailbox-in-mailenable/":{"data":{"":"\nSTEP 1: Firstly, login the server using RDP and then open Mailenable console while searching in the server.¬†Step 2: Once the console is opened, select the ‚Äúmessaging manager‚Äù and then select ‚Äúpost office.‚Äù Please have a look at the screenshot below:\nStep 3: Now we have to click on the post office and there we will find the option to create a mailbox. Please have a look at the screenshot for reference.\nStep 4: We can create any number of mailboxes, depending on the requirements.\nThank you :)"},"title":"How to Create Mailbox in MailEnable"},"/utho-docs/docs/windows/how-to-create-rdp-user-in-windows-server-2012/":{"data":{"":"Follow the steps below to create additional RDP users to connect to your Windows Microhost cloud server.\n1. Access your server using RDP with administrator.\n2. Click on Server Manager and open.\n3. In server Manager click on Tools and expand tools and open Computer Management.\n4. Click On users and groups.\n5. Right Click on Users and add new user. Check option ‚ÄúUser must change password at next login‚Äù if you want to change user password at first login.\n6. Enter user name ,Full name and password.\n7. Now you need to go in user properties for assigned RDP group for access RDP.\n8. Click on add button.\n9. Now click on Advanced tab for search group.\n10. Click on find button.\n11. Here you need to select group ‚ÄúRemote Desktop Users‚Äù and click ok.\n12. Now the search window will close and click on ok.\n13. Again you need to open Server Manager and click on Services.\n14. Click on remote desktop Services and restart the services using restart option.\nNow try to access Remote Desktop with created new RDP user.\nThank You.."},"title":"How to create RDP user in Windows Server 2012"},"/utho-docs/docs/windows/how-to-host-a-domain-on-windows-server-2019/":{"data":{"":"\nPrerequisites\n1.¬†Windows Server\n2. IIS\n3. Domain (html files)\nStep 1. Login into your server via RDP.\nStep 2. Open IIS (Install IIS if you don‚Äôt have it on your system)\nLink on How to Install IIS:\nInstall IIS via Powershell\nInstall IIS through GUI\nStep 3. Paste your domain files in a folder in this location¬†C:\\inetpub\\wwwroot\nNaming the folder according to your domain name We will name it ‚Äúmysite‚Äù\nStep 4. Go to IIS \u003e Sites and click on¬†Add Site.\nInput your site name, physical path, and hostname. Click OK.\nStep 5. Go to this location:¬†C:\\Windows\\System32\\drivers\\etc¬†‚Ä¶.and edit the¬†Hosts¬†file.\nStep 6. Go to IIS and click on ‚ÄúBrowse mysite.local‚Äù\nYour domain will open in the browser.\nThank you!!"},"title":"How to host a domain on Windows Server 2019"},"/utho-docs/docs/windows/how-to-initialize-and-bring-a-disk-online-in-window-server/":{"data":{"":"\nStep 1. Log into your Win server and open Computer Management\nStep 2. Right click on the ‚ÄúNot initialized‚Äù and then click on ‚ÄòInitialize Disk‚Äô\nThen the Disk will show as ‚ÄúOffline‚Äù\nStep 3. Right click on ‚ÄúOffline‚Äù and then click on ‚ÄúOnline‚Äù\nstep 4. Open ‚ÄòRun‚Äô, type ‚Äòdiskpart‚Äô and click OK\nStep 5. Run diskpart according to the article below:\nhttps://utho.com/docs/tutorial/how-to-allocate-unallocated-disk-space-in-windows/\nStep 6. After completing diskpart, go to File Manager, and check your disk space.\nThank you!"},"title":"How to Initialize and bring a disk online in Window Server"},"/utho-docs/docs/windows/how-to-install-active-directory-domain-service-on-windows-server/":{"data":{"":"","#":"\nINTRODUCTION Active Directory¬†Domain Services (AD DS) is a server role in Active Directory that enables administrators to control and store data from applications as well as network resource information in a distributed database. Administrators can control network components, including people and computing devices, and reorder them into a certain hierarchical structure with the use of AD DS. Incorporating security further is AD DS by authenticating logons and restricting access to directory resources. In this tutorial, we will learn how to install Active Directory Domain Service on Windows Servers 2012R2, 2016, 2019 and 2022.\nTo install Active Directory Domain Service via PowerShell, open PowerShell with Admin Privileges, and run the following command.\nPS C:\\Users\\Administrator\u003e Install-WindowsFeature -name AD-Domain-Services -IncludeManagementTools\nNow, run the following command to restart the server to apply changes.\nPS C:\\Users\\Administrator\u003e Restart-Computer -Force\nTo install Active Directory Domain Service via Server Manager\nStep 1. Open Server Manager and click Add roles and features.\nStep 2. Click Next\nStep 3. Select Role-based or feature-based installation.\nStep 4. Select a Host which you‚Äôd like to add services.\nStep 5. Check a box Active Directory Domain Services.\nStep 6. Addtional features are required to add AD DS. Click Add Features button.\nStep 7. Click Next.\nStep 8. Click Next.\nStep 9. Click Install.\nStep 10. After finishing Installation, click Close button.\nThank You!"},"title":"How to install Active Directory Domain Service on Windows Server"},"/utho-docs/docs/windows/how-to-install-apache-tomcat-9-on-windows-server/":{"data":{"":"\nIntroduction Apache Tomcat version 9.0¬†implements the Servlet 4.0 and JavaServer Pages 2.3 specifications from the Java Community Process, and includes many additional features that make it a useful platform for developing and deploying web applications and web services.\nFirstly, we should have Java Development kit (JDK) isntalled on the machine. Please refer to the blog tagged below on how to install JDK on Winodws Server.\nHow to install Java Development kit (JDK) on Windows Server.\nNow, We will download Apache Tomcat setup files.\nDownload Apache Tomcat 9\nStep 1. Login to your winodws server\nStep 2. Double click on the setup media of JDK to begin installation.\nStep 3. Click Next to start installation.\nSet password for your Tomcat Manager\nSelect the path as shown below:\nApache Tomcat 9 successfully installed.\nStep 4. Launch Tomcat application\nIt will launch in cmd\nStep 5. hit http://localhost:8080/ to view tomcat\nStep 6. hit http://localhost:8080/manager/ to go to tomcat manager page.\nUse username and password that we setup during installation.\nStep 7. Open Tomcat 9 manager\nSet the Apache service to start automatically. so that you don‚Äôt have to start the service manually everytime.\nApache Tomcat 9 installation completed on Windows Server.\nThank You."},"title":"How to install Apache Tomcat 9 on Windows Server"},"/utho-docs/docs/windows/how-to-install-configure-printer-pool-in-windows-server/":{"data":{"":"","#":"\nPrerequisites: Windows Server IIS A printer Step 1. Log into your Windows server via RDP.\nStep 2. Open¬†Server Manager,¬†click¬†Add Roles \u0026 Features,¬†then click Next¬†3 times.¬†On the Select Server Roles interface, click¬†Print and Document Services¬†and then click¬†Next.\nStep 3. On the Select Features¬†interface, click¬†Next.\nStep 4. On the Print and Document Services interface, click¬†Next.\nStep 5. On the Select Role Services interface, in the Role Services section, verify that the Print Server¬†check box is selected and then click¬†Next.\nStep 6. In the¬†Confirm Installation Selections¬†interface, click¬†Install.\nStep 7. On the installation progress interface, click¬†‚ÄúClose.‚Äù\nStep 8. On the DC-CLOUD.Windows.ae¬†server, in the¬†Server Manager,¬†click¬†Tools, and then click¬†Print Management.\nStep 9. Expand¬†Print Servers, expand¬†DC-CLOUD (local), right-click¬†DC-CLOUD¬†and then¬†click¬†Add Printer.\nStep 10. On the Network Printer Installation Wizard interface, click¬†Add a TCP/IP or Web Services Printer by¬†IP address or hostname, and then click¬†Next.\nStep 11. On the Printer Address interface,¬†change the¬†Type of Device to TCP/IP Device. Next, in Host name or IP address, type¬†172.16.1.254 (this will be your printer‚Äôs network IP). Clear Auto detect the printer driver to use, and¬†then click¬†Next.\nStep 12. Under Device Type, click¬†Generic Network Card, and then click Next.\nStep 13. On the Printer Driver interface, click¬†Install a new driver, and then click¬†Next.\nStep 14. On the Printer Installation interface, under¬†Manufacturer,¬†click¬†Microsoft. Under¬†Printers, click¬†Microsoft XPS Class Driver, and then¬†click Next.\nStep 15. On the¬†Printer Name and Sharing Settings¬†interface,¬†change the Printer Name to¬†Windows¬†Sales Printer, and then click¬†Next.\nStep 16. Click¬†Next¬†2¬†times to accept the default printer name and share name and to install the printer.\nStep 17. Click¬†Finish¬†to close the Network Printer Installation Wizard.\nStep 18. In the Print Management console,¬†right-click the Windows Sales Printer, and then click¬†Enable¬†Branch Office Direct Printing.\nStep 19. Right-click the Windows Sales Printer, and then select¬†Properties.\nStep 20. On the¬†Windows Sales Printer¬†properties,¬†click the¬†Sharing¬†tab, select¬†List in the directory, and then click¬†OK.\nStep 21. Next, let‚Äôs configure printer pooling¬†in the Print Management console. Under SUB-01 Server,¬†right-click Ports, and then click¬†Add Port.\nStep 22. In the Printer Ports dialog box, click¬†Standard TCP/IP Port, and then click¬†New Port.\nStep 23. In the Add Standard TCP/IP Printer Port Wizard, click¬†Next.\nStep 24. On the Add Port Interface,¬†in¬†Printer Name or IP Address, type¬†172.16.1.200, and then click¬†Next.\nStep 25. In the Additional port information required dialog box, click¬†Next.\nStep 26.Click¬†Finish¬†to close the Add Standard TCP/IP Printer Port Wizard.\nStep 27. Click¬†Close¬†to close the Printer Ports dialog box.\nStep 28. In the Print Management console, right-click¬†Windows¬†Sales Printer, and then click¬†Properties.\nStep 29. In the¬†Windows¬†Sales Printer¬†Properties dialog box, click the¬†Ports¬†tab, select¬†Enable printer¬†pooling, and then click the¬†172.16.1.200 port¬†to select it as the second port.\nStep 30. Switch to your¬†client PC,¬†open the Control Panel, then click¬†Devices and Printers.\nStep 31. In the Devices and Printers console, click¬†Add a printer.\nStep 32. On the Add Printer interface, under Search for Available Printers, click¬†your¬†existing printer name¬†and then click¬†Next.¬†Then¬†the wizard will¬†install the printer driver from the¬†DC-CLOUD. Windows¬†server\nStep 33. On the Add Printer interface (it will state that ‚ÄúYou‚Äôve successfully added a Windows¬†Sales Printer¬†on¬†172.16.1.254‚Äù), click¬†Next.\nStep 34. Click¬†Finish.\nStep 35. Verify that you have a Windows¬†Sales Printer¬†at¬†172.16.1.254 that is listed in your¬†Devices and Printers control panel.\nStep 36. Installation was completed successfully.\nThank You!"},"title":"How to Install \u0026amp; Configure Printer Tool in Windows Server"},"/utho-docs/docs/windows/how-to-install-iis-via-powershell-in-windows-server/":{"data":{"":"","introduction#INTRODUCTION":"Internet Information Services(IIS) is an extensible web server software created by Microsoft for use with the Windows NT family. IIS supports HTTP, HTTP/2, HTTPS, FTP, FTPS, SMTP and NNTP. It has been an integral part of the Windows NT family since¬†Windows NT 4.0, though it may be absent from some editions (e.g. Windows XP Home edition), and is not active by default. In this tutorial, we will learn How to install IIS via Powershell in Windows Server 2012R2, 2016, 2019, 2022.\nPrerequisites - install IIS via Powershell in Windows A server with Microsoft Windows Server¬†operating system installed on it.\nYou are logged on to the Windows Server as an Administrator.\nMicrosoft Powershell in Administrator mode.\nStep 1. Open Powershell as Administrator\nRun the following command:\nInstall-WindowsFeature Web-Server -IncludeManagementTools Open Server Manager and check that IIS is installed in ONE click.\nThank You!"},"title":"How to install IIS via Powershell in Windows Server"},"/utho-docs/docs/windows/how-to-install-java-development-kit-on-windows-server/":{"data":{"":"\nIntroduction The Java Development Kit is a distribution of Java Technology by Oracle Corporation. It implements the Java Language Specification and the Java Virtual Machine Specification and provides the Standard Edition of the Java Application Programming Interface.\nFirstly, we need to download JDK from the Oracle website. Use the link attached below to download JDK 8\nDownload JDK 8\nStep 1. Login to your winodws server\nStep 2. Double click on the setup media of JDK to begin installation.\nStep 3. Click Next to start installation.\nJDK isntalled successfully.\nStep 4. Check JDK installation.\nThank You."},"title":"How to install Java Development kit on Windows Server"},"/utho-docs/docs/windows/how-to-install-mailenable-on-windows-server/":{"data":{"":"\nIntroduction :\nMailEnable is an e-mail server distributed by MailEnable Pty based on Windows. The features of MailEnable include support for IMAP, POP3 and SMTP email protocols, with the support of SSL / TLS and the list server. The management functionality can be used by the Microsoft Management Console or the browser. The functionality of MailEnables can be implemented by using MMAP.\nBefore we begin\nThe user should have admin access to the server.\nThe server should be connected to the internet.\nIIS Manager should be installed on the server.\nStep 1: Firstly, login to the server using RDP and download the Mailenable setup file using the link. Mailenable usually offers two products: one is open source and the other is license-based. Please have a look at the screenshot for your reference.\nSTEP 2: After successfully downloading the setup file, We have to initiate the installation process. We have to execute the downloaded setup file as given in the screenshot below.\nStep 3: While executing the setup file, we will get the prompt as below. We need to click on ‚ÄúOK.‚Äù\nStep 4: While clicking on the ‚Äúok‚Äù button, a new window of installation will appear like below. We have to click on ‚Äúnext.‚Äù\nStep 5: While clicking on ‚Äúnext,‚Äù will move to the next part. Where we have to enter the company name as given below in the screenshot,\nSTEP 6: In the next window, we can see the terms and conditions for the email server. We need to click on ‚Äúnext‚Äù as per the screenshot.\nStep 7: Afterward, we will move to the next window, where we can see the description of the disk space requirement for the Mailenable application. Please see the screenshot for your reference.\nStep 8: In the next window, we can change the installation folder if we want, otherwise simply click on ‚Äúnext‚Äù as per the screenshot.\nStep 9: In the next window, we can change the data directory of Mailenable if we want to store email data at a different location. Please see the screenshot for your reference.\nStep 10: In the next window, we have to enter the post office name. You can give any name as your post office name. However, I‚Äôll recommend you use the domain name as your post office name. Please have a look at the below screenshot for your reference.\nStep 11: In the next window, we have to enter the domain name from which we want to use the email service. Rather than the domain name, everything will be the same as per the screenshot.\nStep 12: In the next window, we will see the message that everything is ready to be installed. It will ask about the configuration of the IIS site configuration for Mailenable. Please see the screenshot for your reference.\nStep 13: While clicking next, another window will ask for another IIS site configuration for Mailenable. We have to simply click on ‚Äùnext‚Äù as per the screenshot.\nStep 14: While clicking next, the installation will automatically start and, in a while, it will be completed. We have to finish the installation process by clicking ‚Äúfinish‚Äù as per the screenshot.\nWe have successfully installed Mailenable.\nThank you :)"},"title":"HOW TO INSTALL MAILENABLE ON WINDOWS SERVER"},"/utho-docs/docs/windows/how-to-install-mssql-express-edition-2019-on-windows-server/":{"data":{"":"SQL Server 2019¬†Express is a free edition of SQL Server, ideal for development and production for desktop, web and small server applications.\nAsp.net 3.5 version should be installed on server before install SQL server and you have access administrator RDP user login details.\n1. Install Asp.net 3.5 version using server manager.\n(i) Open server manager click on server manager button.\n(ii) Click on Add roles and features.\n(iii) Select .NET framework 3.5 Features and install.\n(iv) Click on next.\n(v) When you will get below message in screenshot close and exit the server manager installation wizard.\nNow, download a SQL Server Express offline installer from the Microsoft SQL Server downloads page. On the page, click Download now for the Express edition. Then, open the folder that keeps downloaded files and run the SQL2019-SSEI-Expr.exe file.\nAfter starting the installation, we have to choose the installation type of the SQL Express edition\nThen, accept the license agreement and privacy policy terms. After that, specify the location for the install package and click Install.\nAfter launching the installation process, you will see a progress bar informing that the install package is being downloaded and that SQL Server Express is being installed. During the installation, you can either pause or cancel the installation.\nNow, we will select the New SQL Server stand-alone installation or add features to an existing installation option to start the installation:\nWe will accept the license terms and other details on the¬†Licence Terms¬†screen and click** Next**:\nThe** Global Rules** screen will be launched and, the SQL Server Setup will be checked the computer configurations:\nThe¬†Product Update¬†screen checks the latest updates of the SQL Server Setup and if we don‚Äôt have an internet connection, we can disable the¬†Include SQL Server product updates¬†option:\nIn the¬†Install Rules¬†screen, the potential issues will be checked by SQL Setup that might be occurred during the installation. We will click the¬†Next¬†button and skip the next step:\nIn the *_Feature Selection *_screen, we can select the features which we want to install. For this installation, we will disable the following features:\nMachine Learning Services and Language Extensions\nFull-Text and Semantic Extractions For Search\nPolyBase Query Service for External Data\nLocalDB\nAt the same time, we can specify the installation path of the SQL Server through the Instance root directory option:\nIn the¬†Instance Configuration¬†screen, we will give a name to the SQL instance but we will not change the default name of the installation. Click the¬†Next¬†button:\nIn the¬†Server Configuration¬†screen, we can determine the startup type of the SQL Server Database Engine and SQL Server Browser services. We will not change the default options:\nIn the** Database Engine Configuration** screen, we will specify the authentication mode of the SQL Server. We have two options on this screen.\nIn the *_Windows authentication *_mode, the SQL logins are controlled by the Windows operating systems and it is best practice to use this mode.\nIn the¬†Mixed Mode, the SQL Server can be accessed with both Windows authentication and SQL Server authentication. For this installation, we will select the Mixed Mode option, so we will set the password of the sa login of the SQL Server and we will also add a windows login. We can directly add the current windows user to the windows through the Add Current User button:\nIn the Installation Progress screen, we can follow the progress of the installation:\nIn the final step, the Complete screen meets us and it gives information about the result of the installation:\nTest connection to SQL Server Express\nAfter installing the SQL Express, we are able to connect to the SQL Server Database Engine over the SQL Server Management Studio.\nSo then click on Install SQL Server Management tool and download the SQL server management Studio(SSMS)\nTo connect to the SQL Server using the Microsoft SQL Server Management Studio, you use these steps:\nFirst, launch the Microsoft SQL Server Management Studio from the Start menu:\nNext, from the¬†Connect¬†menu under the¬†Object Explorer, choose the¬†Database Engineer‚Ä¶\nThen, enter the information for the Server name (localhost), Authentication (SQL Server Authentication), and password for the sa user and click the Connect button to connect to the SQL Server. Note that you should use the sa user and password that you entered during the installation.\nThe installation is completed.\nThankyou"},"title":"How to install MsSQL Express Edition 2019 on Windows Server"},"/utho-docs/docs/windows/how-to-install-multipoint-services-in-windows-server-2016/":{"data":{"":"\nThe MultiPoint Services (MPS) role replaces the well-known Windows MultiPoint Server 2012. This role enables multiple users to share simultaneously one single computer. In turn, each user has its own independent Windows experience..\nLet get started.\nStep 1 ‚Äì Open server manager dashboard,¬†‚ÄúAdd roles and features‚Äù¬†Click Next,¬†Choose¬†‚ÄúRole-based or feature-based installation‚Äù¬†radio button and¬†click Next,¬†Scroll down and choose¬†select¬†MultiPoint Services.¬†You may notice there are some additional features are required for MPS such as¬†File And Storage Services, Print and Document Services and etc. Just click on ‚ÄúAdd features‚Äù¬†and¬†click Next.¬†(refer picture).\nStep 2 ‚Äì Read a great explanation from Microsoft¬†‚Äúwhat is MPS?‚Äù.\nRemote Desktop Licensing¬†needs to be activated or use trial period¬†(120 days).\nStep 3 ‚Äì I leave¬†default settings¬†and click Next. Read and¬†click next.\nPrint Server¬†is needed to manage multiple printers\nDistributed Scan Server¬†‚Äì enables you to manage and share networks scanners that support Distributed Scan Management\nInternet Printing¬†creates a web site where users can manage printer jobs on the server .\nIf you have installed Internet Printing client on stations you can connect and print to shared printers using Web Browser and Internet Printing Protocol\nLPD service¬†‚Äì Line Printer Daemon Service enables UNIX-based computers using the Line Printer Remote service to print to shared printers on MPS\nStep 4 ‚Äì I leave¬†default settings¬†and click Next. Read and¬†click next.\nRD Gateway¬†‚Äì to publish RDS (not suitable for MPS)\nRD Connection Broker¬†‚Äì to distribute connections¬†(not suitable for MPS)\nRD Virtualization Host¬†‚Äì for VDI\nRD Web Access¬†‚Äì web access to RD session/vdi/remoteapp collections (not suitable for MPS)\nStep 5 ‚Äì¬†Click at Restart¬†the destination server automatically if required checkbox to restart your server.\nStep 6: ‚Äì When installation progress reaches the end click the¬†Close¬†button to close the Add Roles and Features Wizard.¬†The server restart is required.\nStep 7 ‚Äì Press Start button and open¬†MultiPoint Manager.\nStep 8 ‚Äì¬†Add MultiPoint Servers¬†or¬†personal computers¬†(optional)\nStep 9 ‚Äì Go to Users tab and click ‚ÄúAdd user account‚Äù, click Next and select user type.¬†(refer picture).\nStep 10 ‚Äì Testing, Connect to¬†MultiPoint¬†Server¬†from the user connection using¬†RDP.\nWhen user firstly log on to¬†MultiPoint¬†Services¬†he receives¬†privacy notification¬†‚ÄúTo assist you with your usage of this computer, your activities may be monitored by your system administrator‚Äù\nClick on¬†‚ÄúAccept and continue using this computer‚Äù¬†and¬†go back to MPS server.¬†(refer picture).\nStep 11 ‚Äì On MPS server run¬†MultiPoint Dashboard. All screens from user stations are being added and updated to dashboard.\nYou can see what happens on user‚Äôs station, block this desktop, set message for blocked users, take control, write IM to user, block USB storage or limit web access on selected desktops.\nStep 12 ‚Äì You can also¬†project your desktop to all or selected user desktops.\nIt‚Äôs really needed when trainer or teacher does not have projector so he or she shares screen to all user‚Äôs station.\nIf you are familiar with¬†Lync / Skype¬†there is a similar feature called as¬†‚Äúdesktop sharing‚Äù¬†(refer picture).\nStep 13 ‚Äì If you open MultiPoint Manager you can notice that list of stations has been updated with rlevchenko‚Äôs station.\nThank you :)"},"title":"How to Install MultiPoint Services in Windows Server 2016"},"/utho-docs/docs/windows/how-to-install-mysql-on-windows-server-2019/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now that you have learned how to install MySQL on Windows Server 2019.\nAlso read:¬†How to Block or Allow TCP/IP Port in Windows Firewall\nThank You üôÇ","install-mysql#Install MySQL":"Step 1: Start by installing the Windows Server 2019 MySQL Installer. Select the version that fits on your machine, then click Download.\nStep 2: Run the programme from the pop-up window at the bottom of your computer after you‚Äôve downloaded the file.","introduction#Introduction":"In this article, you will learn how to install MySQL on Windows Server 2019.\nMySQL is a database that can be downloaded for free and is used by many people. It is a relational database management system that is used by a lot of people. Even though Linux servers use it more often, it works just as well with Windows Servers.","mysql-installer-setup#MySQL Installer Setup":"Step 1: When you ran the programme in the last step, a new window came up with the MySQL Installer setup. It asks you which MySQL items you want to put on the host.\nOne choice is to choose the setup type that is already set up and fits your needs. Here‚Äôs a quick look at each mode:\nDeveloper Default: Installs MySQL Server, Shell, Router, Workbench, MySQL for Visual Studio, MySQL Connectors (for.NET, Python, ODBC, Java, and C++), MySQL Documentation, and MySQL Samples and Examples.\nServer only: Only puts the MySQL server in place. This type of setup runs the GA (general availability) or development release server you chose when you downloaded MySQL Installer. It uses the download and data paths that are set by default.\nClient only: Only downloads the latest MySQL applications and MySQL connectors. This setup type is similar to the Developer Default type, but it does not include the MySQL server or the client programmes that usually come with it, such as MySQL or mysqladmin.\nFull: Installs all available MySQL tools.\nCustom: With the custom setup type, you can search for and choose specific MySQL items from the MySQL Installer collection.\nFor this article, select Server only.\nStep 2: Click on Execute.\nStep 3: Click on terms and conditions, then click install.\nStep 4: Once the setup is done, it will show the message below, which says that the installation was successful. Click the close.\nStep 5: Click on Check Requirements, which is on the left side of the screen.\nStep 6: Click on Execute.\nStep 7: Click on next.\nStep 8: Click on Next to configure the MySQL server.\nStep 9: Choose the setting that works your needs, then click Next.\nStep 10: Use Strong Password Encryption for Authentication and click Next in the Authentication Method area.\nStep 11: The Accounts and Role window is the next screen. This is where you can set your MySQL Root Password. Fill out this part, then click Next.\nStep 12: You can set up MySQL Server as a Windows service and give the service a name on the Windows Service screen. The given name for this lesson is MySQL8.0.\nYou can also start the service at System Startup and use the normal system account or a custom user to run it. Choose what you want and then click Next.\nStep 13: Click ‚ÄúExecute‚Äù after selecting ‚ÄúConfiguration Steps.‚Äù¬†Step 14: When the operation is finished, click the Finish button.¬†","using-command-prompt-to-access-mysql#Using Command Prompt to access MySQL":"If you use the Windows Start menu to open Command Prompt and then run the mysql command, you may see an error below.\n# mysql This mistake happens because MySQL Bin has not been added to the path. To find where the MySQL Bin is, use the File Manager to go to This PC. Then go to C drive \u003e Program Files \u003e MySQL \u003e MySQL Server 8.0 \u003e bin. Right-click on ‚Äúbin‚Äù in the bar at the top and choose ‚ÄúCopy address.‚Äù\n# cd C:\\\\Program Files\\\\MySQL\\\\MySQL Server 8.0\\\\bin And then run the following to access your MySQL.\n# mysql -u root -p "},"title":"How to install MySQL on Windows Server 2019"},"/utho-docs/docs/windows/how-to-install-one-ssl-certificate-on-two-different-windows-server/":{"data":{"":"\nIntroduction :\nSSL (Secure Sockets Layer) and its successor, TLS (Transport Layer Security), are protocols for establishing authenticated and encrypted links between networked computers. We can have secure communication over the internet using SSL.\nPrerequisite\nwindows server with administrator permission\nOne Fully qualified domain name\nIIS should be installed on the server\nA SSL certificate\nStep 1: The first thing , we have to do that, we need to generate a CSR certificate from the server, where we have to install the SSL certificate. We can generate the CSR certificate in the IIS (¬†Internet Information Services ) in windows server. Please have a look on the below screenshot for your reference.\nStep 2: We have to click the option \" Click Certificate Request\" given in the write hand side of the screenshot.¬†Step 3: Once you will click on the¬†\" Click Certificate Request\", you will get the new prompt as per the¬†above screenshot¬†. We need to fill all the required details correctly as per the below screenshot and then click on the next button.\nStep 4¬†: While clicking on the next button , a new prompt will appear, there we have to select the encryption bit. It can be of your choice. Usually we create it using 2048 bit.\nStep 5 : While clicking on the next button , a new prompt will appear, there we have to give the CSR file storage location . Weather, we can create a text file prior the installation or we can defile the file at that moment also.¬†Step 6 : Once you will click on finish , a CSR certificate will be create. We can view the CSR file content while opening the file using notepad or with any text editor.\nStep 7: Now we have to share this CSR certificate with the concerned person to generate the SSL certificate. Once we will receive the certificate, we will proceed with the installation. In IIS there is a option of \" Complete Certificate Request\" just below the CSR option. We need to click on that, afterward a new prompt will appear where you need to select the .crt file afterward enter the domain name and then click on OK button.¬†Step 8: Now as per the above screenshot, we have successfully completed the installation of the certificate on the server. Next we will see how we can import and export the certificate.\n***********************************************************************************************************\nHOW TO EXPORT AND IMPORT SSL CERTIFICATE USING IIS (Internet Information Services)\nStep 1: Firstly we need to export the certificate, so we will move to the SERVER CCERTIFICATE option of IIS. There we need to select the certificate which we want to export as .pfx file . While exporting we have to assign a password for the certificate which will be used while importing the ssl in another server.\nPlease have a look on the below screenshot.\nStep 2:¬†One the certificate¬†will be exported we need to import it on another server, where we want to install that ssl certificate. We can find the import option in the Server Certificate . We have to browse to the path of .pfx file then we need to enter the password which had been generated while exporting the ssl certificate. Please have a look on the screenshot.\nWe have completed the export and import task of ssl certificate now.\nThank you :)"},"title":"HOW TO INSTALL ONE SSL CERTIFICATE ON TWO DIFFERENT WINDOWS SERVER"},"/utho-docs/docs/windows/how-to-install-openssh-on-windows-server/":{"data":{"":"","conclusion#Conclusion":"I hope now you understand how to install OpenSSH on Windows Server¬†Thank You üôÇ","configure-firewall#Configure Firewall":"Step 4. Open the Windows start menu, search for ‚ÄúServer Manager,‚Äù and then click on it. In the window for the Server Manager, go to the Tools menu and then choose the Windows Defender Firewall with Advanced Security option from the drop-down list.\nStep 5. Now, within the Firewall window that has been opened, right click at the Inbound Rules button. After that, pick New Rule from the menu located in the right pane.\nStep 6.Choose Port from the list of available options within the New Inbound Rule Wizard.\nStep 7. Then click the Next button. Choose ‚ÄòTCP‚Äô from the drop-down menu, and then type 22 into the box under ‚ÄòSpecific local ports:‚Äô\nStep 8. The next step is to enable the connection, assign the rule to server profiles, and set a custom name for the rule so that it can be easily identified among the list of Firewall rules.\nTo save the new firewall rule, click the Finish button.","install-openssh#Install OpenSSH":"Through Windows Settings\nStep 1. To access the Start Menu in Windows, click the Start button.\nStep 2. Find the Settings menu and click it to open it. In the newly opened window, go to the Apps menu, and then select Optional Features from the Apps \u0026 Features drop-down menu.\nStep 3. After clicking the ‚ÄúAdd Feature‚Äù button, type ‚ÄúOpenSSH‚Äù into the search box to locate the feature. After that, from the list of results, choose OpenSSH Server, and then click the Install button.\nFollowing the completion of the installation, the OpenSSH Client and Server will be added to the list of features that are installed on the system.","introduction#Introduction":"This article will show you how to Install OpenSSH on Windows Server, OpenSSH is an encrypted remote access tool that protects data sent between a client and server. Therefore, it prevents common forms of network assaults like as connection hijacking, sniffing, and eavesdropping. Follow along with this tutorial to set up OpenSSH on a Windows server."},"title":"How to Install OpenSSH on Windows Server"},"/utho-docs/docs/windows/how-to-install-python-3-7-on-windows-server-2012-r2-2016-2019-2022-via-powershell/":{"data":{"":"","introduction-how-to-install-python-37-on-windows-server#INTRODUCTION how to install Python 3.7 on Windows Server":"Python¬†is a¬†high-level,¬†general-purpose programming language. Its design philosophy emphasizes¬†code readability¬†with the use of¬†significant indentation. Python is¬†dynamically-typed¬†and¬†garbage-collected. It supports multiple¬†programming paradigms, including¬†structured¬†(particularly¬†procedural),¬†object-oriented¬†and¬†functional programming. It is often described as a ‚Äúbatteries included‚Äù language due to its comprehensive¬†standard library. how to install Python 3.7 on Windows Server\nPython 3.7, the latest version of the language aimed at¬†making complex tasks simple, is now in production release.¬†The most significant additions and improvements to Python 3.7 include:\nData classes that reduce boilerplate when working with data in classes. A potentially backward-incompatible change involving the handling of exceptions in generators. A ‚Äúdevelopment mode‚Äù for the interpreter. Nanosecond-resolution time objects. UTF-8 mode that uses UTF-8 encoding by default in the environment. A new built-in for triggering the debugger. Prerequisites how to install Python 3.7 on Windows Server Windows Server with Administrator rights Access to Powershell Access to internet Step 1. Login to your Windows Server via RDP\nStep 2. Open Windows Powershell as Administrator\nStep 3. Run the following command to download the python setup\nPS C:\\Users\\Administrator\u003e Invoke-WebRequest -Uri \"https://www.python.org/ftp/python/3.7.4/python-3.7.4-amd64.exe\" -OutFile \"python-3.7.4-amd64.exe\" Step 4. Run the following command to install python and set up path as well\nPS C:\\Users\\Administrator\u003e .\\python-3.7.4-amd64.exe /quiet InstallAllUsers=1 PrependPath=1 Include_test=0 Step 5. Run the following command to reload environment variables\nPS C:\\Users\\Administrator\u003e $env:Path = [System.Environment]::GetEnvironmentVariable(\"Path\",\"Machine\") + \";\" + [System.Environment]::GetEnvironmentVariable(\"Path\",\"User\") Step 6. Run python -V to check the version of python installed.\nPython official downloads:- https://www.python.org/downloads/\nThank You."},"title":"How to install Python 3.7 on Windows Server 2012 R2, 2016, 2019, 2022 via PowerShell"},"/utho-docs/docs/windows/how-to-install-ssl-on-windows-server/":{"data":{"":"\nStep 1: Login into your server using RDP using Administrator login details.\nStep 2: Open Information Services (IIS) Manager\nStep 3: Select the required server and open the ‚ÄúServer Certificates‚Äù option.\nStep 4: On the right hand side options, click on the ‚ÄúCreate Certificate Request‚Äù button.\nStep 5: Enter all the required information for the CSR and then click on Next.\nStep 6: Choose a bit length of 2048 and go to Step 7.\nStep 7: Choose the location where you want to save your CSR and click ‚ÄúFinish.‚Äù\nA CSR TXT file has been generated.¬†Step 8: Download the certificate from the SSL panel.\nStep 9: After receiving the certificate, submit the SSL certificate.\ncompletion of the certificate.\n9.1: Open the IIS and Server Manager and click on the Complete Certificate Request.\n9.2: Add the certificate and fill in all the details, then click on ‚ÄúOK‚Äù to complete the request.\n9.3: The Certificate has been completely submitted.\nStep 12: Bind the certificate and add domains.\n10.1: Open bindings options in default web sites as shown in the below screenshot.\n10**.2:** After opening the bindings , click on ‚Äúadd‚Äù to add the domains for the SSL.\n10.3: Choose Type, IP, port and SSL certificate and then click on OK.\nSSL has been successfully installed on the domain, please verify through the URL.\nThank you."},"title":"How to Install SSL on Windows Server"},"/utho-docs/docs/windows/how-to-install-telnet-client-on-a-server-using-windows-powershell/":{"data":{"":"","introduction#INTRODUCTION":"The Telnet client¬†enables a TCP/IP user to sign on and use applications on a remote system by using a Telnet server application. Telnet allows you to log on to the remote computer and use it as if you were connected directly to it. You can run programs, change configurations, or do just almost anything else you can do. In this tutorial, we will learn how to and use install Telnet Client on Windows Server 2012R2, 2016, 2019, 2022.\nStep 1. Open Windows PowerShell as Administrator Step 2. Run the installation command install Telnet Client on windows\nInstallation Complete.\nThank You."},"title":"How to install Telnet Client on a server using Windows PowerShell"},"/utho-docs/docs/windows/how-to-install-windows-rdp-cal-license-in-windows-servers/":{"data":{"":"\nAn RDS CAL license is also known as a Remote Deskto Service Client Access License. It allows multiple users to access the server concurrently. A user can perform their own task in their RDP session.\nWe will have a brief explanation of the license installation process.\nStep 1: We need to install the role for Remote Desktop Services. We can install the role from the server manager option. Please have a look at the below screenshot.\nSTEP 2: As per the above screenshot, we have to select the Remote Desktop Services option and then click on ‚ÄúNEXT.‚Äù It will take you to the next option, where you have to select the additional features present in the Remote Desktop Services.\nSTEP 3: As per the above screenshot, we have added two features to this. Now we will click on ‚ÄúNEXT‚Äù. Afterward, a new window will appear for the installation of this role. Please have a look at the below screenshot for reference.\nStep 4: Once you click on ‚Äúinstall,‚Äù the installation process will start. It will take 5 to 10 minutes to install the role. Once the installation is completed, we have to close the window. Afterward, ‚Äúreboot‚Äù the server to reflect the applied changes.\nStep 5: Once the server has rebooted successfully, we can see the remote desktop services option in the toolbar of the server manager. Please have a look at the screenshot.\nStep 6: Now we have to click on Remote Desktop Services and then select the ‚ÄúRemote Desktop Licensing Manager‚Äù. While clicking it, you can see the below prompt as per the screenshot.\nStep 7: We can see that the licensing server is not activated yet. Hence, we have to activate the server first.\nStep 8: Further, we have filled in the required details to activate the server.\nStep 9: As per the above screenshot, we have activated the licensing server successfully. Now we will initiate the license installation process by clicking on ‚ÄòNext‚Äô.\nStep 10: While clicking on next, as per the above screenshot, the next thing is to select the license program. Please have a look at the below screenshot for reference.\nStep 11: Now we have to enter the agreement number.\nStep 12: Now we have to select the operating system and then the license type, as well as the license quantity. Afterward, click on ‚ÄúNext‚Äù and then click on ‚ÄúFINISH‚Äù.\nWe have successfully installed the CAL license now.\nThank you¬†üòä"},"title":"How to Install Windows RDP CAL license in windows servers"},"/utho-docs/docs/windows/how-to-install-wordpress-on-iis-in-windows-server-2019/":{"data":{"":"","introduction#Introduction":"","prerequisites#\u003cstrong\u003ePrerequisites\u003c/strong\u003e":"","step-1-login-to-your-win-server-via-rdp#\u003cstrong\u003eStep 1. Login to your win server via RDP.\u003c/strong\u003e":"","step-10-after-the-installation-go-to-start-menu-and-openheidisql#\u003cstrong\u003eStep 10. After the installation, go to Start Menu and open¬†HeidiSQL\u003c/strong\u003e":"","step-11-add-a-new-session-and-change-its-name-from-unamed-to-localhost#\u003cstrong\u003eStep 11. Add a new session and change its name from \u0026ldquo;Unamed\u0026rdquo; to \u0026ldquo;localhost\u0026rdquo;\u003c/strong\u003e":"","step-12-set-a-password-for-the-root-user-click-save-open#\u003cstrong\u003eStep 12. Set a password for the root user.\u003c/strong\u003e Click save, open":"","step-14-go-to-the-user-manager#\u003cstrong\u003eStep 14. Go to the User Manager\u003c/strong\u003e":"","step-15-click-on-add-new-user#\u003cstrong\u003eStep 15. Click on \u0026ldquo;Add new user.\u0026rdquo;\u003c/strong\u003e":"","step-16-set-the-username-as-admindb-set-from-the-host-as-access-from-everywhere#\u003cstrong\u003eStep 16. Set the username as admindb. Set from the host as Access from everywhere\u003c/strong\u003e":"","step-17-set-a-password-for-the-new-user-and-retype-the-password#\u003cstrong\u003eStep 17. Set a password for the new user and retype the password\u003c/strong\u003e":"","step-18-download-wordpress#\u003cstrong\u003eStep 18. Download Wordpress\u003c/strong\u003e":"","step-19-copy-the-contents-of-the-downloaded-wordpress-file#\u003cstrong\u003eStep 19. Copy the contents of the downloaded WordPress file.\u003c/strong\u003e":"","step-2-install-iis-internet-information-services-if-you-dont-have-it-on-your-server#\u003cstrong\u003eStep 2. Install IIS (Internet Information Services) if you don‚Äôt have it on your server.\u003c/strong\u003e":"","step-20-go-to-the-following-location-cinetpubwwwroot#\u003cstrong\u003eStep 20. Go to the following location: C:\\inetpub\\wwwroot\u003c/strong\u003e":"","step-21-open-iisgo-to-sites-and-click-on-add-website#\u003cstrong\u003eStep 21\u003c/strong\u003e. \u003cstrong\u003eOpen IIS\u003c/strong\u003eGo to sites and click on \u003cstrong\u003eAdd Website.\u003c/strong\u003e":"","step-22-click-on-connect-as--specific-user--set#\u003cstrong\u003eStep 22. Click on \u0026ldquo;Connect as\u0026hellip;\u0026rdquo; \u0026gt; Specific user \u0026gt; set\u003c/strong\u003e":"","step-22-set-the-site-name-as-app1#\u003cstrong\u003eStep 22. Set the site name as: APP1\u003c/strong\u003e":"","step-23-openiisand-open-app1#**Step 23. Open¬†IIS¬†and open \u0026lsquo;APP1\u0026rsquo;":"","step-24-click-on-view-recommendations#\u003cstrong\u003eStep 24. Click on \u0026ldquo;View Recommendations,\u0026rdquo;\u003c/strong\u003e":"","step-25-go-toapp1#**Step 25. Go to¬†APP1":"","step-26-select-the-installation-language-and-proceed#\u003cstrong\u003eStep 26. Select the installation language and proceed.\u003c/strong\u003e":"","step-27-set-the-database-name-as-wordp1#\u003cstrong\u003eStep 27. Set the Database Name as: wordp1.\u003c/strong\u003e":"","step-28-set-the-site-title-as-app1#\u003cstrong\u003eStep 28. Set the Site Title as: APP1\u003c/strong\u003e":"","step-29-login-with-your-username-and-password#\u003cstrong\u003eStep 29. Login with your username and password.\u003c/strong\u003e":"","step-3-open-iis#\u003cstrong\u003eStep 3. Open IIS\u003c/strong\u003e":"","step-4-on-the-right-side-click-onget-new-web-platform-components#\u003cstrong\u003eStep 4. On the right side, click on¬†Get New Web Platform Components\u003c/strong\u003e":"","step-5-install-web-platform-installer#\u003cstrong\u003eStep 5. Install Web Platform Installer\u003c/strong\u003e":"","step-6-openweb-platform-isntaller#\u003cstrong\u003estep 6. Open¬†Web Platform Isntaller\u003c/strong\u003e":"","step-7-search-for-php-andinstallphp-7413-x86#\u003cstrong\u003eStep 7. Search for PHP, and¬†install¬†PHP 7.4.13 (x86).\u003c/strong\u003e":"","step-8-now-open-your-browser-and-download-php-manager-for-iis#\u003cstrong\u003eStep 8. Now open your browser and download PHP Manager for IIS.\u003c/strong\u003e¬†":"","step-9-download-mariadb#\u003cstrong\u003eStep 9. Download MariaDB.\u003c/strong\u003e":"","step13-create-a-new-database-under-localhost#\u003cstrong\u003eStep13. Create a new database under localhost\u003c/strong\u003e":"","wordpress-successfully-installed#\u003cstrong\u003eWordpress successfully installed.\u003c/strong\u003e":"\nIntroduction WordPress is a free and open-source content management system written in hypertext preprocessor language and paired with a MySQL or MariaDB database with supported HTTPS. In this article we will learn to install wordpress on IIS in a Windows Server 2019.\nPrerequisites Windows Server¬Æ 2019\nIIS¬†PHP (latest version)\nMySQL (latest version)\nStep 1. Login to your win server via RDP. Step 2. Install IIS (Internet Information Services) if you don‚Äôt have it on your server. IIS is very important to install Wordpress on IIS in WIndows Server 2019.\nPlease see the link below for the IIS installation.\nInstall IIS through Powershell ‚Äî‚Äì Install IIS through GUI\nStep 3. Open IIS Click on your server.\nStep 4. On the right side, click on¬†Get New Web Platform Components Step 5. Install Web Platform Installer step 6. Open¬†Web Platform Isntaller Step 7. Search for PHP, and¬†install¬†PHP 7.4.13 (x86). Step 8. Now open your browser and download PHP Manager for IIS.¬†Use the link below to download\nLINK:¬†Download PHP Manager for IIS\nStep 9. Download MariaDB. Use one of the links below to download\nLINK 1:¬†Download MariaDB 10.6.5\nLINK 2:¬†MariaDB downloads page\nEnter the password for the root user.\nStep 10. After the installation, go to Start Menu and open¬†HeidiSQL Step 11. Add a new session and change its name from ‚ÄúUnamed‚Äù to ‚Äúlocalhost‚Äù Step 12. Set a password for the root user. Click save, open Step13. Create a new database under localhost Name it ‚Äúwordp1‚Äù.\nStep 14. Go to the User Manager Step 15. Click on ‚ÄúAdd new user.‚Äù Step 16. Set the username as admindb. Set from the host as Access from everywhere Step 17. Set a password for the new user and retype the password Allow Global Privileges\nClick save, then close.\nStep 18. Download Wordpress Use the link to download: Wordpress Downloads Page\nStep 19. Copy the contents of the downloaded WordPress file. Step 20. Go to the following location: C:\\inetpub\\wwwroot Create a folder there, (we will name it ‚Äútest‚Äù)\nAnd paste the copied Wordpress contents into this folder.\nStep 21. Open IISGo to sites and click on Add Website. Step 22. Set the site name as: APP1 Set the physical path as: C:\\inetpub\\wwwroot\\test\nStep 22. Click on ‚ÄúConnect as‚Ä¶‚Äù \u003e Specific user \u003e set Set user name as Administrator\nSet user password\nRetype the password; click OK.\nClick on Test Settings..\nProceed if the connection is valid.\nSet port to: 8041\n**Step 23. Open¬†IIS¬†and open ‚ÄòAPP1‚Äô **Open¬†PHP Manager\nStep 24. Click on ‚ÄúView Recommendations,‚Äù change the PHP configuration for your machine. Select both options available and click OK.\n**Step 25. Go to¬†APP1 **Under the¬†Actions¬†section, click on¬†Browse *.8041 (http)\nThe Wordpress installation page will open.\nStep 26. Select the installation language and proceed. Step 27. Set the Database Name as: wordp1. Set Username as: admindb\nType your admindb password\nSet Database Host as:¬†localhost\nSet the table prefix as: wp_\nClick on ‚Äúsubmit.‚Äù\nStep 28. Set the Site Title as: APP1 Set Username as: admin\nSet Password\nEnter your email Id\nCheck the search engine visibility option.\nClick on Install Wordpress.\n**\nInstallation was successful.**\nStep 29. Login with your username and password. Wordpress successfully installed. Thank You."},"title":"How to install Wordpress on IIS in WIndows Server 2019"},"/utho-docs/docs/windows/how-to-install-xampp-on-windows-server-2016-2019-2022/":{"data":{"":"","conclusion#Conclusion":"Hopefully, now you have learned how to install XAMPP on Windows Server 2016/2019/2022.\nThank You üôÇ\nAlso read: How to Block or Allow TCP/IP Port in Windows Firewall","download-and-install-the-xampp-application#Download and install the XAMPP application":"Before getting started, you will need to go to the Apache Friends official website and download the most recent version of XAMPP on your server. You can find the page here. To successfully install XAMPP on Windows Server, please follow the procedures below:\nStep 1. To reach the XAMPP download page, which is seen below, click here:\nStep 2. Choose the version that you want, and then download it to your server.\nStep 3. When the download is finished, you will see the image below.¬†Step 4. You can proceed by clicking the Next button. Your device‚Äôs screen should now be on the component selection screen.\nStep 5. Make your selections and proceed by clicking Next. The installation folder selection screen should appear:\nStep 6. Choose a destination for the installation, then proceed. The language menu should load, which will look like this:\nStep 7. To proceed, choose your language and press the Next button. There‚Äôs a XAMPP info screen that should load:\nStep 8. Click the Next button after reading all of the information. At this point, you should be presented with the Ready to Install screen:\nStep 9. The installation can be started by clicking the Next button. When XAMPP is finished installing, the following window will appear:\nStep 10. To complete the setup process, select Final. When you‚Äôre done setting up XAMPP, the admin panel will look like this:\nStep 11. As shown below, you can start various services by clicking the Start button under the Action column.","introduction#Introduction":"In this article, you will learn how to install XAMPP on Windows Server 2016/2019/2022.\nThe Apache Friends organisation created XAMPP, a cross-platform software stack used to develop and test programmes and web applications. It streamlines your web development process by eliminating the need to install each component separately.\nGenerally known as Cross-Platform, Apache, MySQL, PHP, and Perl, or XAMPP for short, it is the most widely used free software stack available. Apache is included for multiple server delivery and command line executables in this web solution package."},"title":"How to install XAMPP on Windows Server 2016/2019/2022"},"/utho-docs/docs/windows/how-to-make-partition-from-existing-drive-windows-server-support-internal/":{"data":{"":"\nHow to make a partition from an existing drive on Windows Server\nStep1: Firstly, we need to login into the server using Remote Desktop Connection (RDP Client) or type the following commands on the RUN prompt mstsc\nStep2: After executing the mstsc command on the RUN prompt, type the server IP address which you need to connect to.\nStep3: Now you need to login to the server with admin privileges to check the current disk size, and thenyou need to create a new disk size from the existing one.\nAs you see in the above screenshot, there is only 1 logical drive present on the server. Now we are going to create a new drive from the server‚Äôs existing drive.\nNote: To edit disks on a server, you need to access the server with admin rights.\nStep5: Need to go to disk management by typing the following command: diskmgmt.msc\nOr\nStep 5: You have to open Server Manager in the server start option, then click on the tools section, then select ‚ÄúComputer Management,\" and then you need to select ‚ÄúDisk Management‚Äù on the screen.\nStep6: After either of the above 2 methods, you will go on the below screen.\nStep7: Now we are ready to add a new partition on the server. Just follow these steps below.\nStep8: Now we need to click on Shrink Volume, as we are going to make a partition from the existing drive of the server.\nStep 9: Now we have to enter the amount of space we need to give to the new drive.\nStep 10: Now, by clicking on the shrink button, we are going to create a new partition from the drive.\nStep11: Now we need to allocate the new partition from the unallocated zone. Follow the below screenshot.\nStep12: After doing all the above steps, we are now able to access the newly allocated partition on the server, which is shrinked from the existing partition.\nThank you :)"},"title":"How to make partition from existing drive Windows Server"},"/utho-docs/docs/windows/how-to-migrate-the-zimbra-emails-on-plesk-panel-using-email-migrator/":{"data":{"":"\nIntroduction: Zimbra is an business email solution, which can be used on Linux based systems. It is open source MTA.\nPrerequisite :\nWindows server with Plesk installed in it\nAdmin credentials\nDomain should be added in the Plesk panel\nPlesk should be of latest version like 18.01 etc.\nStep 1: First of all, we have to login into the Plesk panel. The domain should be added into the plesk prior to the migration. Once we are logged in, we will see an option for mail import, as per the below screenshot.\nStep 2: We have to click on mail import. Make sure emails should be working on source server , then only we can migrate them on new server.\nStep 3: Once , will click on the import mail message a new prompt will be appeared , where we have to fill all the details as per the screenshot below. We will use option \" create a new mailbox \" as we need to create that email id on new server afterward copying process will be started. You can use existing mailbox option if you have created the email id earlier.\nStep 4: As per the above screenshot, we have to click on show advance setting to expand the all option of migration. Please have a look on the below screenshot. We have to fill all the details which are marked as red and then click on ok to proceed the migration.\nSource email:¬†It should be the complete email id like \"¬†admin@domain.com¬†\"\nSource pass:¬†Password for source email id like \"¬†admin@domain.com¬†‚Äú.Create a new mailbox: Use to create a new mailbox\nUsername: Should be same as source mailbox like \"¬†admin@domain.com¬†‚ÄúPass: You can use any password as per your need on the destination server.\nSource Imap Host: It should be your¬†smtp server hostname of your source server like \" mail.domain.com \"¬†Source Imap encryption: It should be plain.\nSource Imap Timeout: 50\nHere we have completed the docs.\nThank You :)"},"title":"HOW TO MIGRATE THE ZIMBRA EMAILS ON PLESK PANEL USING EMAIL MIGRATOR"},"/utho-docs/docs/windows/how-to-mount-nfs-persistently-in-windows-server/":{"data":{"":"NFS allows a system to share directories and files with others over a network. By using NFS, users and programs can access files on remote systems almost as if they were local files.\nIn other words, Network File System (NFS) is a file sharing solution that allows you to transfer files using the NFS protocol between computers running Windows Server and UNIX operating systems.\nSteps to mount NFS Persistently in Windows Server\nBy Mapping Network Drive\nFirst open up ‚ÄúThis PC‚Äù and select Computer from the menu at the top. From here click on Map network drive, as shown below. The Map Network Drive window will open, select the drive letter that you want to assign to the NFS share, followed by the IP address or hostname of the NFS server as well as the path to the exported NFS directory. Click the Finish button when complete. You may see a pop up window showing that the connection is being attempted. Once complete the shared NFS folder will open up. When you view ‚ÄúThis PC‚Äù you will see the mapped network drive under Network location.\nThankyou"},"title":"How to mount NFS persistently in Windows Server"},"/utho-docs/docs/windows/how-to-mount-virtio-iso-on-windows-server-2016/":{"data":{"":"\nStep 1. Go to a Virtio ISO downloading site.\nCopy the address link of the Virtio ISO\nYou can find the link to the Virtio ISO below:\nLINK:¬†https://github.com/virtio-win/virtio-win-pkg-scripts/blob/master/README.md\nStep 2. Login to the Microhost Cloud Dashboard\nStep 3. Go to the¬†ISO¬†section in the Microhost Cloud Dashboard.\nStep 4. ISO downloading step\n¬∑¬†Select data center same as your server‚Äôs data center location.\n¬∑¬†In the URL section, paste the link copied in step 1.\n¬∑¬†Add any name to the ISO\n¬∑¬†Click on¬†Add ISO\nStep 5. Go to the Cloud Servers\nStep 6. Go to the Manage Cloud¬†of your cloud server.\nStep 7. Go to¬†ISO¬†in your server options\nStep 8.¬†In the mount ISO section, select the Virtio ISO that you downloaded and click on ‚ÄúMount and Boot from ISO‚Äù\nStep 9. Virtio ISO mounted successfully.\nThank you."},"title":"How to mount Virtio ISO"},"/utho-docs/docs/windows/how-to-open-a-port-in-windows-server-firewall/":{"data":{"":"","#":"Open Windows Firewall Hit the Windows key and find ‚Äúfirewall with Advanced Security.‚Äù Select the first option as described below. Once the window of the firewall opens, go to the next step.\nConfigure Inbound rule. In the top-left section , click on the ‚ÄúInbound Rule‚Äù button and in the top right-hand section, select the ‚ÄúNew Rule.‚Äù For a better photo view, see below. A window will open ‚ÄúNew Inbound Rule Wizard.‚Äù Continue to the following step.\nGo to Wizard On the new window, follow the steps shown in the screenshots below\nChoose¬†port¬†and hit¬†next.\nClick on TCP and add the port, whatever port you want to open. Then click next.\nClick on Allow the Connection.\nNow, click on next and select the domain, Private \u0026 Public as per the screenshot:\nIn the last section, Give the name and description.\nWe have successfully completed the Port opening section.\nThank You :)"},"title":"How To open a port in Windows Server Firewall"},"/utho-docs/docs/windows/how-to-reset-a-lost-administrator-password-in-windows-server/":{"data":{"":"","#":"INTRODUCTION This blog will guide you to reset lost Administrator password in Windows Servers 2012R2, 2016, 2019 and 2022.\nAn administrator account is a user account that grants you the ability to make changes that effect other users. Administrators have access to all data on the computer and can alter security settings, install software and hardware. Administrators can also make modifications to other user accounts, such as creating new local user accounts other than the administrator and giving the local user administrative capabilities.\nStep 1. Login to microhost.com reset lost Administrator password Step 2. Got to Manage\nStep 3. Go to ISO section, and mount any other Windows Server ISO than your current OS version. Let‚Äôs say we have Windows Server 2019 OS installed on our VM, so we will be mounting the OS of WIndows Server 2016. You can also do the vice versa.\nThen click on Mount and Boot from ISO.\nStep 4. Go to the console of the VM. reset lost Administrator password Step 5. Power on and off the VM from the Microhost Manage Platform\nStep 6. After powering on the VM from the panel, immediately go to the console and press escape to boot the VM with the ISO we just mounted. On the console we will find OS installation setup of the ISO we just mounted.\nStep 7. Click Next on Windows Setup.\nStep 8. Then click on Repair your computer.\nStep 9. Click on Troubleshoot\nStep 10. Click on Command Prompt\nStep 11. CMD will open. Now type Diskpart, which will tell us which volume does our OS exists on.\nStep 12. exit diskpart and Go to Windows\\System32 in the volume where your old OS exists. Volume F in our case.\nStep 13. Now we will rename the utiman.exe file.\nStep 14. Now we will copy cmd to utilman. (here we have replaced utilman with the command prompt in order to be able to reset our password)\nStep 15. Type exit in cmd and continue to boot the VM with our old Windows OS.\nStep 16. Let the VM boot in our old Windows Server 2019 OS that we already had installed on our VM.\nStep 17. On the console menu, we will go to ease of access. Where cmd will open.\nStep 18. Now we will run the following command to reset the Administartor‚Äôs password.\nnet user administrator Passw0rd\nwhere Passw0rd is the new password that we need to set for the administrator.\nThe command says ‚ÄúThe command completed successfully‚Äù means the password was reset successfully.\nStep 19. Click exit. And login with the newly reset password of the administrator.\nStep 20. Login with the new password. reset lost Administrator password And we have successfully reset the lost password of the Administrator in your Windows Server.\nThank You."},"title":"How to reset a lost Administrator password in Windows Server"},"/utho-docs/docs/windows/how-to-setup-disk-driver-while-deploying-windows-server-with-custom-iso/":{"data":{"":"\nStep 1. Download ISO\nStep 2. Deploy a server using the ISO\nStep 3. Open console to install OS\nStep 4. Begin OS isntallation\nClick ‚ÄúI accept the license terms‚Äù\nStep 5. Select ‚ÄúCustom‚Äù\nstep 6. Select ‚ÄúLoad Driver‚Äù\nStep 7. Click on ‚ÄúBrowse‚Äù\nStep 8. Search for ‚ÄúCd Drive (E:) virtio-win-version‚Äù\nStep 9. Search for the OS version same as your ISO OS version, select ‚Äúand64‚Äù for 64bits\nStep 10. Red Hat Virtio SCSI controller will appear, selct that and click ‚Äúnext‚Äù\nStep 11. Then select ‚ÄúWindows Server Standard Evolution (Server with a Graphical Interface)‚Äù\nStep 12. Allocate Drive (Click on ‚ÄòNew‚Äô)\nStep 13. Select ‚ÄúPrimary Drive‚Äù and click on ‚ÄòNext‚Äô\nStep 14. OS installation begins.\nThank You!"},"title":"How to setup Disk Driver while deploying Windows Server with custom ISO"},"/utho-docs/docs/windows/how-to-setup-network-driver-while-deploying-windows-server-with-custom-iso/":{"data":{"":"\nStep 1. After installing OS using custom ISO, login to Microhost.com and go to ISO section.\nStep 2. Upload Virtio Network Adapter ISO\nLINK: Virtio Win ISO\nStep 3. Check if the ISO is downloaded properly.\nStep 4. Go to Mnage Cloud and Click on ISO sub-section\nStep 5. Select Virtio ISO and click on ‚ÄúMount and Boot from ISO‚Äù\nStep 6. After the ISO is mounted and server is booted from the ISO, go to Console and open This PC \u003e Open Virtio Driver\nStep 7. Double click on ‚Äúvirtio-win-guest-tools‚Äù\nStep 8. Proceed with the installation process as follows:\nNetwork Driver installed once you see the following icon.\nNow follow the below mentioned document to configure IP manually on windows Server.\nURL: How to configure IP manually on Windows Server.\nThank You."},"title":"How to setup Network Driver while deploying Windows Server with custom ISO"},"/utho-docs/docs/windows/how-to-setup-ntp-server-for-time-synchronization-using-powershell/":{"data":{"0x00--not-a-time-server#0x00 : Not a time server":" ","0x01--always-time-server#0x01 : Always time server":" ","0x02--automatic-time-server#0x02 : Automatic time server":" ","0x04--always-reliable-time-server#0x04 : Always reliable time server":" ","0x08--automatic-reliable-time-server#0x08 : Automatic reliable time server":" Set-ItemProperty -Path ‚ÄúHKLM:SYSTEMCurrentControlSetservicesW32TimeConfig‚Äù -Name ‚ÄúAnnounceFlags‚Äù -Value 5\n![setup NTP Server for time synchronization](images/Screenshot_4-26-1024x347.png) Step 6. Allow NTP port if Windows Firewall is running New-NetFirewallRule `\n![](images/Screenshot_5-21.png) -Name ‚ÄúNTP Server Port‚Äù -DisplayName \"NTP Server Port\" -Description ‚ÄòAllow NTP Server Port‚Äô -Profile Any -Direction Inbound -Action Allow -Protocol UDP -Program Any -LocalAddress Any` -LocalPort 123\nThank You! ","meaning-of-numbers#Meaning of numbers":"\nStep 1. Login to your Windows Server\nStep 2. Open Powershell as Administrator setup NTP Server for time synchronization\nIf the computer you want to configure is an Active Directory domain controller, the NTP server function is automatically enabled.¬†Therefore, the following is an example of setting the NTP server function to Windows Server in a workgroup environment.\nStep 3. Run the following command to check current settings\nGet-ItemProperty -Path \"HKLM:SYSTEMCurrentControlSetServicesw32timeTimeProvidersNtpServer\" Step 4. Run the following command to Enable NTP server function\nSet-ItemProperty -Path \"HKLM:SYSTEMCurrentControlSetServicesw32timeTimeProvidersNtpServer\" -Name \"Enabled\" -Value 1 ``` setup NTP Server for time synchronization ![setup NTP Server for time synchronization](images/Screenshot_3-23-1024x366.png) Step 5 . Change AnnounceFlags to 5 Meaning of numbers "},"title":"How to setup NTP Server for time synchronization using Powershell"},"/utho-docs/docs/windows/how-to-setup-openvpn-connect-in-windows-server/":{"data":{"":"","introduction#INTRODUCTION":"In this tutorial, we will learn how to instal and setup OpenVPN Connect in Windows Server 2012R2, 2016, 2019.\nStep 1. Login to your Windows Server Step 2. Download OpenVPN Connect Setup OpenVPN Connect in Windows Download OpenVPN for Windows\nStep 3. Open the installer package Setup OpenVPN Connect in Windows Step 4. Follow the installation window OpenVPN Connect installed. Step 5. Import your OpenVPN profile and you are good to go. Setup OpenVPN Connect in Windows Thank You!"},"title":"How to Setup OpenVPN Connect in Windows Server"},"/utho-docs/docs/windows/how-to-setup-ssh-server-on-windows-server-via-powershell/":{"data":{"":"\nStep 1. Login to your Windows Server setup SSH Server via PowerShell\nStep 2. Open PowerShell as Administrator\nStep 3. Check the names of installable features using the following command\nGet-WindowsCapability -Online | ? Name -like 'OpenSSH*' Name : OpenSSH.Client~~~~0.0.1.0\nState: Installed\nName : OpenSSH.Server~~~~0.0.1.0\nState : NotPresent\nStep 4. Install OpenSSH Server\nAdd-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0 Path:\nOnline : True\nRestartNeeded : False\nStep 5. Start service\nStart-Service -Name \"sshd\" step 6. Set Startup to [Automatic]\nSet-Service -Name \"sshd\" -StartupType Automatic step 7. Check\nGet-Service -Name \"sshd\" | Select-Object * Name : sshd\nRequiredServices : {}\nCanPauseAndContinue : False\nCan Shutdown : False\nCanStop : True\nDisplayName : OpenSSH SSH Server\nDependentServices : {}\nMachineName : .\nServiceName : sshd\nServicesDependedOn : {}\nServiceHandle : SafeServiceHandle\nStatus : Running\nServiceType : Win32OwnProcess\nStartType : Automatic\nSite: Container: setup SSH Server via PowerShell\nStep 8. Allow 22/TCP if Windows Firewall is running\nNew-NetFirewallRule -Name \"SSH\" ` -DisplayName \"SSH\" ` -Description \"Allow SSH\" ` -Profile Any ` -Direction Inbound` -Action Allow` -Protocol TCP` -Program Any` -LocalAddress Any ` -RemoteAddress Any` -LocalPort 22` -Remote Port Any Name : SSH\nDisplayName : SSH\nDescription : Allow SSH\nDisplayGroup:\nGroup :\nEnabled : True\nProfile: Any\nPlatform : {}\nDirection: Inbound\nAction: Allow\nEdgeTraversalPolicy : Block\nLooseSourceMapping : False\nLocalOnlyMapping : False\nOwner:\nPrimary Status : OK\nStatus : The rule was parsed successfully from the store. (65536)\nEnforcementStatus : NotApplicable\nPolicyStoreSource : PersistentStore\nPolicyStoreSourceType : Local\nThank You!"},"title":"How to setup SSH Server on Windows Server via PowerShell"},"/utho-docs/docs/windows/how-to-share-a-folder-over-network-in-windows-servers/":{"data":{"":"","#":"INTRODUCTION In this tutorial, we will learn how to share a folder over network in Windows Servers. There are many ways to setup shared folder in¬†Windows Server. You can use¬†Server Manager to share a folder. We can also use folder properties share option to share a folder. In this tutorial, we will learn the steps to share a folder in¬†Windows Server 2016¬†using folder properties share option.\nPrerequisites share a folder over network Windows Server Internet connectivity NOTE: We recommend you use a user other than Administrator for file sharing. We will be using a test user.\nStep 1. Connect to your Windows server via RDP.\nStep 2. Create/select a folder that you wish to share.\nStep 3. Right-click on the folder and go to properties\nStep 4. Go to sharing tab and click on Share‚Ä¶\nStep 5. Now we will give folder access to the user we wish to access the folder with. In our, it‚Äôs the test user that we are using to share the folder.\nStep 6. Now we will give access to the user so that it can read or write the contents of the folder.\nStep 7. The folder has been shared. share a folder over network\nStep 8. Now we will access the folder remotely using the servers IP address. We will input the folder path as \\\\server_ip\\folder_name\nStep 9. The connection will prompt to provider username and password to access the folder. In our case, we will be using the credentials of the test user.\nStep 10. We have successfully accessed the shared folder.\nThank You."},"title":"How to share a folder over network in Windows Servers"},"/utho-docs/docs/windows/how-to-solve-internal-server-error-while-connecting-to-rdp/":{"data":{"":"\nWhen you face an internal server error while connecting to RDP, please follow these steps:\nLogin to the Windows server through the console.\nNow we need to open the Remote Desktop Group Policy.¬†Press Win + R on the keyboard to open the run window. In the Open field, type ‚Äúgpedit.msc‚Äù and press Enter on the keyboard or click OK.\nStep 3.¬†After Local Group Policy Editor opens, expand Computer Configuration ¬ª Administrative Templates ¬ª Windows Components ¬ª Remote Desktop Services ¬ª Remote Desktop Session Host ¬ª Security.\nstep 4. On the right-side panel. Double-click on ‚ÄúRequire use of specific security layer for remote (RDP) connection and enable it. See below in the screenshot-\nStep 5. Open cmd and type - gpupdate\nThank you!!"},"title":"How to solve internal server error while connecting to RDP"},"/utho-docs/docs/windows/how-to-turn-off-internet-explorer-enhanced-security-configuration-on-windows-server/":{"data":{"":"","introduction#Introduction":"Internet Explorer Enhanced Security Configuration (ESC)¬†establishes security settings that define how users browse the internet and intranet websites. These settings also reduce the exposure of servers to websites that might present a security risk. This process is also known as IEHarden.\nStep 1. Connect to your Windows server¬†with Remote Desktop Protocol (RDP).\nStep 2. Open¬†Server Manager¬†from the Windows start menu.\nStep 3. Open Local Server\nStep 4. Click on ‚ÄúIE Enhanced Security Configuration‚Äù¬†property in the right-hand panel, and click¬†on ‚ÄúON‚Äù\nstep 5. Click on Turn the option¬†‚Äúoff‚Äù¬†for Administrators and Users, then click ‚ÄúOK‚Äù\nStep 6. Select refresh on the Server Manager toolbar for changes to take effect.\nStep 7. Server Manager will now show¬†‚Äúoff‚Äù¬†for the IE Enhanced Security Configuration property, and you will be able to browse the web without any warnings in Internet Explorer.\nThank You."},"title":"How to Turn off Internet Explorer Enhanced Security Configuration on Windows Server"},"/utho-docs/docs/windows/how-to-upgrade-tls-1-1-to-1-2-in-window-server/":{"data":{"":"","conclusion#Conclusion":"Hopefully, you have learned how to upgrade TLS 1.1 to TLS 1.2 in window server.\nAlso read: How to Block or Allow TCP/IP Ports in Windows Firewall\nThank You üôÇ","introduction#Introduction":"In this article, you will learn how to upgrade TLS 1.1 to TLS 1.2 in window server.\nWhat is TLS?\nTLS is a cryptographic protocol that provides end-to-end security of data sent between applications over the Internet. It is mostly familiar to users through its use in secure web browsing, and in particular the padlock icon that appears in web browsers when a secure session is established. However, it can and indeed should also be used for other applications such as e-mail, file transfers, video/audioconferencing, instant messaging and voice-over-IP, as well as Internet services such as DNS and NTP.\nNeed to connect windows server with Administrator privilege, then type below command in search box ‚Äúregedit‚Äù\nSearch regedit ¬ª computer ¬ª HKEY_LOCAL MACHINE ¬ª SYSTEM ¬ª current control set ¬ª control ¬ª security providers ¬ª SCHANNEL ¬ª protocols [right click on protocols] then NEW ¬ª key then rename the new folder with TLS 1.2\nRight click on TLS 1.2 folder then NEW ¬ª key\nRename the new folder with client\nRight click on client name folder ¬ª new ¬ª DWORD [32-bit] value after that in the right side of the page a new folder will be added and renamed it with DisabledByDefault\nThen right click on DisabledByDefault folder go to modify ¬ª value data=0 ¬ª hexadecimal ¬ª ok\nRight click on client name folder ¬ª new ¬ª DWORD [32-bit] value after that in the right side of the page a new folder will be added and renamed it with Enabled\nThen right click on Enabled folder go to modify ¬ª value data=1 ¬ª hexadecimal ¬ª ok\nHow do I know if TLS version is enabled on Windows Server?\nClick on: Start -\u003e Control Panel -\u003e Internet Options Click on the Advanced tab Scroll to the bottom and check the TLS version described in steps 3 Select [use TLS 1.2] ¬ª ok "},"title":"How to upgrade TLS 1.1 to TLS 1.2 in window server"},"/utho-docs/docs/windows/how-to-upgrade-windows-server-2012r2-to-windows-server-2016/":{"data":{"":"","#":"\nIntroduction Unlike upgrading previous versions of¬†Windows¬†Server to 2012, Windows Server 2016 only supports 64-bit architecture. In addition, Windows Server 2012 and 2016 have only 4 versions. Upgrading Windows Server is a complicated process. So, Microsoft recommends that instead of updating the entire Windows server, it upgrades the roles and settings of the Windows 2016 server. Hence, we are going to learn you¬†How to upgrade Windows Server 2012R2 to Windows Server 2016.\nPrerequisites for the upgrade Windows 2012R2 to 2016 1. Upgrading from 32-bit to 64-bit is not possible.\n2. All Windows Server versions before to 2012 are incompatible with the 2016 upgrade.\n3. A version to version upgrade is not supported. You can‚Äôt upgrade the standard version of Windows Server 2012 to the Data Center version of Windows Server 2016.\n4. It is not possible to upgrade from one language to another. For example, you can‚Äôt upgrade the German version of Windows Server to Chinese.\n5. Upgrades between Server Core and Desktop versions (or vice versa) are not supported.\n6. Upgrading from older versions of Windows Server to trial versions of Windows Server is not supported. Trial versions must be installed manually.\n7. If this server uses NIC Teaming, disable it and reactivate it.\n8. Make a full backup of the server, as you may have a problem during the upgrade and want to go back to the previous version.\n-‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\nCheck out disk allocation steps in case you need them after the server upgrade.\n-‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\nStep 1. Log in to Windows Server 2012 and install DVD or Flash. You can also mount an ISO of Windows Server 2016 that we will be using to upgrade the server from 2012R2 to 2016. Open¬†File Explorer¬†and click on¬†DVD Drive¬†as shown below:\nStep 2. Select¬†Download \u0026 install updates¬†to begin the Windows installation process.\nStep 3. Choose a version of Windows Server 2016 that suits your needs. You may need to enlarge the network in the future and require more roles and licenses. So be careful at this point.\nStep 4. In the¬†license¬†stage, click¬†Accept. You will enter the next step.\nStep 5. If you have selected the same version as the previous version, you can keep all the applications and files in the system. The apps and files will be erased if the prior version is not selected. Select¬†Keep personal files and apps¬†to save files and apps, otherwise, click¬†Nothing¬†to delete all files. Then click¬†Next.\nStep 6. Microsoft will check for updates.\nStep 7. You receive a warning prior to beginning the installation, which you must ignore (confirm) before it can proceed.\nStep 8. The install check disk space.\nStep 9. Now the critical moment, review the settings that we have been specified and then select Install to start the process:\nStep 10. Windows Server 2016 Installation begins\nStep 11. The Server will restart several times after that. The whole process will take some time to complete the process.\nStep 12. The new Windows Server 2016 login screen will appear following the upgrade.\nWindows Server 2012R2 successfully upgraded to Windows Server 2016\nThank You!"},"title":"How to upgrade Windows Server 2012R2 to Windows Server 2016"},"/utho-docs/docs/windows/install-plesk-on-windows-server-2012/":{"data":{"":"\nTo Install Plesk in windows using browser,You need to follow below steps.\n1. Login on your server using RDP.\n2. Download plesk installer Plesk Installer.\n3. Double click on downlaoded plesk installer file.\n4. It will open command prompt and your server default browser itself.\n4. Enter your server administrator user password for next.\n5. Click on ‚ÄúInstall or Upgrade your product‚Äù\n6. Choose the latest stable version of your product and press¬†\"Continue\"\n7. The installation type determines which Plesk components and features are to be installed. The following types of installation are available:\nThe Recommended installation type includes all components necessary for web hosting (including web server, mail server, database server, etc.) plus the most popular and widely used features. If you don‚Äôt know what type of installation to choose, going with Recommended is a safe bet. The Full Installation Type includes all Plek components and features. Note that choosing this type of installation will require the most disk space. The Custom installation type allows you to choose and install items from the list of all available components and features. This type of installation is recommended for experienced Plesk administrators. You can choose an installation type that is not suited to your needs-you will be able to add or remove Plesk components at any time after the installation has finished.\nThe custom installation type looks like-mark the components you want to install with the green checkbox icon, select the red cross icon for those you don‚Äôt need. When you‚Äôre satisfied with the components you‚Äôve selected, click Continue to move forward.\n8. You will see following setting in screenshot which you can change according to your requirement.\n9. If you want to install plesk with default setting. Enter the password of administrator user and click on continue for proceed installation.\n10. You will see the output of the console inside the web interface. Wait till the installation process is over.\n11. When you will get the message‚ÄùAll operations with products and components have been successfully completed.‚Äù Click Ok‚Äù and it will exit the installation page.\n12. Now reboot your server and after reboot configure your plesk panel using plesk url¬†http://your-server-ip:8880¬†or¬†https://your-server-ip:8443¬†by entering administrator user and password.\n13. After login in plesk enter the email address for admin notifications and password for admin user.\n14. Enter the Licnese key if you have plesk panel license key or go with ‚ÄúProceed with a full-featured trial license‚Äù and Check the button ‚ÄúI confirm that I've read and accepted the End-User License Agreement¬†‚Äú\n15. Click On¬†enter plesk."},"title":"Install Plesk on Windows Server 2012"},"/utho-docs/docs/windows/install-sql-server-2012-express-edition-in-windows-server-2012/":{"data":{"":"\nAsp.net 3.5 version should be installed on server before install SQL server and you have access administrator RDP user login details.\nStep:- 1. Install Asp.net 3.5 version using server manager.\n(i) Open server manager click on server manager button.\n(ii) Click on Add roles and features.\n(iii) Select .NET framework 3.5 Features and install.\n(iv) Click on next.\n(v) When you will get below message in screenshot close and exit the server manager installation wizard.\nStep:- 2. For download SQL express edition 2012 setup click here and save it on your server.\nStep: 3. When setup will download on your server. Double click on the setup file.\nStep:- 4. Click on ‚ÄúNew Sql stand-alone installation or add features in existing installation‚Äù.\nStep:- 5. Select terms and policy and click on next.\nStep:- 6. Select features which you want to install in SQL database and defined installation directory and click on next.\nStep:- 7 Enter instance name and id of SQL database server.\nStep:- 8 Select mixed mode enter sa user password of sql database server.\nStep:- 9. Select option ‚Äúinstall and configure‚Äù and click on next and next again. Now the installation is started of sql and it will take 20-40 minutes for complete.\nStep:- 10. When you will get the message below means sql installation is completed.\nNow SQL database server is installed and operational.\nThankyou.."},"title":"Install SQL Server 2012 Express Edition in Windows Server 2012"},"/utho-docs/docs/windows/installation-and-configuration-of-iis-web-server-on-windows-server/":{"data":{"":"","#":"Internet Information Services¬†(IIS, formerly¬†Internet Information Server) is an extensible¬†web server¬†software created by¬†Microsoft¬†for use with the¬†Windows NT¬†family.¬†IIS supports¬†HTTP,¬†HTTP/2,¬†HTTPS,¬†FTP,¬†FTPS,¬†SMTP¬†and¬†NNTP. It has been an integral part of the Windows NT family since¬†Windows NT 4.0, though it may be absent from some editions (e.g. Windows XP Home edition), and is not active by default.\nInstall IIS Through GUI Installation of IIS can also take these steps if your server has the graphical user interface component installed.\nOpen Server Manager, this can be found in the start menu. If it‚Äôs not there simply type ‚ÄúServer Manager‚Äù with the start menu open and it should be found in the search.\nStep:1 You can find this in the start menu, Open Server Manager. If not, just enter ‚ÄúServer Manager‚Äù with the open start menu to find it in the search.\nThe output will be shown as follows. Now click on ADD ROLE AND FEATURES\nThe Next output Will be shown as follow. On the ‚ÄúBefore you begin‚Äù window, simply click the Next button .\nSelect ‚ÄúRole-based or feature-based installation‚Äù and click Next.\nWhen we install on our local machine, leave the current machine, and click ‚ÄúSelect a server from the server pool.‚Äù You can also choose another server that you are managing from here or a VHD server.\nCheck the box next to ‚ÄúWeb Server (IIS)‚Äù from the ‚Äúselect server roles‚Äù window. If this is done, you can open a new window to indicate additional functions, just click on the ‚ÄòAdd Features‚Äô button. Once completed, click Next back on Select server roles menu.\nThe next prompt will be shown as follow.\nAt this stage, we won‚Äôt install additional features, so just click Next on the ‚ÄòChose Functions‚Äô window.\nAfter reading the information provided click on the ‚ÄúWeb Server Role (IIS)‚Äù window\nYou can install additional IIS services at this point on the ‚ÄúSelect roles services‚Äù windows if necessary. You do not need to worry about that now because you can always return and add later, just click Next to install the defaults.\nIn the ‚ÄúConfirm installation selections‚Äù window, you will finally check the items to be installed and then you will install the IIS web server when you are ready.\nNo reboot should be required with a standard IIS installation, however, if you remove the role a reboot will be needed.\nClick the close button once the installation is successful. The firewall rules ‚ÄúWorld Wide Web Services‚Äù (HTTP Traffic-In) activated by firewall in the Windows firewall should be enabled by default at this point. at port 80.\nWe have completed the Installation of IIS. Now we will proceed to the configuration portion.\nOpen Server Manager and choose IIS, right-click on your server and select IIS Manager.\nRight-click on the Sites node in the tree in the Connections pane, then click Add site.\nEnter a friendly website name in the Site name box in the Add Website dialog box.\nClick Select if you want to choose another application pool, compared to the application pool box. Choose an application pool from the application pool list and then click OK in the Application pool dialog box.\nEnter the website‚Äôs physical path box or click the browse button (‚Ä¶) to navigate the folder in the file system. (Note: The best way is to build a folder in your C: for your websites.\nClick Connect to specify credentials with permission to access the path if the physical path that is entered in the previous phase has to a remote share. Select the User‚Äôs (pass-through Authentication) option in the Connect As dialog box if you don‚Äôt use specific credentials.\nSelect from the Type list the protocol for the website.\nIn the IP address box, enter the IP address if you are asked for a static IP address for the site (the default is All Unassigned).\nIn the port text box enter a port number.\nEnter a host website header name in the host header box, optionally.\nIn case you do not need to modify the site and want the website to be available immediately, select the Start Website check box immediately.\nThen Click OK.\nNow we have completed the Site addition portion.\nThank You :)"},"title":"Installation and Configuration of IIS Web Server on Windows Server"},"/utho-docs/docs/windows/mssql-database-backup-restore-script/":{"data":{"":"\nUSE [master]\nRESTORE DATABASE [Algeria]\nFROM DISK = N‚ÄôC:\\db\\db\\Algeria_FULL_05282021_131711.BAK‚Äô WITH FILE = 1,\nMOVE N‚ÄôAlgeria‚Äô TO N‚ÄôC:\\Program Files\\Microsoft SQL Server\\MSSQL12.MSSQLSERVER\\MSSQL\\DATA\\Algeria.mdf',\nMOVE N‚ÄôAlgeria_log‚Äô TO N‚ÄôC:\\Program Files\\Microsoft SQL Server\\MSSQL12.MSSQLSERVER\\MSSQL\\DATA\\Algeria_log.ldf',\nNOUNLOAD,\nREPLACE,\nSTATS = 5;\nThank you!"},"title":"Mssql database backup restore script"},"/utho-docs/docs/windows/windows-server-backup-feature-2012r2-2016-2019/":{"data":{"":"","introduction#Introduction":"Windows Server Backup (WSB) is a feature that provides¬†backup¬†and¬†recovery¬†options for Windows server environments. Windows Server Backup is a feature that provides a set of wizards and other tools for you to perform basic backup and recovery tasks for the server it is installed on. Administrators can use Windows Server Backup to back up a full server, the system state, selected storage¬†volumes¬†or specific files or folders, as long as the data volume is less than 2¬†terabytes. Windows Server Backup replaced the Ntbackup feature in earlier Windows Server operating systems.¬†In this article we will learn to take backup of one Windows Server into another Windows Server. For this we obviously need, two windows servers. For better understing, we will call them Server1 and Server2.","prerequisites#Prerequisites":" 2 Windows Servers with IIS and Windows Server Backup feature installed. Internet connectivity on both the servers. Backup user wih same credentials on both the servers. ","stage-one-in-server-1-windows-server-backup-feature#Stage ONE in SERVER-1 Windows Server Backup feature":"Step 1. Login to server 1\nStep 2. Open Server Manager\nStep 3. Click on Add roles and feartures\nInstall Windows Server Backup","stage-three-in-server-1-windows-server-backup-feature#Stage THREE in SERVER-1 Windows Server Backup feature":"Step 1. Click on Tools and open Windows Server Backup\nStep 2. We will a create one backup for this posts‚Äô purpose. Click on ‚ÄòBackup once‚Ä¶‚Äô\nWe will choose the ‚ÄúCustom‚Äù option to backup specific files/folder\nClick on ‚ÄúAdd Items‚Äù to select folders for backup\nWe will select the folder ‚Äúftpbackup‚Äù\nClick on ‚ÄúRemote shared folder‚Äù\nInput the folder path in location and click on next\nInput the server credentials of SERVER-2\nClick on ‚ÄúBackup‚Äù to initiate backup\nBackup created in the shared folder.\nThank You!","stage-two-in-server-2#Stage TWO in SERVER-2":"Step 1. We will create a folder in server-2 and share it over network, in order to be able store the backup from server-1\nCreate a folder\nGo to it‚Äôs Properties\nGo to Sharing tab, and click on ‚ÄúShare‚Ä¶‚Äù\nClick on ‚ÄúShare‚Ä¶‚Äù\nCopy the path of the shared folder \\\\server_ip\\foldername"},"title":"Windows Server Backup feature (2012R2, 2016, 2019)"}}